本文主要整理PMPP Perface要点。

## 1. 背景

本章是经典教材《大规模并行处理器编程：实践方法》（Programming Massively Parallel Processors: A Hands-on Approach）第四版的出版介绍和前言部分。它阐述了该书在新一代异构计算（CPU+GPU）背景下的使命、目标读者、教学方法以及自2016年第三版发布以来的重要更新。

### 要点总结

1.  **核心使命与愿景**：
    *   本书旨在成为推动**计算实验**在科学、工程、医学和商业领域广泛应用的“关键要素”。
    *   目标是让**计算思维**和**并行编程技能**像微积分一样，成为数百万理工科学生普遍掌握的基础能力。

2.  **目标读者**：
    *   **主要读者**：所有需要利用计算思维和并行编程来取得突破的**科学与工程领域的本科生和研究生**。
    *   **次要读者**：需要更新并行计算知识、跟上技术飞速发展的**行业专业开发人员**。文中列举了众多领域，包括机器学习、网络安全、自动驾驶、计算金融、数据分析、生物工程、物理、化学等。

3.  **教学方法与前提**：
    *   **方法**：采用“实践方法”，通过**建立直观的技术理解**来教授并行编程。
    *   **前提**：假设读者具备**基础的C语言编程经验**。
    *   **工具**：使用 **CUDA C** 编程环境（基于NVIDIA GPU），其拥有庞大的用户基础（超4亿活跃程序员和超10亿终端设备），保证了学习成果的实用性。

4.  **第四版更新与时代背景**：
    *   **更新原因**：基于第三版（2016年）的读者反馈和技术的高速发展。
    *   **硬件演进**：重点涵盖了自2016年后推出的**三大新一代GPU计算架构**：Volta（伏特）、Turing（图灵）和Ampere（安培），确保了内容的前沿性。
    *   **市场趋势**：强调了**异构计算系统**（多核CPU + 多线程GPU）的普及，已将“万亿次（TeraScale）计算带入笔记本，而千万亿次（ExaScale）计算带入集群”，并且GPU计算已在金融、电商等关键行业被广泛采用。

## 2. Volta（伏特）、Turing（图灵）和Ampere（安培）

### 1. Volta（伏特架构，2017年）

Volta 是一个为**高性能计算（HPC）和人工智能**设计的划时代架构。它的核心创新是引入了 **Tensor Core**。

*   **核心创新：**
    *   **Tensor Cores**：这是Volta最大的亮点。这是一种专门为执行**矩阵乘法和累加**运算而设计的专用核心，这种运算是深度学习和AI训练的核心。它能够极快地处理4x4矩阵运算，提供远超传统CUDA核心的吞吐量，为AI应用带来了数量级的性能提升。
    *   **NVLink 2.0**：一种高速互连技术，允许多个GPU之间以极高的带宽共享数据，非常适合大规模服务器和超级计算机。
    *   **基于硬件的抢占**：允许更精细地调度任务，避免长线程阻塞整个流式多处理器（SM）。
    *   **独立的线程调度**：简化了并行编程模型。

*   **主要GPU型号：**
    *   **Tesla V100**：这是Volta架构的旗舰产品，面向数据中心和HPC领域。有PCIe和更快的SXM2两种形态，并且首次采用了**HBM2（高带宽内存）**，提供了巨大的内存带宽。
    *   **Titan V**：面向发烧友和研究人员的限量版消费级显卡，也包含了完整的Volta特性（如Tensor Cores）。
    *   **Quadro GV100**：面向专业可视化领域（如CAD、CAE、影视特效）的工作站显卡。

*   **意义**：Volta 奠定了现代AI计算的基石，其Tensor Core概念被后续所有架构继承和发展。

---

### 2. Turing（图灵架构，2018年）

Turing 架构的革命性在于它将**实时光线追踪**带到了消费级市场。它是一个“混合计算”架构，同时专注于传统的光栅化渲染和新的光线追踪渲染。

*   **核心创新：**
    *   **RT Cores**：这是专用于**实时光线追踪**的专用核心。它可以极其高效地处理光线与三角形的求交计算，使得在游戏中实现电影级的逼真光影、反射和阴影效果成为可能。
    *   **Tensor Cores（第二代）**：继承了Volta的Tensor Cores并进行了增强，不仅用于AI计算，还在图形领域用于**DLSS（深度学习超级采样）** 技术。DLSS利用AI对低分辨率图像进行超采样重建，从而在不牺牲画质的前提下大幅提升游戏帧率。
    *   **同时支持INT32和FP32并发执行**：提升了传统游戏计算的效率。

*   **主要GPU型号（消费级GeForce系列）：**
    *   **GeForce RTX 20系列**：这是首款搭载RT Core的消费级显卡，开启了游戏光追时代。
        *   RTX 2080 Ti, RTX 2080, RTX 2070, RTX 2060
    *   **专业级显卡**：
        *   **Quadro RTX** 系列（如RTX 8000, RTX 6000, RTX 5000）：面向专业设计和可视化领域。
    *   **数据中心**：
        *   **Tesla T4**：一款低功耗的推理加速卡，广泛用于数据中心和云服务的AI推理任务。

*   **意义**：Turing 带来了图形技术的范式转变，从纯粹的光栅化进入了光栅化与光线追踪相结合的混合渲染时代。

---

### 3. Ampere（安培架构，2020年）

Ampere 架构是一个全面的进化，在 **HPC、AI计算和图形游戏** 三大领域都实现了巨大的性能飞跃。它可以说是Volta和Turing优势的结合与放大。

*   **核心创新：**
    *   **Tensor Cores（第三代）**：支持新的精度（TF32、BF16），针对AI训练和推理进行了进一步优化，性能相比Volta有数倍提升。
    *   **RT Cores（第二代）**：光线追踪性能相比Turing提升了一倍，并且支持动态模糊等更复杂的光追计算。
    *   **大幅增加SM数量和CUDA核心数**：采用了新的SM设计（每个SM的FP32核心数翻倍），流式多处理器（SM）的整体数量也大幅增加。
    *   **多实例GPU（MIG）**：在数据中心GPU（如A100）中，可以将一块物理GPU分割为多个完全独立的实例，供不同的用户或任务使用，极大提升了数据中心GPU的利用率和安全性。
    *   **PCIe 4.0支持**：提供了更高的CPU-GPU通信带宽。

*   **主要GPU型号：**
    *   **数据中心/HPC**：
        *   **NVIDIA A100**：基于Ampere的旗舰数据中心GPU，是Volta V100的直接继任者，广泛应用于AI训练、推理和超级计算（例如著名的Selene超级计算机和E级超算“前沿”）。
    *   **消费级GeForce系列**：
        *   **GeForce RTX 30系列**：性能提升巨大的一代游戏显卡。
            *   RTX 3090, RTX 3080, RTX 3070, RTX 3060 Ti, RTX 3060
    *   **专业级显卡**：
        *   **NVIDIA A40**（数据中心可视化）和 **RTX A6000**（工作站）替代了之前的Quadro系列。

*   **意义**：Ampere 架构提供了前所未有的综合计算能力，巩固了NVIDIA在AI和HPC领域的领导地位，同时为游戏玩家提供了极具性价比的性能选择。

### 总结对比

| 架构 | 推出年份 | 核心创新 | 目标领域 | 代表性产品 |
| :--- | :--- | :--- | :--- | :--- |
| **Volta** | 2017 | **Tensor Core (第一代)**， NVLink 2.0 | HPC, AI 训练 | Tesla V100, Titan V |
| **Turing** | 2018 | **RT Core (第一代)**， Tensor Core (第二代) | 游戏光追， AI 推理，专业设计 | GeForce RTX 20系列， Tesla T4 |
| **Ampere** | 2020 | **Tensor Core (第三代)**， **RT Core (第二代)**， MIG | HPC, AI 训练/推理， 游戏 | NVIDIA A100, GeForce RTX 30系列 |

## 3. 新增Chapter

本章从**硬件架构**转向**软件生态**和**书籍内容**的更新。它解释了第四版教材为了适应CUDA 9至11的新特性以及新的并行算法，所进行的重大内容重组和扩充，具体表现为新增了四个核心章节并对大量现有章节进行了重写。

### 要点总结

#### 1. 软件（CUDA）演进

*   **CUDA版本**：书籍内容覆盖了从CUDA 9到CUDA 11的重要演进。
*   **更新目的**：这些新版本的CUDA允许程序员访问**新的硬件和系统特性**（例如Volta/Turing/Ampere架构的Tensor Core, RT Core, MIG等），并催生了许多**新算法**的开发。
*   **书籍调整**：为了跟上这些软件和硬件的变化，本书进行了大规模内容更新，包括**新增四章**和**重写大量现有章节**。

#### 2. 新增章节及其动机

新增章节反映了作者对并行编程教学法的深化和对行业需求的回应，旨在提供更系统、更深入、更实用的内容。

| 章节与标题 | 新增动机与核心内容 |
| :--- | :--- |
| **第4章：计算架构与调度** | **动机**：解决之前版本中架构和调度内容分散的问题，进行**系统性整合**。<br>**内容**：将相关讨论集中于一章，成为读者理解硬件如何调度和执行并行任务的**核心参考章**，为后续学习打下坚实的底层基础。 |
| **第8章：Stencil（模板计算）** | **动机**：对重要但之前仅被简略提及的模式进行**深度扩展**。<br>**内容**：<br>1. 更**彻底地讲解**Stencil模式，强调其与卷积相似的**数学背景**及其**关键区别**。<br>2. 基于其独特性和区别，介绍**额外的优化技术**。<br>3. 提供处理**三维网格和数据**的实例，更具实用价值。 |
| **第10章：归约与最小化分化** | **动机**：对另一个基础并行模式（Reduction）进行**完整和系统的教学**。<br>**内容**：<br>1. 采用**渐进式方法**来应用各种优化策略。<br>2. 对性能提升与代价进行**更彻底的分析和权衡**，帮助读者建立性能优化思维。 |
| **第13章：排序** | **动机**：深入探讨一个在数据处理中至关重要且能极大发挥GPU优势的算法类别。<br>**内容**：<br>1. 重点介绍**基数排序（Radix Sort）**，这是一种非常适用于GPU并行化的**非比较型排序算法**。<br>2. 同样采用**渐进方法**来优化它并分析性能权衡。<br>3. 也讨论了合并排序（Merge Sort）。 |

### 核心价值

通过这些更新，第四版教材展现了其核心目标：
1.  **系统性**：整合知识碎片，构建更清晰的知识体系（如第4章）。
2.  **深度化**：对关键算法和模式进行更深入、更专业的讲解（如第8、10、13章）。
3.  **实用性**：强调通过**渐进式优化**和**性能权衡分析**来教授编程，而不仅仅是语法，培养读者解决实际问题的能力。
4.  **前瞻性**：紧密跟随CUDA软件和GPU硬件的发展，确保教学内容不过时。

## 4. 已有章节

本章详细阐述了第四版教材除新增章节外，对**现有章节进行的全面修订和实质性重写**。这些修订的核心指导思想是：
1.  **强化系统性**：构建更清晰、更逻辑的知识体系。
2.  **采用渐进式教学方法**：从基础实现开始，逐步应用优化，并深入分析性能权衡。
3.  **提升深度与广度**：对关键主题进行更全面、更深入的探讨，引入更多替代方案和案例。
4.  **优化内容结构**：重新编排章节顺序以改善学习曲线，并将全书分为四个逻辑部分。

### 要点总结（按章节修订）

#### 1. 系统性优化与教学方法升级

*   **第6章（性能考虑）**：
    *   **内容重组**：将架构内容移至新第4章，归约示例移至新第10章。
    *   **核心新增**：提供了一个**通用性能优化策略清单**，明确指出每种策略针对的性能瓶颈。该清单成为全书后续优化代码的系统性指南，旨在培养学生程序性能优化的方法论。

*   **第9章（并行直方图）**：
    *   **教学方法革新**：采用**更渐进的优化路径**。初始实现不再使用线程粗化，并将**私有化**（减少原子操作竞争）和**使用共享内存**（降低访问延迟）明确区分为两个独立的优化步骤。
    *   **逻辑更清晰**：阐明了线程粗化的一个主要好处是减少需要提交的私有副本数量，因此应在私有化之后应用。
    *   **顺序调整**：该章被移至“归约”和“扫描”模式之前，以便更早地引入原子操作的概念。

*   **第14章（稀疏矩阵计算）**：
    *   **分析方法系统化**：开头引入了一个用于设计稀疏矩阵存储格式的**考量因素清单**。
    *   **贯穿全章**：使用该清单作为框架，**系统化地分析**不同存储格式之间的权衡利弊。

#### 2. 深度与广度的扩展

*   **第7章（卷积）**：
    *   **重点转变**：从主要使用一维卷积示例，转变为**从一开始就聚焦于二维卷积**。
    *   **学习目标**：让学生能处理更高维度分块（Tiling）的复杂性，并为学习第16章的卷积神经网络（CNN）打下更好的基础。

*   **第15章（图遍历）**：
    *   **内容大幅扩展**：不再只关注一种BFS并行策略，而是涵盖了**更全面的替代方案集**。
    *   **策略覆盖**：包括基于顶点的推/拉模型、基于边的实现、线性代数实现等，并分析其间的权衡。这种分类法具有通用性，适用于一般的图算法并行化。

*   **第16章（深度学习）**：
    *   **理论背景强化**：重写以提供**全面且直观的理论背景**，帮助读者理解现代神经网络（全连接层、激活函数、卷积层等）的计算组件。
    *   **降低门槛**：消除了理解卷积神经网络训练核函数的一些常见障碍。

*   **第19章（并行编程与计算思维）**：
    *   **角色转变**：作为第一、二部分的**总结性章节**。
    *   **案例扩充**：不再仅从两个章节中抽取示例，而是**从更多章节中广泛取材**。
    *   **概念深化**：特别扩展了问题分解的讨论，引入了**以输出为中心**和**以输入为中心**的分解的 generalizations，并用大量示例讨论其权衡。

#### 3. 内容聚焦与结构调整

*   **第21章（CUDA动态并行）**：
    *   **重点转移**：从详尽的编程语义细节，转向**更关注应用示例**。编程细节被简要讨论，并指引读者参考CUDA编程指南。

*   **全书结构重组**：
    *   全书被组织为**四个主要部分**（如图P.1所示）：
        1.  **第一部分**：介绍并行编程的基本概念、GPU架构、性能分析与优化。
        2.  **第二部分**：应用第一部分的概念，覆盖六种常见计算模式，展示如何并行化和优化它们。每种模式都引入一个新的编程特性或技术。
        3.  **第三部分**：介绍更多高级模式和应用，继续应用第二部分的优化技术，但更强调探索不同的问题分解形式及分析其权衡。
        4.  **第四部分**：让读者接触高级实践和编程特性。

*   **内容精简**：
    *   将关于数值考虑的章节移至附录，以便为更多并行模式腾出空间，保持全书内容精炼。


## 5. CUDA 9 到 CUDA 11演进

CUDA 9 到 CUDA 11 是NVIDIA GPU计算平台非常重要的三个版本，其演进紧密配合了 **Volta、Turing和Ampere** 三代GPU架构的发布，为开发者提供了访问新硬件强大功能的能力，并引入了许多提升编程效率和性能的新特性。

### **CUDA 9 (2017年) - 为Volta架构而生**

CUDA 9的核心任务是释放 **Volta架构**（如Tesla V100）的革命性功能，特别是Tensor Core。

| 领域 | 主要新特性与演进 |
| :--- | :--- |
| **硬件支持** | • **全面支持Volta架构**：首次为Tesla V100提供生产级支持。<br>• **Tensor Core编程**：引入了用于编写Tensor Core内核的**Warp-Level Matrix Operations (WMMA)** API。这使得开发者可以在CUDA C++中直接使用半精度（FP16）和单精度（FP32）混合精度矩阵运算，极大简化了AI应用的开发。 |
| **编程模型** | • **协同组 (Cooperative Groups)**：这是一个重大的编程模型扩展。它引入了新的线程组（如`thread_block_tile`）抽象，允许开发者以更灵活、更可移植的方式对线程束（Warp）和线程块（Block）内的线程进行分组和同步，取代了部分`__syncthreads()`和warp intrinsic函数的功能，使代码更清晰、更易于维护。 |
| **编译器与语言** | • **增强的C++支持**：对C++14特性的支持更好。<br>• **独立的编译和链接**：改进了对分离编译的支持，允许单独编译CUDA代码然后链接。 |
| **性能分析** | • **Nsight Compute**：引入了新一代的交互式内核性能分析器（项目名称为“NVIDIA Profiler”），替代旧的`nvvp`，提供了更深入的硬件计数器分析和更佳的用户体验。 |

---

### **CUDA 10 (2018年) - 成熟与扩展**

CUDA 10在CUDA 9的基础上进行了大量改进，增强了对新硬件的支持并提升了编程的便捷性。

| 领域 | 主要新特性与演进 |
| :--- | :--- |
| **硬件支持** | • **支持Turing架构**：为GeForce RTX 20系列和Quadro RTX显卡提供全面支持，包括其**RT Core**（用于光线追踪）和**增强的Tensor Core**。<br>• **多进程服务 (MPS) 增强**：显著改进了MPS的性能和可扩展性，允许多个计算进程更高效地共享GPU资源。 |
| **内存管理** | • **异步数据操作**：引入了**`cudaMemcpyAsync`** 的增强功能，与**流**和**默认池**更好地结合。<br>• **统一内存 (UM) 增强**：提供了对**访问计数器迁移**和**按需页面迁移**的改进支持，提升了UM在复杂访问模式下的性能。 |
| **编程模型** | • **协同组扩展**：进一步扩展了Cooperative Groups，支持更多类型的组和集合操作。<br>• **图 (Graphs) API (预览)**：首次引入了基于图的任务提交模型。它将内核启动和内存拷贝等操作定义为节点，依赖关系定义为边，形成一个计算图。一次性启动整个图可以大幅减少CPU端的开销，特别适合迭代计算中重复执行相同操作序列的场景。 |
| **开发工具** | • **Nsight Systems**：引入了系统级的性能分析工具，可以提供从CPU到GPU的整个系统的全景时间线视图，帮助识别系统级的瓶颈。 |

---

### **CUDA 11 (2020年) - 为Ampere架构与未来设计**

CUDA 11是一个里程碑式的版本，它为**Ampere架构**（如A100和RTX 30系列）带来了全面支持，并进行了大量底层革新。

| 领域 | 主要新特性与演进 |
| :--- | :--- |
| **硬件支持** | • **全面支持Ampere架构**：支持A100的诸多新特性，如：<br>  - **第三代Tensor Core**：支持新的精度（TF32, BF16），针对AI训练进行了优化。<br>  - **多实例GPU (MIG)**：允许将一块物理GPU划分为多个独立的、安全的小GPU实例。<br>  - **异步复制**：一种新的编程特性，允许直接从全局内存异步加载数据到共享内存，减少寄存器压力并提升带宽利用率。 |
| **编程模型与API** | • **增强的Graphs API**：从预览版变为正式版，并增加了**节点实例化**和**流捕获**等功能，使得创建计算图更加灵活和方便。<br>• **内存管理**：引入了**`cudaMallocAsync`** 和 **`cudaFreeAsync`** ，支持流序内存分配，可以更高效地与计算重叠，减少内存分配开销。<br>• **C++标准并行算法 (STL-like) 支持 (实验性)**：提供了类似于C++标准模板库的并行算法实现（如`std::for_each`），允许开发者使用标准C++语法编写并行代码，并由CUDA自动在GPU上执行。 |
| **编译器与兼容性** | • **向前兼容性**：引入了**CUDA Forward Compatibility**，允许为未来架构编译的应用程序在当前的、支持该功能的驱动上运行，简化了部署。<br>• **增强的PTX (并行线程执行) ISA**：更新了虚拟指令集以支持新硬件特性。<br>• **NVCC编译器改进**：编译速度更快，对C++17的支持更好。 |
| **开发与部署** | • **容器化与部署**：改进了对容器部署的支持，NVIDIA Container Toolkit更加成熟。 |

### **总结演进脉络**

| 特性类别 | CUDA 9 -> CUDA 10 -> CUDA 11 的演进方向 |
| :--- | :--- |
| **硬件支持** | **Volta (Tensor Core)** -> **Turing (Tensor Core 2 + RT Core)** -> **Ampere (Tensor Core 3 + MIG)** |
| **编程模型** | **引入协同组** -> **扩展协同组，预览Graphs** -> **正式化并增强Graphs，引入流序内存分配** |
| **内存管理** | 基础统一内存 -> **增强统一内存** -> **异步分配 (`cudaMallocAsync`)** |
| **开发生态** | **引入Nsight Compute** -> **引入Nsight Systems** -> **强调向前兼容性和容器化部署** |
| **核心目标** | **解锁Volta革命性功能** -> **提升成熟度与扩展性** -> **为大规模计算和AI的未来铺路** |

总而言之，从CUDA 9到CUDA 11的演进，是一条从**为特定新硬件（Volta）提供关键支持**，到**不断完善平台功能和开发体验**，最终走向**为大规模、异构计算未来（Ampere及以后）构建坚实基础**的道路。这些演进正是《大规模并行处理器编程》第四版大幅更新内容的核心背景。
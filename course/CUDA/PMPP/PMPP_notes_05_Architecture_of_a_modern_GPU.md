本文主要整理PMPP Chapter 4 Compute architecture and scheduling的要点。

## 4.0 前言

### 内容概述

本章（第4章）的重点是**GPU的计算架构**，从一个简化的高层架构视角开始，逐步深入到资源分配、线程调度、延迟容忍等核心概念，并介绍如何利用API和工具来查询资源和分析性能。最后，它指明了后续两章将分别专注于片上（on-chip）和片外（off-chip）内存架构。

### 要点总结

1.  **核心设计哲学**：
    *   **CPU**： 旨在**最小化指令执行的延迟**（Latency），快速完成单个任务。
    *   **GPU**： 旨在**最大化指令执行的吞吐量**（Throughput），同时处理海量任务。

2.  **本章（第4章）核心主题：GPU计算架构**
    *   **学习目标**： 让CU程序员能够理解和分析其内核（kernel）代码的性能表现。
    *   **主要内容将覆盖**：
        *   **高层架构**： 提供一个简化的GPU计算架构视图。
        *   **资源与调度**：
            *   灵活的**资源分配**（如何将计算任务映射到硬件上）。
            *   **线程块（Block）的调度**机制。
            *   **占用率（Occupancy）** 的概念（活跃的线程束数量与最大可能数量的比率，是性能关键指标）。
        *   **线程执行**：
            *   **线程调度**的具体细节（如Warp调度）。
            *   如何通过大量线程来**容忍指令延迟**（Latency Tolerance）。
            *   **控制分支（Control Divergence）** 及其对性能的影响（当线程束内的线程执行不同路径时的性能损失）。
            *   **同步（Synchronization）** 机制和开销。
        *   **实践工具**：
            *   用于**查询GPU可用资源**的API函数。
            *   帮助**估算内核运行时GPU占用率**的工具。

3.  **后续章节预告**：
    *   **第5章**： 将重点讨论GPU的**内存架构**，特别是**片上内存**（如共享内存、缓存）的结构和使用，以及**数据局部性**的重要性。
    *   **第6章**： 将简要介绍**片外内存**（如全局内存）架构，并**综合前两章内容**，全面阐述GPU架构的各种**性能考量**。

4.  **最终目标**：
    *   掌握这些架构概念的程序员，将具备编写和理解**高性能并行内核代码**的能力。

总而言之，这段文字是一个章节的“路线图”，指明了接下来要深入学习的方向是GPU的底层硬件架构（计算和内存），并强调了理解这些架构是进行高性能CUDA编程的关键。

## 4.1 Architecture of a modern GPU

### 内容概述

本节描述了一个现代GPU（以NVIDIA CUDA架构为例）的高层次组织结构。它将GPU描绘成一个由多个**流多处理器（SM）** 组成的阵列。每个SM内部又包含许多更小的处理单元（**CUDA核心**）以及共享的**片上内存资源**。此外，整个GPU还配备了容量更大但速度相对较慢的**片外全局内存**（通常由DRAM技术实现，如GDDR或HBM）。文本简要介绍了这些组件，并指明了关于内存架构的更详细讨论将在后续章节中进行。

### 要点总结

1.  **核心架构单元：流多处理器（SM）**
    *   GPU的核心计算单元是**流多处理器（Streaming Multiprocessor， SM）**。
    *   GPU由一个**SM阵列**组成，这体现了其大规模并行架构的本质。
    *   **举例**： Ampere架构的A100 GPU拥有108个SM。

2.  **SM的内部组成**
    *   每个SM内部包含多个**处理单元**，称为流处理器或**CUDA核心**（文中指出后续将简称为“核心”）。
    *   这些核心**共享控制逻辑和内存资源**，这意味着它们以锁步（warp）的方式高效地执行指令并访问共享的存储空间。
    *   **举例**： A100的每个SM包含64个CUDA核心，因此总核心数为 108 * 64 = 6912个。

3.  **内存层次结构**
    *   **片上内存（On-Chip Memory）**：
        *   每个SM都配有多种**片上内存结构**（图中标为“Memory”）。
        *   这些内存离计算核心很近，**速度极快**，但容量较小。是性能优化的关键。
        *   **提示**： 这将是第5章（内存架构与数据局部性）的重点讨论内容。
    *   **片外全局内存（Off-Chip Global Memory）**：
        *   整个GPU配备有以GB为单位的**设备全局内存**。
        *   它位于芯片之外，**容量大但访问延迟高、带宽低于片上内存**。
        *   **技术演进**： 从早期的GDDR发展到更先进的**HBM**（高带宽内存），后者通过将DRAM与GPU紧密集成在同一封装内来提供极高的带宽。
        *   **术语简化**： 文中为简洁起见，将统一使用**DRAM**来泛指所有这些类型的片外内存。
        *   **提示**： 访问全局内存的性能考量将是第6章的重点。

4. **核心SM资源参数概览**

| 参数 | 常见值与说明 | 影响与重要性 |
| :--- | :--- | :--- |
| **CUDA核心数** | **64 / 128 / 144** (e.g., A100: 64, H100: 128, RTX 4090: 128) | 决定了SM**每个时钟周期能执行的指令数**，是衡量SM计算能力的基础。 |
| **最大线程数 (Thread Slots)** | **1536 / 2048** (e.g., Turing: 1536, Ampere+: 2048) | SM能同时驻留的**最大线程数**。这是计算理论最高占用率的分母。 |
| **Warp数量** | **48 / 64** (e.g., Turing: 48, Ampere+: 64) | SM能同时管理的**最大Warp数**（最大线程数 / 32）。 |
| **最大块数 (Block Slots)** | **16 / 32** (e.g., V100: 32, A100: 32, H100: 32) | SM能同时驻留的**最大块数量**。限制了大块（Block）场景下的并行度。 |
| **寄存器文件大小** | **64K / 128K / 144K** (e.g., V100: 64K, A100: 64K, H100: 144K) | SM**寄存器总量**。与`每个线程使用的寄存器数`共同决定寄存器限制下的最大线程数。 |
| **共享内存大小** | **64KB / 96KB / 128KB / 144KB** (e.g., 可配置，A100: 164KB) | 每个SM的**共享内存总量**。与`每个块使用的共享内存量`共同决定共享内存限制下的最大块数。 |
| **Warp大小** | **32** | **所有现代NVIDIA GPU均固定为32**。是调度和执行的基本单位。 |

总而言之，这段文字勾勒出了GPU架构的基本蓝图：**众多SM（每个SM包含大量核心）构成了强大的计算能力，而分层的内存结构（快速的片上内存和容量大的片外DRAM）则负责为这些计算核心提供数据**。理解这个结构是后续进行性能优化的基础。

## 4.2 Block scheduling

### 内容概述

本节详细阐述了CUDA内核启动后，其线程块（Block）是如何被调度到流多处理器（SM）上执行的。核心机制是**以块为单位进行调度**：一个块内的所有线程被同时分配给同一个SM。由于SM的硬件资源有限，一个SM能同时执行的块数量是有限的。对于包含大量块的网格（Grid），CUDA运行时会采用一个**动态调度队列**来管理：当一个SM上的块执行完毕并释放资源后，运行时系统会从队列中分配新的块到该SM上，直到所有块都执行完毕。这种基于块的调度机制是线程间进行高效协作（如同步和共享内存访问）的基础。

### 要点总结

1.  **调度基本单位：线程块（Block）**
    *   内核启动后，线程不是单个而是**以“块”为单位**被调度和分配到SM上。
    *   一个块内的**所有线程总是被同时分配给同一个SM**。这是CUDA编程模型的一个关键保证。

2.  **SM的资源限制与并行度**
    *   一个SM可以**同时接收和执行多个线程块**（例如，图中显示每个SM分配了3个块）。
    *   然而，每个块在执行时需要占用SM上的硬件资源（如寄存器、共享内存等），因此**一个SM能同时容纳的块数量是有限的**。这个上限取决于多种因素。

3.  **动态调度与队列管理**
    *   由于SM数量和每个SM的块容量限制，整个GPU**能同时执行的块总数是有限的**。
    *   大多数网格的块数远超过这个上限。为了解决这个问题，CUDA运行时系统会维护一个**待执行块列表（队列）**。
    *   当一个SM上的某个块执行完毕并释放资源后，运行时系统会**立即分配一个新的块到该空闲的SM上**。这种动态调度机制确保了所有块最终都能得到执行。

4.  **块内线程协作的架构基础**
    *   这种“同块线程必在同SM”的调度保证，为**块内线程间的紧密协作**提供了硬件基础，这是不同块之间的线程无法实现的。具体体现在：
        *   **屏障同步（Barrier Synchronization）**： 块内线程可以在代码中安全地进行点同步（例如使用 `__syncthreads()` 函数）。
        *   **共享内存（Shared Memory）**： 块内线程可以高效地通过SM上的**低延迟、高带宽的片上共享内存**进行通信和协作。

### Block内线程的通信

**核心机制：通过共享内存（Shared Memory）和同步原语（Synchronization）**

同一个Block内的线程运行在同一个SM上，因此它们可以高效地访问一块**共享的、片上（on-chip）的高速内存**，称为共享内存（Shared Memory）。

1.  **共享内存 (Shared Memory)**
    *   这是每个SM内部的一块小型、超高速的内存池。
    *   同一个Block内的所有线程都可以**读写**这块内存。
    *   典型工作流程：
        *   线程将数据从慢速的全局内存（Global Memory）加载到快速的共享内存中。
        *   使用 `__syncthreads()` 同步所有线程，确保所有数据都已就绪。
        *   线程们从共享内存中高效地读取数据进行计算。
    *   **优点**： 速度极快，延迟比全局内存低一个数量级，带宽非常高。

2.  **同步原语 (Synchronization)**
    *   为了保证通信的正确性，防止数据竞争（Data Race），必须进行同步。
    *   `__syncthreads()`： 这是一个CUDA内置函数，充当**屏障（Barrier）**。当调用它时，同一个Block内的所有线程都必须执行到此点，然后所有线程才会继续执行后面的代码。这确保了在同步点之前的所有内存操作（特别是对共享内存的写入）都对同Block内的其他线程可见。
    *   **注意**： 同步只在同一个Block内有效。无法同步不同Block的线程。

**总结：Block内通信 = 共享内存 + `__syncthreads()`**
这是一种**紧密耦合、高效**的通信方式，是CUDA性能优化的关键。

### Block间线程的通信

**核心机制：通过全局内存（Global Memory）和原子操作（Atomic Operations）**

不同Block的线程可能运行在不同的SM上，它们之间**没有共享的片上内存**，也无法进行同步。因此，通信方式更为间接和昂贵。

1.  **全局内存 (Global Memory)**
    *   这是GPU板载的DRAM，所有Block的所有线程都可以**读写**（但需要小心同步问题）。
    *   一个线程将数据写入全局内存的一个特定位置，另一个线程再从该位置读取。
    *   **缺点**： 全局内存的访问速度**非常慢**，延迟高，带宽也比共享内存低。

2.  **原子操作 (Atomic Operations)**
    *   当多个线程（来自不同Block）同时读写全局内存中的同一位置时，会发生数据竞争，导致结果错误。
    *   **原子操作**（如 `atomicAdd`, `atomicExch`, `atomicCAS` 等）可以确保对全局内存中一个数据的“读-修改-写”操作是**不可分割的**，从而避免竞争条件。
    *   **缺点**： 原子操作会序列化对内存地址的访问，如果大量线程竞争同一个地址，会**严重降低性能**。

3.  **同步问题**
    *   无法使用 `__syncthreads()` 来同步不同Block的线程。
    *   唯一可靠的同步方式是**让内核函数结束**。内核的结束意味着所有Block的所有线程都已完成工作，并且它们对全局内存的写入都已生效。然后，在启动下一个内核时，新的线程才能看到之前线程写入的结果。
    *   更复杂的同步需要通过多次内核启动或使用CUDA Streams和Events来实现。

**总结：Block间通信 = 全局内存 + （原子操作/内核结束同步）**
这是一种**松散耦合、低效**的通信方式，应尽量避免或谨慎使用。

### 对比表格

| 特性 | Block内通信 | Block间通信 |
| :--- | :--- | :--- |
| **硬件基础** | 同一个SM上的**共享内存** | GPU上的**全局内存** (DRAM) |
| **机制** | 共享内存读写 | 全局内存读写 + 原子操作 |
| **同步** | `__syncthreads()` | 内核结束、原子操作、Stream/Event |
| **速度** | **非常快** (片上缓存) | **非常慢** (片外DRAM) |
| **耦合度** | 紧密耦合 | 松散耦合 |
| **设计建议** | **积极使用**，是优化的核心 | **尽量避免**，必要时需非常谨慎 |

**编程建议**： 应尽可能地将需要频繁通信的线程组织在同一个Block内，以利用共享内存和快速同步。将Block间的通信需求降到最低是设计高效CUDA内核的关键。

## 调度的基本单位是Block？还是warp？

这是一个非常核心且重要的问题。答案是：**调度发生在两个不同的层级，各有其基本单位。**

*   **在GPU全局层面，调度的基本单位是线程块（Block）。**
*   **在每个流多处理器（SM）内部，调度的基本单位是线程束（Warp）。**

让我为您详细解释这两个层级的分工：

### 1. 全局调度：以 Block 为单位

*   **谁负责**： CUDA运行时系统（Runtime）和GPU驱动。
*   **做什么**： 当一个内核（Kernel）启动时，其所有的线程块（Grid）会被放入一个全局队列中。**运行时系统负责将这些Block分配到各个可用的SM上**。
*   **特点**：
    *   **顺序任意**： Block被分配到SM的顺序是不确定的，这保证了程序的**透明可扩展性**——代码在不同SM数量的GPU上都能正确运行，无需修改。
    *   **资源检查**： 在分配一个Block到某个SM之前，系统会检查该SM是否有足够的资源（如寄存器、共享内存、线程槽位）来容纳这个新Block。如果有，就分配；如果没有，就等待其他Block执行完毕释放资源后再分配。
    *   **Block间独立**： 不同Block之间**无法同步**（除全局内核结束外），正是因为它们被分配到不同SM且执行顺序任意。

**可以理解为：运行时系统是“经理”，负责把一个个“项目组”（Block）派送到不同的“办公室”（SM）去工作。**

### 2. SM内部调度：以 Warp 为单位

*   **谁负责**： 每个SM内部的硬件调度器（Warp Scheduler）。
*   **做什么**： 一个SM可以同时容纳多个Block。每个Block会被进一步划分成多个Warp（每个Warp包含32个线程）。**SM的硬件调度器在每个时钟周期内，会从所有已经就绪（Ready）的Warp中选择一个或多个，将其指令发送到计算核心上执行。**
*   **特点**：
    *   **隐藏延迟的关键**： 当某个Warp因为等待数据（如访问全局内存）而停滞时，调度器会立刻切换到另一个就绪的Warp执行。通过保持大量活跃的Warp（高占用率），SM可以有效地隐藏指令和内存访问的延迟，让计算单元始终繁忙。
    *   **零开销切换**： Warp间的切换由硬件直接完成，无需保存和恢复线程状态（因为所有Warp的状态都已保存在SM内），效率极高。
    *   **SIMD执行**： 一个Warp中的所有线程会**同步地执行同一条指令**（但操作不同的数据）。

**可以理解为：每个“办公室”（SM）的“小组长”（硬件调度器）管理着所有“项目组”（Block）派来的“小团队”（Warp），并决定下一刻让哪个“小团队”使用办公桌（计算核心）工作。**

### 总结与类比

为了更清晰地理解，我们可以用一个比喻：

| 角色 | GPU概念 | 调度职责 | 基本单位 |
| :--- | :--- | :--- | :--- |
| **公司经理** | CUDA运行时系统 | 将项目组分配到各个办公室 | **项目组 (Block)** |
| **办公室小组长** | SM硬件调度器 | 安排办公室内的小团队使用工位干活 | **小团队 (Warp)** |
| **员工** | Thread | 执行具体的任务 | - |

所以，您的问题的答案是：**两者都是，但处于不同的层级**。

*   您编写CUDA程序时，直接控制的是**Block**的维度（大小和形状）。
*   而GPU硬件为了极致的管理效率，将Block打散成**Warp**进行实际的执行和调度。
*   优化的重要目标之一，就是确保有足够多的**Warp**可供SM调度，从而隐藏延迟。而这通常是通过配置合适的**Block**大小和数量来实现的。

## 4.3 Synchronization and transparent scalability

### 内容概述

本节深入探讨了CUDA编程模型中两个紧密相关的核心概念：**屏障同步** 和 **透明可扩展性**。

1.  **屏障同步 (`__syncthreads()`)**: 它首先详细解释了CUDA中用于块内线程协调的同步机制。通过一个生动的“朋友购物”类比，阐明了屏障同步的作用：确保一个块中的所有线程都必须到达同步点后，才能继续执行后续代码，从而防止“数据竞争”和“有人掉队”的问题。文本特别强调了正确使用 `__syncthreads()` 的规则（必须被块内所有线程一致地执行），否则会导致未定义行为或死锁。

2.  **硬件支持与约束**: 接着，它解释了CUDA硬件如何通过**以块为单位分配资源**来满足屏障同步的要求（所有线程被同时分配到同一SM），从而保证了线程执行的时间接近性，避免了过长的等待。

3.  **透明可扩展性 (Transparent Scalability)**: 最后，文本揭示了CUDA设计中的一个重要权衡：将同步严格限制在块内，使得**块之间无需相互等待**。这一设计赋予了CUDA运行时系统极大的灵活性，可以以任何顺序执行块。这使得同一段CUDA代码无需修改，就能在拥有不同计算资源（从几个SM到上百个SM）的GPU上正确且高效地运行，从移动处理器到高端服务器显卡，这种特性被称为“透明可扩展性”。

### 要点总结

1.  **屏障同步 (`__syncthreads()`)**
    *   **功能**： 用于**同一线程块（Block）内**的线程协调。调用该函数的线程会等待，直到**同一块内的所有线程**都到达这个同步点，才能继续执行。
    *   **类比**： 就像一群朋友约好开车购物，必须所有人都回到车上，车才能开走。
    *   **关键规则**： `__syncthreads()` 的调用必须被**块内所有线程一致地执行**。
        *   **正确用法**： 所有线程都执行包含同步点的路径，或者所有线程都不执行。
        *   **错误用法**： 在条件分支（如if/else）中，如果只有部分线程执行了同步点，会导致**未定义行为**或**死锁**（线程永远互相等待）。

2.  **硬件对同步的支持**
    *   为了实现高效且安全的同步，CUDA运行时系统以**块为单位分配执行资源**。
    *   一个块的所有线程不仅被分配到同一个SM上，而且**是同时被分配**的。这意味着一个块只有在SM为其所有线程都准备好所需资源后才会开始执行。
    *   这种机制确保了块内所有线程在时间上**执行接近**，从而避免了在屏障处出现极端或无限的等待时间。

3.  **透明可扩展性 (Transparent Scalability)**
    *   **设计权衡**： 之所以将同步严格限制在块内，是为了换取一个巨大的优势：**块与块之间完全独立，无需相互同步或等待**。
    *   **带来的好处**： CUDA运行时系统可以**以任何顺序**调度和执行块。这使得同一应用程序代码可以在不同规模的硬件上运行：
        *   **低端设备**（如移动GPU）： 可能每次只执行少量块。
        *   **高端设备**（如桌面/服务器GPU）： 可以同时执行大量（上百个）块。
    *   **核心价值**： 开发者**无需为不同的硬件修改代码**。应用程序 automatically scales across different GPUs based on their available resources. 这极大地降低了开发负担，提高了应用程序的可用性。

总而言之，**块内同步**的严格规则和硬件支持，是实现**块间独立**和**透明可扩展性**这一强大特性的基础。这是CUDA编程模型成功的关键设计哲学之一。

## GPU 有哪些同步机制？如：__syncthreads、__syncwarp

在GPU编程中，同步机制是协调线程执行顺序和数据一致性的核心工具。以下是CUDA中常见的同步机制及其应用场景：

---

### **1. 块内同步：`__syncthreads()`**
- **作用**：  
  确保**同一线程块（Block）内所有线程**都执行到此点后才继续后续代码。
- **特性**：
  - 是**全屏障（Full Barrier）**，同时同步线程和内存（确保共享内存写入对所有线程可见）。
  - 必须被**块内所有线程**执行，否则会导致死锁或未定义行为。
- **典型场景**：  
  协调共享内存访问（如矩阵分块计算中，待所有线程完成数据加载后再计算）。

```cpp
__global__ void kernel() {
    __shared__ int s_data[128];
    int tid = threadIdx.x;
    
    s_data[tid] = ...;  // 写入共享内存
    __syncthreads();    // 等待所有线程完成写入
    
    // 安全读取其他线程写入的s_data
}
```

---

### **2. Warp内同步：`__syncwarp()`**
- **作用**：  
  同步**同一Warp内（32个线程）** 的执行，不保证跨Warp同步。
- **特性**：
  - 在Volta架构后引入，用于解决**独立线程调度（Independent Thread Scheduling）** 导致的Warp内线程执行不一致问题。
  - 可指定掩码（Mask）同步部分线程（如 `__syncwarp(0xFFFFFFFF)` 同步整个Warp）。
- **典型场景**：  
  Warp级原语（如Warp Shuffle）后确保数据一致性。

```cpp
int data = __shfl_sync(0xFFFFFFFF, value, lane_id); // Warp内数据交换
__syncwarp();  // 确保所有线程完成Shuffle操作
```

---

### **3. 内存栅栏（Memory Fences）**
- **作用**：  
  控制内存访问顺序，确保在栅栏前的内存操作对栅栏后的操作可见。
- **类型**：
  - `__threadfence_block()`： 块内线程可见。
  - `__threadfence()`： 设备内全局可见。
  - `__threadfence_system()`： 跨CPU/GPU系统可见（仅Pascal+架构）。
- **典型场景**：  
  无锁编程（Lock-Free）中维护数据一致性。

```cpp
__global__ void lock_free() {
    data[threadIdx.x] = ...;
    __threadfence(); // 确保data写入对其他线程可见
    ... // 后续操作
}
```

---

### **4. 原子操作（Atomic Operations）**
- **作用**：  
  对全局/共享内存中的变量进行**不可分割的读写-修改-写回**操作。
- **常见函数**：  
  `atomicAdd()`, `atomicCAS()`, `atomicExch()` 等。
- **典型场景**：  
  计数器、归约（Reduction）等需避免数据竞争的场合。

```cpp
__global__ void sum(int* output) {
    __shared__ int s_sum;
    atomicAdd(&s_sum, thread_data); // 共享内存原子加
    __syncthreads();
    if (threadIdx.x == 0) atomicAdd(output, s_sum); // 全局内存原子加
}
```

---

### **5. 流与事件同步（Streams & Events）**
- **作用**：  
  协调**主机（CPU）与设备（GPU）** 或**多个GPU内核**之间的执行顺序。
- **关键API**：
  - `cudaStreamSynchronize(stream)`： 主机等待流内所有操作完成。
  - `cudaEventRecord(event, stream)` + `cudaEventSynchronize(event)`： 事件标记与同步。
- **典型场景**：  
  流水线并行（如数据加载、计算、传输重叠）。

```cpp
cudaStream_t stream;
cudaStreamCreate(&stream);
kernelA<<<..., stream>>>();  // 启动内核A
cudaEvent_t event;
cudaEventRecord(event, stream); 
kernelB<<<..., stream>>>();  // 启动内核B（与A并发）
cudaEventSynchronize(event); // 等待A完成（B可能仍在运行）
```

---

### **6. 协作组（Cooperative Groups, CG）**
- **作用**：  
  更灵活的线程分组同步（支持Block/Warp/自定义组）。
- **特性**：
  - 需CUDA 9+，支持动态组划分（如 `coalesced_threads()`）。
  - 提供 `sync()` 方法同步组内线程。
- **典型场景**：  
  复杂并行模式（如动态Warp级协作）。

```cpp
#include <cooperative_groups.h>
__global__ void cg_kernel() {
    auto block = cooperative_groups::this_thread_block();
    block.sync();  // 等效于__syncthreads()
}
```

---

### **同步机制的选择策略**
| **场景**                     | **推荐机制**               |
|------------------------------|----------------------------|
| 块内线程同步共享内存         | `__syncthreads()`          |
| Warp内线程协调（如Shuffle）  | `__syncwarp()` + 内存栅栏   |
| 全局数据原子更新             | `atomicXXX()`              |
| 跨内核/主机-设备同步         | 事件（Events） + 流（Streams） |
| 细粒度线程组协作             | 协作组（Cooperative Groups） |

---

### **注意事项**
1. **死锁风险**：  
   避免在分支条件（如 `if`）中仅让部分线程调用 `__syncthreads()`。
2. **性能开销**：  
   同步会暂停线程执行，过度使用会降低并行效率。
3. **内存一致性**：  
   弱内存模型下，需配合栅栏确保操作可见性（如Warp Shuffle后使用 `__syncwarp()` + `__threadfence_block()`）。

通过合理选择同步机制，可在保证正确性的同时最大化GPU并行效率。


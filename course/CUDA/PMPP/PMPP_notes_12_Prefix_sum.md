本文主要整理PMPP Chapter 11 Prefix sum的要点。

## 11.0 前言

### 内容概况

本页主要介绍了**并行前缀和**（也称为 **scan**）这一重要的并行计算模式。文章阐述了并行扫描的核心概念、其在并行计算中的关键作用、典型应用场景，并特别指出了在使用并行扫描时需要在**算法复杂度**和**并行化收益**之间进行权衡的重要性。

---

### 要点总结

1.  **核心定义**：
    *   并行前缀和（Parallel Scan）是一种基础的并行计算模式。
    *   它的主要作用是将一些看似顺序执行的操作（例如，每个计算项都依赖于前一个项结果的数学递归）并行化。

2.  **重要性**：
    *   在大规模并行计算中至关重要，因为它能有效**消除或减少应用程序中的顺序执行部分**，这些部分是制约整体性能的主要瓶颈。
    *   它被广泛用作许多并行算法中的**基本操作（Primitive Operation）**。

3.  **应用场景**：
    *   文中列举了并行扫描的多种应用，包括：**基数排序、快速排序、字符串比较、多项式求值、解决递归问题、树操作以及流压缩**等。

4.  **关键权衡与挑战**：
    *   并行扫描是**算法复杂度与并行化之间权衡**的典型例子。
    *   某些并行算法可能比对应的顺序算法执行更多的总计算量（即**工作复杂度更高**）。
    *   因此，对于大规模数据集，如果并行算法带来的复杂度增加过多，其运行速度**可能反而会慢于**更高效的顺序算法。

5.  **现实意义**：
    *   在大数据时代，处理海量数据集时，这种对算法复杂度的考量变得**愈发重要**。选择并行算法时必须谨慎评估其工作复杂度，以确保真正的性能提升。

## 11.1 Background

### 内容概况

**背景定义**入手，通过生动的“切香肠”案例解释了Scan的用途，随后详细对比了**包容性扫描（Inclusive Scan）** 和**排他性扫描（Exclusive Scan）** 的异同、输出结果及应用场景，最后给出了一个**顺序实现的Scan算法代码**及其复杂度分析，并为后续介绍并行算法做铺垫。

---

### 要点总结

#### 1. 扫描（Scan）的核心概念
*   **定义**：一种二元关联运算符（如加法）作用于一个数组的累积操作。
*   **类型**：主要分为两种：
    *   **包容性扫描（Inclusive Scan）**：输出数组的每个元素是输入数组从首元素到当前元素的所有元素的累积结果。
        *   公式： `[x₀, (x₀⊕x₁), (x₀⊕x₁⊕x₂), ...]`
    *   **排他性扫描（Exclusive Scan）**：输出数组的每个元素排除了当前输入元素的影响，从运算符的**单位元**开始累积。
        *   公式（以加法为例，单位元为0）： `[0, x₀, (x₀⊕x₁), ... , (x₀⊕...⊕xₙ₋₂)]`

#### 2. 关键区别与联系
*   **根本区别**：输出数组中每个元素是否包含对应输入元素自身的影响。
*   **直观联系**：两者提供的信息是互补的。在“切香肠”的例子中：
    *   **Inclusive Scan** 输出的是**切割点**（每个段的结束位置）。
    *   **Exclusive Scan** 输出的是**起始点**（每个段的开始位置）。
*   **相互转换**：通过简单的**移位**和**填充单位元**操作，可以方便地在两种扫描结果间进行转换。

#### 3. 应用与重要性
*   **核心价值**：将原本需要顺序执行的累积计算并行化，**消除顺序瓶颈**，是众多并行算法的基础构件。
*   **应用场景**：如内存分配、基数排序、流压缩、多项式求值等任何需要计算偏移量或起始地址的场景。
*   **示例**：“切香肠”案例形象地说明，一旦通过Scan预计算出所有切割点，所有切割动作就可以**同时进行**，极大提高效率。

```c
void sequential_scan(float *x, float *y, unsigned int N) {
    y[0] = x[0];
    for (unsigned int i = 1; i < N; ++i) {
        y[i] = y[i - 1] + x[i];
    }
}
```

#### 4. 算法实现与复杂度
*   **顺序算法**：图片中给出了基于加法的顺序Inclusive Scan实现代码。其逻辑是逐个元素进行累积。
*   **复杂度**：顺序算法的时间复杂度为 **O(N)**，即计算量与输入规模N呈线性关系。这为后续评估并行算法的效率提供了基准。
*   **后续内容预告**：文章指出将介绍**并行分段扫描**的方法，即先将数组分成若干段由不同的处理器并行计算，再合并结果。这引出了在并行计算中至关重要的**工作效率**（总计算量）的权衡问题。

### 总结
这三页内容奠定了Scan操作的理论和实践基础，明确了基本概念，并暗示了后续方向：即如何设计一个工作复杂度最优的并行Scan算法，使其在利用多处理器加速的同时，不至于做过多的额外计算。

## 11.2 Parallel scan with the Kogge-Stone algorithm

### 内容概况

这10张图片系统性地介绍了**并行扫描** 的核心算法之一——**Kogge-Stone算法**。内容键编程挑战（如竞争条件）及其解决方案，并扩展说明了如何将其从包容性扫描转换为排他性扫描。这是一个从理论到代码实现的完整技术讲解。

---

### 要点总结

#### 1. 朴素并行方法的缺陷与改进动机
*   **缺陷**：为每个输出元素分配一个线程进行顺序归约，虽然并行，但计算复杂度高达 **O(N²)**，且最慢线程的耗时（O(N)）使其无法超越优化后的顺序算法（O(N)），是一个“坏主意”。
*   **动机**：需要一种能**在不同输出元素的归约树之间共享部分和** 的算法，以降低计算复杂度。Kogge-Stone算法正是为此而生，它借鉴了高速加法器电路的设计思想。

#### 2. Kogge-Stone算法核心思想
*   **操作方式**：是一种**原地扫描**算法。算法迭代地更新一个共享数组`XY`，每次迭代中，每个元素（特定位置除外）会加上其左侧固定距离（`stride`，按2的幂次增长）的元素的值。
*   **迭代效果**：经过 `k` 次迭代后，`XY[i]` 包含了从 `i` 向前最多 `2^k` 个输入元素的和。当 `stride` 超过线程索引时，该线程的计算已完成。
*   **可视化**：图11.2清晰地展示了这一“传播”过程，最终所有位置都计算出正确的前缀和。

```c
__global__ void Kogge_Stone_scan_kernel(float *X, float *Y, unsigned int N) {
    __shared__ float XY[SECTION_SIZE];
    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N) {
        XY[threadIdx.x] = X[i];
    } else {
        XY[threadIdx.x] = 0.0f;
    }
    for (unsigned int stride = 1; stride < blockDim.x; stride *= 2) {
        __syncthreads();
        float temp;
        if (threadIdx.x >= stride)
            temp = XY[threadIdx.x] + XY[threadIdx.x - stride];
        __syncthreads();
        if (threadIdx.x >= stride)
            XY[threadIdx.x] = temp;
    }
    if (i < N) {
        Y[i] = XY[threadIdx.x];
    }
}
```

#### 3. 并行内核实现与关键挑战
*   **内核设计**：图11.3展示了具体的CUDA内核代码。其核心是一个循环，`stride` 从1开始不断翻倍。
*   **竞争条件**：这是实现中最关键的问题。由于算法会**重用部分和**，在同一个迭代中，一个线程准备读取的旧值可能被另一个线程过早地覆盖，从而产生**写后读** 竞争条件。
*   **解决方案**：
    1.  **临时变量与同步**：代码中使用临时变量`temp`存储计算结果，并通过两次`__syncthreads()`屏障同步，确保所有线程完成读取后，再统一执行写入操作，从而避免竞争。
    2.  **与归约算法的对比**：文章特别指出，第10章的归约算法不存在此问题，因为其读写操作不重叠，因此不需要这种保护措施。
    3.  **优化方案：双缓冲**：图片8提出了一种更优的解决方案——使用两个共享内存缓冲区。在每次迭代中，从一个缓冲区读取，向另一个缓冲区写入，从根本上消除了读写冲突，但会增加共享内存的使用量。

#### 4. 从包容性扫描到排他性扫描的转换
*   **核心关系**：排他性扫描与包容性扫描的结果仅相差一个元素的**移位**。排他性扫描的结果相当于将包容性扫描的结果右移一位，并在首位填入单位元（如0）。
*   **代码转换**：将包容性扫描内核（图11.3）转换为排他性扫描内核非常简单，只需修改初始化步骤：将`XY[0]`初始化为0，并将`XY[threadIdx.x]`初始化为`X[i-1]`（如图片10中的代码所示），而算法的迭代部分完全不变。图11.4展示了转换后的数据流图。

```c
if (i < N && threadIdx.x != 0) {
    XY[threadIdx.x] = X[i-1];
} else {
    XY[threadIdx.x] = 0.0f;
}
```

#### 5. 性能与实现考量
*   **控制发散**：当`stride`较小时，部分线程会提前完成工作，导致线程束内的控制发散，但对大规模数据块而言，影响相对较小。
*   **分段扫描**：所示内核处理的是单个数据段。对于大于一个线程块处理能力的大规模输入，需要先进行分段扫描，再通过后续步骤合并结果。

### 总结
这组图片深入探讨了Kogge-Stone这一高效并行扫描算法的实现精髓。它不仅提供了可工作的代码，更重点剖析了并行编程中常见的**竞争条件**问题及其解决方案（临时变量同步与双缓冲），并阐明了不同扫描类型间的转换关系，是理解并行前缀和计算的经典教材。

## 11.3 Speed and work efficiency consideration

### 内容概况

以**Kogge-Stone算法**为例，深入分析了其计算复杂度、在真实硬件上的执行步骤、资源消耗情况，并与顺序算法进行对比，最终对其优缺点及适用场景做出了全面评估。

---

### 要点总结

#### 1. 核心概念：工作效率
*   **定义**：算法完成计算所需的工作量接近该计算所需最小工作量的程度。
*   **基准**：扫描操作的最小工作量为 **N-1 次加法**，即顺序算法的 **O(N)** 复杂度。
*   **对比**：
    *   朴素并行算法工作量为 **O(N²)**，效率极低。
    *   Kogge-Stone 算法工作量为 **N*log₂(N) - (N-1)**，即 **O(N\*logN)**，优于朴素方法，但仍**不如顺序算法高效**。例如，处理512个元素时，其工作量约为顺序算法的8倍。

#### 2. 并行性能与步骤分析
*   **理论加速比**：在拥有无限计算资源的理想情况下，并行算法（步骤数：log₂N）相对于顺序算法（步骤数：N）的加速比约为 **N/log₂(N)**。对于N=512，理论加速比约为56.9倍。
*   **实际执行步骤**：在真实的CUDA GPU上，由于需要管理N个线程，实际消耗的执行资源更接近 **N*log₂(N)**。
    *   引入了 **计算步骤** 作为比较指标。实际步骤数取决于**执行单元数量(P)**，公式为 **(N*log₂(N)) / P**。
    *   举例：用1024个线程、32个执行单元处理1024个元素，需320步，实际加速比仅为3.2倍。**如果硬件资源(P)不足，并行算法可能比顺序算法更慢**。

**示例验证（N=8）**
| **迭代轮次** | 步长 `stride` | 加法次数               | 计算公式          |
|--------------|---------------|------------------------|-------------------|
| 第1轮        | 1             | `8 - 1 = 7`           | `N - 2^0`         |
| 第2轮        | 2             | `8 - 2 = 6`           | `N - 2^1`         |
| 第3轮        | 4             | `8 - 4 = 4`           | `N - 2^2`         |
| **总计**     | -             | **7 + 6 + 4 = 17**    | `K·N - (2^K-1) = 3*8 - (8-1) = 17` |

最小工作量（顺序扫描）= `8-1=7`  
公式结果：`8·log₂(8) - (8-1) = 8*3 - 7 = 17` → **匹配**。

#### 3. Kogge-Stone算法的优缺点与适用场景
*   **主要缺点（问题）**：
    1.  **硬件效率较低**：当硬件计算资源不足时，无法有效利用并行性，可能性能反而不如顺序算法。
    2.  **能耗较高**：额外的计算工作消耗更多能量，使其不适用于对功耗敏感的场景（如移动设备）。
*   **主要优点与适用场景**：
    1.  **高资源下的高速性**：当GPU硬件资源充足时，能实现极高的执行速度。
    2.  **控制分歧有限**：执行过程中的线程分支很少，效率高。
    3.  **现代算法的基础组件**：特别适用于计算**中等大小数据段**（如512或1024个元素）的扫描结果，是构建更复杂、更高速的现代并行扫描算法的**重要组成部分**。
    4.  **新硬件优化**：在支持**Warp内洗牌指令**的新一代GPU架构上，该算法能被高效实现。

### 总结
这三页内容揭示了并行计算中一个至关重要的权衡：**通过增加计算量来换取更短的执行时间**。Kogge-Stone算法并非“万能”解决方案，其价值严重依赖于**可用的硬件规模**和**对能耗的考量**。它是在特定条件下（资源充足、追求速度、处理分段数据）实现高性能的关键技术，但理解其工作效率局限对于正确选择和应用并行算法至关重要。

## 11.4 Parallel scan with the Brent-Kung algorithm

### 内容概况

这6张图片系统性地阐述了**Brent-Kung并行扫描算法**，旨在解决此前介绍的Kogge-Stone算法**工作效率较低**的问题。内容从算法设计动机出发，通过图示（图11.5, 11.6）详细解释了其**两阶段（归约树+反向树）** 工作原理，并给出了完整的CUDA内核实现代码（图11.7）。最后，文章深入分析了该算法的工作量、执行时间，并与Kogge-Stone算法在**理论效率**和**实际硬件（CUDA）上的表现**进行了全面对比。

---

### 要点总结

#### 1. 核心思想与设计动机
*   **解决痛点**：Kogge-Stone算法工作量高达O(NlogN)，效率较低。Brent-Kung算法通过**更策略性地计算和共享中间结果（部分和）**，以减少总操作次数。
*   **设计基础**：借鉴了**Brent-Kung加法器**的设计，采用一种**更平衡的树状结构**。
*   **两阶段过程**：
    1.  **归约树阶段**：自底向上，用最少的操作（N-1次）生成所有部分和以及总和。
    2.  **反向树阶段**：自顶向下，将归约阶段计算出的部分和“推送”或分配到需要它们的输出位置，以完成所有前缀和的计算。

```c
__global__ void Brent_Kung_scan_kernel(float *X, float *Y, unsigned int N) {
    __shared__ float XY[SECTION_SIZE];
    unsigned int i = 2 * blockIdx.x * blockDim.x + threadIdx.x;
    
    if (i < N)
        XY[threadIdx.x] = X[i];
    if (i + blockDim.x < N)
        XY[threadIdx.x + blockDim.x] = X[i + blockDim.x];
    
    for (unsigned int stride = 1; stride <= blockDim.x; stride *= 2) {
        __syncthreads();
        unsigned int index = (threadIdx.x + 1) * 2 * stride - 1;
        if (index < SECTION_SIZE) {
            XY[index] += XY[index - stride];
        }
    }
    
    for (int stride = SECTION_SIZE / 4; stride > 0; stride /= 2) {
        __syncthreads();
        unsigned int index = (threadIdx.x + 1) * stride * 2 - 1;
        if (index + stride < SECTION_SIZE) {
            XY[index + stride] += XY[index];
        }
    }
    
    __syncthreads();
    if (i < N)
        Y[i] = XY[threadIdx.x];
    if (i + blockDim.x < N)
        Y[i + blockDim.x] = XY[threadIdx.x + blockDim.x];
}
```

#### 2. 算法工作原理与图示解析
*   **归约树阶段**：
    *   操作不覆盖所有元素。例如，第一轮只更新奇数索引元素，第二轮只更新索引为4n-1的元素，以此类推。
    *   **目标**：高效地计算出关键位置（如XY[1], XY[3], XY[7], XY[15]）的部分和。
*   **反向树阶段**：
    *   **需求分析**：图11.6清晰地展示了每个位置在反向树开始时已累积的输入值范围（如XY[11]已包含x8~x11），以及它需要从哪些邻居位置累积额外的部分和才能完成计算。
    *   **“推送”操作**： stride由大到小，将高位部分和累加到低位尚未完成的位置。例如，将XY[7]的值加到XY[11]，将XY[3]加到XY[5]等。

#### 3. 内核实现与编程技巧
*   **代码结构**：图11.7给出了完整内核代码，包含数据加载、归约树循环、反向树循环和结果存储。
*   **控制发散优化**：为避免Kogge-Stone算法中因条件判断导致的线程束内控制发散，Brent-Kung使用了一种**复杂的索引映射**公式 `(threadIdx.x + 1) * stride * 2 - 1`，确保每个迭代中活跃的线程是连续的，从而极大改善了控制流的一致性。
*   **数据规模**：一个线程块（最多1024线程）可处理最多2048个元素（每个线程处理2个元素），提升了资源利用率。

#### 4. 工作量与性能分析
*   **工作量**：总操作次数为 **2N - 2 - log₂(N)**，时间复杂度为 **O(N)**。
    * 阶段一：归约树（上行阶段）总操作次数​​：这是一个等比数列求和：N/2 + N/4 + ... + 1 = N - 1
    * 阶段二：反向传播树（下行阶段）操作次数实际上是：(N/2 - 1) + (N/4 - 1) + ... + (1 - 1) = N/2 - log₂(N)
*   **效率优势**：操作数远低于Kogge-Stone算法（O(NlogN)），并且当N很大时，其工作量不超过顺序算法（N-1次）的2倍，在**并行性和效率间取得了良好平衡**。

#### 5. 与Kogge-Stone算法的对比
*   **理论优势**：Brent-Kung具有**更高的工作效率**（操作数更少）。
*   **实际硬件（CUDA）上的局限**：
    *   **SIMD开销**：在CUDA的SIMD架构下，即使线程不活跃，只要在同一线程束中，也可能消耗部分执行资源，削弱了Brent-Kung的理论效率优势。
    *   **潜在更长的执行时间**：由于需要**两倍于Kogge-Stone的步骤**（归约+反向），在计算资源无限的理论情况下，其执行时间可能更长。
*   **有限资源下的优势**：在**执行单元有限**的现实情况下，Brent-Kung由于总工作量小，实际执行时间可能**显著短于**Kogge-Stone。例如，用32个执行单元处理1024个元素时，Brent-Kung预计可获得约14倍加速，而Kogge-Stone仅3.2倍。
*   **选择依据**：当硬件执行资源非常充足时，Kogge-Stone可能因步骤更少而占优；但在资源受限或能效敏感的场景下，Brent-Kung是更优的选择。

### 总结
Brent-Kung算法提供了一种不同于Kogge-Stone的并行扫描设计范式，它通过增加计算步骤（反向树）来换取总工作量的显著降低，从而实现了更高的**工作效率**。尽管在CUDA的SIMD架构下其理论优势会打折扣，并且在理想情况下执行时间可能更长，但在**现实资源受限的硬件环境**中，它通常能提供更好的性能，尤其适用于对能效和操作总数有要求的应用场景。这两种算法的对比深刻体现了并行计算中在**步骤数、工作量、硬件资源利用率**之间的经典权衡。

## 11.5 Coarsening for even more work efficiency

### 内容概况

从分析并行扫描固有开销入手，提出了一种**三阶段并行扫描方法**。该方法通过在传统的并行扫描前后增加**完全独立的顺序扫描阶段**，将大规模计算任务分解为更高效的组合，从而在保持并行性的同时显著提升了工作效率。文章详细阐述了每个阶段的任务、数据流、内存访问优化策略，并通过具体示例和量化分析展示了该方法的显著优势。

---

### 要点总结

#### 1. 优化动机：解决并行扫描的核心开销
*   **并行开销**：传统的树形并行扫描（如Kogge-Stone）存在硬件利用不足和同步开销。
*   **工作效率低**：并行算法本身的总计算量（工作量）通常高于最优的顺序算法。
*   **核心思路**：如果硬件无法实现完全并行，不如主动进行**线程粗化**，通过优化任务分配来**自行串行化部分计算**，从而换取更高的工作效率。

#### 2. 三阶段并行扫描方法
该方法将一个大规模扫描任务分解为三个清晰的阶段，如上图11.8所示：

*   **第一阶段：独立的顺序扫描**
    *   **任务**：将线程块处理的数据段进一步划分为更小的、连续的子段，每个线程**独立地**对自己的子段执行**顺序扫描**。
    *   **示例**：一个包含4个线程的块处理16个元素，则每个线程处理4个元素，并生成4个独立的顺序扫描结果。
    *   **关键产出**：每个子段的**最后一个元素**即为该子段所有元素的和（如图中的7, 7, 6, 11）。
    *   **内存优化**：通过协作式、合并访问的方式将数据从全局内存加载到共享内存，然后在共享内存中进行可能非合并的访问模式，以提升内存效率。

*   **第二阶段：子段和的并行扫描**
    *   **任务**：收集所有子段的最后一个元素（即各子段和），形成一个更小的逻辑数组，并对此数组执行**并行扫描**（可使用Kogge-Stone或Brent-Kung算法）。
    *   **可行性**：由于此数组大小等于线程数，规模适中，非常适合并行扫描。
    *   **作用**：此步骤计算出了每个子段相对于整个数据段起始位置的**基础偏移量**。

*   **第三阶段：全局偏移量传播**
    *   **任务**：每个线程将**前一个子段**的基础偏移量（由第二阶段计算得出）加到**当前子段（除最后一个元素外）** 的每一个扫描结果上。
    *   **原因**：第一阶段计算的子段扫描结果是独立的，需要加上前面所有子段的总和才能得到在全局中的正确前缀和。
    *   **例外**：各子段的最后一个元素在第一阶段结束时已是正确的子段和，在第二阶段已被处理为正确的全局前缀和，因此在第三阶段**无需再次更新**。

#### 3. 线程粗化的核心优势与量化分析
*   **核心优势**：**高效利用执行资源**。允许用**远少于数据元素数量的线程**来处理大规模数据段，突破了“线程数限制任务规模”的约束，任务规模上限由共享内存大小决定。
*   **工作量分析**：
    *   阶段1工作量：~ `N - T`（T为线程数）
    *   阶段2工作量：~ `T * log₂(T）`（使用Kogge-Stone算法）
    *   阶段3工作量：~ `N - T`
    *   总工作量近似为 `(N - T) + T*log₂(T) + (N - T) = 2N - 2T + T*log₂(T)`
*   **性能收益**：假设有P个执行单元，总执行步骤约为总工作量除以P。例如，用64线程和32个执行单元处理1024个元素，仅需约72步，相比纯Kogge-Stone算法（约320步）有巨大提升。

### 总结
线程粗化三阶段扫描法是一种非常实用的高性能并行编程技术。它通过**增加计算阶段**（顺序扫描和偏移量相加）这种“迂回”策略，巧妙地**将大规模并行问题转化为“局部顺序+全局并行”的混合问题**，从而在整体上实现了更高的工作效率和更优的资源利用率。这是在并行计算中通过算法设计来平衡**工作量、并行度和资源约束**的典范。

## 11.6 Segmented parallel scan for arbitrary-length inputs

### 内容概况

这四张图片系统性地阐述了**针对超大规模数据的分段并行扫描方法**。内容从问题背景（处理百万至数十亿元素）出发，提出了一种**分层扫描策略**，通过图示（Fig. 11.9, 11.10）和文字详细解释了其**三个核心步骤**。最后，文章将算法原理与计算机算术中的**进位预测** 进行类比，并给出了使用**三个CUDA内核**实现该算法的具体代码框架和设计思路。这是一个从理论、示例到代码实现的完整解决方案。

---

### 要点总结

#### 1. 核心问题与解决方案
*   **问题**：需要扫描（计算前缀和）的数据集过于庞大（数百万/数十亿元素），无法一次性装入单个线程块的共享内存进行处理。
*   **解决方案**：**分层/分段并行扫描**。将大规模问题分解为可管理的层次：
    1.  **局部扫描**：将输入数据划分为多个**扫描块**，每个块由单个线程块独立处理，生成**局部前缀和**结果。
    2.  **全局整合**：收集各扫描块的**块总和**，对这些总和进行**高层级扫描**，计算出每个块相对于全局起始位置的**基础偏移量**。
    3.  **结果合并**：将每个块的基础偏移量加到该块内所有元素的局部前缀和上，得到最终的**全局前缀和**。

#### 2. 分层扫描的三步流程详解
*   **第一步：局部块扫描**
    *   使用任何已有的并行扫描内核（Kogge-Stone, Brent-Kung, 或线程粗化内核）处理各个数据块。
    *   **关键产出**：每个扫描块最后一个输出元素的值等于**该块所有输入元素的总和**。例如，图11.10中，块0总和为7，块1总和为7等。
*   **第二步：收集与扫描块总和**
    *   **收集**：每个线程块将其最后一个输出元素（即块总和）写入一个全局数组 `S` 中。索引通常使用 `blockIdx.x`。
    *   **扫描**：对数组 `S` 执行一次扫描操作。由于 `S` 的大小等于扫描块的数量，规模很小，此步骤可由**单个线程块**完成。
    *   **结果含义**：数组 `S` 扫描后的结果 `S[i]` 表示从全局起始位置到第 `i` 个扫描块末尾的**累积总和**。例如，`S[1]=14` 是块0和块1的总和。
*   **第三步：添加偏移量，完成最终结果**
    *   **操作**：启动第三个内核。对于第 `i` 个扫描块（`i>=1`），将其**前一个**扫描块在数组 `S` 中的值（即 `S[i-1]`）加到该块**每一个**元素的局部扫描结果上。
    *   **原因**：`S[i-1]` 包含了前 `i` 个块的总和，正是第 `i` 个块完成全局扫描所缺的偏移量。
    *   **特例**：第0个扫描块无需添加任何值，其局部结果已是全局最终结果。

#### 3. 算法原理与硬件类比
*   **原理类比**：该分段扫描算法的思想与计算机CPU中**加法器的进位预测** 原理非常相似。两者都是通过**预先计算和传播关键信息（块和/进位）** 来打破顺序依赖，实现并行加速。

#### 4. 三内核实现框架
文章给出了用三个CUDA内核实现该算法的具体代码框架：

*   **内核1（局部扫描内核）**：
    *   基于已有的扫描内核（如三阶段粗化内核）。
    *   **修改**：增加一个全局内存数组 `S` 作为参数。在每个线程块结束时，由**最后一个线程**将本块的最终总和（`XY[SECTION_SIZE-1]`）写入 `S[blockIdx.x]`。
*   **内核2（高层级扫描内核）**：
    *   使用一个**仅包含单个线程块**的配置来启动现有的任一扫描内核。
    *   该内核以数组 `S` 为输入和输出，计算块总和的全局前缀和。
*   **内核3（偏移量添加内核）**：
    *   每个线程计算其对应的全局索引 `i`。
    *   执行操作：`Y[i] += S[blockIdx.x - 1]`。
    *   **注意**：第0个块（`blockIdx.x == 0`）的线程不执行加法，因为 `S[-1]` 无效。

### 总结
分段并行扫描是处理海量数据前缀和问题的标准且高效的方法。它通过“**分而治之**”的策略，将全局扫描分解为**独立的局部扫描**和**小规模的全局整合**两个阶段，巧妙地解决了单块内存限制和跨块通信问题。这种层次化设计与硬件中的先进思想一脉相承，并通过结构清晰的多个内核协作实现，是高性能并行计算中的关键技术。

## 11.7 Single-pass scan for memory access efficiency

### 内容概况

这四张图片系统性地阐述了用于**提升大规模分段并行扫描内存访问效率**的**单遍扫描算法**，也称为**流式扫描**或**多米诺式扫描**。内容从分析传统三内核分段扫描的**额外内存存储/加载开销**入手，提出了一种通过**相邻线程块间单向传递部分和**来避免全局同步的流式方法。文章详细介绍了该算法的核心思想、执行流程，并重点给出了使用**原子操作实现相邻块间同步**的具体代码示例。同时，也指出了该算法潜在的**死锁风险**及其解决方案——**动态块索引分配**，确保了扫描块的线性调度。这是一个从问题、方案、实现到风险应对的完整技术剖析。

---

### 要点总结

#### 1. 优化动机：解决三内核分段扫描的内存瓶颈
*   **核心问题**：传统的三内核分段扫描（局部扫描 -> 扫描块总和 -> 添加偏移量）需要将中间结果（扫描块的部分和）**存储到全局内存**，再由后续内核**重新加载**。
*   **性能影响**：这些额外的全局内存存储和加载操作**无法与计算重叠**，且发生在独立的内核中，会显著影响算法执行速度。

#### 2. 流式（多米诺式）扫描的核心思想
*   **核心观察**：高层级的全局扫描（即块总和的整合）**并不需要严格的全局网格同步**，可以以“多米诺骨牌”的方式流水线式完成。
*   **工作原理**：
    1.  **并行局部扫描**：所有线程块并行扫描各自的数据块。
    2.  **链式传递**：每个线程块 `i` 完成后，等待并接收来自左邻块 `i-1` 传递来的累积和。
    3.  **累加与传播**：块 `i` 将接收到的累积和与自身的块总和相加，得到新的累积和，并传递给右邻块 `i+1`。
    4.  **本地更新**：块 `i` 将接收到的累积和（来自块 `i-1`）加到本块内**每一个**局部扫描结果上，生成最终输出。
*   **优势**：将原本集中式的全局整合步骤**分散并重叠**到各个块的执行过程中，避免了中间结果的显式全局内存写回和重载，潜在地提升了内存访问效率和整体性能。

#### 3. 关键实现技术：相邻同步
*   **需求**：为确保数据在块间正确传递，需要实现**相邻线程块间的同步**。
*   **机制**：采用**生产者-消费者**模型，通过**全局内存中的标志** 和**原子操作** 来实现。
    *   **生产者（块 i-1）**：计算并存储其累积和到全局数组（如 `scan_value[i]`），然后通过原子操作设置一个标志（如 `flags[i]`），表示数据已就绪。
    *   **消费者（块 i）**：其**领导线程**（如 `threadIdx.x == 0`）不断检查标志 `flags[i]`，一旦发现被设置，便从 `scan_value[i]` 加载数据。
    *   **内存栅栏**：在设置标志前，使用 `_threadfence()` 确保数据已持久化到全局内存，防止其他线程看到旧数据。
*   **协作**：块内其他线程在 `_syncthreads()` 处等待领导线程完成数据加载。

#### 4. 潜在风险与解决方案：动态块索引分配
*   **风险**：GPU的流多处理器调度器**不一定按 `blockIdx.x` 的顺序线性调度线程块**。可能导致先调度块 `i+1`，而它又在等待块 `i` 的数据，而块 `i` 可能尚未被调度，从而引起**死锁**。
*   **解决方案**：动态块索引分配。
    *   **目的**：解耦块的实际执行顺序与逻辑上的依赖顺序。
    *   **方法**：在内核开始时，由每个块的领导线程**原子地递增一个全局计数器**，从而为该块获取一个动态的、线性的索引 `bid`。
    *   **作用**：这保证了获取到索引 `i` 的块，其前驱块（索引 `i-1`）**一定已经被调度并执行了原子操作**，从而确保了执行顺序符合多米诺链的依赖关系，从根本上避免了死锁。

#### 5. 性能与可行性分析
*   **内存访问重叠**：虽然原子操作和全局数组访问会产生流量，但在支持缓存的现代GPU上，这些操作很可能与其他块的计算阶段（阶段1和阶段3）**在时间上重叠**，从而隐藏了延迟。
*   **并行性**：在数据传递阶段，块执行是串行的。但一旦某个块收到数据，它就可以立即进行本地更新，从而在第三个阶段，多个块可以并行进行。

### 总结
单遍流式扫描算法是对传统分段扫描的重要优化。它通过**将全局同步转化为局部的、链式的数据传递**，巧妙地规避了中间结果的全局内存瓶颈，有望提升执行效率。其实现依赖于**原子操作实现的相邻块同步**机制，并需要通过**动态索引分配**等策略来应对GPU动态调度带来的死锁风险。这种算法体现了在并行计算中，通过精细的同步和调度控制，在**减少内存开销**和**维持并行效率**之间取得平衡的高级设计技巧。

## 11.8 Summary

### 内容概况

总结系统性地回顾了本章探讨的多种并行扫描算法及其核心思想。内容从并行扫描的基本概念和重要性入手，依次总结了**Kogge-Stone算法**、**Brent-Kung算法**、**线程粗化**、**分层扫描方法**等关键技术与设计模式，深入分析了它们在**工作效率**、**执行步骤**和**硬件资源利用率**之间的不同权衡。最后，总结了为提升全局内存效率而设计的**多米诺式单遍扫描算法**的复杂性，并给出了使用现成库（如Thrust）而非从头实现的实用建议。

---

### 要点总结

#### 1. 并行扫描的核心价值
*   **定义**：并行扫描（前缀和）是一种重要的并行计算模式。
*   **作用**：用于实现**非均匀需求的资源并行分配**，能将**基于数学递归的、看似顺序的计算**转换为并行计算，从而减少应用中的顺序瓶颈。
*   **基准**：简单的顺序扫描算法仅需 **N-1** 次加法，时间复杂度为 **O(N)**。

#### 2. 主要并行扫描算法对比
总结重点对比了两种核心算法：

*   **Kogge-Stone 算法**：
    *   **优点**：快速、概念简单。
    *   **缺点**：**工作效率不高**，执行 **O(Nlog₂N)** 次操作，超过顺序算法。
    *   **适用场景**：通常在**执行资源充足**的处理器中，用于处理**中等规模**的扫描块。

*   **Brent-Kung 算法**：
    *   **优点**：**工作效率高**（数据可扩展算法），通过归约树和反向树阶段，仅执行 **O(N)** 次操作。
    *   **缺点**：概念更复杂，**需要更多的执行步骤**完成。
    *   **权衡**：在拥有足够执行资源的系统中，尽管Kogge-Stone工作效率较低，但因步骤少，**性能可能更优**。这体现了**工作量**与**执行步骤数**之间的经典权衡。

#### 3. 关键优化技术
总结回顾了为提升算法实用性而引入的几种重要技术：

*   **线程粗化**：
    *   **目的**：缓解硬件利用率不足和同步开销，进一步提升工作效率。
    *   **方法**：让块内每个线程先对自己的输入子段执行高效的顺序扫描，然后再协作执行块范围的、工作效率较低的并行扫描。

*   **分层扫描**：
    *   **目的**：将并行扫描算法扩展到能处理任意大小的输入集。
    *   **挑战**：简单的三内核实现会导致冗余的、延迟无法与计算重叠的全局内存访问。

*   **多米诺式分层扫描**：
    *   **目的**：改善分层扫描的全局内存访问效率。
    *   **特点**：支持单遍、单内核实现。
    *   **复杂性**：需要精心设计**相邻块同步机制**（使用原子操作、线程内存栅栏、屏障同步），并需通过**动态块索引分配**来防止死锁。

#### 4. 结论与建议
*   **优化空间**：存在进一步的优化机会，例如使用**Warp级别的洗牌操作**。
*   **实现复杂性**：在GPU上实现和优化并行扫描算法是一个复杂的过程。
*   **实用建议**：对于大多数用户而言，更可能使用现有的GPU并行扫描库（如**Thrust**），而非从头开始实现自己的扫描内核。
*   **核心价值**：尽管实现复杂，但并行扫描作为一个重要的并行模式，为理解优化并行模式时所涉及的各种权衡提供了一个精彩而相关的案例研究。
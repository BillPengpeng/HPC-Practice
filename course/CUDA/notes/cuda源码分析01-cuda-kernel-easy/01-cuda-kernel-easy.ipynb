{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 相关接口函数\n",
    "\n",
    "C++主要是cmath/math.h，见[link](https://legacy.cplusplus.com/reference/cmath/);  \n",
    "CUDA主要是cuda-math-api，见[link](https://docs.nvidia.com/cuda/cuda-math-api/index.html)。\n",
    "\n",
    "### python加载cuda kernal示例\n",
    "\n",
    "-U...: Undefine restrictions for numerical types  ——取消定义宏定义。  \n",
    "__functionName()命名约定的函数直接映射到硬件级别。 它们速度更快，但准确度稍低（如__sinf(x)和__expf(x)），functionName()命名约定的函数较慢但具有较高的准确性（例如，sinf(x)和expf(x)，-use_fast_math编译器选项强制每个functionName()调用等效的__functionName()调用。  \n",
    "--expt-relaxed-constexpr/--expt-extended-lambda，**含义未理解**。\n",
    "\n",
    "\n",
    "```\n",
    "lib = load(name='elementwise_lib', \n",
    "           sources=['elementwise.cu'], \n",
    "           extra_cuda_cflags=[\n",
    "               \"-O3\",\n",
    "                \"-U__CUDA_NO_HALF_OPERATORS__\",\n",
    "                \"-U__CUDA_NO_HALF_CONVERSIONS__\",\n",
    "                \"-U__CUDA_NO_HALF2_OPERATORS__\",\n",
    "                \"-U__CUDA_NO_BFLOAT16_CONVERSIONS__\",\n",
    "                \"--expt-relaxed-constexpr\",\n",
    "                \"--expt-extended-lambda\",\n",
    "                \"--use_fast_math\",\n",
    "            ], \n",
    "           extra_cflags=['-std=c++17'])\n",
    "```\n",
    "\n",
    "### benchmark示例\n",
    "\n",
    "先执行一段warmup，完成Kernel launch；再通过批量测试统计耗时情况。两步均通过**torch.cuda.synchronize()**完成同步。\n",
    "\n",
    "```\n",
    "def run_benchmark(perf_func: callable, a: torch.Tensor, b: torch.Tensor, tag: str, \n",
    "                  out: Optional[torch.Tensor] = None, warmup: int = 10, \n",
    "                  iters: int = 1000, show_all: bool = False):\n",
    "    # torch.dot vs custom dot_prod kernel\n",
    "    if out is not None: \n",
    "        out.fill_(0)    \n",
    "    # warmup\n",
    "    if out is not None:\n",
    "        for i in range(warmup):\n",
    "            perf_func(a, b, out)\n",
    "    else:\n",
    "        for i in range(warmup):\n",
    "            _ = perf_func(a, b) \n",
    "    torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    # iters\n",
    "    if out is not None:\n",
    "        for i in range(iters):\n",
    "            perf_func(a, b, out)\n",
    "    else:\n",
    "        for i in range(iters):\n",
    "            out = perf_func(a, b) \n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "    total_time = (end - start) * 1000 # ms\n",
    "    mean_time = total_time / iters\n",
    "    out_info = f\"out_{tag}\"\n",
    "    out_val = out.flatten().detach().cpu().numpy().tolist()[:2]\n",
    "    out_val = [round(v, 8) for v in out_val]\n",
    "    print(f\"{out_info:>18}: {out_val}, time:{mean_time:.8f}ms\")\n",
    "    if show_all: print(out)\n",
    "    return out, mean_time\n",
    "```\n",
    "\n",
    "### cuda kernal实现要点\n",
    "\n",
    "1. f32实现、f16实现分别执行2个f32、2个f16的elementwise运算，[Half Arithmetic Functions](https://docs.nvidia.com/cuda/cuda-math-api/cuda_math_api/group__CUDA__MATH____HALF__ARITHMETIC.html)。\n",
    "\n",
    "```\n",
    "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "if (idx < N) c[idx] = a[idx] + b[idx];\n",
    "\n",
    "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "if (idx < N) c[idx] = __hadd(a[idx], b[idx]);\n",
    "```\n",
    "\n",
    "2. f32x4、f16x2、f16x8类似向量化，执行多个f32或f16的elementwise运算。\n",
    "```\n",
    "int idx = 4 * (blockIdx.x * blockDim.x + threadIdx.x);\n",
    "if (idx < N) {\n",
    "    float4 reg_a = FLOAT4(a[idx]);\n",
    "    float4 reg_b = FLOAT4(b[idx]);\n",
    "    float4 reg_c;\n",
    "    reg_c.x = reg_a.x + reg_b.x;\n",
    "    reg_c.y = reg_a.y + reg_b.y;\n",
    "    reg_c.z = reg_a.z + reg_b.z;\n",
    "    reg_c.w = reg_a.w + reg_b.w;\n",
    "    FLOAT4(c[idx]) = reg_c;\n",
    "}\n",
    "\n",
    "int idx = 2 * (blockIdx.x * blockDim.x + threadIdx.x);\n",
    "if (idx < N) {\n",
    "    half2 reg_a = HALF2(a[idx]);\n",
    "    half2 reg_b = HALF2(b[idx]);\n",
    "    half2 reg_c;\n",
    "    reg_c.x = __hadd(reg_a.x, reg_b.x);\n",
    "    reg_c.y = __hadd(reg_a.y, reg_b.y);\n",
    "    HALF2(c[idx]) = reg_c;\n",
    "}\n",
    "\n",
    "int idx = 8 * (blockIdx.x * blockDim.x + threadIdx.x);\n",
    "half2 reg_a_0 = HALF2(a[idx + 0]);\n",
    "half2 reg_a_1 = HALF2(a[idx + 2]);\n",
    "half2 reg_a_2 = HALF2(a[idx + 4]);\n",
    "half2 reg_a_3 = HALF2(a[idx + 6]);\n",
    "half2 reg_b_0 = HALF2(b[idx + 0]);\n",
    "half2 reg_b_1 = HALF2(b[idx + 2]);\n",
    "half2 reg_b_2 = HALF2(b[idx + 4]);\n",
    "half2 reg_b_3 = HALF2(b[idx + 6]);\n",
    "half2 reg_c_0, reg_c_1, reg_c_2, reg_c_3;\n",
    "reg_c_0.x = __hadd(reg_a_0.x, reg_b_0.x);\n",
    "reg_c_0.y = __hadd(reg_a_0.y, reg_b_0.y);\n",
    "reg_c_1.x = __hadd(reg_a_1.x, reg_b_1.x);\n",
    "reg_c_1.y = __hadd(reg_a_1.y, reg_b_1.y);\n",
    "reg_c_2.x = __hadd(reg_a_2.x, reg_b_2.x);\n",
    "reg_c_2.y = __hadd(reg_a_2.y, reg_b_2.y);\n",
    "reg_c_3.x = __hadd(reg_a_3.x, reg_b_3.x);\n",
    "reg_c_3.y = __hadd(reg_a_3.y, reg_b_3.y);\n",
    "if ((idx + 0) < N) { HALF2(c[idx + 0]) = reg_c_0; }\n",
    "if ((idx + 2) < N) { HALF2(c[idx + 2]) = reg_c_1; }\n",
    "if ((idx + 4) < N) { HALF2(c[idx + 4]) = reg_c_2; }\n",
    "if ((idx + 6) < N) { HALF2(c[idx + 6]) = reg_c_3; }\n",
    "\n",
    "```\n",
    "\n",
    "3. f16x8_pack特点在于**同时加载128位**，运算过程同f16x8基本等效。\n",
    "\n",
    "```\n",
    "int idx = 8 * (blockIdx.x * blockDim.x + threadIdx.x);\n",
    "  // temporary register(memory), .local space in ptx, addressable\n",
    "  half pack_a[8], pack_b[8], pack_c[8]; // 8x16 bits=128 bits.\n",
    "  // reinterpret as float4 and load 128 bits in 1 memory issue.\n",
    "  LDST128BITS(pack_a[0]) = LDST128BITS(a[idx]); // load 128 bits\n",
    "  LDST128BITS(pack_b[0]) = LDST128BITS(b[idx]); // load 128 bits\n",
    "\n",
    "  #pragma unroll\n",
    "  for (int i = 0; i < 8; i += 2) {\n",
    "    // __hadd2 for half2 x 4\n",
    "    HALF2(pack_c[i]) = __hadd2(HALF2(pack_a[i]), HALF2(pack_b[i]));\n",
    "  }\n",
    "  // reinterpret as float4 and store 128 bits in 1 memory issue.\n",
    "  if ((idx + 7) < N) { LDST128BITS(c[idx]) = LDST128BITS(pack_c[0]); }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!TORCH_CUDA_ARCH_LIST=Ampere python3 elementwise.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sigmoid/relu/elu/gelu\n",
    "\n",
    "### sigmoid cuda kernal实现\n",
    "\n",
    "当x的值非常大时，expf(x)会溢出。对于float类型的输入，这个溢出点通常发生在x大约等于88.7；当x的值非常小时，expf(x)会趋近于零，下溢点通常发生在x小于大约-88.7时。  \n",
    "对于公式中常量，采用__float2half由f32精度转换至half精度。\n",
    "\n",
    "```\n",
    "// -------------------------------------- FP32 -------------------------------------- \n",
    "// Sigmoid x: N, y: N y=1/(1+exp(-x))\n",
    "// grid(N/256), block(K=256) \n",
    "__global__ void sigmoid_f32_kernel(float* x, float* y, int N) {\n",
    "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  if (idx < N) {\n",
    "    float v = x[idx];\n",
    "    // fminf/fmaxf/expf 出自cmath/math.h\n",
    "    v = fminf(fmaxf(v, MIN_EXP_F32), MAX_EXP_F32); \n",
    "    y[idx] = 1.0f / (1.0f + expf(-v));\n",
    "  }\n",
    "}\n",
    "\n",
    "// -------------------------------------- FP16 -------------------------------------- \n",
    "__global__ void sigmoid_f16_kernel(half* x, half* y, int N) {\n",
    "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  const half f = __float2half(1.0f);\n",
    "  if (idx < N) {\n",
    "    half v = x[idx];\n",
    "    // __hmin/__hmax/hexp 出自Half Precision Intrinsics\n",
    "    v = __hmin(__hmax(v, MIN_EXP_F16), MAX_EXP_F16);\n",
    "    y[idx] = f / (f + hexp(-v));\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### relu cuda kernal实现\n",
    "\n",
    "```\n",
    "// -------------------------------------- FP32 -------------------------------------- \n",
    "// Relu x: N, y: N y=max(0,x)\n",
    "// grid(N/256), block(K=256) \n",
    "__global__ void relu_f32_kernel(float* x, float* y, int N) {\n",
    "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  if (idx < N) y[idx] = fmaxf(0.0f, x[idx]);\n",
    "}\n",
    "\n",
    "// -------------------------------------- FP16 -------------------------------------- \n",
    "__global__ void relu_f16_kernel(half* x, half* y, int N) {\n",
    "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  if (idx < N) y[idx] = __hmax(__float2half(0.0f), x[idx]);\n",
    "}\n",
    "```\n",
    "\n",
    "### elu cuda kernal实现\n",
    "\n",
    "$$\n",
    "\\text{ELU}(x) = \n",
    "\\begin{cases} \n",
    "x & \\text{if } x > 0 \\\\\n",
    "\\alpha (\\exp(x) - 1) & \\text{if } x \\leq 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "```\n",
    "// ELU 计算函数\n",
    "// inline 是标准C/C++的一部分,__forceinline__ 是MSVC特有的指令\n",
    "// -------------------------------------- FP32 --------------------------------------\n",
    "__device__ __forceinline__ float elu(float x) {\n",
    "  return x > 0.f ? x : ALPHA * (expf(x) - 1.f);\n",
    "}\n",
    "\n",
    "// -------------------------------------- FP16 --------------------------------------\n",
    "__device__ __forceinline__ half elu_half(half x) {\n",
    "  return __hgt(x, __float2half(0.f)) ? x : __hmul(__float2half(ALPHA), __hsub(hexp(x), __float2half(1.f)));\n",
    "}\n",
    "\n",
    "// -------------------------------------- FP32 --------------------------------------\n",
    "__global__ void elu_f32_kernel(float* x, float* y, int N) {\n",
    "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  if (idx < N) y[idx] = elu(x[idx]);\n",
    "}\n",
    "\n",
    "// -------------------------------------- FP16 --------------------------------------\n",
    "__global__ void elu_f16_kernel(half* x, half* y, int N) {\n",
    "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  if (idx < N) y[idx] = elu_half(x[idx]);\n",
    "}\n",
    "```\n",
    "\n",
    "### gelu cuda kernal实现\n",
    "\n",
    "$$\n",
    "\\text{GELU}(x) = 0.5x \\left(1 + \\tanh\\left(\\sqrt{\\frac{2}{\\pi}} \\left(x + 0.044715 x^3\\right)\\right)\\right)\n",
    "$$\n",
    "\n",
    "```\n",
    "__inline__ __device__ float gelu_tanh_approximate(float x){\n",
    "  return 0.5f * x * (1.0f + tanhf(SQRT_2_PI * (x + 0.044715f * x * x * x)));\n",
    "}\n",
    "\n",
    "__inline__ __device__ half gelu_tanh_approximate(half x){\n",
    "  half x_cube = x * x * x;\n",
    "  // compute mid value : inner = 0.7978845608 * (x + 0.044715 * x * x * x)\n",
    "  half inner = HALF_SQRT_2_PI * (x + HALF_V_APP * x_cube);\n",
    "  // compute tanh\n",
    "  return HALF_DIV2 * x * (HALF_1 + ((hexp(inner * HALF_2) - HALF_1) / (hexp(inner * HALF_2) + HALF_1))); \n",
    "}\n",
    "\n",
    "// -------------------------------------- FP32 -------------------------------------- \n",
    "// GELU tanh approximate: x, y:x 0.5 * x * (1.0 + tanh(0.7978845608 * x * (1.0 + 0.044715 * x * x)))\n",
    "// grid(N/256), block(K=256) \n",
    "__global__ void gelu_f32_kernel(float* x, float* y, int N) {\n",
    "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  if (idx < N) {\n",
    "    float v = fminf(fmaxf(x[idx], MIN_EXP_F32), MAX_EXP_F32);\n",
    "    y[idx] = GELU_OPS(v);\n",
    "  }\n",
    "}\n",
    "\n",
    "__global__ void gelu_f16_kernel(half* x, half* y, int N) {\n",
    "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  if (idx < N) {\n",
    "    half v = x[idx];\n",
    "    v = __hmin(__hmax(v, MIN_EXP_F16), MAX_EXP_F16);\n",
    "    \n",
    "    y[idx] = HALF_GELU_OPS(v);\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!TORCH_CUDA_ARCH_LIST=Ampere python3 sigmoid.py\n",
    "!TORCH_CUDA_ARCH_LIST=Ampere python3 relu.py\n",
    "!TORCH_CUDA_ARCH_LIST=Ampere python3 elu.py\n",
    "!TORCH_CUDA_ARCH_LIST=Ampere python3 gelu.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## swish/hardswish/hardshrink\n",
    "\n",
    "### swish\n",
    "\n",
    "$$\n",
    "\\text{Swish}(x) = x \\cdot \\sigma(x) = \\frac{x}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "```\n",
    "// -------------------------------------- FP32 --------------------------------------\n",
    "// Swish x: N, y: N y=x*sigmoid(x)\n",
    "__device__ __forceinline__ float swish(float x) {\n",
    "  return x / (1.0f + expf(-x));\n",
    "}\n",
    "\n",
    "__global__ void swish_f32_kernel(float* x, float* y, int N) {\n",
    "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  if (idx < N) y[idx] = swish(x[idx]);\n",
    "}\n",
    "\n",
    "// -------------------------------------- FP16 --------------------------------------\n",
    "__device__ __forceinline__ half swish_half(half x) {\n",
    "  return __hmul(x, __hdiv(\n",
    "    __float2half(1.0f), __hadd(__float2half(1.0f), hexp(__hneg(x)))));\n",
    "}\n",
    "\n",
    "__global__ void swish_f16_kernel(half* x, half* y, int N) {\n",
    "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  if (idx < N) y[idx] = swish_half(x[idx]);\n",
    "}\n",
    "```\n",
    "\n",
    "### hardswish\n",
    "\n",
    "$$\n",
    "\\text{HardSwish}(x) = \n",
    "\\begin{cases} \n",
    "0, & \\text{if } x \\leq -3 \\\\\n",
    "x \\left( \\frac{x}{6} + \\frac{1}{2} \\right), & \\text{if } -3 < x < 3 \\\\\n",
    "x, & \\text{if } x \\geq 3 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "```\n",
    "// -------------------------------------- FP32 --------------------------------------\n",
    "__device__ __forceinline__ float hardswish(float x) {\n",
    "  if (x >= THRESHOLD_A) {\n",
    "    return x;\n",
    "  } else if (x <= THRESHOLD_B) {\n",
    "    return 0;\n",
    "  } else {\n",
    "    return x * (x + 3) / 6;\n",
    "  }\n",
    "}\n",
    "__global__ void hardswish_f32_kernel(float* x, float* y, int N) {\n",
    "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  if (idx < N) y[idx] = hardswish(x[idx]);\n",
    "}\n",
    "\n",
    "// -------------------------------------- FP16 --------------------------------------\n",
    "__device__ __forceinline__ half hardswish_half(half x) {\n",
    "  if (x > __float2half(THRESHOLD_A)) {\n",
    "    return x;\n",
    "  } else if (x < __float2half(THRESHOLD_B)) {\n",
    "    return __float2half(0.f);\n",
    "  } else {\n",
    "    return x * (x + __float2half(3.f)) / __float2half(6.f);\n",
    "  }\n",
    "}\n",
    "__global__ void hardswish_f16_kernel(half* x, half* y, int N) {\n",
    "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  if (idx < N) y[idx] = hardswish_half(x[idx]);\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "### hardshrink\n",
    "\n",
    "$$\n",
    "\\text{HardShrink}(x) = \n",
    "\\begin{cases} \n",
    "x, & \\text{if } x > \\lambda \\\\\n",
    "x, & \\text{if } x < -\\lambda \\\\\n",
    "0, & \\text{otherwise} \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "```\n",
    "// -------------------------------------- FP32 --------------------------------------\n",
    "__device__ __forceinline__ float hardshrink(float x) {\n",
    "  if (x > LAMBD || x < -LAMBD) {\n",
    "    return x;\n",
    "  } else {\n",
    "    return 0;\n",
    "  }\n",
    "}\n",
    "__global__ void hardshrink_f32_kernel(float* x, float* y, int N) {\n",
    "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  if (idx < N) y[idx] = hardshrink(x[idx]);\n",
    "}\n",
    "\n",
    "// -------------------------------------- FP16 --------------------------------------\n",
    "__device__ __forceinline__ half hardshrink_half(half x) {\n",
    "  if(x > __float2half(LAMBD) || x < __float2half(-LAMBD)) {\n",
    "    return x;\n",
    "  } else {\n",
    "    return __float2half(0.f);\n",
    "  }\n",
    "}\n",
    "__global__ void hardshrink_f16_kernel(half* x, half* y, int N) {\n",
    "  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "  if (idx < N) y[idx] = hardshrink_half(x[idx]);\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!TORCH_CUDA_ARCH_LIST=Ampere python3 swish.py\n",
    "!TORCH_CUDA_ARCH_LIST=Ampere python3 hardswish.py\n",
    "!TORCH_CUDA_ARCH_LIST=Ampere python3 hardshrink.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

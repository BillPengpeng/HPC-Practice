{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cuda基本函数\n",
    "\n",
    "### 限定词\n",
    "\n",
    "| Qualifier keyword  | Callable From | Executed on| Executed by | \n",
    "| :--:   | :--:    | :--:      | :--:    | \n",
    "| \\_\\_host\\_\\_ | Host | Host | Caller host thread | \n",
    "| \\_\\_global\\_\\_ | Host | Device | New grid of device threads | \n",
    "| \\_\\_device\\_\\_ | Device | Device | Caller device thread | \n",
    "\n",
    "### 内存相关\n",
    "\n",
    "设备分配内存：cudaMalloc  \n",
    "CPU分配内存：cudaMallocHost  \n",
    "分配统一内存，CPU/GPU均可访问 ：cudaMallocManaged  \n",
    "CPU/GPU内存拷贝（复制方向包括：cudaMemcpyHostToDevice/cudaMemcpyDeviceToHost）：cudaMemcpy  \n",
    "数据异步预取  cudaMemPrefetchAsync\n",
    "\n",
    "```\n",
    "void foo(cudaStream_t s) {\n",
    "  char *data;\n",
    "  cudaMallocManaged(&data, N);\n",
    "  init_data(data, N);                                   // execute on CPU\n",
    "  cudaMemPrefetchAsync(data, N, myGpuId, s);            // prefetch to GPU\n",
    "  mykernel<<<..., s>>>(data, N, 1, compare);            // execute on GPU\n",
    "  cudaMemPrefetchAsync(data, N, cudaCpuDeviceId, s);    // prefetch to CPU\n",
    "  cudaStreamSynchronize(s);\n",
    "  use_data(data, N);\n",
    "  cudaFree(data);\n",
    "}\n",
    "```\n",
    "\n",
    "### CUDA stream\n",
    "\n",
    "```\n",
    "cudaStream_t stream;       // CUDA streams are of type `cudaStream_t`.\n",
    "cudaStreamCreate(&stream); // Note that a pointer must be passed to `cudaCreateStream`.\n",
    "someKernel<<<number_of_blocks, threads_per_block, 0, stream>>>(); // `stream` is passed as 4th EC argument.\n",
    "cudaStreamDestroy(stream); // Note that a value, not a pointer, is passed to `cudaDestroyStream`.\n",
    "```\n",
    "\n",
    "### Compiler\n",
    "\n",
    "The host code is straight ANSI C code, which is compiled with the host’s standard C/C++ compilers and is run as a traditional CPU process.  \n",
    "The device code, which is marked with CUDA keywords that designate CUDA kernels and their associated helper functions and data structures, is compiled by NVCC into virtual binary files called **PTX files**. Graphics driver translates PTX into executable binary code (**SASS**).\n",
    "\n",
    "### Architecture of a modern GPU\n",
    "\n",
    "A typical CUDA-capable GPU is organized into an array of highly threaded **streaming multiprocessors (SMs)**. Each SM has several processing units called **streaming processors or CUDA cores**. **Multiple blocks** are likely to be simultaneously assigned to the same SM. However, blocks need to reserve hardware resources to execute, so only a limited number of blocks can be simultaneously assigned to a given SM.  \n",
    "\n",
    "In most implementations to date, once a block has been assigned to an SM, it is further divided into **32-thread units called warps**. The size of warps is implementation specific and can vary in future generations of GPUs. (threadIdx.x, threadIdx.y, threadIdx.z) 组织warps时， **优先级 x > y > z， 参考cuda mode Lecture 4**。 When threads in the same warp follow different execution paths, we say that these threads exhibit **control/warp divergence**, that is, they diverge in their execution. If all threads in a warp must complete a phase of their execution before any of them can move on, one must use a **barrier synchronization mechanism such as __syncwarp()** to ensure correctness.\n",
    "\n",
    "### Getting good occupancy – balance resources\n",
    "\n",
    "Have 82 SM → **many blocks = good** (for comparison Jetson Xavier has 8 Volta SM).  \n",
    "Can schedule up to 1536 threads per SM → power of two **block size < 512** desirable (some other GPUs 2048).  \n",
    "**Avoid divergence** to execute an entire warp (32 threads) at each cycle.  \n",
    "**Avoid FP64/INT64** if you can on Gx102 (GeForce / Workstation GPUs).  \n",
    "Shared Memory and Register File → **limits number of scheduled on SM**. (use __launch_bounds__ / C10_LAUNCH_BOUNDS to advise compiler of # of threads for register allocation, but register spill makes things slow).  \n",
    "Use torch.cuda.get_device_properties(<gpu_num>) to get properties (e.g. max_threads_per_multi_processor)\n",
    "\n",
    "### 工具相关\n",
    "安装NsightSystems: https://developer.nvidia.com/nsight-systems/get-started#platforms  \n",
    "jupter配置nsys：https://pypi.org/project/jupyterlab-nvidia-nsight/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\tNumber of SMs: 20\n",
      "Error: invalid device ordinal\n",
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o 01-vector-add 01-vector-add.cu -run\n",
    "# !nsys nvprof ./01-vector-add\n",
    "# !rm report*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## python集成cuda基础\n",
    "\n",
    "### profiler\n",
    "\n",
    "torch.autograd.profiler.profile  \n",
    "torch.profiler.profile，采用prof.export_chrome_trace可导出[网页可视化](chrome://tracing/)json格式  \n",
    "\n",
    "### Custom cpp extensions\n",
    "\n",
    "#### 方式一：load_inline\n",
    "\n",
    "```\n",
    "cpp_source = \"\"\"\n",
    "std::string hello_world() {\n",
    "  return \"Hello World!\";\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "my_module = load_inline(\n",
    "    name='my_module',\n",
    "    cpp_sources=[cpp_source],\n",
    "    functions=['hello_world'],\n",
    "    verbose=True,\n",
    "    build_directory='./tmp'\n",
    ")\n",
    "```\n",
    "\n",
    "#### 方式二：Integrate a triton kernel\n",
    "\n",
    "通过TORCH_LOGS=\"output_code\"及torch.compile自动生成triton kernel\n",
    "\n",
    "|        | CUDA | TRITION | \n",
    "| :--:   | :--: | :--:    |\n",
    "| Memory Coalescing | Muaual | Automatic | \n",
    "| Shared Memory Management | Muaual | Automatic | \n",
    "| Scheduling (Within SMs) | Muaual | Automatic | \n",
    "| Scheduling (Across SMs) | Muaual | Muaual | \n",
    "\n",
    "#### 方式三：numba\n",
    "\n",
    "```\n",
    "@cuda.jit\n",
    "def square_matrix_kernel(matrix, result):\n",
    "    # Calculate the row and column index for each thread\n",
    "    row, col = cuda.grid(2)\n",
    "\n",
    "    # Check if the thread's indices are within the bounds of the matrix\n",
    "    if row < matrix.shape[0] and col < matrix.shape[1]:\n",
    "        # Perform the square operation\n",
    "        result[row, col] = matrix[row, col] ** 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emitting ninja build file ./tmp/build.ninja...\n",
      "Building extension module my_module...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module my_module...\n",
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "# !python3 pytorch_square.py\n",
    "!python3 hello_load_inline.py\n",
    "# !TORCH_LOGS=\"output_code\" python3 pytorch_square_compiler.py\n",
    "# !python3 numba_square.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

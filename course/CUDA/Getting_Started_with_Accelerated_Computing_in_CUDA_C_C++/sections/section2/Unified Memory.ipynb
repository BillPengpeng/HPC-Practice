{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><div align=\"center\">Managing Accelerated Application Memory with CUDA C/C++ Unified Memory</div></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CUDA](./images/CUDA_Logo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [*CUDA Best Practices Guide*](http://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-optimizations), a highly recommended followup to this and other CUDA fundamentals labs, recommends a design cycle called **APOD**: **A**ssess, **P**arallelize, **O**ptimize, **D**eploy. In short, APOD prescribes an iterative design process, where developers can apply incremental improvements to their accelerated application's performance, and ship their code. As developers become more competent CUDA programmers, more advanced optimization techniques can be applied to their accelerated code bases.\n",
    "\n",
    "This lab will support such a style of iterative development. You will be using the Nsight Systems command line tool **nsys** to qualitatively measure your application's performance, and to identify opportunities for optimization, after which you will apply incremental improvements before learning new techniques and repeating the cycle. As a point of focus, many of the techniques you will be learning and applying in this lab will deal with the specifics of how CUDA's **Unified Memory** works. Understanding Unified Memory behavior is a fundamental skill for CUDA developers, and serves as a prerequisite to many more advanced memory management techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prerequisites\n",
    "\n",
    "To get the most out of this lab you should already be able to:\n",
    "\n",
    "- Write, compile, and run C/C++ programs that both call CPU functions and launch GPU kernels.\n",
    "- Control parallel thread hierarchy using execution configuration.\n",
    "- Refactor serial loops to execute their iterations in parallel on a GPU.\n",
    "- Allocate and free Unified Memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Objectives\n",
    "\n",
    "By the time you complete this lab, you will be able to:\n",
    "\n",
    "- Use the Nsight Systems command line tool (**nsys**) to profile accelerated application performance.\n",
    "- Leverage an understanding of **Streaming Multiprocessors** to optimize execution configurations.\n",
    "- Understand the behavior of **Unified Memory** with regard to page faulting and data migrations.\n",
    "- Use **asynchronous memory prefetching** to reduce page faults and data migrations for increased performance.\n",
    "- Employ an iterative development cycle to rapidly accelerate and deploy applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Iterative Optimizations with the NVIDIA Command Line Profiler\n",
    "\n",
    "The only way to be assured that attempts at optimizing accelerated code bases are actually successful is to profile the application for quantitative information about the application's performance. `nsys` is the Nsight Systems command line tool. It ships with the CUDA toolkit, and is a powerful tool for profiling accelerated applications.\n",
    "\n",
    "`nsys` is easy to use. Its most basic usage is to simply pass it the path to an executable compiled with `nvcc`. `nsys` will proceed to execute the application, after which it will print a summary output of the application's GPU activities, CUDA API calls, as well as information about **Unified Memory** activity, a topic which will be covered extensively later in this lab.\n",
    "\n",
    "When accelerating applications, or optimizing already-accelerated applications, take a scientific and iterative approach. Profile your application after making changes, take note, and record the implications of any refactoring on performance. Make these observations early and often: frequently, enough performance boost can be gained with little effort such that you can ship your accelerated application. Additionally, frequent profiling will teach you how specific changes to your CUDA code bases impact its actual performance: knowledge that is hard to acquire when only profiling after many kinds of changes in your code bases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Profile an Application with nsys\n",
    "\n",
    "[01-vector-add.cu](01-vector-add/01-vector-add.cu) (<------ you can click on this and any of the source file links in this lab to open them for editing) is a naively accelerated vector addition program. Use the two code execution cells below (`CTRL` + `ENTER`). The first code execution cell will compile (and run) the vector addition program. The second code execution cell will profile the executable that was just compiled using `nsys profile`.\n",
    "\n",
    "`nsys profile` will generate a report file which can be used in a variety of manners, including for use in visual profiling with Nsight Systems, which we will look at in more detail in the following section.\n",
    "\n",
    "Here we use the `--stats=true` flag to indicate we would like summary statistics printed. In this section this summary will be the focus of our attention. There is quite a lot of information printed:\n",
    "\n",
    "- Operating System Runtime Summary (`osrt_sum`)\n",
    "- **CUDA API Summary (`cuda_api_sum`)**\n",
    "- **CUDA Kernel Summary (`cuda_gpu_kern_sum`)**\n",
    "- **CUDA Memory Time Operation Summary (`cuda_gpu_mem_time_sum`)**\n",
    "- **CUDA Memory Size Operation Summary (`cuda_gpu_mem_size_sum`)**\n",
    "\n",
    "In this section you will primarily be using the 4 summaries in **bold** above. In the next section, you will be using the generated report files to give to the Nsight Systems GUI for visual profiling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After profiling the application, answer the following questions using information displayed in the `cuda_gpu_kern_sum` section of the profiling output:\n",
    "\n",
    "- What was the name of the only CUDA kernel called in this application?\n",
    "- How many times did this kernel run?\n",
    "- How long did it take this kernel to run? Record this time somewhere: you will be optimizing this application and will want to know how much faster you can make it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o single-thread-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-eb8e.qdstrm'\n",
      "[1/8] [========================100%] report1.nsys-rep\n",
      "[2/8] [========================100%] report1.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report1.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     90.4       6153520010        318  19350691.9  10072051.5      3581  100149601   27615757.9  poll                  \n",
      "      8.7        590028082        283   2084904.9   2064142.0       180   20491409    1273608.0  sem_timedwait         \n",
      "      0.7         45091463        499     90363.7     14310.0       400   10013859     594991.9  ioctl                 \n",
      "      0.3         19083977         24    795165.7      5335.0       960    7237594    2143845.5  mmap                  \n",
      "      0.0           920884         27     34106.8      4370.0      3050     547994     103870.0  mmap64                \n",
      "      0.0           549664         44     12492.4     11050.0      3260      51792       7930.0  open64                \n",
      "      0.0           205008          4     51252.0     45042.0     33251      81673      21365.7  pthread_create        \n",
      "      0.0           186092         29      6417.0      3951.0      1650      37042       7236.3  fopen                 \n",
      "      0.0           168457         12     14038.1      3850.0      1480      94944      26367.9  munmap                \n",
      "      0.0           137438         11     12494.4     14821.0      1100      20711       6552.4  write                 \n",
      "      0.0            51692         26      1988.2        70.0        60      49932       9778.7  fgets                 \n",
      "      0.0            40500          6      6750.0      7705.0      2490       9380       2781.1  open                  \n",
      "      0.0            38531         52       741.0       540.0       160       6470        879.6  fcntl                 \n",
      "      0.0            31990         22      1454.1      1125.0       560       4270        858.9  fclose                \n",
      "      0.0            22452         14      1603.7      1250.5       730       4300       1041.1  read                  \n",
      "      0.0            16611          2      8305.5      8305.5      4580      12031       5268.7  socket                \n",
      "      0.0            14761          5      2952.2      1740.0        70       8340       3473.4  fread                 \n",
      "      0.0            11171          1     11171.0     11171.0     11171      11171          0.0  connect               \n",
      "      0.0             5970         64        93.3       120.0        40        190         48.6  pthread_mutex_trylock \n",
      "      0.0             5400          1      5400.0      5400.0      5400       5400          0.0  pipe2                 \n",
      "      0.0             2280          1      2280.0      2280.0      2280       2280          0.0  bind                  \n",
      "      0.0             1140          1      1140.0      1140.0      1140       1140          0.0  listen                \n",
      "      0.0              150          1       150.0       150.0       150        150          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  ---------------------\n",
      "     94.3       2469992445          1  2469992445.0  2469992445.0  2469992445  2469992445          0.0  cudaDeviceSynchronize\n",
      "      4.9        129132720          3    43044240.0       29771.0       15301   129087648   74515777.5  cudaMallocManaged    \n",
      "      0.7         19231773          3     6410591.0     6095204.0     5848072     7288497     770264.7  cudaFree             \n",
      "      0.0           111135          1      111135.0      111135.0      111135      111135          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  ----------------------------------------------\n",
      "    100.0       2470049184          1  2470049184.0  2470049184.0  2470049184  2470049184          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     75.5         34165722   2304   14828.9    4367.0      1983     80256      22493.1  [CUDA Unified Memory memcpy HtoD]\n",
      "     24.5         11061142    768   14402.5    3743.5      1247     80768      22784.5  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    402.653   2304     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy HtoD]\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report1.nsys-rep\n",
      "    /dli/task/report1.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./single-thread-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worth mentioning is that by default, `nsys profile` will not overwrite an existing report file. This is done to prevent accidental loss of work when profiling. If for any reason, you would rather overwrite an existing report file, say during rapid iterations, you can provide the `-f` flag to `nsys profile` to allow overwriting an existing report file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize and Profile\n",
    "\n",
    "Take a minute or two to make a simple optimization to [01-vector-add.cu](01-vector-add/01-vector-add.cu) by updating its execution configuration so that it runs on many threads in a single thread block. Recompile and then profile with `nsys profile --stats=true` using the code execution cells below. Use the profiling output to check the runtime of the kernel. What was the speed up from this optimization? Be sure to record your results somewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o multi-thread-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-1e20.qdstrm'\n",
      "[1/8] [========================100%] report2.nsys-rep\n",
      "[2/8] [========================100%] report2.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report2.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     88.5       2479494409        132  18784048.6  10073386.0      2140  100151341   27192215.5  poll                  \n",
      "      8.9        249984234        111   2252110.2   2066644.0       130   20471461    2094231.1  sem_timedwait         \n",
      "      1.8         50072440        497    100749.4     14220.0       380   13680302     752827.9  ioctl                 \n",
      "      0.7         19348032         24    806168.0      4940.5      1160    7177782    2170730.4  mmap                  \n",
      "      0.0          1111234         27     41156.8      4481.0      3480     740403     140560.5  mmap64                \n",
      "      0.0           551076         44     12524.5     11725.0      4720      33611       5288.4  open64                \n",
      "      0.0           179085          4     44771.3     42351.5     38311      56071       7858.7  pthread_create        \n",
      "      0.0           175409         29      6048.6      3931.0      1750      35331       6727.5  fopen                 \n",
      "      0.0           163496         11     14863.3      9600.0       830      56282      15134.4  write                 \n",
      "      0.0            62021         11      5638.3      3830.0      1800      23221       5966.6  munmap                \n",
      "      0.0            60281         26      2318.5        90.0        70      57991      11355.0  fgets                 \n",
      "      0.0            40040          6      6673.3      6465.0      3420      10120       2387.4  open                  \n",
      "      0.0            35722         52       687.0       540.0       150       5780        784.7  fcntl                 \n",
      "      0.0            32730         22      1487.7      1365.0       730       3270        637.7  fclose                \n",
      "      0.0            19931         14      1423.6      1040.0       510       4021       1121.7  read                  \n",
      "      0.0            15550          2      7775.0      7775.0      3640      11910       5847.8  socket                \n",
      "      0.0             9691          1      9691.0      9691.0      9691       9691          0.0  connect               \n",
      "      0.0             7550          5      1510.0      1460.0        90       2940       1369.0  fread                 \n",
      "      0.0             6230         64        97.3       120.0        40        390         59.7  pthread_mutex_trylock \n",
      "      0.0             6200          1      6200.0      6200.0      6200       6200          0.0  pipe2                 \n",
      "      0.0             2380          1      2380.0      2380.0      2380       2380          0.0  bind                  \n",
      "      0.0             1280          1      1280.0      1280.0      1280       1280          0.0  listen                \n",
      "      0.0              350          1       350.0       350.0       350        350          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ---------------------\n",
      "     74.4        450093970          1  450093970.0  450093970.0  450093970  450093970          0.0  cudaDeviceSynchronize\n",
      "     22.4        135798561          3   45266187.0      37311.0      18131  135743119   78355322.2  cudaMallocManaged    \n",
      "      3.2         19381378          3    6460459.3    6128629.0    6045237    7207512     648308.8  cudaFree             \n",
      "      0.0            42261          1      42261.0      42261.0      42261      42261          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ----------------------------------------------\n",
      "    100.0        450084924          1  450084924.0  450084924.0  450084924  450084924          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     75.5         34149196   2304   14821.7    4431.0      1983     80191      22488.2  [CUDA Unified Memory memcpy HtoD]\n",
      "     24.5         11064172    768   14406.5    3727.5      1247     80703      22784.2  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    402.653   2304     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy HtoD]\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report2.nsys-rep\n",
      "    /dli/task/report2.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./multi-thread-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize Iteratively\n",
    "\n",
    "In this exercise you will go through several cycles of editing the execution configuration of [01-vector-add.cu](01-vector-add/01-vector-add.cu), profiling it, and recording the results to see the impact. Use the following guidelines while working:\n",
    "\n",
    "- Start by listing 3 to 5 different ways you will update the execution configuration, being sure to cover a range of different grid and block size combinations.\n",
    "- Edit the [01-vector-add.cu](01-vector-add/01-vector-add.cu) program in one of the ways you listed.\n",
    "- Compile and profile your updated code with the two code execution cells below.\n",
    "- Record the runtime of the kernel execution, as given in the profiling output.\n",
    "- Repeat the edit/profile/record cycle for each possible optimization you listed above\n",
    "\n",
    "Which of the execution configurations you attempted proved to be the fastest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o iteratively-optimized-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-7521.qdstrm'\n",
      "[1/8] [========================100%] report4.nsys-rep\n",
      "[2/8] [========================100%] report4.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report4.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     87.6       1787578999        100  17875790.0  10072695.0      1820  100144500   26375516.5  poll                  \n",
      "      9.4        192059533         90   2133994.8   2067622.5       190   20428004    2305592.9  sem_timedwait         \n",
      "      1.9         39171991        497     78816.9     11031.0       400    8311043     484625.1  ioctl                 \n",
      "      0.9         19313298         24    804720.8      5240.5       950    7214906    2168465.3  mmap                  \n",
      "      0.0           921690         27     34136.7      4350.0      2790     577760     109396.5  mmap64                \n",
      "      0.0           521350         44     11848.9     10636.0      3350      35672       5946.8  open64                \n",
      "      0.0           192400          4     48100.0     38817.0     34272      80494      21825.1  pthread_create        \n",
      "      0.0           174289         29      6010.0      3890.0      1380      40342       7506.1  fopen                 \n",
      "      0.0           143118         11     13010.7     14431.0      1160      18771       4838.6  write                 \n",
      "      0.0           102564         11      9324.0      3910.0      1460      39382      11359.8  munmap                \n",
      "      0.0            50593         26      1945.9        70.0        60      48832       9562.9  fgets                 \n",
      "      0.0            39162          6      6527.0      7190.0      2500       9371       2487.5  open                  \n",
      "      0.0            33930         52       652.5       480.0       190       5200        716.5  fcntl                 \n",
      "      0.0            28052         14      2003.7      1320.0       880       6960       1740.6  read                  \n",
      "      0.0            27261         22      1239.1      1075.0       510       3500        693.8  fclose                \n",
      "      0.0            17901          2      8950.5      8950.5      4730      13171       5968.7  socket                \n",
      "      0.0            13001          1     13001.0     13001.0     13001      13001          0.0  connect               \n",
      "      0.0             6490          5      1298.0      1260.0        80       2770       1203.8  fread                 \n",
      "      0.0             6320          1      6320.0      6320.0      6320       6320          0.0  pipe2                 \n",
      "      0.0             5630         64        88.0        50.0        40        350         56.0  pthread_mutex_trylock \n",
      "      0.0             2830          1      2830.0      2830.0      2830       2830          0.0  bind                  \n",
      "      0.0             1650          1      1650.0      1650.0      1650       1650          0.0  listen                \n",
      "      0.0              350          1       350.0       350.0       350        350          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ---------------------\n",
      "     51.4        126989528          3   42329842.7      29711.0      16561  126943256   73277365.7  cudaMallocManaged    \n",
      "     40.8        100768482          1  100768482.0  100768482.0  100768482  100768482          0.0  cudaDeviceSynchronize\n",
      "      7.9         19411022          3    6470340.7    6193473.0    5929639    7287910     720220.0  cudaFree             \n",
      "      0.0           104845          1     104845.0     104845.0     104845     104845          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ----------------------------------------------\n",
      "    100.0        100819427          1  100819427.0  100819427.0  100819427  100819427          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     73.5         30372915   8191    3708.1    2175.0      1823     70593       7452.6  [CUDA Unified Memory memcpy HtoD]\n",
      "     26.5         10964483    758   14465.0    3743.5      1279     80737      22889.2  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    232.292   8191     0.028     0.008     0.004     0.918        0.100  [CUDA Unified Memory memcpy HtoD]\n",
      "    133.169    758     0.176     0.033     0.004     1.044        0.302  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report4.nsys-rep\n",
      "    /dli/task/report4.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./iteratively-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Streaming Multiprocessors and Querying the Device\n",
    "\n",
    "This section explores how understanding a specific feature of the GPU hardware can promote optimization. After introducing **Streaming Multiprocessors**, you will attempt to further optimize the accelerated vector addition program you have been working on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following video presents upcoming material visually, at a high level. Click watch it before moving on to more detailed coverage of their topics in following sections.\n",
    "\n",
    "<script>console.log('hi');</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video controls width=\"640\" height=\"360\">\n",
       "    <source src=\"https://d36m44n9vdbmda.cloudfront.net/assets/s-ac-04-v1/task2/NVPROF_UM_1.mp4\" type=\"video/mp4\">\n",
       "    Your browser does not support the video tag.\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "video_url = \"https://d36m44n9vdbmda.cloudfront.net/assets/s-ac-04-v1/task2/NVPROF_UM_1.mp4\"\n",
    "\n",
    "video_html = f\"\"\"\n",
    "<video controls width=\"640\" height=\"360\">\n",
    "    <source src=\"{video_url}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(video_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Multiprocessors and Warps\n",
    "\n",
    "The GPUs that CUDA applications run on have processing units called **streaming multiprocessors**, or **SMs**. During kernel execution, blocks of threads are given to SMs to execute. In order to support the GPU's ability to perform as many parallel operations as possible, performance gains can often be had by *choosing a grid size that has a number of blocks that is a multiple of the number of SMs on a given GPU.*\n",
    "\n",
    "Additionally, SMs create, manage, schedule, and execute groupings of 32 threads from within a block called **warps**. A more [in depth coverage of SMs and warps](http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation) is beyond the scope of this course, however, it is important to know that performance gains can also be had by *choosing a block size that has a number of threads that is a multiple of 32.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programmatically Querying GPU Device Properties\n",
    "\n",
    "In order to support portability, since the number of SMs on a GPU can differ depending on the specific GPU being used, the number of SMs should not be hard-coded into a code bases. Rather, this information should be acquired programatically.\n",
    "\n",
    "The following shows how, in CUDA C/C++, to obtain a C struct which contains many properties about the currently active GPU device, including its number of SMs:\n",
    "\n",
    "```cpp\n",
    "int deviceId;\n",
    "cudaGetDevice(&deviceId);                  // `deviceId` now points to the id of the currently active GPU.\n",
    "\n",
    "cudaDeviceProp props;\n",
    "cudaGetDeviceProperties(&props, deviceId); // `props` now has many useful properties about\n",
    "                                           // the active GPU device.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Query the Device\n",
    "\n",
    "Currently, [01-get-device-properties.cu](04-device-properties/01-get-device-properties.cu) contains many unassigned variables, and will print gibberish information intended to describe details about the currently active GPU.\n",
    "\n",
    "Build out [01-get-device-properties.cu](04-device-properties/01-get-device-properties.cu) to print the actual values for the desired device properties indicated in the source code. In order to support your work, and as an introduction to them, use the [CUDA Runtime Docs](http://docs.nvidia.com/cuda/cuda-runtime-api/structcudaDeviceProp.html) to help identify the relevant properties in the device props struct. Refer to [the solution](04-device-properties/solutions/01-get-device-properties-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\n",
      "Number of SMs: 80\n",
      "Compute Capability Major: 8\n",
      "Compute Capability Minor: 6\n",
      "Warp Size: 32\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o get-device-properties 04-device-properties/01-get-device-properties.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize Vector Add with Grids Sized to Number of SMs\n",
    "\n",
    "Utilize your ability to query the device for its number of SMs to refactor the `addVectorsInto` kernel you have been working on inside [01-vector-add.cu](01-vector-add/01-vector-add.cu) so that it launches with a grid containing a number of blocks that is a multiple of the number of SMs on the device.\n",
    "\n",
    "Depending on other specific details in the code you have written, this refactor may or may not improve, or significantly change, the performance of your kernel. Therefore, as always, be sure to use `nsys profile` so that you can quantitatively evaluate performance changes. Record the results with the rest of your findings thus far, based on the profiling output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o sm-optimized-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-558d.qdstrm'\n",
      "[1/8] [========================100%] report5.nsys-rep\n",
      "[2/8] [========================100%] report5.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report5.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     87.0       1771805106         98  18079643.9  10075080.0      2190  100153169   26534242.7  poll                  \n",
      "      9.4        192306820         86   2236125.8   2070590.0       200   20451571    2578644.0  sem_timedwait         \n",
      "      2.5         49967929        497    100539.1     14100.0       390   14324649     773020.1  ioctl                 \n",
      "      0.9         19259598         24    802483.3      5040.0      1150    7220392    2162671.3  mmap                  \n",
      "      0.1          1104430         27     40904.8      4490.0      3590     735717     139533.8  mmap64                \n",
      "      0.0           534075         44     12138.1     11575.0      4500      29901       4905.1  open64                \n",
      "      0.0           192656          4     48164.0     48671.5     37651      57662      10451.0  pthread_create        \n",
      "      0.0           187078         29      6451.0      4350.0      1480      47322       8541.8  fopen                 \n",
      "      0.0           139404         11     12673.1     14490.0      1090      22801       6990.6  write                 \n",
      "      0.0           128295         11     11663.2      4120.0      1480      69403      19948.9  munmap                \n",
      "      0.0            59592         26      2292.0        90.0        70      57312      11221.9  fgets                 \n",
      "      0.0            41844          6      6974.0      6776.0      3770      10921       2661.0  open                  \n",
      "      0.0            36381         52       699.6       565.0       150       6040        809.6  fcntl                 \n",
      "      0.0            32584         22      1481.1      1290.0       750       3201        650.4  fclose                \n",
      "      0.0            22100         14      1578.6      1145.0       480       4630       1221.6  read                  \n",
      "      0.0            16231          2      8115.5      8115.5      4211      12020       5521.8  socket                \n",
      "      0.0            12781          1     12781.0     12781.0     12781      12781          0.0  connect               \n",
      "      0.0             6751          5      1350.2      1120.0        70       3071       1267.2  fread                 \n",
      "      0.0             6440          1      6440.0      6440.0      6440       6440          0.0  pipe2                 \n",
      "      0.0             5791         64        90.5        50.0        40        370         59.2  pthread_mutex_trylock \n",
      "      0.0             2150          1      2150.0      2150.0      2150       2150          0.0  bind                  \n",
      "      0.0             1250          1      1250.0      1250.0      1250       1250          0.0  listen                \n",
      "      0.0              280          1       280.0       280.0       280        280          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ---------------------\n",
      "     54.9        128480865          3  42826955.0     28951.0     14001  128437913   74141264.8  cudaMallocManaged    \n",
      "     36.8         86241255          1  86241255.0  86241255.0  86241255   86241255          0.0  cudaDeviceSynchronize\n",
      "      8.3         19358981          3   6452993.7   6172774.0   5933965    7252242     702393.0  cudaFree             \n",
      "      0.0            44222          1     44222.0     44222.0     44222      44222          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ----------------------------------------------\n",
      "    100.0         86230632          1  86230632.0  86230632.0  86230632  86230632          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     82.0         50257166  12725    3949.5    2175.0      1791     80223       8072.8  [CUDA Unified Memory memcpy HtoD]\n",
      "     18.0         11051023    768   14389.4    3743.0      1343     80703      22781.0  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    402.653  12725     0.032     0.008     0.004     1.044        0.108  [CUDA Unified Memory memcpy HtoD]\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report5.nsys-rep\n",
      "    /dli/task/report5.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./sm-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Unified Memory Details\n",
    "\n",
    "You have been allocating memory intended for use either by host or device code with `cudaMallocManaged` and up until now have enjoyed the benefits of this method - automatic memory migration, ease of programming - without diving into the details of how the **Unified Memory** (**UM**) allocated by `cudaMallocManaged` actual works.\n",
    "\n",
    "`nsys profile` provides details about UM management in accelerated applications, and using this information, in conjunction with a more-detailed understanding of how UM works, provides additional opportunities to optimize accelerated applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following video presents upcoming material visually, at a high level. Click watch it before moving on to more detailed coverage of their topics in following sections.\n",
    "\n",
    "<script>console.log('hi');</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video controls width=\"640\" height=\"360\">\n",
       "    <source src=\"https://d36m44n9vdbmda.cloudfront.net/assets/s-ac-04-v1/task2/NVPROF_UM_2.mp4\" type=\"video/mp4\">\n",
       "    Your browser does not support the video tag.\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "video_url = \"https://d36m44n9vdbmda.cloudfront.net/assets/s-ac-04-v1/task2/NVPROF_UM_2.mp4\"\n",
    "\n",
    "video_html = f\"\"\"\n",
    "<video controls width=\"640\" height=\"360\">\n",
    "    <source src=\"{video_url}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(video_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unified Memory Migration\n",
    "\n",
    "When UM is allocated, the memory is not resident yet on either the host or the device. When either the host or device attempts to access the memory, a [page fault](https://en.wikipedia.org/wiki/Page_fault) will occur, at which point the host or device will migrate the needed data in batches. Similarly, at any point when the CPU, or any GPU in the accelerated system, attempts to access memory not yet resident on it, page faults will occur and trigger its migration.\n",
    "\n",
    "The ability to page fault and migrate memory on demand is tremendously helpful for ease of development in your accelerated applications. Additionally, when working with data that exhibits sparse access patterns, for example when it is impossible to know which data will be required to be worked on until the application actually runs, and for scenarios when data might be accessed by multiple GPU devices in an accelerated system with multiple GPUs, on-demand memory migration is remarkably beneficial.\n",
    "\n",
    "There are times - for example when data needs are known prior to runtime, and large contiguous blocks of memory are required - when the overhead of page faulting and migrating data on demand incurs an overhead cost that would be better avoided.\n",
    "\n",
    "Much of the remainder of this lab will be dedicated to understanding on-demand migration, and how to identify it in the profiler's output. With this knowledge you will be able to reduce the overhead of it in scenarios when it would be beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Explore UM Migration and Page Faulting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nsys profile` provides output describing UM behavior for the profiled application. In this exercise, you will make several modifications to a simple application, and make use of `nsys profile` after each change, to explore how UM data migration behaves.\n",
    "\n",
    "[01-page-faults.cu](06-unified-memory-page-faults/01-page-faults.cu) contains a `hostFunction` and a `gpuKernel`, both which could be used to initialize the elements of a `2<<24` element vector with the number `1`. Currently neither the host function nor GPU kernel are being used.\n",
    "\n",
    "For each of the 4 questions below, given what you have just learned about UM behavior, first hypothesize about what kind of page faulting should happen, then, edit [01-page-faults.cu](06-unified-memory-page-faults/01-page-faults.cu) to create a scenario, by using one or both of the 2 provided functions in the code bases, that will allow you to test your hypothesis.\n",
    "\n",
    "In order to test your hypotheses, compile and profile your code using the code execution cells below. Be sure to record your hypotheses, as well as the results, obtained from `nsys profile --stats=true` output. In the output of `nsys profile --stats=true` you should be looking for the following:\n",
    "\n",
    "- Is there a _CUDA Memory Operation Statistics_ section in the output?\n",
    "- If so, does it indicate host to device (HtoD) or device to host (DtoH) migrations?\n",
    "- When there are migrations, what does the output say about how many _Operations_ there were? If you see many small memory migration operations, this is a sign that on-demand page faulting is occurring, with small memory migrations occurring each time there is a page fault in the requested location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the scenarios for you to explore, along with solutions for them if you get stuck:\n",
    "\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed only by the CPU? ([solution](06-unified-memory-page-faults/solutions/01-page-faults-solution-cpu-only.cu))\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed only by the GPU? ([solution](06-unified-memory-page-faults/solutions/02-page-faults-solution-gpu-only.cu))\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed first by the CPU then the GPU? ([solution](06-unified-memory-page-faults/solutions/03-page-faults-solution-cpu-then-gpu.cu))\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed first by the GPU then the CPU? ([solution](06-unified-memory-page-faults/solutions/04-page-faults-solution-gpu-then-cpu.cu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o page-faults 06-unified-memory-page-faults/01-page-faults.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating '/tmp/nsys-report-afa0.qdstrm'\n",
      "[1/8] [========================100%] report14.nsys-rep\n",
      "[2/8] [========================100%] report14.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report14.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls  Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ---------  ----------  --------  --------  -----------  ----------------------\n",
      "     62.3        150429336         17  8848784.5  10071927.0      2520  38090068   10200843.5  poll                  \n",
      "     18.7         45116948         15  3007796.5     46053.0       140  20499147    7022406.1  sem_timedwait         \n",
      "     16.3         39355369        482    81650.1     12421.0       390   7975203     478199.9  ioctl                 \n",
      "      1.8          4449757         18   247208.7      5976.0      1010   4328898    1018685.2  mmap                  \n",
      "      0.4           904860         27    33513.3      4341.0      2890    543745     103091.5  mmap64                \n",
      "      0.2           529145         44    12026.0     10791.0      3811     39152       6496.1  open64                \n",
      "      0.1           189182         29     6523.5      4080.0      1340     43933       8638.4  fopen                 \n",
      "      0.1           189132          4    47283.0     44218.0     34032     66664      16035.4  pthread_create        \n",
      "      0.1           164980         11    14998.2     12901.0       860     30872       7555.4  write                 \n",
      "      0.0            52314         26     2012.1        70.0        60     50494       9888.4  fgets                 \n",
      "      0.0            41911          6     6985.2      8040.0      2510      9670       2752.0  open                  \n",
      "      0.0            37752         22     1716.0      1050.0       520     11621       2356.0  fclose                \n",
      "      0.0            36272         52      697.5       515.0       160      6040        816.7  fcntl                 \n",
      "      0.0            30452         14     2175.1      1260.0       930      8471       2134.2  read                  \n",
      "      0.0            25763          7     3680.4      3370.0      1780      6121       1427.7  munmap                \n",
      "      0.0            18492          2     9246.0      9246.0      5131     13361       5819.5  socket                \n",
      "      0.0            11150          1    11150.0     11150.0     11150     11150          0.0  connect               \n",
      "      0.0             8940          5     1788.0      1320.0        80      4980       2005.5  fread                 \n",
      "      0.0             7100          1     7100.0      7100.0      7100      7100          0.0  pipe2                 \n",
      "      0.0             5580         64       87.2        50.0        40       170         46.1  pthread_mutex_trylock \n",
      "      0.0             2260          1     2260.0      2260.0      2260      2260          0.0  bind                  \n",
      "      0.0             1970          1     1970.0      1970.0      1970      1970          0.0  listen                \n",
      "      0.0              400          1      400.0       400.0       400       400          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ---------------------\n",
      "     87.2        121792807          1  121792807.0  121792807.0  121792807  121792807          0.0  cudaMallocManaged    \n",
      "      9.7         13483226          1   13483226.0   13483226.0   13483226   13483226          0.0  cudaDeviceSynchronize\n",
      "      3.2          4400903          1    4400903.0    4400903.0    4400903    4400903          0.0  cudaFree             \n",
      "      0.0            29572          1      29572.0      29572.0      29572      29572          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)            Name          \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ------------------------\n",
      "    100.0         13470463          1  13470463.0  13470463.0  13470463  13470463          0.0  deviceKernel(int *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "SKIPPED: /dli/task/report14.sqlite does not contain GPU memory data.\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "SKIPPED: /dli/task/report14.sqlite does not contain GPU memory data.\n",
      "Generated:\n",
      "    /dli/task/report14.nsys-rep\n",
      "    /dli/task/report14.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./page-faults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Revisit UM Behavior for Vector Add Program\n",
    "\n",
    "Returning to the [01-vector-add.cu](01-vector-add/01-vector-add.cu) program you have been working on throughout this lab, review the code bases in its current state, and hypothesize about what kinds of memory migrations and/or page faults you expect to occur. Look at the profiling output for your last refactor (either by scrolling up to find the output or by executing the code execution cell just below), observing the _CUDA Memory Operation Statistics_ section of the profiler output. Can you explain the kinds of migrations and the number of their operations based on the contents of the code base?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-34ca.qdstrm'\n",
      "[1/8] [========================100%] report15.nsys-rep\n",
      "[2/8] [========================100%] report15.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report15.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     87.2       1794885584        100  17948855.8  10073334.0      2610  100139245   26344330.5  poll                  \n",
      "      9.7        199788967         90   2219877.4   2071618.0       110   20403263    2752313.0  sem_timedwait         \n",
      "      1.9         39078078        497     78627.9     12371.0       370    7987015     472614.1  ioctl                 \n",
      "      1.1         21963199         24    915133.3      5365.0      1020    8914640    2493386.7  mmap                  \n",
      "      0.0           883723         27     32730.5      4811.0      2861     550753     104276.0  mmap64                \n",
      "      0.0           531674         44     12083.5     10590.5      3630      41883       7280.0  open64                \n",
      "      0.0           164712         29      5679.7      3591.0      1540      28262       5699.7  fopen                 \n",
      "      0.0           160099          4     40024.8     36472.0     27462      59693      13827.6  pthread_create        \n",
      "      0.0           155149         11     14104.5     13231.0      1110      26602       6060.5  write                 \n",
      "      0.0            93695         11      8517.7      4301.0      1460      46003      12870.7  munmap                \n",
      "      0.0            58044         26      2232.5        70.0        50      56234      11014.2  fgets                 \n",
      "      0.0            35963          6      5993.8      6675.5      2561       8300       2212.0  open                  \n",
      "      0.0            33771         52       649.4       480.0       150       5190        716.2  fcntl                 \n",
      "      0.0            27240         22      1238.2      1075.0       530       3190        633.1  fclose                \n",
      "      0.0            21841         14      1560.1      1190.0       890       3530        877.1  read                  \n",
      "      0.0            17681          2      8840.5      8840.5      4010      13671       6831.4  socket                \n",
      "      0.0            13461          1     13461.0     13461.0     13461      13461          0.0  connect               \n",
      "      0.0             7171          1      7171.0      7171.0      7171       7171          0.0  pipe2                 \n",
      "      0.0             7020         64       109.7       120.0        40        480         91.1  pthread_mutex_trylock \n",
      "      0.0             6530          5      1306.0       940.0        70       3310       1376.8  fread                 \n",
      "      0.0             3051          1      3051.0      3051.0      3051       3051          0.0  bind                  \n",
      "      0.0             1310          1      1310.0      1310.0      1310       1310          0.0  listen                \n",
      "      0.0              240          1       240.0       240.0       240        240          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ---------------------\n",
      "     50.1        116850468          3  38950156.0     43352.0     18991  116788125   67409659.6  cudaMallocManaged    \n",
      "     40.4         94119426          1  94119426.0  94119426.0  94119426   94119426          0.0  cudaDeviceSynchronize\n",
      "      9.5         22045641          3   7348547.0   6735261.0   6347137    8963243    1411769.3  cudaFree             \n",
      "      0.0            44263          1     44263.0     44263.0     44263      44263          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ----------------------------------------------\n",
      "    100.0         94110151          1  94110151.0  94110151.0  94110151  94110151          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     81.6         49177073  11960    4111.8    2175.0      1823     79969       8398.8  [CUDA Unified Memory memcpy HtoD]\n",
      "     18.4         11059258    768   14400.1    3727.5      1375     80673      22787.3  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    402.653  11960     0.034     0.008     0.004     1.044        0.112  [CUDA Unified Memory memcpy HtoD]\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report15.nsys-rep\n",
      "    /dli/task/report15.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./sm-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Initialize Vector in Kernel\n",
    "\n",
    "When `nsys profile` gives the amount of time that a kernel takes to execute, the host-to-device page faults and data migrations that occur during this kernel's execution are included in the displayed execution time.\n",
    "\n",
    "With this in mind, refactor the `initWith` host function in your [01-vector-add.cu](01-vector-add/01-vector-add.cu) program to instead be a CUDA kernel, initializing the allocated vector in parallel on the GPU. After successfully compiling and running the refactored application, but before profiling it, hypothesize about the following:\n",
    "\n",
    "- How do you expect the refactor to affect UM memory migration behavior?\n",
    "- How do you expect the refactor to affect the reported run time of `addVectorsInto`?\n",
    "\n",
    "Once again, record the results. Refer to [the solution](07-init-in-kernel/solutions/01-vector-add-init-in-kernel-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o initialize-in-kernel 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-0084.qdstrm'\n",
      "[1/8] [========================100%] report17.nsys-rep\n",
      "[2/8] [========================100%] report17.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report17.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     80.9        481830552         32  15057204.8  10069351.0     24911  100126799   23967491.1  poll                  \n",
      "     10.5         62544190         28   2233721.1   2064738.5       170   20438356    4071312.1  sem_timedwait         \n",
      "      5.5         32940666        497     66279.0     13211.0       380    7960460     392228.6  ioctl                 \n",
      "      2.7         16062527         24    669272.0      4095.5       840    7375191    1869200.1  mmap                  \n",
      "      0.2           903743         27     33472.0      4420.0      3010     566738     107272.3  mmap64                \n",
      "      0.1           649595         44     14763.5     11055.0      4720      51362      11360.4  open64                \n",
      "      0.0           199261         29      6871.1      5460.0      1460      36162       6708.1  fopen                 \n",
      "      0.0           174758          4     43689.5     36816.5     33752      67373      15877.0  pthread_create        \n",
      "      0.0           141586         11     12871.5     11271.0      9001      19951       3724.2  write                 \n",
      "      0.0            70364         11      6396.7      4110.0      1540      20071       6140.1  munmap                \n",
      "      0.0            60363         26      2321.7        90.0        70      58043      11365.0  fgets                 \n",
      "      0.0            43672          6      7278.7      7655.5      4311       9670       2300.8  open                  \n",
      "      0.0            37713         22      1714.2      1695.0       710       3460        589.8  fclose                \n",
      "      0.0            35461         52       681.9       500.5       160       5160        709.3  fcntl                 \n",
      "      0.0            21990         14      1570.7      1275.0       430       4490       1309.0  read                  \n",
      "      0.0            20171          1     20171.0     20171.0     20171      20171          0.0  connect               \n",
      "      0.0            18071          2      9035.5      9035.5      4240      13831       6781.9  socket                \n",
      "      0.0             8512          5      1702.4      1181.0        70       4051       1533.8  fread                 \n",
      "      0.0             7160          1      7160.0      7160.0      7160       7160          0.0  pipe2                 \n",
      "      0.0             5921         64        92.5        80.0        40        270         52.3  pthread_mutex_trylock \n",
      "      0.0             2820          1      2820.0      2820.0      2820       2820          0.0  bind                  \n",
      "      0.0             1320          1      1320.0      1320.0      1320       1320          0.0  listen                \n",
      "      0.0              160          1       160.0       160.0       160        160          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ---------------------\n",
      "     68.3        108431737          3  36143912.3     22781.0     13861  108395095   62571359.8  cudaMallocManaged    \n",
      "     21.4         33982176          2  16991088.0  16991088.0    867073   33115103   22802800.7  cudaDeviceSynchronize\n",
      "     10.2         16142620          3   5380873.3   4379020.0   4335827    7427773    1772798.7  cudaFree             \n",
      "      0.1            93565          4     23391.3     18576.0      4850      51563      20248.2  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ----------------------------------------------\n",
      "     97.5         33164444          3  11054814.7  11028841.0  10993545  11142058      77588.7  initWith(float, float *, int)                 \n",
      "      2.5           864419          1    864419.0    864419.0    864419    864419          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    100.0         11074248    768   14419.6    3743.5      1375     80736      22791.5  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report17.nsys-rep\n",
      "    /dli/task/report17.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./initialize-in-kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Asynchronous Memory Prefetching\n",
    "\n",
    "A powerful technique to reduce the overhead of page faulting and on-demand memory migrations, both in host-to-device and device-to-host memory transfers, is called **asynchronous memory prefetching**. Using this technique allows programmers to asynchronously migrate unified memory (UM) to any CPU or GPU device in the system, in the background, prior to its use by application code. By doing this, GPU kernels and CPU function performance can be increased on account of reduced page fault and on-demand data migration overhead.\n",
    "\n",
    "Prefetching also tends to migrate data in larger chunks, and therefore fewer trips, than on-demand migration. This makes it an excellent fit when data access needs are known before runtime, and when data access patterns are not sparse.\n",
    "\n",
    "CUDA Makes asynchronously prefetching managed memory to either a GPU device or the CPU easy with its `cudaMemPrefetchAsync` function. Here is an example of using it to both prefetch data to the currently active GPU device, and then, to the CPU:\n",
    "\n",
    "```cpp\n",
    "int deviceId;\n",
    "cudaGetDevice(&deviceId);                                         // The ID of the currently active GPU device.\n",
    "\n",
    "cudaMemPrefetchAsync(pointerToSomeUMData, size, deviceId);        // Prefetch to GPU device.\n",
    "cudaMemPrefetchAsync(pointerToSomeUMData, size, cudaCpuDeviceId); // Prefetch to host. `cudaCpuDeviceId` is a\n",
    "                                                                  // built-in CUDA variable.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Prefetch Memory\n",
    "\n",
    "At this point in the lab, your [01-vector-add.cu](01-vector-add/01-vector-add.cu) program should not only be launching a CUDA kernel to add 2 vectors into a third solution vector, all which are allocated with `cudaMallocManaged`, but should also be initializing each of the 3 vectors in parallel in a CUDA kernel. If for some reason, your application does not do any of the above, please refer to the following [reference application](07-init-in-kernel/solutions/01-vector-add-init-in-kernel-solution.cu), and update your own code bases to reflect its current functionality.\n",
    "\n",
    "Conduct 3 experiments using `cudaMemPrefetchAsync` inside of your [01-vector-add.cu](01-vector-add/01-vector-add.cu) application to understand its impact on page-faulting and memory migration.\n",
    "\n",
    "- What happens when you prefetch one of the initialized vectors to the device?\n",
    "- What happens when you prefetch two of the initialized vectors to the device?\n",
    "- What happens when you prefetch all three of the initialized vectors to the device?\n",
    "\n",
    "Hypothesize about UM behavior, page faulting specifically, as well as the impact on the reported run time of the initialization kernel, before each experiment, and then verify by running `nsys profile`. Refer to [the solution](08-prefetch/solutions/01-vector-add-prefetch-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\tNumber of SMs: 80\n",
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o prefetch-to-gpu 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\tNumber of SMs: 80\n",
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-e9fe.qdstrm'\n",
      "[1/8] [========================100%] report20.nsys-rep\n",
      "[2/8] [========================100%] report20.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report20.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     85.6       1626985031         94  17308351.4  10075686.0      2451  100127307   25456035.8  poll                  \n",
      "      9.4        178293605         82   2174312.3   2070337.0       180   20456230    2560146.5  sem_timedwait         \n",
      "      3.7         69561955        500    139123.9     10880.5       370   11415428     975897.7  ioctl                 \n",
      "      1.1         20654248         24    860593.7      5505.0       880    7643727    2318239.3  mmap                  \n",
      "      0.1          2620600          2   1310300.0   1310300.0     91185    2529415    1724089.0  sem_wait              \n",
      "      0.0           891713         27     33026.4      4170.0      2970     545494     103259.7  mmap64                \n",
      "      0.0           515985         44     11726.9     11136.0      4001      33512       5028.0  open64                \n",
      "      0.0           238697          5     47739.4     39093.0     29442      69855      18400.8  pthread_create        \n",
      "      0.0           198670         29      6850.7      4750.0      1950      37262       7201.1  fopen                 \n",
      "      0.0           154470         12     12872.5     13400.5      1080      22291       4857.6  write                 \n",
      "      0.0            57494         26      2211.3        80.0        60      55354      10839.0  fgets                 \n",
      "      0.0            52092         11      4735.6      3401.0      1860      13171       3625.0  munmap                \n",
      "      0.0            45632          6      7605.3      8155.0      3940      10731       3033.3  open                  \n",
      "      0.0            37841         52       727.7       520.0       160       6320        865.5  fcntl                 \n",
      "      0.0            35382         22      1608.3      1515.0       700       3610        709.4  fclose                \n",
      "      0.0            22651         15      1510.1       980.0       370       4520       1319.6  read                  \n",
      "      0.0            17341          2      8670.5      8670.5      5181      12160       4934.9  socket                \n",
      "      0.0            10950          1     10950.0     10950.0     10950      10950          0.0  connect               \n",
      "      0.0             9032          5      1806.4      1430.0        90       4451       1855.7  fread                 \n",
      "      0.0             7180          1      7180.0      7180.0      7180       7180          0.0  pipe2                 \n",
      "      0.0             6520         64       101.9        60.0        40        450         75.2  pthread_mutex_trylock \n",
      "      0.0             2460          1      2460.0      2460.0      2460       2460          0.0  bind                  \n",
      "      0.0             1460          1      1460.0      1460.0      1460       1460          0.0  listen                \n",
      "      0.0              240          1       240.0       240.0       240        240          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ---------------------\n",
      "     67.9        116789785          3  38929928.3     29942.0     15441  116744402   67389311.4  cudaMallocManaged    \n",
      "     12.0         20701834          3   6900611.3   6556350.0   6459365    7686119     681995.8  cudaFree             \n",
      "     11.3         19449018          3   6483006.0   8444776.0    203062   10801180    5564749.1  cudaMemPrefetchAsync \n",
      "      8.8         15080521          1  15080521.0  15080521.0  15080521   15080521          0.0  cudaDeviceSynchronize\n",
      "      0.0            32482          1     32482.0     32482.0     32482      32482          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  --------  --------  --------  --------  -----------  ----------------------------------------------\n",
      "    100.0           852044          1  852044.0  852044.0    852044    852044          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     73.4         30526117    192  158990.2  159042.0    158721    159554        169.3  [CUDA Unified Memory memcpy HtoD]\n",
      "     26.6         11057381    768   14397.6    3679.5      1374     81089      22791.6  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    402.653    192     2.097     2.097     2.097     2.097        0.000  [CUDA Unified Memory memcpy HtoD]\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report20.nsys-rep\n",
      "    /dli/task/report20.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./prefetch-to-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Prefetch Memory Back to the CPU\n",
    "\n",
    "Add additional prefetching back to the CPU for the function that verifies the correctness of the `addVectorInto` kernel. Again, hypothesize about the impact on UM before profiling in `nsys` to confirm. Refer to [the solution](08-prefetch/solutions/02-vector-add-prefetch-solution-cpu-also.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\tNumber of SMs: 80\n",
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o prefetch-to-cpu 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\tNumber of SMs: 80\n",
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-834d.qdstrm'\n",
      "[1/8] [========================100%] report21.nsys-rep\n",
      "[2/8] [========================100%] report21.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report21.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     78.7        501581996         34  14752411.6  10068922.5      2080  100127095   22869668.1  poll                  \n",
      "     11.3         71877842         30   2395928.1   2069045.5       210   20525106    4417905.7  sem_timedwait         \n",
      "      7.1         44965546        500     89931.1     12856.0       370   11778419     651568.2  ioctl                 \n",
      "      2.6         16415738         24    683989.1      5005.5       990    7752294    1926108.2  mmap                  \n",
      "      0.2          1105012         27     40926.4      4870.0      3030     760861     144381.0  mmap64                \n",
      "      0.1           518123         44     11775.5     11230.5      4361      34371       5375.0  open64                \n",
      "      0.0           195990          4     48997.5     49137.5     28761      68954      17866.7  pthread_create        \n",
      "      0.0           189198         29      6524.1      4151.0      1840      33761       7117.0  fopen                 \n",
      "      0.0           163891         11     14899.2     14161.0       971      30412       7178.4  write                 \n",
      "      0.0            74154         12      6179.5      3575.0      1310      35102       9257.5  munmap                \n",
      "      0.0            59994         26      2307.5        90.0        70      57763      11310.8  fgets                 \n",
      "      0.0            46602          6      7767.0      8765.5      3560      11031       2796.5  open                  \n",
      "      0.0            36883         52       709.3       520.0       160       6680        899.3  fcntl                 \n",
      "      0.0            32893         22      1495.1      1300.5       711       4340        784.4  fclose                \n",
      "      0.0            21222         14      1515.9      1165.0       460       4181       1187.8  read                  \n",
      "      0.0            17591          2      8795.5      8795.5      4410      13181       6202.0  socket                \n",
      "      0.0            12351          1     12351.0     12351.0     12351      12351          0.0  connect               \n",
      "      0.0            11022          5      2204.4      1190.0        90       6211       2405.2  fread                 \n",
      "      0.0             6441          1      6441.0      6441.0      6441       6441          0.0  pipe2                 \n",
      "      0.0             5900         64        92.2        65.0        40        210         49.4  pthread_mutex_trylock \n",
      "      0.0             2540          1      2540.0      2540.0      2540       2540          0.0  bind                  \n",
      "      0.0             1130          1      1130.0      1130.0      1130       1130          0.0  listen                \n",
      "      0.0              240          1       240.0       240.0       240        240          0.0  pthread_cond_broadcast\n",
      "      0.0              170          1       170.0       170.0       170        170          0.0  pthread_mutex_lock    \n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ---------------------\n",
      "     68.6        126742037          3  42247345.7     30152.0     14681  126697204   73135723.1  cudaMallocManaged    \n",
      "     22.4         41386580          2  20693290.0  20693290.0    884857   40501723   28013354.6  cudaDeviceSynchronize\n",
      "      8.9         16451729          3   5483909.7   4410936.0   4258577    7782216    1991849.0  cudaFree             \n",
      "      0.1           190820          3     63606.7     51312.0     48563      90945      23715.6  cudaMemPrefetchAsync \n",
      "      0.0            63984          4     15996.0     15721.0      4400      28142      12418.8  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ----------------------------------------------\n",
      "     97.9         40501870          3  13500623.3  13522906.0  13377561  13601403     113572.4  initWith(float, float *, int)                 \n",
      "      2.1           882085          1    882085.0    882085.0    882085    882085          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    100.0         11075336    768   14421.0    3727.5      1407     80609      22791.7  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report21.nsys-rep\n",
      "    /dli/task/report21.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./prefetch-to-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this series of refactors to use asynchronous prefetching, you should see that there are fewer, but larger, memory transfers, and, that the kernel execution time is significantly decreased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "At this point in the lab, you are able to:\n",
    "\n",
    "- Use the Nsight Systems command line tool (**nsys**) to profile accelerated application performance.\n",
    "- Leverage an understanding of **Streaming Multiprocessors** to optimize execution configurations.\n",
    "- Understand the behavior of **Unified Memory** with regard to page faulting and data migrations.\n",
    "- Use **asynchronous memory prefetching** to reduce page faults and data migrations for increased performance.\n",
    "- Employ an iterative development cycle to rapidly accelerate and deploy applications.\n",
    "\n",
    "In order to consolidate your learning, and reinforce your ability to iteratively accelerate, optimize, and deploy applications, please proceed to this lab's final exercise. After completing it, for those of you with time and interest, please proceed to the *Advanced Content* section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Exercise: Iteratively Optimize an Accelerated SAXPY Application\n",
    "\n",
    "A basic accelerated SAXPY (Single Precision a\\*x+b) application has been provided for you [here](09-saxpy/01-saxpy.cu). It currently works and you can compile, run, and then profile it with `nsys profile` below.\n",
    "\n",
    "Record the runtime of the `saxpy` kernel without making any modifications and then work *iteratively* to optimize the application, using `nsys profile` after each iteration to notice the effects of the code changes on kernel performance and UM behavior.\n",
    "\n",
    "Utilize the techniques from this lab. To support your learning, utilize [effortful retrieval](http://sites.gsu.edu/scholarlyteaching/effortful-retrieval/) whenever possible, rather than rushing to look up the specifics of techniques from earlier in the lesson.\n",
    "\n",
    "Your end goal is to profile an accurate `saxpy` kernel, without modifying `N`, to run in under *200,000 ns*. Check out [the solution](09-saxpy/solutions/02-saxpy-solution.cu) if you get stuck, and feel free to compile and profile it if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\tNumber of SMs: 80\n",
      "c[0] = 5, c[1] = 0, c[2] = 0, c[3] = 0, c[4] = 0, \n",
      "c[4194299] = 0, c[4194300] = 0, c[4194301] = 0, c[4194302] = 0, c[4194303] = 0, \n"
     ]
    }
   ],
   "source": [
    "!nvcc -o saxpy 09-saxpy/01-saxpy.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\tNumber of SMs: 80\n",
      "c[0] = 5, c[1] = 0, c[2] = 0, c[3] = 0, c[4] = 0, \n",
      "c[4194299] = 0, c[4194300] = 0, c[4194301] = 0, c[4194302] = 0, c[4194303] = 0, \n",
      "Generating '/tmp/nsys-report-6b11.qdstrm'\n",
      "[1/8] [========================100%] report24.nsys-rep\n",
      "[2/8] [========================100%] report24.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report24.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ----------------------\n",
      "     73.1        291551551         24  12147981.3  10068131.0     24661  96577730   19892643.6  poll                  \n",
      "     13.4         53305178        500    106610.4     12360.5       370  14545305     762337.4  ioctl                 \n",
      "     12.3         48967362         19   2577229.6     61313.0       160  20500778    5575346.5  sem_timedwait         \n",
      "      0.6          2250422         23     97844.4      6320.0       830    721583     240368.9  mmap                  \n",
      "      0.2           893808         27     33104.0      4670.0      2830    550385     104164.9  mmap64                \n",
      "      0.1           471864         44     10724.2     10246.0      3770     28801       3940.5  open64                \n",
      "      0.1           431520          2    215760.0    215760.0    109285    322235     150578.4  sem_wait              \n",
      "      0.1           247921          5     49584.2     52912.0     34452     63213      13815.2  pthread_create        \n",
      "      0.0           174020         12     14501.7     14041.0     11581     22091       3056.2  write                 \n",
      "      0.0           139991         29      4827.3      3760.0      1520     18290       3787.2  fopen                 \n",
      "      0.0            51772         26      1991.2        70.0        50     49962       9784.2  fgets                 \n",
      "      0.0            33203          6      5533.8      5195.5      2540      8571       2199.1  open                  \n",
      "      0.0            32962         52       633.9       480.0       210      4501        621.2  fcntl                 \n",
      "      0.0            30852         10      3085.2      2745.5      1470      5010       1254.8  munmap                \n",
      "      0.0            25591         22      1163.2      1000.5       580      3240        601.6  fclose                \n",
      "      0.0            22251         15      1483.4      1320.0       551      3820        884.8  read                  \n",
      "      0.0            17292          2      8646.0      8646.0      4531     12761       5819.5  socket                \n",
      "      0.0            16110          5      3222.0      1890.0       120      8050       3565.7  fread                 \n",
      "      0.0            12081          1     12081.0     12081.0     12081     12081          0.0  connect               \n",
      "      0.0             7100          1      7100.0      7100.0      7100      7100          0.0  pipe2                 \n",
      "      0.0             6151         64        96.1       115.0        40       480         67.2  pthread_mutex_trylock \n",
      "      0.0             2390          1      2390.0      2390.0      2390      2390          0.0  bind                  \n",
      "      0.0             1200          1      1200.0      1200.0      1200      1200          0.0  listen                \n",
      "      0.0              250          1       250.0       250.0       250       250          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)   Med (ns)   Min (ns)  Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ----------  ---------  --------  ---------  -----------  ---------------------\n",
      "     94.8        134330897          3  44776965.7    21501.0     18751  134290645   77521120.3  cudaMallocManaged    \n",
      "      2.1          2965815          3    988605.0  1062128.0    213270    1690417     741313.1  cudaMemPrefetchAsync \n",
      "      1.6          2208941          3    736313.7   740454.0    705432     763055      29033.8  cudaFree             \n",
      "      1.5          2121817          1   2121817.0  2121817.0   2121817    2121817          0.0  cudaDeviceSynchronize\n",
      "      0.0            34842          1     34842.0    34842.0     34842      34842          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)             Name           \n",
      " --------  ---------------  ---------  --------  --------  --------  --------  -----------  --------------------------\n",
      "    100.0            82624          1   82624.0   82624.0     82624     82624          0.0  saxpy(int *, int *, int *)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     99.6          3815899     24  158995.8  159040.0    158719    159264        205.1  [CUDA Unified Memory memcpy HtoD]\n",
      "      0.4            14462      4    3615.5    3583.5      1439      5856       2477.1  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     50.332     24     2.097     2.097     2.097     2.097        0.000  [CUDA Unified Memory memcpy HtoD]\n",
      "      0.131      4     0.033     0.033     0.004     0.061        0.033  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report24.nsys-rep\n",
      "    /dli/task/report24.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./saxpy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

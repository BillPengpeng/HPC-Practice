本文主要整理CUDA MODE lecture_002 (pmpp book ch. 1-3)的要点。

## 1.0 Ch 1: Introduction

1.  **核心动机：提升 GPU 性能 (FLOPS)**
    *   核心目标是让 GPU 运行得更快、更强大（“GPU go brrr”），追求更高的浮点运算能力（FLOPS）。
    *   驱动因素是：**模拟和构建世界模型**（如游戏、天气预测、蛋白质折叠、机器人技术）。

2.  **终极目标：实现 AGI 以解决重大问题**
    *   更大规模的模型通常更智能。
    *   最终目标是实现人工通用智能（AGI）。
    *   AGI 被期望用于解决人类面临的重大挑战：防止战争、应对气候变化、治愈癌症等。

3.  **GPU 的关键作用**
    *   GPU 是现代深度学习的**基础支撑（backbone）**。

4.  **对比：传统 CPU 的局限与演变**
    *   **经典软件**依赖于顺序执行的程序。
    *   CPU 单纯提升时钟频率的趋势在 **2003 年左右放缓**，主要受制于**能耗和散热问题**。
    *   作为应对，**多核 CPU** 架构兴起。
    *   这迫使开发者必须学习**多线程编程**，并面临其带来的复杂性（如**死锁、竞态条件**等问题）。

## 1.1 The rise of CUDA

本节阐述了 CUDA 作为现代并行编程模型的核心地位，解释了 GPU 相较于多核 CPU 在高性能计算（尤其是峰值 FLOPS）上的优势。它说明了 CUDA 的基本原理（工作分配到线程）和 GPU 的设计哲学（追求海量线程的执行吞吐量）。文本对比了 CPU 和 GPU 的适用场景（CPU 处理顺序部分，GPU 处理计算密集型部分），并简要回顾了 CUDA 出现前通过图形 API（GPGPU）进行计算的困难历史，最后指出 CUDA 的出现使得 GPU 编程因其强大的并行计算能力和广泛的硬件支持而对开发者极具吸引力。

**要点总结：**

1.  **CUDA 的本质：** CUDA 是现代软件的核心，专注于**并行程序**开发。
2.  **GPU 的核心优势：** GPU 拥有**远超多核 CPU 的峰值 FLOPS**（浮点运算能力）。
3.  **CUDA 基本原理：** 核心思想是将计算任务**划分（Divide work）** 给大量的**线程（Threads）** 来执行。
4.  **GPU 设计哲学：** GPU 的设计目标是最大化**海量线程的执行吞吐量（Execution throughput）**，而非单个线程的速度。
5.  **GPU 编程关键点：** 程序必须启动**足够多的线程**才能充分利用 GPU 性能；线程少的程序在 GPU 上**表现不佳**。
6.  **CPU + GPU 协同：** 现代计算架构中，**顺序执行部分**在 **CPU** 上运行，**计算密集型部分**则交给 **GPU** 处理。
7.  **CUDA 含义：** CUDA 代表 **Compute Unified Device Architecture**。
8.  **CUDA 前的历史（GPGPU）：** 在 CUDA 出现之前，开发者需要使用技巧通过**图形 API（如 OpenGL 或 Direct3D）** 来进行通用计算（GPGPU），过程复杂且不直观。
9.  **GPU 编程的吸引力：** 得益于 **CUDA 的出现**和 **GPU 硬件的广泛普及（Massive availability）**，利用 GPU 进行高性能并行计算现在对开发者来说变得**非常便捷和有吸引力**。

## 1.2 阿姆达尔定律（Amdahl's Law）

本节介绍了阿姆达尔定律的核心概念。该定律指出，通过并行化所能获得的系统加速比（定义为慢系统时间除以快系统时间）受到程序中可并行部分比例 `p` 的限制。其理论上限由公式 `speedup < 1/(1-p)` 给出（实际应用中通常考虑 `≤`）。例如，如果 `p=90%`，则最大加速比小于 10 倍。然而，作者指出一个乐观的事实：对于许多处理大型数据集的实际应用，可并行部分 `p` 通常超过 99%，因此实际能达到超过 100 倍的加速比。

**要点总结：**

1.  **阿姆达尔定律的核心观点：** 通过并行化实现的**加速比（Speedup）** 存在一个**理论上的上限**。
2.  **加速比定义：** `speedup = 慢系统时间 (slow_sys_time) / 快系统时间 (fast_sys_time)`。
3.  **关键限制因素：** 加速比的上限由程序中**可并行化部分的比例 `p`** 决定。
4.  **数学表达式（上限）：** `speedup < 1/(1-p)`（或 `speedup ≤ 1/(1-p)`）。
5.  **示例说明：** 如果可并行部分 `p = 90%` (0.9)，则最大加速比 `< 1/(1-0.9) = 1/0.1 = 10` 倍。
6.  **实际应用中的乐观情况：**
    *   对于许多**现实世界的应用**（尤其是处理**大型数据集**时），**可并行部分 `p` 非常高（> 99%）**。
    *   因此，在这些情况下，**实际可达到的加速比可以非常高（> 100 倍）**，远超过基于较低 `p` 值的理论预测上限。

## 1.3 Challenges

**内容概括：**

- if you do not care about performance, parallel programming is very easy

本节通过一句反讽性的引言（“如果你不关心性能，并行编程就很容易”）引出了并行编程在实际设计中的核心挑战。它指出设计并行算法通常比设计顺序算法更困难，并举了并行化递归计算（如前缀和）需要非直观思维作为例子。文本强调了性能瓶颈常常在于内存（延迟或吞吐量），而非计算本身。它还说明并行程序的性能表现会因输入数据的特性而有巨大差异，并且点明并非所有应用都能轻松并行化（“令人尴尬的并行”），同步操作（如等待）会带来显著的开销。

**要点总结：**

1.  **核心难点（反讽点题）：** 并行编程的**真正困难在于实现高性能优化**。如果完全不在乎性能，并行编程可能显得“简单”。
2.  **算法设计复杂度：** **设计高效的并行算法**在实践中通常**比设计顺序算法更困难**。
3.  **非直观思维需求：** 并行化某些计算（如**递归计算**）需要**非直观的、创新的思维方式**（例如，高效并行实现**前缀和（prefix sum）** 算法）。
4.  **常见性能瓶颈：** 并行程序的**速度（性能）经常受限于内存访问**，即**内存延迟（访问速度）** 和**内存吞吐量（带宽）**，而非处理器本身的计算能力（此时程序是 **memory bound**）。
5.  **输入数据依赖性：** 并行程序的**性能表现会因输入数据的具体特性（如大小、分布、结构）而产生巨大差异**。
6.  **同步开销：** 并非所有应用程序都是“令人尴尬的并行”（**embarrassingly parallel**，指可以完美分割、几乎不需要通信/同步）。**同步操作**（例如，线程/进程间的协调和等待）**会引入显著的额外开销（overhead）**，降低整体效率。

## 1.4 Main Goals of the Book

**内容概括：**

本节阐述了 PMPP 的核心目标：为通用的并行编程奠定坚实基础。它强调了三个关键的学习/实践支柱：1) 并行编程思维与计算思维；2) 确保程序的正确性、可靠性和性能调试能力；3) 通过规范化和局部化内存访问来实现可扩展性。文本说明 PMPP 选择使用 GPU 作为主要的教学工具和实践平台，但指出所学技术同样适用于其他类型的加速器。最后，它强调课程将以实践为导向，通过具体的 CUDA 编程示例来引入和讲解核心概念。

**要点总结：**

1.  **PMPP 的核心目标：** 为**通用的并行编程**建立**坚实的基础（foundation）**。
2.  **三大核心支柱/学习重点：**
    *   **并行编程与计算思维：** 培养适应并行环境的思维方式和问题解决方法。
    *   **正确性、可靠性与调试：** 确保并行程序功能正确、运行可靠，并具备调试功能问题和性能问题的能力。
    *   **可扩展性：** 关注如何设计程序以实现良好的扩展性，关键方法是**规范化（regularize）** 和**局部化（localize）内存访问**（优化内存使用模式）。
3.  **教学工具选择：** 使用 **GPU** 作为主要的**学习载体（learning vehicle）**。
4.  **技术的普适性：** 在 GPU 上学到的并行编程**技术同样适用于其他类型的加速器**（如 FPGA, TPU 等）。
5.  **教学方法：** 采用**实践导向（hands-on）** 的方式，通过**具体的 CUDA 编程示例**来引入和讲解核心的并行编程概念。

## 2.0 Ch 2: Heterogeneous data parallel computing

**内容概括：**

本节介绍了**异构数据并行计算**的核心概念，即利用 **CPU + GPU** 协同工作。核心思想是 **数据并行**：将大规模计算任务分解成大量**独立的、可并行执行**的小计算单元。文本通过两个具体例子说明这种并行性：**向量加法** 和 **RGB图像转灰度图**。特别强调了在图像转换中，每个像素的转换操作（使用公式 `L = r*0.21 + g*0.72 + b*0.07` 计算亮度值）是**完全独立**的，这正是数据并行的理想应用场景（“令人尴尬的并行”）。

**要点总结：**

1.  **核心概念：异构计算**
    *   系统由不同类型的处理器组成，本章指 **CPU + GPU** 协同工作。

2.  **核心范式：数据并行**
    *   核心策略是将大型计算任务**分解（Break down）** 成大量**独立的（Independent）** 小计算单元。
    *   这些独立的小计算单元可以**同时执行（Executed in parallel）**。

3.  **关键特性：独立性**
    *   数据并行成功的关键在于计算单元之间的**独立性**。每个单元的计算不依赖于其他单元的结果。

4.  **示例应用 1：向量加法**
    *   将两个大向量相加，每个元素（如 `C[i] = A[i] + B[i]`）的计算是独立的。

5.  **示例应用 2：RGB 转灰度图像**
    *   将彩色（RGB）图像转换为灰度图像。
    *   **核心操作：** 对图像中的**每个像素**独立应用转换公式。
    *   **转换公式：** `Luminance (L) = r * 0.21 + g * 0.72 + b * 0.07` (这是一个简单的**加权求和**，权重基于人眼对不同颜色的敏感度)。
    *   **独立性体现：** 每个像素的灰度值计算**仅依赖于该像素自身的 RGB 值**，与其他像素无关。💡

## 2.1 CUDA C

**内容概括：**

本节介绍了 CUDA C 编程语言的基本特性及其核心术语和概念。CUDA C 被描述为对 ANSI C 的扩展，仅添加了少量新语法。它明确了关键术语：CPU 称为 **host**，GPU 称为 **device**。CUDA C 源文件可以混合包含主机（CPU）代码和设备（GPU）代码。其中，在设备上执行的函数被称为 **kernels**（内核）。当启动一个内核时，会创建一个由大量线程组成的 **grid**（网格）来执行它。CPU 和 GPU 代码可以**并发（同时、重叠）** 执行。文本强调在 GPU 编程中，应该大胆地启动**非常多的线程**，例如，为输出张量（或数组）中的每个元素分配一个线程是常见且合适的做法。

**要点总结：**

1.  **CUDA C 的本质：**
    *   是 **ANSI C** 的扩展。
    *   只引入了**极少量（minimal）的新语法**。

2.  **核心术语：**
    *   **CPU = Host (主机)**
    *   **GPU = Device (设备)**

3.  **代码结构：**
    *   CUDA C 源文件是**混合体（mixture）**，可以同时包含在 **Host (CPU)** 上运行的代码和在 **Device (GPU)** 上运行的代码。

4.  **内核（Kernel）：**
    *   在 **Device (GPU)** 上执行的函数被称为 **kernels**。
    *   内核是并行计算任务的入口点。

5.  **执行模型：**
    *   当启动一个内核时，会创建一个由**大量线程（many threads）** 组成的 **grid（网格）** 来执行该内核。
    *   **Grid** 是线程组织的最高层级结构。

6.  **并发执行：**
    *   **Host (CPU) 代码和 Device (GPU) 代码可以并发运行（runs concurrently）**，即它们的执行在时间上可以**重叠（overlapped）**，CPU 不需要等待 GPU 完成（反之亦然，取决于任务依赖关系）。

7.  **GPU 编程关键原则：**
    *   **不要害怕启动大量线程（don't be afraid of launching many threads）**：GPU 的设计就是为了高效处理海量线程。
    *   **线程分配粒度：** 一个常见的、有效的模式是 **为每个输出元素（如张量或数组中的一个元素）分配一个线程**（例如，向量加法中每个线程计算 `C[i] = A[i] + B[i]`）。

## 2.2 Example: Vector Addition

**内容概括：**

本节以向量加法为例，展示了如何将传统的顺序循环计算（CPU）转化为 GPU 上的并行计算。它强调了向量加法是“令人尴尬的并行”问题，因为每个元素加法 (`c[i] = a[i] + b[i]`) 完全独立。文本描述了一个基本的（“Naïve”）GPU 实现流程：分配设备内存、将输入数据从主机内存复制到设备内存、启动执行加法操作的内核、将结果从设备内存复制回主机内存、最后释放设备内存。最后，它指出在实际优化中，为了最大化性能，数据应尽可能长时间地驻留在 GPU 内存中，以便异步调度多个内核执行，减少昂贵的数据传输。

**要点总结：**

1.  **核心概念：循环 -> 线程**
    *   将传统的顺序 **循环计算** 转化为由 **大量线程** 并行执行是 GPU 并行化的基本思想。
2.  **理想并行性：**
    *   向量加法是**高度可并行化（Easily parallelizable）** 的经典例子，因为**所有加法操作（每个元素的计算）都是相互独立的**。
3.  **基础 GPU 实现流程 ("Naïve"):**
    1.  **分配设备内存：** 在 GPU (Device) 上为输入向量 (`a`, `b`) 和输出向量 (`c`) 分配内存。
    2.  **数据传输 (H2D)：** 将输入向量数据从 **主机内存 (Host memory - CPU)** **传输 (Transfer)** 到 **设备内存 (Device memory - GPU)**。
    3.  **启动内核：** **启动 (Launch)** 一个 CUDA **内核 (Kernel)**。这个内核会创建大量线程，每个线程负责计算一个输出元素 (`c[i] = a[i] + b[i]`)。
    4.  **数据传输 (D2H)：** 将计算结果向量 `c` 从 **设备内存 (Device memory)** **复制 (Copy)** 回 **主机内存 (Host memory)**。
    5.  **释放设备内存：** **释放 (Free)** 之前在 GPU 上分配的内存。
4.  **关键性能优化原则：**
    *   在实际应用中，为了获得最佳性能，应**尽可能长时间地将数据保留在 GPU 内存中 (keep data on the GPU as long as possible)**。
    *   这样做的主要好处是可以**异步调度多个内核执行 (asynchronously schedule many kernel launches)**，让 GPU 持续进行计算工作，同时减少或隐藏昂贵的主机与设备之间的数据传输时间。这是对“Naïve”流程的重要优化方向。

## 2.3 CUDA Essentials: Memory allocation

**内容概括：**

本节介绍了 CUDA 编程中关于设备内存的两个最核心、最基础的操作。它首先指出 NVIDIA GPU 设备拥有自己独立的 **DRAM**，称为**设备全局内存（Device Global Memory）**（并提到第 5 章会介绍其他类型的内存）。文本重点说明了两个关键函数：`cudaMalloc` 用于在设备全局内存中**分配**空间，`cudaFree` 用于**释放**之前分配的设备内存。同时，它也点明了需要在**主机内存（CPU 内存）** 和**设备内存（GPU 内存）** 之间**复制数据**。

**要点总结：**

1.  **GPU 独立内存：** NVIDIA GPU 设备配备有自己的 **DRAM**，称为**设备内存（Device Memory）**。
2.  **内存类型：** 本章讨论的是设备上的 **全局内存（Global Memory）**。这是设备上容量最大、最通用（但通常访问延迟较高）的内存空间。
    *   *(补充说明：文中提到第 5 章会学习其他类型的内存，如共享内存 Shared Memory、常量内存 Constant Memory、纹理内存 Texture Memory 等，它们各有不同的特性和用途)。*
3.  **核心内存管理函数：**
    *   **`cudaMalloc`：** 用于在设备的**全局内存**中**动态分配**指定大小的内存空间。它返回一个指向设备内存的指针（类型为 `void*`，通常需要强制转换为所需类型）。
    *   **`cudaFree`：** 用于**释放**之前通过 `cudaMalloc` (或其他分配函数) 在设备全局内存中分配的空间。
4.  **数据传输：** 除了分配和释放，管理设备内存的一个核心任务是**在主机内存（Host Memory - CPU RAM）和设备全局内存（Device Global Memory - GPU DRAM）之间复制数据**。
    *   cudaMemcpy(A_d, A_h, size, cudaMemcpyHostToDevice);
    *   cudaMemcpy(B_d, B_h, size, cudaMemcpyHostToDevice);

## 2.4 Kernel functions  fn<<>>

**内容概括：**

本节介绍了 CUDA 编程中核心的执行单元：**内核函数（Kernel functions）** 及其启动方式。它说明了启动内核会创建一个由大量线程组成的 **网格（grid）**。所有线程执行**相同的代码**，体现了 **SPMD（Single Program Multiple Data）** 并行模型。文本进一步解释了 CUDA 线程的**层级组织**结构：网格（Grid）由多个**线程块（Thread blocks）** 组成，而每个线程块又包含多个线程（Threads）。最后，它给出了一个关键限制：每个线程块最多可包含 **1024 个线程**（这是当前主流 GPU 硬件的常见限制）。

**要点总结：**

1.  **内核函数声明与启动：**
    *   内核函数使用特殊的语法 `fn<<<...>>>` 来声明和启动。
    *   `<<<...>>>` 中的参数指定了线程的组织方式（网格和线程块的维度）。

2.  **内核启动 = 网格启动：**
    *   **启动一个内核（Launching kernel）** 意味着启动一个由**大量线程（many threads）** 组成的 **网格（grid）**。

3.  **执行模型：SPMD：**
    *   网格中的所有线程**执行相同的代码**（即内核函数的代码）。
    *   这种模式称为 **SPMD（Single Program Multiple Data）**：**单个程序（内核代码）** 在**多个数据元素（由不同线程处理）** 上并行执行。

4.  **线程层级组织：**
    *   CUDA 线程采用**层级结构（hierarchically organized）**：
    *   **网格（Grid）**：最高层级结构，包含执行一个内核的所有线程。
    *   **线程块（Thread block）**：网格由多个线程块组成。线程块是线程分组的基本单位。
    *   **线程（Thread）**：线程块由多个线程组成。线程是执行计算的最小单位。
    *   *(补充：线程块内的线程可以通过共享内存和同步进行协作，不同线程块内的线程通常独立执行)。*

5.  **线程块大小限制：**
    *   每个**线程块（Thread block）** 最多可以包含 **1024 个线程**。
    *   这是一个**硬件限制**（由 GPU 架构决定，如 NVIDIA Ampere, Turing, Pascal 等架构的主流限制）。开发者需要根据算法和硬件能力来设计线程块的形状（维度）和大小（总线程数不超过 1024）。

## 2.5 Kernel Coordinates

**内容概括：**

本节解释了 CUDA 内核中如何通过内置变量让线程识别自身身份和计算任务。核心是三个内置变量：`blockIdx`（线程块索引）、`threadIdx`（线程块内线程索引）和 `blockDim`（线程块维度）。这些“坐标”使得所有执行相同代码（SPMD）的线程能够确定自己应该处理哪部分数据。文本使用电话区号（`blockIdx`）和本地号码（`threadIdx`）的类比来说明线程的唯一标识方式。最后，以向量加法为例，展示了如何利用这些变量计算线程对应的全局数组索引 `i`：`i = blockIdx.x * blockDim.x + threadIdx.x`。

**要点总结：**

1.  **核心目的：线程标识与任务分配**
    *   在内核函数内部，需要一种机制让**所有执行相同代码的线程**知道自己**具体负责处理哪部分数据或执行什么特定任务**。
2.  **关键内置变量：**
    *   **`blockIdx`：** 表示当前线程所在的**线程块（Thread block）** 在**网格（Grid）** 中的**索引（坐标）**。它是一个 `dim3` 结构体（通常使用 `.x`, `.y`, `.z` 成员访问）。
    *   **`threadIdx`：** 表示当前线程在其所属的**线程块（Thread block）** 内部的**索引（坐标）**。同样是一个 `dim3` 结构体。
    *   **`blockDim`：** 表示**线程块（Thread block）** 的**维度（大小）**（即每个维度上有多少个线程）。它是一个 `dim3` 结构体，在启动内核时通过 `<<<...>>>` 语法指定。
3.  **线程唯一标识：**
    *   通过 **`blockIdx` 和 `threadIdx`** 的组合，可以**唯一确定网格中的任何一个线程**。
4.  **电话系统类比：**
    *   **`blockIdx` 类比为“区号（Area Code）”**：标识线程块所在的大区域（网格中的位置）。
    *   **`threadIdx` 类比为“本地电话号码（Local Phone Number）”**：标识线程块内部的特定线程。
    *   两者结合才能唯一确定一个“电话号码”（线程）。
5.  **计算全局索引（核心应用）：**
    *   对于像向量加法这样需要线性索引访问数组的任务，可以通过 `blockIdx`, `threadIdx` 和 `blockDim` 计算线程对应的全局数组索引 `i`。
    *   **公式：`int i = blockIdx.x * blockDim.x + threadIdx.x;`**
        *   `blockIdx.x * blockDim.x`：计算当前线程块**起始位置**在全局数组中的索引。
        *   `threadIdx.x`：计算当前线程在**线程块内部**的偏移量。
        *   两者相加得到当前线程负责处理的**全局数组元素索引 `i`**。

## 2.6 __global__ & __host__

**内容概括：**

本节解释了 CUDA C 中用于声明函数执行位置和调用权限的关键限定符。核心是 `__global__`，用于声明内核函数（Kernel），其调用会启动一个 CUDA 线程网格（Grid）。`__device__` 用于声明设备端函数，只能被设备端代码（如内核或其他 `__device__` 函数）调用。`__host__` 用于声明主机端函数（标准 C 函数），只能被主机端代码调用。文本还特别说明了一个函数可以同时使用 `__host__` 和 `__device__` 限定符，这将导致编译器为该函数生成 CPU 和 GPU 两个版本。

**要点总结：**

1.  **`__global__`：**
    *   **用途：** 声明**内核函数（Kernel function）**。
    *   **调用位置：** 只能从 **主机代码（Host code - CPU）** 调用。
    *   **执行位置：** 在 **设备（Device - GPU）** 上执行。
    *   **调用行为：** 调用一个 `__global__` 函数（使用 `<<<...>>>` 语法）会**启动一个新的 CUDA 线程网格（Grid of threads）**。
    *   **返回值：** 必须声明为 `void`。

2.  **`__host__`：**
    *   **用途：** 声明**主机端函数（Host function）**。
    *   **调用位置：** 只能从 **主机代码（Host code - CPU）** 调用。
    *   **执行位置：** 在 **主机（Host - CPU）** 上执行。
    *   **默认行为：** 如果函数没有任何限定符，它默认是 `__host__` 函数。

3.  **`__device__`：**
    *   **用途：** 声明**设备端函数（Device function）**。
    *   **调用位置：** 只能从 **设备代码（Device code - GPU）** 调用（例如，从 `__global__` 内核或另一个 `__device__` 函数内部）。
    *   **执行位置：** 在 **设备（Device - GPU）** 上执行。

4.  **组合限定符 `__host__ __device__`：**
    *   **用途：** 同时使用 `__host__` 和 `__device__` 限定符声明**同一个函数**。
    *   **效果：** 编译器会**为该函数生成两个版本**的代码：
        *   一个在 **主机（CPU）** 上运行的版本（`__host__`）。
        *   一个在 **设备（GPU）** 上运行的版本（`__device__`）。
    *   **目的：** 允许在 CPU 和 GPU 代码中**共享相同的函数实现**（只要该函数的代码在语法和语义上对两者都有效）。

**列表对比 (`__global__`, `__host__`, `__device__`)：**

| 特性                 | `__global__`                     | `__host__`                     | `__device__`                     |
| :------------------- | :------------------------------- | :----------------------------- | :------------------------------- |
| **主要用途**         | 声明内核函数 (Kernel)           | 声明主机端函数 (CPU函数)       | 声明设备端函数 (GPU内部函数)     |
| **从哪里调用**       | **仅限主机代码 (CPU)**           | **仅限主机代码 (CPU)**         | **仅限设备代码 (GPU)**           |
| **在哪里执行**       | **设备 (GPU)**                   | **主机 (CPU)**                 | **设备 (GPU)**                   |
| **调用行为**         | 调用启动一个**线程网格 (Grid)** | 标准函数调用                   | 标准函数调用                     |
| **返回值**           | 必须为 `void`                   | 任意类型                       | 任意类型 (非 `void`)             |
| **默认限定符**       | 否                               | 是 (无限定符函数默认 `__host__`) | 否                               |
| **能否与 `__host__` 组合** | 否                               | -                              | **是** (`__host__ __device__`) |
| **能否与 `__device__` 组合** | 否                               | **是** (`__host__ __device__`) | -                                |

## 2.7 Vector Addition Example

**内容概括：**

```c
01 // compute vector sum C = A + B
02 // each thread peforms one pair-wise addition
03 __global__
04 void vecAddKernel(float* A, float* B, float* C, int n) {
05  int i = threadIdx.x + blockDim.x * blockIdx.x;
06  if (i < n) {	// check bounds
07    C[i] = A[i] + B[i];
08  }
09 }


dim3 numThreads(256);
dim3 numBlocks((n + numThreads - 1) / numThreads);
vecAddKernel<<<numBlocks, numThreads>>>(A_d, B_d, C_d, n);
```

本节完整展示了如何实现一个 GPU 加速的向量加法 (`C = A + B`)。核心策略是将顺序循环替换为由线程网格执行的并行计算。它强调了**边界检查** (`if (i < n)`) 的**关键重要性**，以防止数据大小 `n` 不能被线程块大小整除时，边界线程块中的线程访问越界内存。文本提供了内核函数 (`vecAddKernel`) 的代码示例，并详细说明了如何在主机端使用 `<<<...>>>` 语法配置内核启动参数（线程块数量和每个线程块的线程数）。最后，给出了一个具体的**主机端配置示例**：使用 `dim3` 类型定义线程块大小 (`numThreads = 256`) 和计算所需的线程块数量 (`numBlocks = (n + numThreads - 1) / numThreads`)，然后启动内核。文末提到后续会学习额外的内核启动参数。

**要点总结：**

1.  **核心策略：循环 -> 线程网格**
    *   将传统的顺序循环计算替换为由**线程网格（Grid of threads）** 执行的并行计算。

2.  **边界检查至关重要：**
    *   数据大小 `n` 可能**无法被线程块大小整除**。
    *   **必须进行边界检查 (`if (i < n)`)**，以防止边界线程块中的线程尝试**读取或写入超出分配内存范围**的位置（导致未定义行为或崩溃）。

3.  **内核代码 (`vecAddKernel`):**
    *   **`__global__`:** 声明为内核函数。
    *   **索引计算 (`int i = ...`):** 使用 `blockIdx.x`, `blockDim.x`, `threadIdx.x` 计算当前线程负责的全局数组索引 `i`。
    *   **边界检查 (`if (i < n)`):** 确保索引 `i` 在有效范围内 (`0` 到 `n-1`)。
    *   **核心计算 (`C[i] = A[i] + B[i];`):** 每个线程执行一个元素对的加法。

4.  **内核启动配置 (`<<<...>>>`):**
    *   在主机代码中调用内核函数时，使用 `<<<...>>>` 语法配置执行参数。
    *   **主要配置参数：**
        *   **线程块数量 (Grid size):** 指定网格中包含多少个线程块。
        *   **每个线程块的线程数 (Block size):** 指定每个线程块包含多少个线程（即 `blockDim`）。

5.  **主机端配置示例:**
    *   **`dim3 numThreads(256);`**
        *   定义一个 `dim3` 变量 `numThreads`，设置每个线程块包含 **256 个线程**（`blockDim.x = 256`）。
        *   *(`dim3` 是 CUDA 提供的用于表示三维维度的结构体类型，这里只用了 `.x` 维度)*。
    *   **`dim3 numBlocks((n + numThreads - 1) / numThreads);`**
        *   定义一个 `dim3` 变量 `numBlocks`，计算所需的**线程块数量**。
        *   **计算公式：`(n + numThreads - 1) / numThreads`**
            *   这是一个**向上取整**的技巧，确保有足够的线程块覆盖所有 `n` 个元素。
            *   例如，`n = 1000`, `numThreads = 256`: `(1000 + 256 - 1) / 256 = 1255 / 256 = 4.90...` -> 整数除法结果为 `4` (需要 `4` 个线程块，共 `4*256=1024` 个线程，其中最后 `24` 个线程会被边界检查 `if (i < 1000)` 过滤掉)。
    *   **内核启动：`vecAddKernel<<<numBlocks, numThreads>>>(A_d, B_d, C_d, n);`**
        *   使用计算好的 `numBlocks` 和 `numThreads` 启动内核。
        *   传入设备内存指针 `A_d`, `B_d`, `C_d` 和数据大小 `n`。

6.  **后续学习内容：**
    *   文中提到后续会学习内核启动的**额外参数**：
        *   **共享内存大小 (Shared memory size):** 指定每个线程块可用的动态分配的共享内存量。
        *   **CUDA 流 (CUDA stream):** 用于管理内核执行和数据传输的并发与依赖关系。

## 2.8 Compiler

**内容概括：**

本节简要描述了 CUDA 程序的编译和执行流程。核心工具是 NVIDIA 的 CUDA 编译器 **`nvcc`**。它的主要作用是将包含 CUDA 内核的源代码（通常为 `.cu` 文件）编译成一种名为 **PTX (Parallel Thread Execution)** 的低级虚拟指令集。PTX 代码并非直接在 GPU 上运行，而是由安装在系统上的 **图形驱动程序 (Graphics Driver)** 在运行时进一步编译（即时编译，JIT）成针对特定 GPU 硬件的原生机器码，称为 **SASS (Shader Assembly)**，最终由 GPU 执行。

**要点总结：**

1.  **核心编译器：`nvcc`**
    *   **`nvcc`** 是 NVIDIA 提供的 **CUDA C/C++ 编译器**。
    *   它的主要任务之一是**将 CUDA 内核代码编译成 PTX 代码**。

2.  **编译产物：PTX (Parallel Thread Execution)**
    *   **PTX** 是一种**低级虚拟机 (VM) 指令集**。
    *   它代表 **Parallel Thread Execution**。
    *   PTX 是**独立于具体 GPU 架构的中间表示 (Intermediate Representation, IR)**，提供了一定程度的硬件兼容性。
    *   *(可以理解为 GPU 的“汇编语言”，但并非最终硬件指令)*。

3.  **运行时编译：图形驱动程序**
    *   PTX 代码**不能直接在 GPU 硬件上执行**。
    *   在程序运行时，系统的 **NVIDIA 图形驱动程序 (Graphics Driver)** 负责**将 PTX 代码即时编译 (Just-In-Time Compile, JIT)**。

4.  **最终执行代码：SASS**
    *   图形驱动程序将 PTX 编译成的最终可执行代码称为 **SASS (Shader Assembly)**。
    *   SASS 是**特定于当前 GPU 硬件架构的原生机器码指令集**。
    *   GPU 的流式多处理器 (SM) 直接执行 SASS 指令。

**流程总结：**
CUDA 源代码 (.cu) -> `nvcc` 编译 -> **PTX (中间码)** -> 图形驱动程序 JIT 编译 -> **SASS (目标 GPU 原生机器码)** -> GPU 执行
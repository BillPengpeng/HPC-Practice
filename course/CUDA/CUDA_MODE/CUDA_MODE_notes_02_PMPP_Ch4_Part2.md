本文主要整理CUDA MODE lecture_004 Compute and Memory basics (based on ch 4 + 5 of the PMPP book) 的要点。

## 1.5 warp级别shuffle操作

![warp shuffle](https://pic1.zhimg.com/v2-ce2aea5da5fef3193325d2410c47d628_1440w.jpg)
![warp shuffle](https://pic2.zhimg.com/v2-3d3568bcdcb4a11e20b082d2c2a81bad_1440w.jpg)

### **内容概括**
1. **内核启动机制**：定义线程块（Block）的布局（线程数/块）和网格（Grid）布局（块数）。
2. **线程块执行逻辑**：线程块在流多处理器（SM）上并行执行，块内线程可共享内存，不同块相互独立。
3. **线程束调度**：线程块在SM上被划分为32线程的线程束（Warps），由SM的硬件单元调度执行。
4. **跨平台对比**：对比AMD的Wavefronts（默认64线程）与NVIDIA的Warps差异。

---

### **核心要点总结**
1. **内核启动配置**  
   - **Block Layout**：指定每个线程块包含的线程数量（如128、256线程/块）。  
   - **Grid Layout**：指定启动的线程块总数（如1024个块）。  
   - *作用*：共同定义GPU并行任务的规模（总线程数 = 块数 × 每块线程数）。

2. **线程块（Block）特性**  
   - **执行单元**：一个线程块在**同一个SM**上执行，块内线程**可访问共享内存（Shared Memory）**。  
   - **独立性**：不同线程块间**无通信能力**（除非使用Hopper+架构的新特性）。  
   - **动态分配**：CUDA运行时**自动分配块到空闲SM**，执行顺序**不可预测**（开发者需避免顺序依赖）。

3. **线程束（Warp）核心机制**  
   - **组成**：每个线程块被划分为**32线程的线程束**（NVIDIA默认）。  
   - **执行单元**：一个线程束在SM的**固定处理单元**上运行（如CUDA Core组）。  
   - **调度策略**：  
     - SM同时管理多个活跃线程束（如16-32个）。  
     - 当线程束因内存访问等操作停滞时，SM**立即切换至其他就绪线程束**（零开销切换）。  
     - **寄存器保留**：切换时线程的寄存器状态保留，确保上下文快速恢复。  

4. **AMD对比（Wavefronts）**  
   - **术语差异**：AMD GPU中对应概念为 **Wavefronts**。  
   - **规模差异**：默认包含**64线程**（NVIDIA Warp为32线程）。  

5. **硬件执行示意图（图4.2）**  
   - 展示多个线程块被动态分配到不同SM的过程。  
   - 每个SM内部划分为线程束，由硬件调度器管理执行。  

---

### **关键结论**
- **并行粒度**：线程块是资源分配单位（共享内存/寄存器），线程束是实际调度单位。  
- **性能关键**：  
  - 避免线程分化（Warp Divergence）：确保线程束内32线程执行相同指令。  
  - 利用共享内存减少全局访问延迟。  
- **跨平台注意**：NVIDIA（32线程/Warp）与AMD（64线程/Wavefront）的差异影响代码优化策略。  

此内容为理解CUDA并行编程模型的基础框架，后续可深入探讨SM资源限制（如寄存器/共享内存分配）、线程束调度优化等进阶主题。

## 1.6 Warp Divergence (<= Pascal)

![Warp Divergence](https://picx.zhimg.com/v2-6323bdda7fb3e4246a98b6cfbe10b2d3_1440w.jpg)

### **内容概括**
该图直观展示了 **Pascal及更早架构GPU中线程束（Warp）的条件分支执行机制**：
1. **核心问题**：当Warp内线程执行条件分支（如`if-else`）时，因线程路径不同导致**执行效率下降**。
2. **硬件机制**：通过**活动掩码（Active Mask）** 控制线程执行状态，共享程序计数器（PC）但串行化不同分支路径。
3. **执行流程**：分支点发散（Diverge）→ 串行执行各分支 → 最终汇合（Reconverge）。
4. **编程警示**：避免在分支内进行未同步的线程通信。

---

### **核心要点总结**

#### **1. 线程束发散机制（<= Pascal架构）**
- **触发条件**：Warp内线程执行条件分支（如`if (threadIdx.x < 4)`）。
- **执行过程**：
  - **发散（Diverge）**：条件判断后，Warp分裂为多个子组（如图中绿色线程执行`A;B`，灰色线程执行`X;Y`）。
  - **串行化**：硬件按分支路径**顺序执行**（先执行`A;B`路径，再执行`X;Y`路径）。
  - **汇合（Reconverge）**：所有线程在分支结束点（`Z`指令）同步继续执行。
- **硬件支持**：
  - **共享PC**：所有线程共享同一程序计数器。
  - **活动掩码**：通过掩码动态启用/禁用线程（如执行`A;B`时仅激活条件为真的线程）。

#### **2. 性能影响与编程警示**
- **性能损失**：  
  - 分支路径越多，串行化时间越长，Warp利用率越低（理想情况：32线程并行 → 实际：部分线程闲置）。
- **关键警告**：  
  - ❗ **禁止在分支内无同步的线程通信**（如未用`__syncwarp()`的共享内存读写），因部分线程可能被掩码禁用，导致数据冲突或死锁。
  - ✅ **自动汇合**：硬件自动在分支结束点同步线程，无需开发者干预。

#### **3. 特殊场景说明**
- **无发散的情况**：  
  - 典型内存操作（如 `cond ? x[i] : y[i]`）**不会触发发散**，因`load/store`指令天然支持非一致地址访问（无需分裂Warp）。
- **例外提示**：  
  - 图中注释强调：**“典型模式（条件选择）不会导致发散”**（N.B.部分）。

#### **4. 时间轴解析（图右侧）**
- **水平时间轴**：展示指令执行顺序。
- **执行块**：  
  - 绿色块：活跃线程执行指令（如`A;B`或`X;Y`）。  
  - 灰色块：线程被掩码禁用（等待状态）。  
- **关键标记**：  
  - `diverge`：分支分裂点（线程分组）。  
  - `reconverge`：分支汇合点（线程同步）。  

---

### **关键结论**
- **根本矛盾**：SIMT模型要求Warp内线程执行相同指令，但分支导致路径冲突。
- **规避策略**：  
  - 减少深度嵌套分支，尽量使用**向量化操作**（如 `select()` 函数）。  
  - 确保分支条件具有**Warp内一致性**（如所有线程同时满足/不满足条件）。  
- **架构演进**：  
  - Volta+架构引入**独立线程调度**（每线程独立PC），缓解分支串行化问题（但仍有代价）。  

此图揭示了传统GPU架构中分支处理的底层逻辑，是优化CUDA内核性能的核心知识之一。

## 1.7 Warp Divergence (>= Volta)

![Warp Divergence](https://picx.zhimg.com/v2-6323bdda7fb3e4246a98b6cfbe10b2d3_1440w.jpg)
![Warp Divergence](https://pic1.zhimg.com/v2-1b13829149bbf131c40bd10f09b625b0_1440w.jpg)

### **内容概括**
两张图共同揭示了 **Volta架构（及后续）GPU中线程束发散的执行逻辑变化**：
1. **架构革新**：Volta+ 引入 **独立线程程序计数器（Per-Thread PC）**，打破传统SIMT模型的严格锁步限制。
2. **执行机制**：线程束内不同分支的线程可**交错执行**（非串行化），但需开发者手动控制同步。
3. **编程适配**：新增 `__syncwarp()` 指令实现显式线程束同步，替代旧架构的自动汇合机制。

---

### **核心要点总结**

#### **1. 执行机制变革（Volta+）**
- **独立程序计数器（PC）**：  
  - 每个线程拥有独立PC，可执行不同指令流（突破Warp内指令强制同步限制）。  
  - 硬件按线程PC调度指令，**分支线程可并行交错执行**（如图中绿色块交错执行 `A/B` 与 `X/Y`）。  
- **无自动重聚合（No Auto-Reconvergence）**：  
  - 分支结束后**不会自动同步线程**（旧架构在分支结束点自动汇合）。  
  - 需开发者用 `__syncwarp()` **显式同步**线程束（如图2右侧黄色同步块）。  

#### **2. 性能影响与优势**
- **潜在优势**：  
  - **延迟隐藏优化**：不同分支线程可同时执行内存访问（如DRAM加载），利用带宽掩盖延迟（图1底部说明）。  
  - **减少闲置**：非活跃线程无需等待（旧架构串行执行时半数线程闲置）。  
- **风险与代价**：  
  - **同步责任转移**：开发者需手动插入 `__syncwarp()` 确保线程一致性，否则导致数据竞争或未定义行为。  
  - **复杂调试**：线程执行流更自由，但调试难度增加（需跟踪独立PC）。  

#### **3. 编程关键适配**
- **同步指令**：  
  - `__syncwarp()`：**显式同步线程束内所有线程**（图2），是分支后恢复一致性的必备操作。  
  - `__syncthreads()`：**同步整个线程块（Block）**，粒度大于 `__syncwarp()`（图2底部说明）。  
- **线程通信限制**：  
  - 线程间通信（如Shuffle指令）**隐式同步参与线程**，但仍需 `__syncwarp()` 确保全Warp一致性（图2注释）。  
- **分支内禁忌**：  
  - ❗ **禁止在分支内无同步的线程通信**（与旧架构一致），因独立PC可能导致部分线程未执行到通信点。  

#### **4. 执行流程对比（图1 vs 图2）**
- **图1（基础流程）**：  
  - 分支线程交错执行 → 结束点无同步 → 后续指令（`Z`）可能因线程未汇合导致错误。  
- **图2（正确实践）**：  
  - 分支线程交错执行 → 结束点插入 `__syncwarp()` → 所有线程同步后执行后续指令（`Z`）。  

---

### **关键结论**
- **范式转变**：Volta+ 架构将 **Warp一致性责任从硬件转移至软件**，开发者需主动管理同步。  
- **优化新思路**：  
  - 利用独立PC提升内存访问并行度（如同时加载不同分支所需数据）。  
  - 减少无效等待（旧架构的串行化闲置问题）。  
- **必备实践**：  
  - 在**分支结束点**、**线程通信前**、**共享内存访问后**插入 `__syncwarp()`。  
  - 区分 `__syncwarp()`（Warp级）和 `__syncthreads()`（Block级）的应用场景。  

> **注**：此机制是NVIDIA GPU向更灵活并行模型演进的关键一步，但要求开发者更深入理解硬件行为以实现安全高效的代码。

## 1.8 Getting good occupancy – balance resources

### **内容概括**
该幻灯片聚焦 **GPU资源平衡策略以实现高占用率（High Occupancy）**，核心涵盖：
1. **硬件资源特性**：SM数量、线程调度上限、指令执行限制。
2. **关键优化原则**：线程块配置、分支控制、数据类型选择。
3. **资源限制因素**：共享内存与寄存器文件的容量约束。
4. **工具与方法**：占用率计算工具、设备属性查询接口。

---

### **核心要点总结**

#### **1. 硬件资源与配置优化**
| **资源项**               | **优化策略**                                                                 |
|--------------------------|-----------------------------------------------------------------------------|
| **SM数量（82个）**       | 多线程块并行 → 充分利用多SM优势（对比：Jetson Xavier仅8个SM）。             |
| **每SM线程上限（1536）** | 线程块大小设为 **2的幂**（如256/512），避免超过设备限制（部分GPU支持2048）。 |
| **指令执行效率**         | 避免分支发散（Divergence）→ 确保**每周期执行完整32线程的Warp**。             |
| **数据类型选择**         | **避免FP64/INT64运算**（消费级/工作站GPU如Gx102性能差）。                  |

#### **2. 关键资源限制与应对**
- **共享内存（Shared Memory）**：  
  - 容量有限 → 影响SM可调度的线程块数量。  
  - 优化：合理分配共享内存（如动态调整`shmem`大小）。  
- **寄存器文件（Register File）**：  
  - 寄存器数量限制线程块并发 → 溢出（Spill）到全局内存会严重降速。  
  - 优化：  
    - 使用 `__launch_bounds__`（CUDA）或 `C10_LAUNCH_BOUNDS`（PyTorch）**提示编译器优化寄存器分配**。  
    - 平衡线程数 vs 寄存器使用量（如减少线程数可分配更多寄存器/线程）。  

#### **3. 工具与调试支持**
- **占用率计算**：  
  - 旧方法：手动计算（Excel表格） → **现集成至Nsight Compute**（可视化分析工具）。  
- **设备属性查询**：  
  - PyTorch API：`torch.cuda.get_device_properties(<gpu_num>)`  
    - 关键属性：`max_threads_per_multi_processor`（每SM最大线程数）。  
  - 完整属性参考：  
    https://developer.download.nvidia.com/compute/DevZone/docs/html/C/doc/html/group__CUDART__DEVICE_g5aa4f47938af8276f08074d09b7d520c.html  

---

### **关键优化结论**
- **占用率核心公式**：  
  `实际占用率 = min(线程块数限制, 寄存器限制, 共享内存限制)`  
- **黄金法则**：  
  1. **线程块大小**：256或512（2的幂，适配Warp调度）。  
  2. **分支控制**：确保Warp内线程执行路径一致。  
  3. **数据类型**：优先FP32/INT32，规避FP64/INT64（除非专业卡）。  
  4. **资源提示**：通过 `__launch_bounds__` 指导编译器优化寄存器分配。  
- **工具链**：  
  - **Nsight Compute**：实时分析占用率与性能瓶颈。  
  - **PyTorch/CUDA API**：动态获取设备参数指导配置。  

> **注**：高占用率（接近100%）未必直接等同高性能，需结合内存访问效率、指令吞吐量综合优化（如避免盲目增大线程块导致寄存器溢出）。
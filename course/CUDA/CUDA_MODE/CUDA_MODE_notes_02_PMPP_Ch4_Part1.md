本文主要整理CUDA MODE lecture_004 Compute and Memory basics (based on ch 4 + 5 of the PMPP book) 的要点。

## 1.0 CPU、GPU架构对比

- **Actually 1SM ~ 4 “CPU-Cores”**, 
- **originally (<=Pascal) GPU threads in a warp shared a Program Counter, now each has one**

**核心设计目标差异：**

*   **CPU：** 设计目标是**低延迟**和**通用性**。它擅长快速处理**单个复杂任务**或**少量并行任务**，需要强大的单线程性能来处理各种不可预测的计算和控制流（如操作系统、应用程序逻辑、数据库查询等）。它追求的是**尽快完成一个任务**。
*   **GPU：** 设计目标是**高吞吐量**。它擅长同时处理**大量相对简单、高度并行**的任务（如图形渲染中的像素着色、科学计算中的矩阵运算、AI中的张量运算）。它追求的是**在单位时间内完成尽可能多的任务**。

**架构差异详解：**

| 特性             | CPU                                   | GPU                                   | 差异原因                                                                                                 |
| :--------------- | :------------------------------------ | :------------------------------------ | :------------------------------------------------------------------------------------------------------- |
| **核心数量**     | **少** (几核到几十核)                 | **极多** (几百到上万核心)             | GPU 通过大量简单核心并行工作来实现高吞吐量。CPU 的核心更复杂、功能更强。                                 |
| **核心复杂度**   | **高** (复杂指令集，强大分支预测)     | **低** (相对简单指令集，弱分支预测)   | CPU 核心需要处理复杂的控制流和指令依赖，需要复杂的逻辑单元和缓存。GPU 核心专注于执行大量相同的简单计算。 |
| **缓存层次**     | **大而复杂** (多级缓存：L1, L2, L3) | **小而简单** (通常只有 L1/L2，较小)   | CPU 需要大缓存来减少访问主存的延迟，这对单线程性能至关重要。GPU 依赖高带宽而非低延迟，且线程切换开销小。 |
| **控制单元占比** | **高** (相对核心面积)                 | **极低** (相对核心面积)               | CPU 需要复杂的控制逻辑来处理指令调度、分支预测、乱序执行等。GPU 的控制逻辑被简化并集中管理。             |
| **ALU 占比**     | **相对较低**                          | **极高** (核心大部分面积是 ALU)       | GPU 的核心设计极度偏向于执行实际计算操作 (ALU)，因为它的任务就是大量并行计算。                          |
| **内存子系统**   | **低延迟，带宽适中**                  | **高带宽，延迟较高**                  | CPU 优化内存访问延迟以加速单线程。GPU 优化带宽以同时喂饱大量核心。                                      |
| **线程管理**     | **硬件线程少** (通常每核1-2个线程)    | **硬件线程极多** (每核心可处理多线程) | GPU 通过大量线程隐藏内存访问延迟。当一个线程等待数据时，硬件可以立即切换到另一个就绪线程。              |
| **SIMD/SIMT**    | **支持 SIMD** (如 SSE, AVX)           | **基于 SIMT** (单指令多线程)          | CPU 的 SIMD 单元较窄（如 4/8/16 通道）。GPU 核心本质上是超宽的 SIMD 处理器，一个指令可以驱动多个线程（如 32 线程的 Warp/Wavefront）执行相同的指令。 |
| **适用任务**     | **通用计算**，复杂逻辑，低延迟任务    | **高度并行计算**，数据密集型任务      | CPU 处理任务的控制流和多样性。GPU 处理任务中可大规模并行的计算部分。                                    |
| **编程模型**     | **相对灵活** (C/C++ 等通用语言)       | **需并行思维** (CUDA, OpenCL, HIP)    | GPU 编程需要显式地表达并行性，将问题分解为大量线程。                                                    |

**关键架构差异总结：**

1.  **核心哲学：**
    *   **CPU：** **处理复杂任务的高手**。像一个拥有博士学位、能解决各种疑难杂症、但一次只能专注解决一个或几个问题的专家。
    *   **GPU：** **处理简单任务的专家**。像一个由成千上万名高中生组成的军队，每个人只做非常简单的任务（比如给一个像素上色），但可以同时做海量的这种任务，效率极高。

2.  **资源分配：**
    *   **CPU：** 将大量晶体管资源用于**控制逻辑**（分支预测、乱序执行、复杂缓存）和**优化单线程性能**。
    *   **GPU：** 将绝大部分晶体管资源用于**计算单元**，控制逻辑被大幅简化并集中管理，通过**海量核心**和**高带宽内存**来实现并行吞吐量。

3.  **并行粒度：**
    *   **CPU：** 擅长**任务级并行**和**数据级并行**，但并行规模相对较小（受限于核心数）。
    *   **GPU：** 擅长**大规模数据级并行**，尤其是**细粒度并行**（处理大量独立的数据元素）。

**类比：**

*   **CPU 像跑车：** 速度快（低延迟），能去任何地方（通用性强），但一次只能载很少的人（并行度低）。
*   **GPU 像大型巴士/火车：** 速度相对慢（高延迟），路线相对固定（适合特定任务），但一次能运送非常非常多的人（高吞吐量）。

**结论：**

CPU 和 GPU 的架构差异源于它们截然不同的设计目标：CPU 追求**低延迟**和**通用性**，GPU 追求**高吞吐量**和**大规模并行计算能力**。它们不是相互替代的关系，而是**互补**的关系。现代计算系统（包括游戏主机、PC、数据中心、超算）通常同时使用 CPU 和 GPU，让 CPU 处理复杂的逻辑和控制流，而将大规模并行计算部分卸载给 GPU 执行，以达到最佳的整体性能。理解这些差异对于进行高性能计算、图形渲染、人工智能等领域的开发和优化至关重要。

## 1.1 Streaming Multiprocessor简介

![SM](https://picx.zhimg.com/v2-63f1977bd2c2b8059f10b8a50b0c559d_1440w.jpg)

**核心概念：SM 是 GPU 上执行计算任务的核心单元。**

1.  **`A thread block is assigned to one SM (max 1536 threads assignable to SM)`**
    *   **含义：** 当你在 GPU 上启动一个核函数（Kernel）时，你定义了一个由许多线程块（Thread Block）组成的网格（Grid）。GPU 的工作分配器会将网格中的**线程块**分配给**可用的 SM**。
    *   **关键点：**
        *   **分配单位是线程块（Block），不是单个线程。**
        *   一个线程块**只能被分配给一个 SM**。它不能拆分到多个 SM 上执行。
        *   一个 SM **可以同时容纳多个线程块**（具体数量取决于 SM 的资源限制）。
        *   `max 1536 threads assignable to SM`：这是指**一个 SM 最多可以同时容纳的线程总数**。例如，如果每个线程块有 256 个线程，那么一个 SM 最多可以同时运行 1536 / 256 = 6 个线程块。这个最大值由 SM 的硬件资源（主要是寄存器文件大小和共享内存大小）决定。

2.  **`NO control which block in a grid when where (ok, in Hopper+ we can have thread block groups)`**
    *   **含义：** 在传统的 CUDA 编程模型（Hopper 架构之前）中，程序员**无法精确控制**：
        *   **哪个**线程块 (`which block`)
        *   **在什么时候** (`when`)
        *   **被分配到哪个** SM (`where`)
    *   **原因：** 这是由 GPU 的硬件调度器（称为 GigaThread Engine 或类似）动态决定的，以实现负载均衡。程序员只能指定网格和线程块的维度。
    *   **例外 (Hopper+):** NVIDIA 的 Hopper 架构（及后续）引入了 **Thread Block Cluster** 的概念。这允许程序员将一组线程块（一个 Cluster）**绑定在一起**，确保它们被分配到同一个 **GPU Processing Cluster (GPC)** 内的 SM 上执行（注意：不是*同一个* SM，而是同一个 GPC *内*的 SM）。这提供了**一定程度的协同定位控制**，主要用于优化需要块间通信或共享 L2 缓存数据的新算法。

3.  **`4 warps or "part-warp" can compute at a cycle, those SHARE one instruction (but Volta+ does have per-thread program counter)`**
    *   **含义：** 描述了 SM 的指令发射和执行能力。
    *   **`4 warps ... can compute at a cycle`：** 指 SM 的 **Warp Scheduler** 数量。一个 Warp Scheduler 负责调度一个 Warp（32 个线程）。这句话表明这个 SM 有 **4 个 Warp Scheduler**。这意味着在每个时钟周期（cycle），**最多可以有 4 个不同的 Warp 被发射指令**（即开始执行一条指令）。
    *   **`or "part-warp"`：** 可能指代一些架构（如 Pascal）中 Warp Scheduler 可以调度半个 Warp（16 个线程）的能力。但在现代架构中，通常以调度整个 Warp 为主。
    *   **`those SHARE one instruction`：** 这是 **SIMT (Single Instruction, Multiple Thread)** 模型的核心。**同一个 Warp 内的所有 32 个线程，在同一个时钟周期内，执行的是完全相同的指令**。它们共享同一个程序计数器（PC），操作的是各自不同的数据。
    *   **`(but Volta+ does have per-thread program counter)`：** **重要澄清！** 从 Volta 架构开始，NVIDIA 引入了 **Independent Thread Scheduling**。这确实意味着硬件为**每个线程**维护了独立的程序计数器（PC）、调用栈和状态。**但是：**
        *   **执行单元层面：** 执行单元（CUDA Core）仍然是按 Warp 组织的。一个 Warp Scheduler 仍然一次发射一条指令给一个 Warp。
        *   **目的：** 引入独立状态主要是为了更**精确地处理线程分化（Warp Divergence）** 和实现**更细粒度的同步**（如 `syncwarp`）。当一个 Warp 发生分化时，硬件能更有效地跟踪和管理不同线程的执行路径，不再需要像以前那样严格地串行化所有分支路径。**在 Warp 内所有线程执行相同指令流的部分，它们仍然是共享指令的。** 独立 PC 主要用于处理分支和同步点。

4.  **`32 FP32 units (one per thread), 16 of which know INT32`**
    *   **含义：** 描述了 SM 内部**浮点 (FP32) 和整数 (INT32) 计算单元**的数量和关系。
    *   **`32 FP32 units`：** 表示该 SM 有 **32 个单精度浮点 (FP32) 计算单元**。
    *   **`(one per thread)`：** 这是一个**理想化的简化描述**。它意味着在硬件设计上，有足够的 FP32 单元，使得**一个包含 32 个线程的 Warp** 可以在一个时钟周期内（理论上）**同时执行一条 FP32 指令**（因为 32 个线程 * 1 个单元/线程 = 32 个单元）。但这并不意味着每个线程物理上独占一个单元。单元是共享资源，由 Warp Scheduler 调度使用。
    *   **`16 of which know INT32`：** 表示在这 32 个 FP32 单元中，**有 16 个单元还具备执行整数 (INT32) 指令的能力**。这通常意味着：
        *   有 16 个**独立的 INT32 单元**。
        *   或者，有 16 个**混合单元**，可以在不同时间执行 FP32 或 INT32（但通常设计为独立的）。
    *   **关键点：** 这种配置允许 SM 在一个周期内执行：
        *   32 条 FP32 指令（使用所有 32 个 FP32 单元）。
        *   或者 16 条 INT32 指令（使用那 16 个 INT32 单元）。
        *   或者混合执行（例如，16 条 FP32 + 16 条 INT32，如果指令类型和单元匹配）。
    *   **目的：** 提供灵活的混合精度计算能力。

5.  **`16k 32-bit registers shared between things scheduled on the same block)`**
    *   **含义：** 描述了 **寄存器文件 (Register File)** 的大小、组织方式和分配原则。
    *   **`16k 32-bit registers`：** 指该 SM 拥有的**寄存器文件总容量**是 **16,384 (16K) 个 32 位寄存器**。
    *   **`shared between things scheduled on the same block`：** **关键！** 寄存器文件资源是按**线程块 (Thread Block)** 来分配和管理的。
        *   当一个线程块被调度到一个 SM 上时，SM 会**从总的寄存器文件中划出一部分**专供这个线程块使用。
        *   划出的这部分寄存器**由该线程块内的所有线程共享使用**。
        *   编译器会根据核函数代码中每个线程声明的变量数量，计算出**每个线程需要多少寄存器**。
        *   一个线程块所需的寄存器总数 = `线程数 * 每个线程所需寄存器数`。
        *   SM 能同时容纳的线程块数量受限于：1) 总的寄存器文件大小 (16K)，2) 每个线程块所需的寄存器数量，3) 共享内存需求，4) 线程块最大线程数限制 (1536)。
    *   **重要性：** 寄存器是访问速度最快的内存。合理利用寄存器对性能至关重要。使用过多寄存器会限制 SM 上同时运行的线程块数量（减少并行度），使用过少可能导致寄存器溢出（spilling）到慢速的本地内存。

6.  **`L1 cache and shared memory share hardware (128KB) directly on the SM`**
    *   **含义：** 描述了 SM 上**快速片上存储资源**的物理实现。
    *   **`share hardware (128KB)`：** SM 上有一块**物理上统一**的、总大小为 **128KB** 的**高速 SRAM 存储区域**。
    *   **`L1 cache and shared memory`：** 这块统一的 128KB SRAM **在逻辑上被划分为两个部分**：
        *   **共享内存 (Shared Memory / Shmem)：** 这是一块**软件可管理**的高速内存。由程序员在核函数中显式声明和使用（`__shared__` 变量）。同一个线程块内的线程通过共享内存进行高效通信和协作。
        *   **L1 缓存 (L1 Cache)：** 这是一块**硬件自动管理**的高速缓存。它透明地缓存来自全局内存（Global Memory）的数据，以减少访问延迟。程序员通常不直接控制它（除了通过一些内存访问提示）。
    *   **`directly on the SM`：** 强调这块内存位于 SM 芯片上，访问速度极快（比全局显存快 1-2 个数量级）。

7.  **`shmem can be 0/8/16/32/64/100KB L1 Cache the remainder (>=28KB)`**
    *   **含义：** 详细说明了如何**在共享内存 (Shmem) 和 L1 缓存之间分配**那 128KB 的统一存储。
    *   **`shmem can be 0/8/16/32/64/100KB`：** 程序员（或编译器/驱动）可以**配置**分配给共享内存的大小。可选的配置有 **0KB, 8KB, 16KB, 32KB, 64KB, 100KB**。*(注意：100KB 是一个特殊选项，通常需要 128KB 总池，所以 L1 只剩 28KB。其他选项如 48KB 在某些架构上也可能存在)。*
    *   **`L1 Cache the remainder (>=28KB)`：** 分配给共享内存后，**剩下的 SRAM 容量**就自动用作 **L1 缓存**。`(>=28KB)` 表示无论怎么配置共享内存，L1 缓存**至少**能分到 28KB。这对应于共享内存配置为最大值（100KB）时的情况：128KB - 100KB = 28KB L1 Cache。
    *   **配置方法：**
        *   在核函数调用时，使用 `<<<...>>>` 语法中的第三个配置参数：`cudaFuncSetSharedMemConfig` 或直接在 Kernel Launch 配置中指定。
        *   在 Kernel 代码中使用 `__shared__` 定义的变量大小也会影响实际需求，但配置选项决定了最大可用量。
    *   **策略：** 选择多少共享内存取决于你的核函数算法：
        *   如果算法需要大量线程块内线程通信或暂存中间结果，就分配更多共享内存（如 32KB, 48KB, 64KB）。
        *   如果算法对共享内存需求不大，但访问全局内存模式不规则（难以合并），那么分配更多 L1 缓存（如只分配 16KB 或 32KB 共享内存）可能更有助于缓存全局数据，提升性能。

**总结理解这段描述：**

1.  **任务分配：** 线程块是分配给 SM 的基本单位。一个 SM 能同时跑多个块，但总线程数有限制（如 1536）。
2.  **调度与执行：** SM 有多个 Warp Scheduler（如 4 个），每个周期可调度多个 Warp。一个 Warp（32 线程）执行相同的指令（SIMT），尽管 Volta+ 有独立线程状态用于处理分支。
3.  **计算单元：** SM 有大量专用计算单元（如 32 FP32 + 16 INT32），设计目标是在一个周期内处理一个 Warp 的特定类型指令。
4.  **寄存器：** 巨大的寄存器文件（如 16K 寄存器）按线程块分配，块内线程共享其分配到的寄存器份额。
5.  **片上存储：** 关键的 128KB 高速 SRAM 是一个统一池，在软件可管理的共享内存和硬件管理的 L1 缓存之间按需分配（如 Shmem: 64KB, L1: 64KB）。共享内存用于块内线程协作，L1 缓存用于加速全局内存访问。

## 1.2 Streaming Multiprocessor硬件架构

**核心目标：** 最大化**计算吞吐量**和**内存带宽利用率**，通过**极致的多线程**来隐藏各种延迟（特别是内存访问延迟）。

**关键硬件组件：**

1.  **CUDA Cores / Stream Processors (SPs)：**
    *   **数量：** 这是 SM 的核心计算资源。一个现代 SM 通常包含 **64 个、128 个甚至更多** 的 CUDA Core（NVIDIA）或 Stream Processor（AMD）。
    *   **功能：** 这些是**极其精简的算术逻辑单元 (ALU)**。它们主要执行：
        *   单精度浮点运算 (FP32)
        *   整数运算 (INT32)
        *   双精度浮点运算 (FP64) - 通常在专业卡或特定 SM 中更强大
        *   特殊函数 (SFU) - 如正弦、余弦、倒数、平方根等（有时有独立单元）
    *   **设计哲学：** 牺牲单个核心的复杂性和功能（如乱序执行、复杂分支预测），换取在有限芯片面积内塞入**最大数量**的 ALU，实现极高的**峰值计算吞吐量 (FLOPS)**。
    *   **组织方式：** 通常组织成多个 **执行单元 (Execution Units)** 或 **处理块 (Processing Blocks)**。例如，NVIDIA 的 SM 通常将 CUDA Cores 分组为多个 **处理块 (如 4 个处理块，每个包含 16 个 FP32 Core + 16 个 INT32 Core)**。这些块可以独立或协同工作。

2.  **Tensor Cores (NVIDIA) / Matrix Cores (AMD)：**
    *   **功能：** **专用硬件加速单元**，用于执行**矩阵乘累加 (MMA)** 操作，这是深度学习和科学计算的核心运算。
    *   **性能：** 相比传统的 CUDA Core，Tensor Core 在特定精度（如 FP16, BF16, INT8, FP8）下能提供**数量级更高的吞吐量**。
    *   **集成：** 通常集成在 SM 内部，与 CUDA Core 共享部分资源（如寄存器、内存接口）。

3.  **RT Cores (NVIDIA)：**
    *   **功能：** **专用硬件加速单元**，用于加速**光线追踪**计算中的关键步骤：**包围盒求交 (Bounding Volume Hierarchy Traversal, BVH)** 和 **三角形求交 (Ray-Triangle Intersection)**。
    *   **目的：** 显著加速实时光线追踪性能，使其在游戏中变得可行。

4.  **Warp Schedulers：**
    *   **数量：** 一个 SM 通常有 **2 个或 4 个** Warp Scheduler。
    *   **功能：** 这是 SM 的**大脑**。它负责：
        *   从分配给该 SM 的 **Thread Blocks (线程块)** 中选择 **Warps (线程束，通常 32 个线程)** 准备执行。
        *   检查 Warp 的指令是否就绪（操作数在寄存器中？）。
        *   每个调度周期（例如，每个时钟周期），**每个 Warp Scheduler 可以发射 1 条指令** 给其负责的一组执行单元（CUDA Cores/Tensor Cores）。
        *   **核心策略：** **零开销线程切换**。当一个 Warp 因为等待内存访问或长延迟操作（如 SFU）而停滞时，Warp Scheduler **立即** 切换到另一个就绪的 Warp 执行，最大化硬件利用率，**隐藏延迟**。

5.  **Dispatch Units：**
    *   **功能：** 位于 Warp Scheduler 和执行单元之间。负责将 Warp Scheduler 选定的指令**分发**到具体的执行流水线（如 FP32 单元、INT32 单元、Tensor Core 单元、SFU 单元、LD/ST 单元）。

6.  **Register File (寄存器文件)：**
    *   **容量：** **巨大**！这是 SM 上仅次于 Cache 的宝贵资源。现代 SM 的寄存器文件容量可达 **数万到数十万** 个 32 位寄存器。
    *   **组织：** 被**划分**给当前在 SM 上执行的所有 **Thread Blocks**。每个线程块内的每个线程都会被分配一定数量的寄存器（由编译器决定）。
    *   **重要性：** 寄存器访问速度最快。将频繁使用的数据保存在寄存器中是 GPU 高性能编程的关键。

7.  **Shared Memory / L1 Cache：**
    *   **功能：** **片上、软件可管理的高速内存**。
    *   **容量：** 相对较小（通常 **64KB, 96KB, 128KB, 192KB** 等），但速度极快（比全局显存快 1-2 个数量级）。
    *   **可配置性：** 在 NVIDIA GPU 上，这部分内存通常可以在 **L1 Cache** 和 **Shared Memory** 之间**按比例划分**（例如 64KB L1 + 0KB SM，或 48KB L1 + 16KB SM，或 0KB L1 + 64KB SM）。AMD GPU 有类似的 Local Data Share (LDS)。
    *   **作用：**
        *   **Shared Memory：** 供**同一个 Thread Block 内的线程**进行**高效通信和协作**。用于暂存数据、实现核函数内的局部累加、减少对全局内存的访问。
        *   **L1 Cache：** 缓存从全局内存或纹理内存中读取的数据，加速后续访问。

8.  **Load/Store Units (LD/ST)：**
    *   **功能：** 负责处理内存访问请求（读取 LD / 写入 ST）。
    *   **作用：** 执行从 **寄存器/Shared Memory <-> 全局内存/常量内存/纹理内存** 的数据传输。
    *   **地址计算：** 通常包含地址生成单元 (AGU) 来计算内存地址。

9.  **Texture Units / Texture Cache：**
    *   **功能：** 专为图形纹理采样和科学计算中的插值访问优化。
    *   **特点：** 支持各种寻址模式（Wrap, Clamp, Mirror 等）、滤波模式（Nearest, Linear）和多级渐远纹理 (Mipmapping)。
    *   **缓存：** 通常有专用的只读纹理缓存。

10. **Constant Cache：**
    *   **功能：** 缓存只读的常量数据。所有线程访问相同的常量数据时效率极高。

11. **Instruction Cache / Instruction Buffer：**
    *   **功能：** 缓存即将执行的指令，减少从更慢的指令缓存或全局内存取指令的次数。

12. **Interconnect Network：**
    *   **功能：** 连接 SM 内部所有组件（寄存器、执行单元、共享内存、LD/ST 单元、调度器等）的高速互连网络。确保数据能在不同组件间高效流动。

**SM 内部数据流与执行流程（简化）：**

1.  **任务分配：** GPU 驱动程序将计算任务（Kernel）分解成大量线程，组织成 **Grid** 和 **Thread Blocks**。工作分配单元将 **Thread Blocks** 分配给空闲的 **SM**。
2.  **资源分配：** SM 为接收到的 Thread Block 分配资源：
    *   在 **寄存器文件** 中为 Block 内的每个线程分配寄存器槽位。
    *   在 **Shared Memory** 中为该 Block 分配一块空间。
    *   在 **Warp 调度槽** 中注册该 Block 包含的 Warps。
3.  **Warp 调度：** Warp Schedulers 不断轮询所有注册的 Warps，检查其下一条指令是否就绪（操作数在寄存器中）。
4.  **指令发射：** 就绪的 Warp 被选中。Warp Scheduler 通过 Dispatch Unit 将该 Warp 的指令发送到相应的执行单元（FP32, INT32, Tensor, LD/ST, SFU）。
5.  **指令执行：** 执行单元接收指令和该 Warp 内所有线程的操作数（来自寄存器文件），并行执行相同的操作（SIMT）。结果写回寄存器文件。
6.  **内存访问：** 如果指令是 Load/Store，LD/ST 单元处理请求：
    *   访问 Shared Memory：极快完成。
    *   访问全局内存：请求发送到 SM 外的 **L2 Cache** 和 **Memory Controllers**。这是一个高延迟操作。
7.  **隐藏延迟：** 当 Warp 因内存访问或其他长延迟操作而停滞时，Warp Scheduler **立即切换**到另一个就绪的 Warp 执行，保持执行单元忙碌。
8.  **完成与释放：** 当 Thread Block 的所有指令执行完毕，其占用的资源（寄存器、Shared Memory、Warp 槽位）被释放，SM 可以接收新的 Thread Block。

**总结：**

Streaming Multiprocessor 是一个高度并行化、深度流水线化、资源密集型的计算引擎。它的核心设计围绕以下几点：

*   **海量简单核心 (CUDA Core/SP)：** 提供峰值计算能力。
*   **专用加速单元 (Tensor Core, RT Core)：** 针对特定负载优化。
*   **巨大寄存器文件：** 为海量线程提供快速存储。
*   **软件可管理共享内存：** 实现线程块内高效协作。
*   **智能 Warp 调度器：** 通过零开销切换和极致的多线程隐藏延迟。
*   **高带宽内存接口：** 喂饱计算单元。

理解 SM 的内部架构对于进行高性能 GPU 编程（如 CUDA, OpenCL, HIP）至关重要，因为它直接影响如何设计算法、组织线程、利用内存层次结构以及避免性能瓶颈（如分支分化、bank conflict）。

## 1.3  ​Threads、Warps和Blocks

### **内容概括**
1. **内核启动机制**：定义线程块（Block）的布局（线程数/块）和网格（Grid）布局（块数）。
2. **线程块执行逻辑**：线程块在流多处理器（SM）上并行执行，块内线程可共享内存，不同块相互独立。
3. **线程束调度**：线程块在SM上被划分为32线程的线程束（Warps），由SM的硬件单元调度执行。
4. **跨平台对比**：对比AMD的Wavefronts（默认64线程）与NVIDIA的Warps差异。

---

### **核心要点总结**
1. **内核启动配置**  
   - **Block Layout**：指定每个线程块包含的线程数量（如128、256线程/块）。  
   - **Grid Layout**：指定启动的线程块总数（如1024个块）。  
   - *作用*：共同定义GPU并行任务的规模（总线程数 = 块数 × 每块线程数）。

2. **线程块（Block）特性**  
   - **执行单元**：一个线程块在**同一个SM**上执行，块内线程**可访问共享内存（Shared Memory）**。  
   - **独立性**：不同线程块间**无通信能力**（除非使用Hopper+架构的新特性）。  
   - **动态分配**：CUDA运行时**自动分配块到空闲SM**，执行顺序**不可预测**（开发者需避免顺序依赖）。

3. **线程束（Warp）核心机制**  
   - **组成**：每个线程块被划分为**32线程的线程束**（NVIDIA默认）。  
   - **执行单元**：一个线程束在SM的**固定处理单元**上运行（如CUDA Core组）。  
   - **调度策略**：  
     - SM同时管理多个活跃线程束（如16-32个）。  
     - 当线程束因内存访问等操作停滞时，SM**立即切换至其他就绪线程束**（零开销切换）。  
     - **寄存器保留**：切换时线程的寄存器状态保留，确保上下文快速恢复。  

4. **AMD对比（Wavefronts）**  
   - **术语差异**：AMD GPU中对应概念为 **Wavefronts**。  
   - **规模差异**：默认包含**64线程**（NVIDIA Warp为32线程）。  

5. **硬件执行示意图（图4.2）**  
   - 展示多个线程块被动态分配到不同SM的过程。  
   - 每个SM内部划分为线程束，由硬件调度器管理执行。  

---

### **关键结论**
- **并行粒度**：线程块是资源分配单位（共享内存/寄存器），线程束是实际调度单位。  
- **性能关键**：  
  - 避免线程分化（Warp Divergence）：确保线程束内32线程执行相同指令。  
  - 利用共享内存减少全局访问延迟。  
- **跨平台注意**：NVIDIA（32线程/Warp）与AMD（64线程/Wavefront）的差异影响代码优化策略。  
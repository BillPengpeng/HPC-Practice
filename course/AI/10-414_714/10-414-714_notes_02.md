## 7 - Neural Network Library Abstractions

### Programming abstractions

The programming abstraction of a framework defines the common ways to **implement, extend and execute** model computations.

#### Forward and backward layer interface

Example framework: Caffe 1.0. Defines the forward computation and backward(gradient) operations.

#### Computational graph and declarative programming

Example framework: Tensorflow 1.0. First declare the computational graph. Then execute the graph by feeding input value. ——符号式编程

#### Imperative automatic differentiation

Example framework: PyTorch (needle:). Executes computation as we construct the computational graph. Allow easy mixing of python control flow and construction. ——命令式编程

### High level modular library components

Deep learning is modular in nature. nn.Module: compose things together. 

Loss functions as a special kind of module.

Optimizer takes a list of weights from the model perform steps of optimization. Keep tracks of auxiliary states (momentum). Two ways to incorporate regularization: Implement as part of loss function; Directly incorporate as part of optimizer update, **SGD with weight decay (L2 regularization)**.

Initialization.

Data loader and preprocessing.

## 9 - Normalization and Regularization

### Normalization

#### Layer normalization

$$
\text{LayerNorm}(x) = \gamma \odot \left( \frac{x - \mu}{\sigma} \right) + \beta
$$

其中：

- $x$：是输入向量，通常是某一层神经元的激活输出。
- $\mu$：是输入$x$在特征维度（或神经元维度）上的均值，计算公式为 $\mu = \frac{1}{d} \sum_{i=1}^{d} x_i$，其中$d$是特征维度（或神经元数量）。
- $\sigma$：是输入$x$在特征维度上的标准差,计算公式为 $\sigma = \sqrt{\frac{1}{d} \sum_{i=1}^{d} (x_i - \mu)^2 + \epsilon}$。
- $\gamma$ 和 $\beta$：是可学习的仿射变换参数，分别用于缩放和平移归一化后的结果。
- $\odot$：表示元素级乘法。

#### Batch Normalization

$$
\text{BN}(x_i) = \gamma \odot \left( \frac{x_i - \hat{\mu}_B}{\hat{\sigma}_B} \right) + \beta
$$

其中：

- $x_i$：表示mini-batch中的第$i$个样本在某一层的激活值。
- $\hat{\mu}_B$：是mini-batch $B$中所有样本在某一层的激活值的均值，计算公式为 $\hat{\mu}_B = \frac{1}{m} \sum_{i=1}^{m} x_i$，其中$m$是mini-batch的大小。
- $\hat{\sigma}_B$：是mini-batch $B$中所有样本在某一层的激活值的标准差，通常还会加上一个很小的正数$\epsilon$以防止除零错误，计算公式为 $\hat{\sigma}_B = \sqrt{\frac{1}{m} \sum_{i=1}^{m} (x_i - \hat{\mu}_B)^2 + \epsilon}$。
- $\gamma$ 和 $\beta$：是可学习的仿射变换参数（scale和shift参数），用于对归一化后的结果进行缩放和平移，以恢复数据的表达能力。
- $\odot$：表示元素级乘法。

### Regularization

L2 Regularization a.k.a. weight decay.

Dropout.

### Interaction of optimization, initialization, normalization, regularization

Many design choices meant to ease optimization ability of deep networks  
• Choice of optimizer learning rate / momentum  
• Choice of weight initialization  
• Normalization layer  
• Reguarlization  

## 10 - Convolutional Networks

### Convolutional operators in deep networks

#### How convolutions “simplify” deep networks 
Convolutions combine two ideas that are well-suited to processing images  
1. Require that activations between layers occur only in a “local” manner, and treat hidden layers themselves as spatial images. ——局部感知  
2. Share weights across all spatial locations. ——权重共享  

### Elements of practical convolutions

假设：
- 输入特征图的形状为 $(N, C_{\text{in}}, H_{\text{in}}, W_{\text{in}})$
- 卷积核的形状为 $(C_{\text{out}}, C_{\text{in}}, K_H, K_W)$
- 步幅（stride）为 $(S_H, S_W)$
- 填充（padding）为 $(P_H, P_W)$
- 空洞率（dilations）为 $(D_H, D_W)$
- 输出特征图的形状为 $(N, C_{\text{out}}, H_{\text{out}}, W_{\text{out}})$，则：

$$
H_{\text{out}} = \left\lfloor \frac{H_{\text{in}} + 2P_H - (K_H - 1) \times D_H - 1}{S_H} \right\rfloor + 1
$$

$$
W_{\text{out}} = \left\lfloor \frac{W_{\text{in}} + 2P_W - (K_W - 1) \times D_W - 1}{S_W} \right\rfloor + 1
$$

###  Differentiating convolutions

#### im2col

输入图像每个局部区域被展开成$C_{\text{in}} \times K_H \times K_W$的列，总共有$H_{\text{out}} \times W_{\text{out}} \times N$这样的列，所以im2col后的矩阵形状是($C_{\text{in}} \times K \times K, N \times H_{\text{out}} \times W_{\text{out}}$)。而卷积核会被展开成($C_{\text{out}}，C_{\text{in}} \times K \times K$)矩阵，这样两者的矩阵相乘结果就是输出矩阵，形状为$C_{\text{out}} \times N \times H_{\text{out}} \times W_{\text{out}}$，再变形回$N \times C_{\text{out}} \times H_{\text{out}} \times W_{\text{out}}$。

## 11 - Hardware Acceleration

###  General acceleration techniques

#### Vectorization

```
Adding two arrays of length 256
 void vecadd(float* A, float *B, float* C) {
    for (int i = 0; i < 64; ++i) {
        float4 a = load_float4(A + i*4);
        float4 b = load_float4(B + i*4);
        float4 c = add_float4(a, b);  
        store_float4(C + i* 4, c);
    }
 }
```

#### Parallelization

```
void vecadd(float* A, float *B, float* C) {
    #pragma omp parallel for
    for (int i = 0; i < 64; ++i) {
        float4 a = load_float4(A + i*4);
        float4 b = load_float4(B + i*4);
        float4 c = add_float4(a, b);
        store_float4(C * 4, c);
    }
 }
```
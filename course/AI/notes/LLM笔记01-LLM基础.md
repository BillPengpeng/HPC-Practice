语言模型旨在预测一个词元或词元序列出现的概率，通常基于规则、统计或学习来构建。语言模型的概率预测与上下文和语料库息息相关。

## LLM基础

基于统计语言模型，基于统计的语言模型通过直接统计语言符号在语料库中出现的频率来预测语言符号的概率。其中，n-grams 是最具代表性的统计语言模型。n-grams 语言模型基于马尔可夫假设和离散变量的极大似然估计给出语言符号的概率，**应用n-grams可以最大概率生成当前语料库**。n-grams对未知序列有一定的**泛化性**，但也容易陷入“***零概率***”的困境。

基于学习的语言模型，学习的要素包括：训练数据（语料库）、假设类（神经网络）、归纳偏置（上下文间存在关联）、学习算法、学习范式（自监督）、学习目标（减少泛化误差），语言模型有“一个字接一个字”的串行输入，如：RNN、LSTM、GRU，“自回归”过程存在着两个问题：(1) 错误级联放大，选用模型自己生成的词作为输入可能会有错误，这样的错误循环输入，将会不断的放大错误，导致模型不能很好拟合训练集；（2）串行计算效率低，因为下一个要预测的词依赖上一次的预测，每次预测之间是串行的，难以进行并行加速。改进有**Teacher Forcing**、**Scheduled Sampling**。“一股脑”的并行输入，如：Transformer，由注意力（Attention）模块、全连接前馈（Fully-connected Feedforwad）模块构成，自注意力模块包括：自注意力层（Self-Attention Layer）、残差连接（Residual Connections）和层正则化（Layer Normalization），全连接前馈包括：全连接前馈层、残差连接、层正则化，**注意力层采用加权平均的思想将前文信息叠加到当前状态上**，LN适合RNN、LSTM、Transformer网络，支持文本长度不定以及batch size较小或batch size为1的情况（参考博文[Batch Normalization vs Layer Normalization](https://zhuanlan.zhihu.com/p/452827651)），将层正则化置于残差连接之后的网络结构被称为 Post-LN Transformer,应对表征坍塌（Representation Collapse）的能力更强，但处理梯度消失略弱,将层正则化置于残差连接之前的网络结构，称之为**Pre-LN,应对梯度消失能力强**，但处理表征坍塌的能力略弱。

语言模型的输出为一个向量，该向量的每一维代表着词典中对应词的概率。在采用自回归范式的文本生成任务中，语言模型将依次生成一组向量并将其解码为文本。需要根据概率值从词表中选出本轮输出的词元，**选择词元的过程被称为采样**。两类主流的解码方法可以总结为 (1).概率最大化方法; (2).随机采样方法。概率最大化方法，包括：贪心搜索（Greedy Search）、波束搜索（Beam Search），在开放式文本生成中，容易生成一些“废话文学”—重复且平庸的文本，缺乏多样性。随机采样方法，包括：Top-K采样、Top-P采样，在采样方法中加入 Temperature 机制可以对候选词的概率分布进行调整。

评测语言模型生成能力的方法可以分为两类。第一类方法不依赖具体任务，直接通过语言模型的输出来评测模型的生成能力，称之为内在评测（Intrinsic Evaluation）。第二类方法通过某些具体任务，如机器翻译、摘要生成等，来评测语言模型处理这些具体生成任务的能力，称之为外在评测（Extrinsic Evaluation）。最为常用的内部评测指标是困惑度（Perplexity），度量了语言模型对测试文本感到“困惑”的程度，如果语言模型对测试文本越“肯定”（即生成测试文本的概率越高），则困惑度的值越小。由于测试文本和预训练文本同分布，预训练文本代表了我们想要让语言模型学会生成的文本，如果语言模型在这些测试文本上越不“困惑”，则说明语言模型越符合我们对其训练的初衷。外在评测方法通常可以分为基于统计指标的评测方法和基于语言模型的评测方法两类，基于统计指标是评测语言模型的输出与标准答案间的契合程度，BLEU（BiLingual Evaluation Understudy）和 ROUGE（Recall-Oriented Understudy for Gisting Evaluation）是应用最为广泛，**BLEU 是精度导向的指标**，被提出用于评价模型在机器翻译（Machine Translation, MT）任务上的效果，**ROUGE是召回导向的指标**，被提出用于评价模型在摘要生成（Summarization）任务上的效果，这样的评分**无法完全适应生成的样本具有较强的创造性和多样性的时候**。基于语言模型的评测方法主要分为两类：（1）基于上下文词嵌入（Contextual Embeddings）的评测方法，如：BERTScore，BERTScore 依赖于人类给出的参考文本，这使其无法应用于缺乏人类标注样本的场景中；（2）基于生成模型的评测方法，如：**G-EVAL**，通过提示工程（Prompt Engineering）引导 GPT-4 输出评测分数，G-EVAL 的 Prompt 分为三部分：(1) 任务描述与评分标准，任务描述指明需要的评测的任务式什么（如摘要生成），评分标准给出评分需要的范围，评分需要考虑的因素等内容；(2) 评测步骤，在第一部分内容的基础上由 GPT-4 自己生成的思维链（Chain-of-Thoughts, CoT）；(3) 输入文本与生成的文本，例如摘要生成任务中的输入文本是原文，而生成的文本就是生成摘要。将上述三部分组合在一个prompt 里面然后输入给 GPT-4，GPT-4 便可给出对应的评分。直接将 GPT-4 给出的得分作为评分会出现区分度不够的问题，因此，G-EVAL 还引入了对所有可能得分进行加权平均的机制来进行改进。

## LLM架构

GPT系列模型的不断迭代更新，GPT-2（1.5b，简单文本生成、语言翻译、文本填充），GPT-3（175b，大规模文本生成、语言翻译、文本生成），GPT-4（万亿，更强的文本生成、更强的推理能力、多模态能力、编程能力），能解决的问题也越来越丰富，参数量参考博文[OpenAI的GPT1,GPT2,GPT3,GPT4系列的模型概览](https://zhuanlan.zhihu.com/p/650057910)。这些新能力并非通过在特定下游任务上通过训练获得，而是随着模型复杂度的提升凭空自然涌现，这些能力因此被称为**的涌现能力（Emergent Abilities）**。最为突出的有：上下文学习能力、常识推理、逻辑推理。LLM能力扩展与增强，主要得益于模型规模和数据规模的增长，有一系列关于模型能力与参数/数据规模之间的定量关系作为理论支撑，即扩展法则（Scaling Law），以OpenAI提出的**Kaplan-McCandlish扩展法则**以及 DeepMind 提出的**Chinchilla扩展法则**最为著名，Kaplan-McCandlish扩展法则指出，为了达到最优模型性能，数据集规模、模型规模应同步增加，但模型规模的增长速度应该略快于数据规模的增长速度，如果总计算预算增加了10倍，模型规模应扩大约5.37倍，而数据规模应扩大约1.86倍，以实现模型的最佳性能；Chinchilla扩展法则指出，数据集量、模型规模几乎同等重要，如果总计算预算增加了10倍，那么模型规模以及数据规模都应当扩大约3.16倍，理想的数据集大小应当是模型规模的20倍。绝大多数LLM均以Transformer框架为核心，并进一步演化出了三种经典架构，分别是Encoder-only架构，Decoder-only架构以及Encoder-Decoder架构。

Encoder-only 架构仅选取了 Transformer 中的编码器（Encoder）部分，用于接收输入文本并生成与上下文相关的特征。具体来说，Encoder-only 架构包含三个部分，分别是输入编码部分，特征编码部分以及任务处理部分。其中输入编码部分包含分词、向量化以及添加位置编码三个过程。而特征编码部分则是由多个相同的编码模块（Encoder Block）堆叠而成，其中每个编码模块包含自注意力模块（Self-Attention）和全连接前馈模块，**全注意力机制**。**适用于情感识别为代表的判别任务**，不适用于生成任务。Encoder-only 架构的代表性方法是BERT及其变体，如RoBERTa、ALBERT。BERT 模型的结构与 Transformer 中的编码器几乎一致，都是由多个编码模块堆叠而成，每个编码模块包含一个多头自注意力模块和一个全连接前馈模块。根据参数量的不同，BERT模型共有BERT-Base（1.1亿）和BERT-Large（3.4亿）两个版本；BERT使用小说数据集BookCorpus（包含约 8 亿个 Token）和英语维基百科数据集 6（包含约 25 亿个 Token）进行预训练，总计约33亿个Token，总数据量达到了15GB左右；在预训练任务上，BERT开创性地提出了掩码语言建模（MaskedLanguage Model, MLM）和下文预测（Next Sentence Prediction, NSP）两种任务来学习生成上下文嵌入。**RoBERTa旨在解决 BERT 在训练程度上不充分这一问题**，以提升预训练语言模型的性能。RoBERTa 在结构上与 BERT 基本一致，采用了更大的数据集（包括更多的英文书籍、维基百科和其他网页数据，总数据量达到约160GB）、更长的训练时间（包括更大的批次大小和更多的训练步数）以及更细致的超参数调整（包括学习率、训练步数等的设置）；移除了BERT中的下文预测任务，并将BERT原生的静态掩码语言建模任务更改为**动态掩码语言建模**。ALBERT旨在通过**因子参数分解、跨层参数共享**来减少模型的参数量和内存占用，因子参数分解将Embedding层的矩阵先进行分解，将参数数量拆解为V × E + E × H；跨层参数共享只学习第一层编码模块的参数，并将其直接共享给其他所有层；使用与 BERT 完全一致的数据集来进行预训练，保留了 BERT 中的掩码语言建模任务，并将下文预测任务替换为**句序预测**（Sentence Order Prediction, SOP）；共四个版本的模型，分别是ALBERT-Base（0.12亿）、ALBERT-Large（0.18亿）、ALBERT-XLarge（0.6亿） 以及 ALBERT-XXLarge（2.2亿）。ELECTRA通过**生成器-判别器架构**、**替换语言模型任务**优化BERT在下游任务表现，生成器（Generator）是一个能进行掩码预测的模型（例如 BERT 模型），负责将掩码后的文本恢复原状。而判别器（Discriminator）则使用替换词检测（Replaced TokenDetection, RTD）预训练任务，负责检测生成器输出的内容中的每个Token是否是原文中的内容；共提出了三个版本的模型，分别是ELECTRA-Small（0.28亿）、ELECTRA-Base（2.2亿）以及ELECTRA-Large（6.6亿）。

| 模型    | 发布时间 | 参数量（亿）| 语料规模 | 预训练任务   |
| :--:    | :--:    | :--:       | :--:    | :--:        |
| BERT    | 2018.10 | 1.1, 3.4   | 约 15GB | MLM+NSP     |
| RoBERTa | 2019.07 | 1.2, 3.5   | 160GB   | Dynamic MLM |
| ALBERT  | 2019.09 | 0.12, 0.18, 0.6, 2.2 | 约 15GB | MLM+SOP |
| ELECTRA | 2020.03 | 0.28, 2.2, 6.6 | 约 20-200GB | RTD |

为了弥补 Encoder-only 架构在文本生成任务上的短板，Encoder-Decoder 架构在其基础上引入了一个解码器（Decoder），并采用交叉注意力机制来实现编码器与解码器之间的有效交互，包括：全注意力机制、带mask注意力机制、交叉注意力机制。缺陷是**训练和推理成本高**。基于Encoder-Decoder 是架构的代表性LLM是T5、BART。BART 旨在通过多样化的预训练任务来提升模型在文本生成任务和文本理解任务上的表现，共有两个版本，分别是 BART-Base（1.4亿）以及 BART-Large（4亿）；BART使用了与RoBERTa相同的语料库，总数据量达到约160GB；在预训练任务上，BART 以重建被破坏的文本为目标。其通过**Token 遮挡任务（Token Masking）、Token 删除任务（Token Deletion）、连续文本填空任务（Text Infilling）、句子打乱任务（Sentence Permutation）以及文档旋转任务（Document Rotation）等五个任务**来破坏文本，然后训练模型对原始文本进行恢复。T5（Text-to-Text Transfer Transformer）采用了统一的文本到文本的转换范式来处理多种任务，提供了五个不同的版本，分别是T5-Small（6000万）、T5-Base（2.2亿）、T5-Large（7.7亿）、T5-3B（28亿） 以及T5-11B（110亿）；采用C4 数据集（Colossal Clean Crawled Corpus），其覆盖了各种网站和文本类型，总规模达到了约 750GB；提出了名为**Span Corruption**的预训练任务，从原始输入中选择 15% 的 Token 进行破坏，每次都选择连续三个 Token 作为一个小段（span）整体被掩码成mask，与BERT模型中采用的单个Token预测不同，T5模型需要对整个被遮挡的连续文本片段进行预测；通过不同的输入前缀来指示模型执行不同任务，然后生成相应的任务输出，这种方法可以视为**早期的提示（Prompt）**，通过构造合理的输入前缀，T5 模型能够引导自身针对特定任务进行优化，而无需对模型架构进行根本性的改变。

| 模型    | 发布时间 | 参数量（亿）| 语料规模 | 
| :--:    | :--:    | :--:       | :--:    | 
| T5      | 2019.10 | 0.6-110 亿 | 750GB
| mT5（多语言版本）   | 2020.10 | 3-130 亿   | 9.7TB
| T0（更强的零样本能力）    | 2021.10 | 30-110 亿  | 约 400GB
| BART  | 2019.10 | 1.4-4 亿   | 约 20GB
| mBART（多语言版本） | 2020.06 | 0.4-6.1 亿 | 约 1TB

Decoder-only 架构同样包含了三个部分，分别是输入编码部分、特征解码部分以及输出生成部分，Decoder-only 架构的核心特点在于省略了每个编码模块中的交叉注意力子模块，采用**带mask注意力机制**。**更适合生成任务**，Decoder-only架构逐渐成为主流，代表性方法有OpenAI 提出的GPT系列、Meta提出的LLaMA系列等。其中，GPT系列是起步最早的 Decoder-only架构，在性能上也成为了时代的标杆。但从第三代开始，GPT系列逐渐走向了闭源。而LLaMA系列虽然起步较晚，但凭借着同样出色的性能以及始终坚持的开源道路。GPT-1 使用了 Transformer 架构中的 Decoder部分，结构上与BERT-Base高度类似,两者之间的本质区别在于BERT-Base中的自注意力模块是**双向的自注意力机制**，而 GPT-1 中的自注意力模块则是**带有掩码的单向自注意力机制**；使用小说数据集BookCorpus来进行预训练，该数据集包含约8亿个Token，总数据量接近5GB；在预训练方法上，GPT-1采用下一词预测任务，即基于给定的上文预测下一个可能出现的 Token。GPT-2 模型延续了GPT-1的Decoder-only架构，并在此基础上进一步加大了参数数量，发布了四个版本，分别是GPT-2 Small（1.24亿）、GPT-2 Medium（3.55亿）、GPT-2 Large （7.44亿）以及 GPT-2 XL（15亿）；在预训练中，GPT-2 继续采用下一词预测任务，其采用了全新的 WebText 数据集，该数据集由 40GB 经过精心筛选和清洗的网络文本组成；GPT-2 的任务泛化能力得到了改善，**在某些任务上可以不进行微调，直接进行零样本学习**。GPT-3 在模型规模和预训练语料上进一步提升，继承并扩展了前两代的架构，显著增加了解码块的数量、隐藏层的维度和自注意力头的数量，参数量最高达到1750亿；继续采用下一词预测作为预训练任务，使用了更大规模和更多样化的互联网文本数据集，数据量接近1TB；开始在文本生成、问答系统、语言翻译等众多自然语言处理任务中崭露头角，涌现出了优良的上下文学习（In-Context Learning, ICL）能力；在 GPT-3 的基础上，OpenAI进一步推出了一系列衍生模型，如：专注代码生成的Codex、能够进行网页检索的WebGPT、具有良好指令跟随能力的InstructGPT模型。其中，在**人类反馈强化学习（Reinforcement Learning from Human Feedback, RLHF）**中，人类评估者首先提供关于模型输出质量的反馈，然后使用这些反馈来微调模型，整体可以分为以下三个步骤：1）有监督微调：收集大量“问题-人类回答”对作为训练样本，对LLM进行微调。2）训练奖励模型：针对每个输入，让模型生成多个候选输出，并由人工对其进行质量评估和排名，构成偏好数据集。用此偏好数据集训练一个奖励模型，使其可以对输出是否符合人类偏好进行打分。3）强化学习微调：基于上一步中得到的奖励模型，使用强化学习方法优化第一步中的语言模型，即在语言模型生成输出后，奖励模型对其进行评分，强化学习算法根据这些评分调整模型参数，以提升高质量输出的概率。为了克服RLHF在计算效率上的缺陷，斯坦福大学2023年在其基础上提出了一种新的算法**直接偏好优化（Direct Preference Optimization, DPO）**，直接利用人类偏好数据来训练模型，省略了单独构建奖励模型以及应用复杂强化学习算法的步骤。ChatGPT标志着一种新的服务模式 LLMaaS(LLM as a Service) 的出现。GPT-4还引入了对图文双模态的支持，扩展了其在图像描述和视觉问题解答等应用领域的可能性。GPT-4o 模型在前代 GPT-4 的基础上，大幅提升了响应速度，显著降低了延迟，并且还增强了多模态处理能力以及多语言支持能力。其在客户支持、内容创作和数据分析等领域表现亮眼。**GPT-4o 的推出标志着 AIGC 的应用日趋成熟**。LLaMA借鉴了GPT系列的设计理念，同时在技术细节上进行了创新和优化。LLaMA与GPT系列的主要区别在于：GPT系列的升级主线聚焦于模型规模与预训练语料的同步提升，而LLaMA则在模型规模上保持相对稳定，更专注于提升预训练数据的规模。LLaMA1旨在以大规模的优质数据训练相对较小的模型。相对较小的参数规模可以赋能更快的推理速度，使其可以更好的应对计算资源有限的场景,在模型架构方面，LLaMA1 采用了与GPT 系列同样的网络架构,在Transformer 原始词嵌入模块、注意力模块和全连接前馈模块上进行了优化,使用**旋转位置编码（Rotary Positional Embeddings, RoPE）**替代绝对位置编码、使用Pre-Norm 层正则化策略、RELU 激活函数改为SwiGLU 激活函数。LLaMA2 在 LLaMA1 的基础上进一步优化和扩充了训练数据，将语料库的规模扩展至约7TB；在预训练阶段之后，LLaMA2 采纳了人类反馈强化学习的方法，训练了 RLHF 奖励模型，并基于**近似策略优化（Proximal Policy Optimization, PPO）**以及**拒绝采样（Rejection Sampling）**进行强化学习对模型进行更新；在模型架构上，LLaMA2 继承了 LLaMA1 的架构。LLaMA2 共推出了四个版本的模型，LLaMA2-34B 和 LLaMA2-70B 还额外增加了**分组查询注意力（Grouped Query Attention, GQA）**，以提升计算效率。LLaMA3 挑选了规模高达50TB 的预训练语料，是LLaMA2的7倍之多，不仅包含丰富的代码数据以增强模型的逻辑推理能力，还涵盖了超过 5% 的非英文数据，覆盖 30 多种语言，显著扩展了模型的跨语言处理能力；沿用人类反馈强化学习，这一策略已被证明能显著提升模型性能；在模型架构上，LLaMA3 与前一代 LLaMA2 几乎完全相同，只是**在分词（tokenizer）阶段，将字典长度扩大了三倍**，极大提升了推理效率。有三类主流的LLaMA衍生模型，包括：性能改进类（Alpaca、Vicuna、Guanaco）、垂直任务类（CodeLLaMA、LawGPT、GOA、Cornucopia）、多模态任务类（LLaVA、MiniGPT4）。

| 模型    | 发布时间 | 参数量（亿）| 语料规模 | 
| :--:    | :--:    | :--:       | :--:    | 
| GPT-1 | 2018.06 | 1.17 | 约5GB |
| GPT-2 | 2019.02 | 1.24 / 3.55 / 7.74 / 15 | 40GB
| GPT-3 | 2020.05 | 1.25 / 3.5 / 7.62 / 13 / 27 / 67 / 130 / 1750 | 1TB |
| ChatGPT | 2022.11 | 未知 | 未知 |
| GPT-4 | 2023.03 | 未知 | 未知 |
| GPT-4o | 2024.05 | 未知 | 未知 |
| LLAMA-1 | 2023.02 | 67 / 130 / 325 / 652 | 约 5TB |
| LLAMA-2 | 2023.07 | 70 / 130 / 340 / 700 | 约 7TB |
| LLAMA-3 | 2024.04 | 80 / 700 | 约 50TB |
| LLAMA-3.1 | 2024.07 | 80 / 700 / 4050 | 约 50TB |

Transformer 结构是当前LLM的主流模型架构，其缺陷是模型规模随输入序列长度平方增长，导致其在处理长序列时面临计算瓶颈。为了提高计算效率和性能，解决 Transformer 在长序列处理中的瓶颈问题，可以选择基于RNN的语言模型。RNN在生成输出时，只考虑之前的隐藏状态和当前输入，理论上可以处理无限长的序列。然而，传统的 RNN 模型（如 GRU、LSTM 等）在处理长序列时可能难以捕捉到长期依赖关系，且面临着梯度消失或爆炸问题。为了克服这些问题，近年来，研究者提出了两类现代 RNN 变体，分别为**状态空间模型（StateSpace Model，SSM）**和**测试时训练（Test-Time Training，TTT）**。SSM范式可以有效处理长文本中存在的长程依赖性（Long-Range Dependencies, LRDs）问题，并且可以有效降低语言模型的计算和内存开销，代表性模型：RWKV和Mamba。SSM范式通过将上下文信息压缩到固定长度的隐藏状态中，成功将计算复杂度降低至线性级别，有效扩展了模型处理长上下文的能力。然而，随着上下文长度的持续增长，基于 SSM 范式的模型可能会过早出现性能饱和。对此，TTT范式提供了一种有效的解决方案。TTT利用模型本身的参数来存储隐藏状态、记忆上文；并在每一步推理中，对模型参数进行梯度更新，已实现上文的不断循环流入。

## Prompt工程

传统的自然语言处理研究遵循“预训练-微调-预测”范式，即先在大规模语料库上作预训练，然后在下游任务上微调，最后在微调后的模型上进行预测。然而，随着语言模型在规模和能力上的显著提升，一种新的范式——“预训练-提示预测”应运而生，即在预训练模型的基础上，通过精心设计 Prompt 引导大模型直接适应下游任务，而无需进行繁琐微调。Prompt通常以自然语言文本的形式出现。Prompt的核心目的是清晰地描述模型应该执行的任务，以引导模型生成特定的文本、图像、音频等内容。经过良好设计的Prompt通常由**任务说明**、**上下文**、**问题**、**输出格式**四个基本元素组成。

上下文学习（In-Context Learning, ICL）是一种通过**构造特定的Prompt**，来使得语言模型理解并学习下游任务的范式，这些特定的Prompt中可以包含演示示例，任务说明等元素。上下文学习实现的关键在于如何设计有效的 Prompt，以引导模型理解任务的上下文和目标。通常，这些 Prompt 会包含任务说明以及一系列的示例，模型能够从这些上下文信息中学习任务的逻辑和规则，从而在没有额外训练的情况下，生成符合任务要求的输出。按照示例数量的不同，上下文学习可以呈现出多种形式：零样本（Zero-shot）上下文学习、单样本（One-shot）上下文学习和少样本（Few-shot）上下文学习。**合理选择演示示例**对提升上下文学习性能至关重要，演示示例选择主要依靠相似性和多样性，**直接检索**的方法依据候选示例与待解决问题间的相似性对候选示例进行排序，然后选取排名靠前的K个示例，代表性方法是KATE；**聚类检索**方法采用先聚类后检索的方法来保证检索结果的多样性。其先把所有候选示例划分为K个簇，然后从每个簇中选取最为相似的一个示例，代表性方法是Self-Prompting；迭代检索首先挑选与问题高度相似的示例，随后在迭代过程中，结合当前问题和已选示例，动态选择下一个示例，从而确保所选示例的相似性和多样性，**迭代检索**的代表性方法是RetICL。上下文学习的性能受到多种因素的共同影响，包括：预训练数据（领域丰富度、任务多样性、训练数据的分布）、预训练模型（模型的参数规模），以及演示示例。

随着语言模型参数规模的持续扩张，其可以更好的捕捉语言特征和结构，从而在语义分析、文本分类、机器翻译等自然语言处理任务中的表现显著增强。但是，在面对算术求解、常识判断和符号推理等需要复杂推理能力的任务时，模型参数规模的增长并未带来预期的性能突破，这种现象被称作“**Flat Scaling Curves**”。**思维链提示（Chain-of-Thought，CoT）**通过模拟人类解决复杂问题时的思考过程，引导LLM在生成答案的过程中引入一系列的中间推理步骤。这种方法不仅能够显著提升模型在推理任务上的表现，而且还能够揭示模型在处理复杂问题时的内部逻辑和推理路径。在 CoT 核心思想的指引下，衍生出了一系列的扩展的方法，按照其推理方式的不同，可以归纳为三种模式：**按部就班**、**三思后行**和**集思广益**。按部就班模式强调的是逻辑的连贯性和步骤的顺序性，原始的少样本思维链（CoT）方法通过手工构造几个一步一步推理回答问题的例子作为示例放入Prompt 中，来引导模型一步一步生成推理步骤，并生成最终的答案，缺点是费时费力且过度依赖于CoT的编写质量，针对这些问题，研究者在原始CoT的基础上进行了扩展，代表性方法有：**Zero-Shot CoT**和**Auto-CoT**，Zero-Shot CoT通过简单的提示，如“Let’s think step by step”，引导模型自行生成一条推理链，Auto-CoT引入与待解决问题相关的问题及其推理链作为示例，以继续提升 CoT 的效果。三思后行模式强调的是在决策过程中的融入审慎和灵活性，研究者在 CoT 的基础上提出了**思维树（Tree of Thoughts, ToT）**、**思维图（Graph of Thoughts, GoT）**等三思后行模式下的CoT变体，ToT 将推理过程构造为一棵思维树，其从以下四个角度对思维树进行**构造，拆分、衍生、搜索**，在 ToT 的基础上，GoT 将树扩展为有向图，以提供了每个思维自我评估修正以及思维聚合的操作。该图中，顶点代表某个问题（初始问题、中间问题、最终问题）的一个解决方案，有向边代表使用“出节点”作为直接输入，构造出思维“入节点”的过程。集思广益模式强调的是通过汇集多种不同的观点和方法来优化决策过程，**Self-Consistency**的实现过程可以分为三个步骤：(1) 在随机采样策略下，使用 CoT 或 Zero-Shot CoT 的方式来引导LLM针对待解决问题生成一组多样化的推理路径; (2) 针对LLM生成的每个推理内容，收集其最终的答案，并统计每个答案在所有推理路径中出现的频率; (3) 选择出现频率最高的答案作为最终的、最一致的答案。**Universal Self-Consistency**利用LLM自身来选择最一致答案，显著拓宽了Self-Consistency的使用场景。三种思维连均是在作用于模型推理侧，OpenAI尝试在训练和推理时融合思维链技术，并提出了**GPT-o1**，包括三个版本：o1、o1-preview、o1-mini三个版本。

**编写规范的Prompt**是我们与LLM进行有效沟通的基础。经典的 Prompt通常由任务说明，上下文，问题，输出格式等部分中的一个或几个组成。在与LLM的交互中，提问的质量直接影响到信息触达的效率和深度。一个精心设计的提问不仅能够明确表达需求，还能引导模型聚焦于问题的核心，从而获得精准且有价值的答案。可以利用两个高级提问策略：“**复杂问题拆解**”和“**追问**”。在处理复杂问题时，我们可以将问题分解为更小、更易于理解的子问题，并逐一解决，这一过程包括两个关键步骤：**分步引导和归纳总结**；从追问的形式和目的角度来看，追问可以分为三种形式：**深入追问、扩展追问、反馈追问**。CoT是在处理涉及算术、常识和符号推理等复杂推理的任务时的理想选择。在处理这类任务的过程中，通过 CoT 引导模型理解和遵循中间步骤，能够显著提高得出答案的准确率。在决定何时使用 CoT 时，需要对**任务类别、模型规模以及模型能力**三方面因素进行考虑。通过模仿自信和乐观的心态，一个人可以在他们的现实生活中实现这些品质。这种现象不仅局限于人类的行为，积极的心理暗示也可用于激发LLM的潜力。这种心理暗示可以通过**角色扮演和情景代入**的方式传达给LLM。为了构建一个有效的角色，需要在指令中包含具体属性、职责、知识和技能；情景代入指的是将特定情境下所需的专业知识、历史背景等信息嵌入到模型的响应中。

## 参数高效微调

对于**预训练数据涉及较少的垂直领域**，LLM需要对这些领域及相应的下游任务进行适配。上下文学习和指令微调是进行下游任务适配的有效途径，但它们在效果或效率上存在缺陷。为弥补这些不足，参数高效微调（Parameter-Efficient Fine-Tuning, PEFT）技术应运而生。

主流的下游任务适配方法有两种：a) 上下文学习（In-context learning）；b）指令微调（Instruction Tuning）。指令微调旨在对模型进行任务指令的学习，使其能更好地理解和执行各种自然语言处理任务的指令。**指令微调需首先构建指令数据集，然后在该数据集上进行监督微调**。**参数高效微调（Parameter-Efficient Fine-Tuning，PEFT）**旨在避免微调全部参数，减少在微调过程中需要更新的参数数量和计算开销，从而提高微调LLM的效率。主流的 PEFT 方法可以分为三类：**参数附加方法（Additional ParametersMethods）**、**参数选择方法（Parameter Selection Methods）**、**低秩适配方法（Low Rank Adaptation Methods）**。

参数附加方法（Additional Parameter Methods）通过增加并训练新的附加参数或模块对LLM进行微调。参数附加方法按照附加位置可以分为三类：**加在输入、加在模型以及加在输出**。加在输入的方法将额外参数附加到模型的输入嵌入（Embedding）中，其中最经典的方法是**Prompt-tuning**，在模型的输入中引入可微分的连续张量，通常也被称为软提示（Soft prompt），设置**合适的软提示长度和合理初始化**至关重要，具备内存效率高、多任务能力、缩放特性。加在模型的方法将额外的参数或模型添加到预训练模型的隐藏层中，其中经典的方法有Prefix-tuning、Adapter-tuning和AdapterFusion，Prefix-tuning 将一系列连续的可训练前缀（Pre-fixes，即 Soft-prompt）**插入到输入嵌入以及 Transformer注意力模块**中，**Pk和Pv**是插入到 Transformer block 中的前缀；Adapter-tuning 在 Transformer 的每一个多头注意力层（Multi-headAttention Layer，图中红色块）和全连接层（Feed-forward Network Layer）之后添加适配器，**适配器模块通常采用瓶颈（Bottomneck）结构，即一个上投影层、一个非线性映射和一个下投影层组成的全连接模块**；**代理微调（Proxy-tuning）**提供了一种轻量级的解码时（Decoding-time）算法，允许我们在不直接修改LLM权重的前提下，通过仅访问模型输出词汇表预测分布，涉及**代理模型、专家模型、反专家模型**。

参数选择方法（Parameter Selection Methods）选择性的对预训练模型中的某个参数子集进行微调。和参数附加方法不同的是，参数选择方法无需向模型添加额外的参数，避免了在推理阶段引入额外的计算成本。通常，参数选择方法分为两类：**基于规则的方法**和**基于学习的方法**。基于规则的方法中最具代表性的方法是**BitFit**，仅优化神经网络中的每一层的偏置项（Biases）以及任务特定的分类头来实现参数高效微调，该方法仅在小模型（如BERT、RoBERT 等）上进行验证性能，在更大模型上的性能表现如何尚且未知，还有仅对 **BERT和RoBERTa的最后四分之一层进行微调**、**PaFi选择具有最小绝对值的模型参数**作为可训练参数。基于学习的方法在模型训练过程中自动地选择可训练的参数子集，最为典型方法是 Child-tuning，梯度掩码矩阵策略实现仅对选中的择子网络进行梯度更新，而屏蔽子网络梯度以外的梯度。参数选择方法目前不是主流，主要应用在早期BERT相关模型。

低秩适配方法（Low-rank Adaptation Methods）通过低秩矩阵来近似原始权重权更新矩阵，并仅微调低秩矩阵，以大幅降低模型参数量。该方法**将参数更新矩阵（d*k）低秩分解为两个小矩阵（d*r、r*k）**。在微调时，通过微调这两个小矩阵来对LLM进行更新，大幅节省了微调时的内存开销。LoRA通常施加在注意力层的权重矩阵、FFN层，性能**主要受权重初始化、秩(r)、施加位置**三方面影响，通常将下投影矩阵B用0初始化、上投影矩阵A高斯初始化；简单任务上用较低的秩有不错的效果，跨领域迁移和复杂任务上高秩往往表现更好；只微调一种权重矩阵时，Wo效果最好。LoRA具备三方面优势：参数效率、**插件化特性**、跨任务泛化。许多 LoRA 变体方法被提出，以进一步提升 LoRA 在下游任务中的适配性
能，主要从以下几个角度进行改进：(1) 性能改进，如AdaLoRA；(2) 任务泛化，如LoRAHub；(3) 训练优化，如Q-LoRA；（4）推理优化，如S-LoRA。

## 模型编辑

LLM有时会产生一些不符合人们期望的结果，如**偏见、毒性和知识错误**等。偏见是指模型生成的内容中包含刻板印象和社会偏见等不公正的观点，毒性是指模型生成的内容中包含有害成分，而知识错误则是指模型提供的信息与事实不符。，模型编辑旨在精准、高效地**修正LLM中的特定知识点**，能够满足LLM对特定知识点进行更新的需求。模型编辑的性质归纳为五个方面，分别为准确性（Accuracy）、泛化性（Generality）、可迁移性（Portability）、局部性（Locality）和高效性（Efficiency）。

现有编辑方法分为外部拓展法和内部修改法，外部拓展法通过设计特定的训练程序，使模型在保持原有知识的同时学习新信息；内部修改法通过调整模型内部特定层或神经元，来实现对模型输出的精确控制。**外部拓展法包括知识缓存法和附加参数法，内部修改法包括元学习法和定位编辑法**。知识缓存法中包括三个主要组件，分别为门控单元、编辑缓存和推理模块；CALINET和 T-Patcher通过修改模型最后一层Transformer的全连接前馈模块来实现。内部修改法旨在通过更新原始模型的内部参数来为模型注入新知识，可以分为**元学习法和定位编辑法**。

**T-Patcher**是附加参数法中的代表性方法，其在模型最后一个Transformer层的全连接前馈层中添加额外参数（称为“补丁”），然后对补丁进行训练来完成特定知识的编辑。T-Patcher将全连接前馈层视为**键值存储体**，全连接前馈层的隐藏层维度可被理解为其“记忆”的文本模式的数量。因此，T-Patcher在**全连接前馈层**中增加额外参数，即添加补丁。而且，T-Patcher仅在模型的最后一层添加补丁，以确保补丁能够充分修改模型的输出，而不被其他模型结构干扰。T-Patcher 从编辑的准确性和局部性两个角度出发对损失函数进行设计，对于补丁的准确性T-Patcher 主要关注两个方面：（1）确保补丁可以在目标输入下可以被激活；（2）一旦被激活，补丁应该能够准确地调整模型输出以符合预期的结果；为了保证编辑的局部性，T-Patcher 设计了特定的损失函数来限制补丁的激活范围，确保其只在相关的输入上被激活。

定位编辑首先定位知识存储在神经网络中的哪些参数中，然后再针对这些定位到的参数进行精确的编辑。**ROME（Rank-One Model Editing）**是其中的代表性方法。ROME通过因果跟踪实验和阻断实验发现知识存储于模型中间层的全连接前馈层。因果跟踪包含三个步骤：**正常推理、干扰推理和恢复推理**。其中，正常推理旨在保存模型在未受干扰情况下的内部状态，用于后续恢复推理中内部状态的恢复；干扰推理旨在干扰模型的所有内部状态，作为控制变量的基准线；恢复推理则将每个内部状态的恢复作为变量，通过对比内部状态恢复前后的输出差异，精确评估每个模块与知识回忆的相关性。模型的中间层Transformer在处理s的最后一个Token s(−1)（如示例中的“马”）时，表现出显著的因果效应。与T-Patcher相似，ROME同样将全连接前馈层视为一个键值存储体。但不同的是，**T-patcher将上投影矩阵的参数向量看作键向量，将下投影矩阵的参数向量看作值向量，而ROME则是将下投影矩阵的输入向量看作键向量，将其输出向量看作值向量**。ROME 编辑方法主要包括三个步骤：1. 确定键向量，拼接随机的不同前缀文本进行多次推理，计算平均的向量；2. 优化值向量，通过设计损失函数L(v) = L1 (v) + L2 (v)以确保编辑的准确性和局部性，其中v是优化变量，用于替换全连接前馈层的输出；3. 插入知识，在插入新知识的同时，尽量避免影响Wproj中的原有信息，ROME将这一问题建模为一个带约束的最小二乘问题。

## 检索增强生成

LLM在面对某些问题时无法给出正确答案，甚至出现“**幻觉**”，即生成看似合理实则逻辑混乱或违背事实的回答。为了解决这些问题并进一步提升LLM的生成质量，我们可以将相关信息存储在外部数据库中，供LLM进行检索和调用。这种从外部数据库中检索出相关信息来辅助改善LLM生成质量的系统被称之为检索增强生成（Retrieval-Augmented Generation，RAG）。出现幻觉的原因有：1. 训练数据导致的幻觉，知识过时、知识边界、知识偏差、数据标注对齐不当；2. 模型自身导致的幻觉，知识长尾、曝光偏差、解码偏差。

针对不同的业务场景，RAG 中的生成器可以选用不同的LLM，如 GPT-4、LLaMA等。考虑到LLM的开源/闭源、微调成本等问题，RAG 中的LLM可以是参数不可感知/调节的“黑盒”模型，也可以是参数可感知和微调的“白盒”模型。RAG架构可分为两大类：**黑盒增强架构**和**白盒增强架构**，黑盒增强架构可根据是否对检索器进行微调分为两类：无微调、检索器微调，白盒增强架构也可根据是否对检索器进行微调分为两类：仅微调LLM、检索器与LLM协同微调（下文简称为协同微调）。

**知识检索**通常包括知识库构建、查询增强、检索器、检索结果重排序四部分，知识库构建主要涉及**数据采集及预处理与知识库增强**两个步骤，数据清洗旨在清除文本中的干扰元素，如特殊字符、异常编码和无用的HTML标签，以及删除重复或高度相似的冗余文档，从而提高数据的清晰度和可用性；文本分块是将长文本分割成较小文本块的过程，例如把一篇长文章分为多个短段落；知识库增强是通过改进和丰富知识库的内容和结构，以提升其质量和实用性，通常涉及查询生成与标题生成等多个步骤，以此为文档建立语义“锚点”，方便检索时准确定位到相应文本。查询增强分为**查询语义增强和查询内容增强**，查询语义增强旨在通过**同义改写和多视角分解**等方法来扩展、丰富用户查询的语义，以提高检索的准确性和全面性；查询内容增强旨在通过生成与原始查询相关的背景信息和上下文，例如：**生成背景文档**，从而丰富查询内容，提高检索的准确性和全面性。给定知识库和用户查询，检索器旨在找到知识库中与用户查询相关的知识文本，可分为**判别式检索器、生成式检索器和图检索器**三类，判别式检索器包括：稀疏检索器（如：**TF-IDF**）、双编码检索器、交叉编码检索器，为提升检索效率，可以引入向量数据库来实现检索中的高效向量存储和查询，向量数据库的核心是设计高效的相似度索引算法；生成式检索器直接将知识库中的文档信息记忆在模型参数中，在接收到查询请求时，能够直接生成相关文档的标识符（即DocID）。精选的主要途径是对检索到的文档进行重新排序，简称重排，然后从中选择出排序靠前的文档，主要分为两类：基于交叉编码的方法和基于上下文学习的方法。

检索器得到相关信息后，将其传递给LLM以期增强模型的生成能力。围绕四个方面展开讨论：（1）何时增强，确定何时需要检索增强，以确保非必要不增强；（2）何处增强，确定在模型中的何处融入检索到的外部知识，以最大化检索的效用；（3）多次增强，如何对复杂查询与模糊查询进行多次迭代增强，以提升 RAG 在困难问题上的效果；（4）降本增效，如何进行知识压缩与缓存加速，以降低增强过程的计算成本。判断是否需要增强的核心在于判断LLM是否具有**内部知识**，外部观测法，通过直接对LLM进行询问或者观测调查其训练数据来推断其是否具备内部知识；在模型参数可访问的情况下，可以通过观测模型内部的隐藏状态来更精确地评估其知识掌握情况，**中间层的内部隐藏状态**能够有效地反映模型对问题的理解和相关知识储备。在确定LLM需要外部知识后，我们需要考虑在何处利用检索到的外部知识，即何处增强的问题，**在输入端，可以将问题和检索到的外部知识拼接在Prompt中**，然后输入给LLM；在中间层，可以采用交叉注意力将外部知识直接编码到模型的隐藏状态中；在输出端，可以利用外部知识对生成的文本进行后矫正。复杂问题往往涉及多个知识点，需要多跳（multi-hop）的理解，常采用分解式增强的方案，将**多跳问题分解**为一个个子问题，然后在子问题间迭代地进行检索增强，最后得出正确结论；而模糊问题往往指代范围不明，难以一次就理解问题的含义，常采用渐进式增强的方案，对问题进行渐进式地**拆解、细化**，然后对细化后的问题进行检索，利用检索到的信息增强大模型。降本增效从**去除冗余文本与复用计算结果**两个角度进行解决，除冗余文本的方法主要分为三类：Token 级别的方法，子文本级别的方法以及全文本级别的方法；为了避免对每个 Token 都重新计算前面的Key和Value的结果，我们可以将之前计算的Key和Value的结果进行缓存（即**KV-cache**），在需要是直接从KV-cache中调用相关结果，从而避免重复计算。

## 参考文献
[大模型基础](https://github.com/ZJU-LLMs/Foundations-of-LLMs)
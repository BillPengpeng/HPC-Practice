
## 1 - Introduction / Logistics

### Aim of this course

You will learn about the underlying concepts of modern deep learning systems like **automatic differentiation**, **neural network architectures**, **optimization**, and **efficient operations** on systems like GPUs.  
To solidify your understanding, along the way (in your homeworks), you will build (from scratch) **needle**, a deep learning library loosely similar to PyTorch, and implement many common architectures in the library. ——构建类似Pytorch的简单深度学习框架。

### Elements of deep learning systems  

**Compose** multiple tensor operations to build modern machine learning models.  
**Transform** a sequence of operations (automatic differentiation).  
**Accelerate** computation via specialized hardware.  
**Extend** more hardware backends, more operators.  

## 2 - ML Refresher / Softmax Regression

### Three ingredients of a machine learning algorithm
Every machine learning algorithm consists of three different elements:  
1. **The hypothesis class**: the “program structure”, parameterized via a set of parameters, that describes how we map inputs (e.g., images of digits) to outputs (e.g., class labels, or probabilities of different class labels). ———假设类。  
2. **The loss function**: a function that specifies how “well” a given hypothesis (i.e., a choice of parameters) performs on the task of interest.  
3. **An optimization method**: a procedure for determining a set of parameters that (approximately) minimize the sum of losses over the training set.  

### Softmax求导

#### Softmax函数及其导数

Softmax函数是机器学习领域，特别是多分类问题中常用的激活函数。它将一个实数向量转换为一个概率分布。给定一个实数向量$\mathbf{z} = (z_1, z_2, \ldots, z_n)$，Softmax函数定义为：

$$
\sigma_i(\mathbf{z}) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}} \quad \text{for } i = 1, 2, \ldots, n
$$

其中，$\sigma_i(\mathbf{z})$表示向量$\mathbf{z}$中第$i$个元素经过Softmax函数处理后的输出值。

Softmax函数的导数通常指其对输入向量$\mathbf{z}$的每个元素的偏导数。设$\mathbf{s} = \sigma(\mathbf{z})$为Softmax函数的输出向量，则$\mathbf{s}$的第$i$个元素$s_i$对$\mathbf{z}$的第$j$个元素$z_j$的偏导数为：
$$
\frac{\partial s_i}{\partial z_j} = \begin{cases}
s_i (1 - s_j), & \text{if } i = j \\
-s_i s_j, & \text{if } i \neq j
\end{cases}
$$

这个导数公式可以通过以下步骤推导得到：

1. 当$i = j$时，

$$
\frac{\partial s_i}{\partial z_i} = \frac{\partial}{\partial z_i} \left( \frac{e^{z_i}}{\sum_{k=1}^{n} e^{z_k}} \right) = \frac{e^{z_i} \sum_{k=1}^{n} e^{z_k} - e^{z_i} e^{z_i}}{\left( \sum_{k=1}^{n} e^{z_k} \right)^2} = \frac{e^{z_i}}{\sum_{k=1}^{n} e^{z_k}} \left( 1 - \frac{e^{z_i}}{\sum_{k=1}^{n} e^{z_k}} \right) = s_i (1 - s_i)
$$

2. 当$i \neq j$时，

$$
\frac{\partial s_i}{\partial z_j} = \frac{\partial}{\partial z_j} \left( \frac{e^{z_i}}{\sum_{k=1}^{n} e^{z_k}} \right) = -\frac{e^{z_i} e^{z_j}}{\left( \sum_{k=1}^{n} e^{z_k} \right)^2} = -s_i s_j
$$

因此，Softmax函数的导数矩阵（也称为Jacobian矩阵）是一个对角线上为$s_i (1 - s_i)$，非对角线上为$-s_i s_j$的矩阵。这个导数矩阵在反向传播算法中用于计算梯度，从而更新模型的参数。


#### Softmax交叉熵损失函数的求导

给定一个样本的真实标签$y$（一个one-hot编码的向量）和模型的预测输出$\mathbf{p}$（一个经过Softmax函数处理后的概率分布向量），Softmax交叉熵损失函数定义为：

$$
L = -\sum_{i=1}^{n} y_i \log(p_i)
$$

其中，$n$是类别的数量，$y_i$是真实标签向量中第$i$个元素的值（对于真实类别，其值为1；对于其他类别，其值为0），$p_i$是预测输出向量中第$i$个元素的值（表示样本属于第$i$个类别的概率）。

在反向传播过程中，我们更关心的是损失函数对输入到Softmax函数之前的原始得分向量$\mathbf{z}$的导数。设$\mathbf{p} = \sigma(\mathbf{z})$，其中$\sigma$表示Softmax函数，则：

$$
\frac{\partial L}{\partial z_i} = \frac{\partial L}{\partial p_y} \frac{\partial p_y}{\partial z_i}
$$

根据Softmax函数的导数公式，可得：

$$
\frac{\partial L}{\partial z_i} = \begin{cases}
p_y - 1, & \text{if } i = y \\
p_i, & \text{if } i \neq y
\end{cases}
$$

或者更简洁地表示为：

$$
\frac{\partial L}{\partial \mathbf{z}} = \mathbf{p} - \mathbf{y}
$$

其中，$\mathbf{y}$是真实标签的one-hot编码向量。

## 3 - “Manual” Neural Networks

### Neural networks / deep learning

A neural network refers to **a particular type of hypothesis class**, consisting of multiple, parameterized differentiable functions (a.k.a. “layers”) composed together in any manner to form the output.


考虑一个两层的人工神经网络，其中第一层是隐藏层，第二层是输出层并接Softmax激活函数。设输入向量为$\mathbf{x}$，隐藏层权重矩阵为$\mathbf{W}^{(1)}$，隐藏层偏置向量为$\mathbf{b}^{(1)}$，输出层权重矩阵为$\mathbf{W}^{(2)}$，输出层偏置向量为$\mathbf{b}^{(2)}$。

##### 前向运算

1.**隐藏层计算**  
隐藏层的输入为$\mathbf{z}^{(1)} = \mathbf{W}^{(1)}\mathbf{x} + \mathbf{b}^{(1)}$。 
隐藏层的输出（激活值）为$\mathbf{a}^{(1)} = \sigma(\mathbf{z}^{(1)})$，其中$\sigma$是隐藏层的激活函数（如ReLU、tanh等）。

2.**输出层计算（Softmax前）**：

输出层的输入为$\mathbf{z}^{(2)} = \mathbf{W}^{(2)}\mathbf{a}^{(1)} + \mathbf{b}^{(2)}$。

3.**Softmax计算**：

Softmax层的输入是$\mathbf{z}^{(2)}$，输出是概率分布向量$\mathbf{p}$，其中第$i$个元素为：

$$
p_i = \frac{e^{z_i^{(2)}}}{\sum_{j=1}^{n} e^{z_j^{(2)}}}
$$

其中，$n$是输出层的神经元数量（即类别数）。

##### 反向梯度计算

假设损失函数为交叉熵损失$L = -\sum_{i=1}^{n} y_i \log(p_i)$，其中$\mathbf{y}$是真实标签的one-hot编码向量。

1.**对输出层权重和偏置的梯度**：

   设$\delta^{(2)} = \frac{\partial L}{\partial \mathbf{z}^{(2)}}$为损失函数对输出层输入的梯度。由于交叉熵损失和Softmax函数的特性，我们有：

   $$
   \delta^{(2)} = \mathbf{p} - \mathbf{y}
   $$

   输出层权重$\mathbf{W}^{(2)}$的梯度为：

   $$
   \frac{\partial L}{\partial \mathbf{W}^{(2)}} = \delta^{(2)}(\mathbf{a}^{(1)})^\top
   $$

   输出层偏置$\mathbf{b}^{(2)}$的梯度为：

   $$
   \frac{\partial L}{\partial \mathbf{b}^{(2)}} = \delta^{(2)}
   $$

2.**对隐藏层权重和偏置的梯度**：

   设$\delta^{(1)} = \frac{\partial L}{\partial \mathbf{z}^{(1)}}$为损失函数对隐藏层输入的梯度。我们需要**先计算它对隐藏层输出的梯度**，然后利用链式法则回传到隐藏层：

   $$
   \frac{\partial L}{\partial \mathbf{a}^{(1)}} = \mathbf{W}^{(2)\top}\delta^{(2)}
   $$

   由于隐藏层通常有一个非线性激活函数$\sigma$，我们需要计算激活函数的导数$\sigma'(\mathbf{z}^{(1)})$（也称为**雅可比矩阵**）：

   $$
   \delta^{(1)} = \sigma'(\mathbf{z}^{(1)}) \odot \frac{\partial L}{\partial \mathbf{a}^{(1)}}
   $$

   其中，$\odot$表示逐元素乘法。

   隐藏层权重$\mathbf{W}^{(1)}$的梯度为：

   $$
   \frac{\partial L}{\partial \mathbf{W}^{(1)}} = \delta^{(1)}\mathbf{x}^\top
   $$

   隐藏层偏置$\mathbf{b}^{(1)}$的梯度为：

   $$
   \frac{\partial L}{\partial \mathbf{b}^{(1)}} = \delta^{(1)}
   $$

注意：在实际计算中，激活函数$\sigma$的导数$\sigma'(\mathbf{z}^{(1)})$需要根据具体的激活函数来计算。例如，对于ReLU激活函数，其导数为：

$$
\sigma'(z_i^{(1)}) = \begin{cases}
1, & \text{if } z_i^{(1)} > 0 \\
0, & \text{if } z_i^{(1)} \leq 0
\end{cases}
$$

或者采用一个小的正数$\epsilon$来避免梯度消失（即当$z_i^{(1)} \leq 0$时，令$\sigma'(z_i^{(1)}) = \epsilon$）。

## 4 - Automatic Differentiation

自动微分（Automatic Differentiation，AD）的原理是一种对计算机程序进行高效准确求导的技术，其基本原理主要基于导数运算法则和链式法则,概括为正向传播和反向传播两个阶段：

开始  
│  
├── 1. 构建计算图（Computational Graph）  
│    │  
│    └── 将计算过程表示为有向无环图（DAG），节点为操作，边为数据流  
│  
├── 2. 前向计算（Forward Pass）  
│    │  
│    ├── 按拓扑排序遍历节点  
│    ├── 计算每个节点的输出值  
│    └── 记录中间结果（用于反向传播）  
│  
├── 3. 初始化梯度（Gradient Initialization）  
│    │  
│    ├── 目标函数梯度 ∂L/∂L = 1  
│    └── 其他节点梯度初始化为 0  
│  
├── 4. 反向传播（Backward Pass）  
│    │  
│    ├── 按逆拓扑排序遍历节点  
│    │  
│    ├── 对当前节点 u：  
│    │    │  
│    │    ├── 获取其输出梯度 ∂L/∂u  
│    │    │  
│    │    ├── 遍历 u 的输入节点 v：  
│    │    │    │  
│    │    │    └── 计算局部梯度 ∂u/∂v（根据前向操作）  
│    │    │  
│    │    └── 累加梯度到输入节点：∂L/∂v += ∂L/∂u * ∂u/∂v  
│    │  
│    └── 重复直到所有节点梯度计算完成  
│  
└── 5. 提取输入变量梯度  
     │  
     └── 返回输入参数（如权重、输入数据）的梯度 ∂L/∂θ  
│  
结束  

## 6 - Fully connected networks, optimization, initialization

### Key questions for fully connected networks

In order to actually train a fully-connected network (or any deep network), we need to address a certain number of questions:  
• How do we choose the **width and depth** of the network?
• How do we actually **optimize** the objective? (“SGD” is the easy answer, but not the algorithm most commonly used in practice)  
• How do we **initialize** the weights of the network?  
• How do we ensure the network can continue to be trained easily over multiple optimization iterations?  


### optimization

#### 随机梯度下降（SGD, Stochastic Gradient Descent）

$$
θ = θ - α * g_t
$$

#### 动量（Momentum）

动量算法通过引入动量项来加速SGD在相关方向上的收敛速度，并抑制震荡。

$$
v_t = β * v_{t-1} + (1 - β) * g_t \\
θ = θ - α * v_t
$$

### “Unbiasing” momentum terms

对动量项偏差校正，在更新参数时使用校正后的动量项。  

$$
v_t = β * v_{t-1} + (1 - β) * g_t \\
θ = θ - α * v_t / (1 - β^{t+1})
$$

#### Nesterov Momentum

在计算梯度时，不是基于当前参数位置，而是基于沿着动量方向预测的未来位置。

$$
v_t = β * v_{t-1}  + (1 - β) * \nabla_{\theta} J(\theta - v_{t-1}) \\
θ = θ - α * v_t
$$

#### RMSProp（Root Mean Square Propagation）

RMSProp算法通过计算梯度平方的移动平均线，并以此来调整每个参数的学习率。
$$
s_t = β * s_{t-1} + (1 - β) * (g_t)^2 \\
θ = θ - α * g_t / (√s_t + ε)
$$
其中，$s_t$表示梯度的平方的指数移动平均。

#### Adam（Adaptive Moment Estimation）

Adam算法结合了动量算法和RMSProp算法的优点，通过计算梯度的一阶矩估计和二阶矩估计来动态调整每个参数的学习率。  
1.更新一阶矩、二阶矩估计（动量）：
$$
   m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t \\
   v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
$$

2.对一阶矩估计和二阶矩估计进行偏差修正：
$$
   \hat{m}_t = \frac{m_t}{1 - \beta_1^t} \\
   \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
$$

3.更新参数：
$$
   \theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
$$

### Initialization of weights

#### Xavier uniform
`xavier_uniform(fan_in, fan_out, gain=1.0, **kwargs)`

Fills the input Tensor with values according to the method described in [Understanding the difficulty of training deep feedforward neural networks](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf), using a uniform distribution. The resulting Tensor will have values sampled from $\mathcal{U}(-a, a)$ where 

$$
a = \text{gain} \times \sqrt{\frac{6}{\text{in} + \text{out}}}
$$

#### Xavier normal
`xavier_normal(fan_in, fan_out, gain=1.0, **kwargs)`

Fills the input Tensor with values according to the method described in [Understanding the difficulty of training deep feedforward neural networks](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf), using a normal distribution. The resulting Tensor will have values sampled from $\mathcal{N}(0, \text{std}^2)$ where 

$$
\text{std} = \text{gain} \times \sqrt{\frac{2}{\text{in} + \text{out}}}
$$


#### Kaiming uniform
`kaiming_uniform(fan_in, fan_out, nonlinearity="relu", **kwargs)`

Fills the input Tensor with values according to the method described in [Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification](https://arxiv.org/pdf/1502.01852.pdf), using a uniform distribution. The resulting Tensor will have values sampled from $\mathcal{U}(-\text{bound}, \text{bound})$ where 

$$
\text{bound} = \text{gain} \times \sqrt{\frac{3}{\text{in}}}
$$

Use the recommended gain value for ReLU: $\text{gain}=\sqrt{2}$.


#### Kaiming normal
`kaiming_normal(fan_in, fan_out, nonlinearity="relu", **kwargs)`

Fills the input Tensor with values according to the method described in [Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification](https://arxiv.org/pdf/1502.01852.pdf), using a uniform distribution. The resulting Tensor will have values sampled from $\mathcal{N}(0, \text{std}^2)$ where 

$$
\text{std} = \frac{\text{gain}}{\sqrt{\text{in}}}
$$

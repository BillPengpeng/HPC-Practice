本文主要整理CS336 Lecture 15 RLHF / ALIGNMENT章节的主要内容。

## 5.0 PPO in language modeling

### 内容概况

这两张图从宏观流程（“做什么”）深入到微观算法（“怎么做”），系统性地阐述了RLHF，特别是其强化学习阶段的关键细节：

1.  **图1（PPO in language modeling - From InstructGPT）**：聚焦于**强化学习优化阶段**。它详细说明了如何将PPO算法应用于语言模型，并介绍了为防止模型“遗忘”基础能力而引入的**混合训练目标函数（PPO-ptx）**。该图是RLHF流程的**策略优化**部分。
2.  **图2（More details and background from Stiennon）**：聚焦于**奖励模型训练**和**策略优化的完整奖励设计**。它解释了奖励模型如何从人类偏好中学习，并明确了在PPO优化中使用的完整奖励函数包含一个关键的**KL惩罚项**。该图是RLHF流程的**基础构建**部分。

---

### 要点总结

#### 1. 核心组件一：奖励模型——将人类偏好量化为分数
*   **目标**：训练一个模型，使其能够像人类一样，对模型生成的回答（如摘要）进行打分。
*   **方法**（基于图2）：
    *   使用**配对比较**数据：给定一个输入和两个模型的输出，让人类判断哪个更好。
    *   训练一个**回归模型**，其目标是让“更好”的回答的得分（rθ(x, yi)）显著高于“较差”的回答（rθ(x, y1-i)）。其损失函数鼓励拉大两者得分差距。
    *   训练完成后，对奖励模型输出进行**归一化**，以便于后续使用。

#### 2. 核心组件二：策略优化——利用奖励模型改进语言模型
*   **目标**：利用训练好的奖励模型作为“指挥棒”，通过强化学习来优化语言模型（即“策略”），使其生成能获得更高奖励的回答。
*   **算法**：主要使用**PPO算法**，这是深度强化学习中的一种强大且稳定的算法。

#### 3. 关键技术一：KL散度惩罚——防止“奖励黑客”和模式崩溃
这是RLHF成功的关键技巧，在两幅图中均被强调。
*   **公式**：完整奖励 = 奖励模型得分 - β * KL(当前策略 || 初始SFT策略)
*   **双重作用**（图2中明确解释）：
    1.  **鼓励探索**：作为熵奖励，防止模型为了获得高奖励而迅速“躺平”在某个单一的、可能取巧的输出模式上（模式崩溃）。
    2.  **避免过度优化**：防止模型生成那些奖励模型在训练时从未见过的、奇怪但可能能“骗”得高分的输出（奖励黑客），确保输出不会偏离初始SFT模型太远。系数β用于控制惩罚的强度。

#### 4. 关键技术二：混合预训练目标（PPO-ptx）——防止灾难性遗忘
这是InstructGPT（图1）提出的一项重要改进。
*   **问题**：单纯优化RL目标（即奖励模型得分）会导致模型在传统的语言建模任务上性能下降（**灾难性遗忘**）。
*   **解决方案**：在PPO的目标函数中，**增加一项预训练损失**。**即要求模型在优化奖励的同时，不能忘记如何像基础语言模型一样进行正常的预测下一个词的任务**。
*   **目标函数**（图1中的公式(2)）：最终的优化目标是RL目标项和预训练损失项的加权和，由系数γ控制预训练项的强度。

#### 5. 工程实践：价值函数与策略网络分离
图2中提到一个重要的工程细节：**价值函数使用与策略网络完全独立的参数**。
*   **目的**：防止在训练早期，对价值函数的更新“污染”或破坏已经预训练好的策略网络参数，从而保证训练稳定性。

### 总结

总而言之，这两张技术图清晰地揭示了RLHF不仅仅是一个概念框架，更是一套包含**精心设计的奖励函数（含KL惩罚）、稳定的PPO算法、以及防止遗忘的混合训练目标**的精密工程系统。其成功的关键在于多个**正则化技术**（KL惩罚、混合训练）的协同应用，从而在“朝着奖励方向改进”和“保持模型原有核心能力与稳定性”之间取得了至关重要的平衡。

## 5.1 PPO – at a conceptual level

### 内容概况

这张幻灯片以清晰的概念层面，介绍了**近端策略优化算法** 的演进思路和核心思想。它采用了“尝试1 -> 尝试2 -> 尝试3”的递进结构，分别阐述了：

1.  **策略梯度法**：作为起点，指出了其核心问题——方差过高。
2.  **信任区域策略优化**：作为改进，通过引入约束来限制策略更新的步幅，但计算复杂。
3.  **PPO算法本身**：作为最终方案，通过一种巧妙的“裁剪”机制，实现了与TRPO相似的目标（稳定训练），但大大简化了实现。

幻灯片的核心在于阐明PPO如何通过一个简化的技巧，解决了策略梯度算法中的核心稳定性难题。

---

### 要点总结

1.  **演进脉络：从“激进”到“稳定”**
    *   **策略梯度（尝试1）**：方法直接，但更新步长难以控制，导致训练过程不稳定（方差高），容易“冲过头”从而破坏已学到的策略。
    *   **TRPO（尝试2）**：认识到了问题，提出了“信任区域”概念，即每次更新只在一个可信的范围内进行（由KL散度约束）。虽然有效，但需要通过复杂的二阶优化方法求解，计算成本高。
    *   **PPO（尝试3）**：目标是保留TRPO的稳定性，同时使其像策略梯度一样简单。它放弃了复杂的约束条件，转而采用一种直观的“裁剪”机制来自然地限制策略的变化幅度。

2.  **PPO的核心创新：裁剪概率比**
    *   PPO的核心是那个包含 $min$ 和 $clip$ 函数的损失函数。
    *   **核心思想**：在更新时，确保新策略（π_θ）与旧策略（π_θk）的差异不会太大。它通过**限制概率比**（新策略概率/旧策略概率）在一个区间 $[1-ε, 1+ε]$ 内来实现这一点。
    *   **工作原理**：
        *   如果某个动作的优势函数（A）为正（即这是个好动作），我们鼓励新策略更多地选择它，但概率比的增长上限被裁剪在 $1+ε$。
        *   如果优势函数为负（即这是个坏动作），我们减少选择它的概率，但概率比的下降下限被裁剪在 $1-ε$。
    *   **效果**：这确保了每次策略更新都是一个“小而安全”的步骤，从而保证了训练的稳定性和可靠性。

3.  **核心价值：在效果和效率间取得绝佳平衡**
    PPO的成功在于它用一种非常简洁的方法，近似实现了TRPO的信任区域效果。它避免了复杂的计算，使得算法更易于实现、调试和扩展，从而成为深度强化学习领域最流行、最通用的算法之一。

---

### 打印公式

以下是幻灯片中出现的三个关键公式的清晰呈现：

#### 尝试1：策略梯度法
**问题：方差过高**
$$ \nabla_{\theta} E_{p_{\theta}}[R(z)]=E_{p_{\theta}}\left[R(z) \nabla_{\theta} \log p_{\theta}(z)\right] $$

#### 尝试2：TRPO
**方法：线性化问题并施加约束**
$$
\begin{array}{ll}
\underset{\theta}{\operatorname{maximize}} & \hat{\mathbb{E}}_{t}\left[\frac{\pi_{\theta}\left(a_{t} \mid s_{t}\right)}{\pi_{\theta_{\text {old }}}\left(a_{t} \mid s_{t}\right)} \hat{A}_{t}\right] \\
\text { subject to } & \hat{\mathbb{E}}_{t}\left[\mathrm{KL}\left[\pi_{\theta_{\text {old }}}\left(\cdot \mid s_{t}\right), \pi_{\theta}(\cdot \mid s_{t}\right)\right] \leq \delta .
\end{array}
$$

#### 尝试3：PPO（最终方案）
**方法：将概率比裁剪在某个ε值附近**
$$ L\left(s, a, \theta_{k}, \theta\right)=\min \left(\frac{\pi_{\theta}(a \mid s)}{\pi_{\theta_{k}}(a \mid s)} A^{\pi_{\theta_{k}}}(s, a), \operatorname{clip}\left(\frac{\pi_{\theta}(a \mid s)}{\pi_{\theta_{k}}(a \mid s)}, 1-\epsilon, 1+\epsilon\right) A^{\pi_{\theta_{k}}}(s, a)\right) $$

## 5.2 Can we get rid of PPO?

### 内容概况

这张幻灯片提出了一个在大型语言模型对齐研究领域至关重要且备受关注的问题：**我们能否避免使用复杂的策略强化学习算法（如PPO），而采用更简单的方法来实现模型与人类偏好的对齐？**

---

### 要点总结

#### 1. 核心问题：寻求RLHF的简化替代方案
幻灯片的核心是探索绕过PPO的可行性。PPO作为一种在线强化学习算法，以其实现复杂、训练不稳定和计算成本高而闻名。因此，社区一直在寻找更简单、更稳健的替代路径。

#### 2. 列举的四种替代方案及其逻辑
幻灯片列出了四种具有代表性的思路：

1.  **使用控制令牌进行训练**
    *   **方法**：在偏好数据对的“优选回答”前加上$[GOOD]$令牌，在“劣选回答”前加上$[BAD]$令牌，然后进行标准的监督微调。
    *   **逻辑**：让模型通过上下文学习，理解“好”与“坏”回答的区别。

2.  **仅对优选回答进行监督微调**
    *   **方法**：直接丢弃偏好数据中的劣选回答，只使用优选回答来微调模型。
    *   **逻辑**：假设只让模型学习“好”的示范，它就能学会只生成好的输出。

3.  **奖励模型筛选后训练**
    *   **方法**：先训练一个奖励模型，然后用基础模型生成一些回答，用奖励模型挑选出得分最高的回答，最后用这些“最优”回答对模型进行监督微调。
    *   **逻辑**：用奖励模型自动识别高质量数据，然后用这些数据“教”模型变得更好。

4.  **奖励模型筛选最优样本**
    *   **方法**：是上一种方案的极端版本。从大量（如1024个）模型输出中，直接用奖励模型选出最好的一个。
    *   **逻辑**：通过海量采样和严格筛选，找到“最优解”来训练模型。

#### 3. 深层含义与未言明的结论
尽管幻灯片没有直接给出答案，但通过提出这个问题并列举这些方案，暗示了以下深层要点：

*   **这些简单方法存在局限性**：如果这些方法足够有效，PPO可能早就被替代了。它们的缺陷可能包括：
    *   **方法1、2**：无法让模型学会在生成过程中进行**精细的比较和修正**，模型能力可能很快达到天花板。
    *   **方法3、4**：存在**模型自我抄袭**和**泛化性不足**的风险。模型只是在模仿自己已有的输出，难以实现真正的突破和优化。
*   **PPO的不可替代性**：PPO的核心优势在于它是一种**在线、迭代的优化过程**。模型在PPO中会主动探索新的回答策略，并根据奖励信号进行微调，而不仅仅是模仿静态数据。这种“探索-利用”机制对于充分挖掘模型潜力可能至关重要。
*   **引出更优方案**：这张幻灯片很可能是一个“引子”，旨在引出后续更先进的替代方案（例如，**DPO** 或**IPO** 等算法），这些算法试图在保持性能的同时，简化训练流程。

## 6.0 DPO – RLHF without tears?

### 内容概况

这张幻灯片清晰地介绍了一种名为**直接偏好优化** 的创新方法，并将其与传统的**基于人类反馈的强化学习** 流程进行了直观对比。

幻灯片的核心论点在于：**DPO 旨在实现与 RLHF 相同的目标（使模型输出与人类偏好对齐），但通过一个极其简化的数学转换，避免了 RLHF（特别是其 PPO 实现）的复杂性和训练不稳定性。** 它通过左右并列的流程图，一目了然地展示了两者在流程上的根本差异。

---

### 要点总结

#### 1. 核心目标：简化对齐过程，实现“无痛”RLHF
幻灯片标题“DPO – RLHF without tears?” 开宗明义。这里的“tears”指代的是传统RLHF-PPO pipeline的几大痛点：
*   **实现复杂**：需要同时维护和协调策略模型、奖励模型和价值函数模型。
*   **训练不稳定**：PPO算法涉及在线采样、多轮迭代和超参数敏感性问题。
*   **计算成本高**：需要大量的采样和梯度更新步骤。

DPO的目标就是**消除这些“泪水”**，提供一个更简单、更稳定的替代方案。

#### 2. 核心方法：绕过奖励模型，直接优化偏好
幻灯片通过对比两个流程图，清晰地指出了DPO的革命性思路：

*   **传统RLHF（左侧流程图）**:
    1.  **步骤1**：用人类偏好数据训练一个**奖励模型**，让其学会区分回答的好坏。
    2.  **步骤2**：使用强化学习算法（如PPO），以该奖励模型作为指导信号，去优化语言模型策略。这个过程需要**在线采样**（让当前模型生成回答）并迭代优化。
    *   **本质**：一个**两步间接**过程。`偏好数据 -> 奖励模型 -> RL优化 -> 最终模型`

*   **DPO（右侧流程图）**:
    1.  **步骤**：直接利用人类偏好数据，通过一个精心设计的**最大似然目标函数**来微调语言模型本身。
    *   **本质**：一个**一步直接**过程。`偏好数据 -> 最大似然优化 -> 最终模型`

#### 3. 技术精髓：将RL问题转化为损失函数问题
幻灯片中的文字“Take gradient steps on log-loss of good stuff, take negative gradient steps on bad stuff” 揭示了DPO的数学精髓。

*   DPO通过一个巧妙的数学变换，证明了**基于奖励模型的RL优化问题，可以等价地转化为一个仅针对偏好数据的分类问题**。
*   具体来说，它设计了一个损失函数，该函数会：
    *   对**优选回答**，增加其生成概率（正梯度步）。
    *   对**劣选回答**，降低其生成概率（负梯度步）。
*   这个损失函数内部隐含地实现了“偏好排序”，从而**无需再显式地训练一个独立的奖励模型**，也完全避免了复杂的强化学习过程。

### 总结

总而言之，这张幻灯片精辟地阐述了DPO的核心价值：**它通过一个数学上的“捷径”，将复杂、不稳定的强化学习对齐问题，优雅地简化为了一个稳定、直观的监督学习问题。** 这种方法大幅降低了技术门槛和计算成本，使更多研究者能够更容易地参与到大模型的对齐研究中，成为近年来该领域一项重要的技术进步。

## 6.1 DPO – derivation from the RLHF formula

### 内容概况

这两张图片从不同角度、分步骤地展示了**DPO算法如何从传统的RLHF目标推导而来**。它们共同构成了一个完整的逻辑链条，揭示了DPO的核心创新：**通过一个巧妙的数学变换，将复杂的强化学习问题转化为一个简单的监督学习问题。**

*   **图1：理论基础与逆向推导**
    *   从RLHF的标准优化目标（带KL约束的奖励最大化）出发。
    *   在**非参数假设**下，直接给出了该优化问题的最优解（策略π）与奖励函数（r）和参考模型（π_ref）之间的**解析关系**。
    *   然后，通过**逆向求解**，得到了用最优策略和参考模型表示的**隐含奖励函数**。这一步是连接RLHF和DPO的关键桥梁。

*   **图2：目标构建与算法实现**
    *   承接图1的结论，将得到的“隐含奖励”函数代入到另一个用于学习奖励模型的**偏好损失函数**中。
    *   通过代入和简化，**神奇地消去了奖励模型（r）和难以处理的配分函数（Z(x)）**，得到了一个**可以直接用策略模型（π_θ）参数化和优化**的最终损失函数——即DPO目标。

---

### 要点总结

#### 1. 起点：RLHF的固有复杂性
RLHF的目标是最大化奖励，同时控制模型不要偏离初始参考模型太远（通过KL散度惩罚）。这通常需要一个**两步流程**：
1.  训练一个奖励模型来拟合人类偏好。
2.  使用强化学习（如PPO）来优化策略，以最大化该奖励模型给出的分数。

这个过程复杂且不稳定。

#### 2. 关键洞见：建立策略与奖励的直接联系
DPO的核心洞见来自于图1中的**非参数假设**和**逆向求解**。它发现，对于给定的RLHF目标，其最优策略π_r和奖励函数r之间存在一个**精确的、可逆的数学关系**（即图1中的两个公式）。这意味着，**我们可以用策略来唯一地确定奖励函数**。

#### 3. 核心创新：用策略参数化奖励，从而绕过RL
图2展示了DPO的巧妙之处：
*   它没有显式地学习一个奖励模型，而是**将策略模型本身作为奖励函数的替代参数化方式**。
*   当把这个替代关系代入到偏好学习的目标函数中时，所有难以处理的项（如配分函数Z(x)）都相互抵消了。
*   最终结果是一个**仅依赖于策略模型π_θ和参考模型π_ref**的损失函数。优化这个损失函数，就等价于在优化原始的RLHF目标。

#### 4. 最终成果：一个简洁有效的监督学习目标
最终的DPO目标函数是一个标准的**二元交叉熵损失**。它直接学习区分“优选回答”和“劣选回答”，其“逻辑”来自于两个策略概率比的对数之差。这使得模型对齐训练变得像监督微调一样简单和稳定。

### 打印公式

以下是两张图片中出现的所有核心公式的清晰呈现：

#### 图1中的公式

1.  **RLHF 优化目标**：
    $$ \max _{\pi_{\theta}} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_{\theta}(y \mid x)}\left[r_{\phi}(x, y)\right]-\beta \mathbb{D}_{\mathrm{KL}}\left[\pi_{\theta}(y \mid x) \| \pi_{\text {ref }}(y \mid x)\right] $$

2.  **最优策略的解析解（在非参数假设下）**：
    $$ \pi_{r}(y \mid x)=\frac{1}{Z(x)} \pi_{\text {ref }}(y \mid x) \exp \left(\frac{1}{\beta} r(x, y)\right) $$
    *其中 $ Z(x) $ 是配分函数，用于归一化。*

3.  **逆向求解得到的“隐含奖励”**：
    $$ r(x, y)=\beta \log \frac{\pi_{r}(y \mid x)}{\pi_{\text {ref }}(y \mid x)}+\beta \log Z(x) $$

#### 图2中的公式

1.  **偏好建模的损失函数**：
    $$ \operatorname{loss}\left(r_{\phi}\right)=-\mathbb{E}_{\left(x, y_{w}, y_{l}\right) \sim D}\left[\log \sigma\left(r_{\phi}\left(x, y_{w}\right)-r_{\phi}\left(x, y_{l}\right)\right)\right] $$

2.  **将“隐含奖励”代入上述损失函数，并消元后得到的DPO最终目标**：
    $$ \mathcal{L}_{\mathrm{DPO}}\left(\pi_{\theta} ; \pi_{\mathrm{ref}}\right)=-\mathbb{E}_{\left(x, y_{w}, y_{l}\right) \sim \mathcal{D}}\left[\log \sigma\left(\beta \log \frac{\pi_{\theta}\left(y_{w} \mid x\right)}{\pi_{\mathrm{ref}}\left(y_{w} \mid x\right)}-\beta \log \frac{\pi_{\theta}\left(y_{l} \mid x\right)}{\pi_{\mathrm{ref}}\left(y_{l} \mid x\right)}\right)\right] $$

### 总结
总而言之，这两张图清晰地揭示了DPO的本质：**它通过数学上的深刻洞察，将策略模型和奖励模型的关系显式化，从而把复杂的强化学习问题“降维”成了一个优雅且高效的监督学习问题。** 这是大模型对齐领域的一项重大进展。

## 6.2 DPO解析解推导过程

**最优策略的解析解**是从RLHF的原始优化目标中，通过**变分法**或**拉格朗日乘子法**推导出来的。其核心在于做了一个关键的 **“非参数假设”**。

### 第1步：RLHF的优化目标

首先，我们有一个明确的优化目标（图片1中的第一个公式）：

$$
\max _{\pi_{\theta}} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_{\theta}(y \mid x)}\left[r_{\phi}(x, y)\right]-\beta \mathbb{D}_{\mathrm{KL}}\left[\pi_{\theta}(y \mid x) \| \pi_{\text {ref }}(y \mid x)\right]
$$

这个目标可以改写为期望形式：
$$
\max _{\pi} \sum_{x} P(x) \sum_{y} \pi(y|x) \left[ r(x, y) - \beta \log \frac{\pi(y|x)}{\pi_{\text{ref}}(y|x)} \right]
$$

**约束条件**是：对于任何一个给定的输入 $ x $，策略 $ \pi(\cdot | x) $ 必须是一个有效的概率分布。即：
$$
\sum_{y} \pi(y|x) = 1 \quad \text{（对于所有x）}, \quad \text{且} \quad \pi(y|x) \ge 0
$$

### 第2步：关键假设——“非参数假设”

幻灯片中明确指出：“Assume that the policy π is the set of all policies (nonparametric assumption)”。

*   **“参数化”策略**：通常我们假设策略 $ \pi_\theta $ 是一个神经网络，其输出由参数 $ \theta $ 决定。优化是在参数空间 $ \theta $ 上进行，这个问题非常复杂，通常需要PPO这样的迭代算法。
*   **“非参数”策略**：这里我们做一个思想实验。我们暂时**抛开神经网络的约束**，假设策略 $ \pi(\cdot | x) $ 可以是**任意一个**在输出空间 $ y $ 上的概率分布。也就是说，对于每一个 $ x $，我们把 $ \pi(y|x) $ 本身（即每个可能输出 $ y $ 的概率）都当作可以自由优化的变量。

这个假设将问题从一个在高维参数空间上的约束优化，简化为了一个在**概率单纯形**上的、为每个 $ x $ 独立进行的约束优化。这使得我们可以使用经典的优化理论直接求解。

### 第3步：构建拉格朗日函数并求解

对于**每一个固定的 $ x $**，我们的优化问题变为：
$$
\max _{\pi(\cdot | x)} \sum_{y} \pi(y|x) \left[ r(x, y) - \beta \log \pi(y|x) + \beta \log \pi_{\text{ref}}(y|x) \right]
$$
**约束条件为：** $ \sum_{y} \pi(y|x) = 1 $。

我们引入拉格朗日乘子 $ \lambda $ 来处理求和为1的约束，构建拉格朗日函数 $ \mathcal{L} $：
$$
\mathcal{L}(\pi, \lambda) = \sum_{y} \pi(y|x) \left[ r(x, y) - \beta \log \pi(y|x) + \beta \log \pi_{\text{ref}}(y|x) \right] + \lambda \left( 1 - \sum_{y} \pi(y|x) \right)
$$

为了找到极大值，我们对变量 $ \pi(y|x) $ 求偏导数并令其为零：
$$
\frac{\partial \mathcal{L}}{\partial \pi(y|x)} = r(x, y) - \beta \log \pi(y|x) - \beta + \beta \log \pi_{\text{ref}}(y|x) - \lambda = 0
$$

整理上式：
$$
r(x, y) - \beta + \beta \log \pi_{\text{ref}}(y|x) - \lambda = \beta \log \pi(y|x)
$$

两边取指数：
$$
\pi(y|x) = \exp\left( \frac{r(x, y) - \beta - \lambda}{\beta} \right) \cdot \pi_{\text{ref}}(y|x)
$$

我们可以将常数项合并。令 $ \frac{-\beta - \lambda}{\beta} = C $，则：
$$
\pi(y|x) = \pi_{\text{ref}}(y|x) \cdot \exp\left( \frac{r(x, y)}{\beta} + C \right) = \frac{1}{Z(x)} \pi_{\text{ref}}(y|x) \cdot \exp\left( \frac{1}{\beta} r(x, y) \right)
$$

这里，为了满足概率归一化约束 $ \sum_y \pi(y|x) = 1 $，我们必须引入一个**配分函数（Partition Function）** $ Z(x) $ 来对所有的 $ y $ 进行归一化：
$$
Z(x) = \sum_y \pi_{\text{ref}}(y|x) \cdot \exp\left( \frac{1}{\beta} r(x, y) \right)
$$

### 最终结果

这样，我们就推导出了图片中给出的**最优策略的解析解**：
$$
\boxed{\pi_{r}(y\mid x)=\frac{1}{Z(x)}\pi_{\text {ref}}(y\mid x)\exp\left(\frac{1}{\beta} r(x,y)\right)}
$$

### 总结与直观理解

这个推导过程的关键在于：

1.  **非参数假设**：暂时忽略策略的参数化形式，将问题简化为一个更易处理的凸优化问题。
2.  **拉格朗日法**：通过引入拉格朗日乘子，将有约束的优化问题转化为无约束问题求解。
3.  **归一化**：解的形式天然地包含了一个配分函数 $ Z(x) $，以确保结果是有效的概率分布。

这个公式有非常直观的意义：**最优策略 $ \pi_r $ 正比于参考策略 $ \pi_{\text{ref}} $ 乘以一个指数化的奖励项**。系数 $ \beta $ 控制着偏离参考策略的“代价”：
*   当 $ \beta $ 很大时，策略会紧密跟随 $ \pi_{\text{ref}} $。
*   当 $ \beta $ 很小时，策略会强烈倾向于选择高奖励的动作。

这个优美的解析解正是DPO算法能够“绕过”强化学习的基石。

## 6.3 DPO updates and components

### 第1步：回顾DPO损失函数

$$
\mathcal{L}_{\mathrm{DPO}}(\pi_{\theta} ; \pi_{\mathrm{ref}}) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}}\left[\log \sigma\left(\beta \log \frac{\pi_{\theta}(y_w \mid x)}{\pi_{\mathrm{ref}}(y_w \mid x)} - \beta \log \frac{\pi_{\theta}(y_l \mid x)}{\pi_{\mathrm{ref}}(y_l \mid x)}\right)\right]
$$

为了简化推导，我们定义两个关键量：

1.  **隐含奖励**： $\hat{r}_{\theta}(x, y) = \beta \log \frac{\pi_{\theta}(y \mid x)}{\pi_{\mathrm{ref}}(y \mid x)}$
2.  **奖励差**：我们令 $ u = \hat{r}_{\theta}(x, y_w) - \hat{r}_{\theta}(x, y_l) $

代入后，损失函数简化为：
$$
\mathcal{L}_{\mathrm{DPO}} = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}}\left[\log \sigma(u)\right]
$$

我们的目标是计算这个损失函数关于参数 $\theta$ 的梯度：$\nabla_{\theta}\mathcal{L}_{\mathrm{DPO}}$。

---

### 第2步：计算梯度（运用链式法则）

我们对期望内部的损失项进行求导。这是一个标量对向量的求导，应用链式法则：

$$
\nabla_{\theta}\left[ -\log \sigma(u) \right] = - \frac{1}{\sigma(u)} \cdot \nabla_{\theta} \sigma(u)
$$

接下来，我们对 $\sigma(u)$ 求导。回忆一下，sigmoid函数 $\sigma(u)$ 的导数有很好的性质：$\frac{d}{du}\sigma(u) = \sigma(u)(1-\sigma(u))$。再次应用链式法则：

$$
\nabla_{\theta} \sigma(u) = \frac{d \sigma(u)}{d u} \cdot \nabla_{\theta} u = \sigma(u)(1-\sigma(u)) \cdot \nabla_{\theta} u
$$

现在我们将两部分组合起来：
$$
\nabla_{\theta}\left[ -\log \sigma(u) \right] = - \frac{1}{\sigma(u)} \cdot \left[ \sigma(u)(1-\sigma(u)) \cdot \nabla_{\theta} u \right] = - (1-\sigma(u)) \cdot \nabla_{\theta} u
$$

利用sigmoid函数的性质 $1 - \sigma(u) = \sigma(-u)$，我们可以写成：
$$
\nabla_{\theta}\left[ -\log \sigma(u) \right] = - \sigma(-u) \cdot \nabla_{\theta} u
$$

现在，我们恢复 $u$ 的定义：$ u = \hat{r}_{\theta}(x, y_w) - \hat{r}_{\theta}(x, y_l) $，所以 $-u = \hat{r}_{\theta}(x, y_l) - \hat{r}_{\theta}(x, y_w)$。因此：
$$
\nabla_{\theta}\left[ -\log \sigma(u) \right] = - \sigma(\hat{r}_{\theta}(x, y_l) - \hat{r}_{\theta}(x, y_w)) \cdot \nabla_{\theta} \left[ \hat{r}_{\theta}(x, y_w) - \hat{r}_{\theta}(x, y_l) \right]
$$

---

### 第3步：展开奖励差项的梯度

我们展开括号内的梯度项：
$$
\nabla_{\theta} \left[ \hat{r}_{\theta}(x, y_w) - \hat{r}_{\theta}(x, y_l) \right] = \nabla_{\theta} \hat{r}_{\theta}(x, y_w) - \nabla_{\theta} \hat{r}_{\theta}(x, y_l)
$$

现在，我们计算 $\nabla_{\theta} \hat{r}_{\theta}(x, y)$。代入隐含奖励的定义：
$$
\hat{r}_{\theta}(x, y) = \beta \log \pi_{\theta}(y \mid x) - \beta \log \pi_{\mathrm{ref}}(y \mid x)
$$

注意，第二项 $\log \pi_{\mathrm{ref}}(y \mid x)$ 与 $\theta$ 无关，其梯度为零。因此：
$$
\nabla_{\theta} \hat{r}_{\theta}(x, y) = \beta \nabla_{\theta} \log \pi_{\theta}(y \mid x)
$$

这是一个非常重要的结果：**隐含奖励的梯度，正比于策略的对数梯度**。

将其代入回原式：
$$
\nabla_{\theta} \left[ \hat{r}_{\theta}(x, y_w) - \hat{r}_{\theta}(x, y_l) \right] = \beta \left( \nabla_{\theta} \log \pi_{\theta}(y_w \mid x) - \nabla_{\theta} \log \pi_{\theta}(y_l \mid x) \right)
$$

---

### 第4步：得到最终梯度公式

现在，我们将所有结果组合起来，并加上外部的期望：

$$
\begin{align*}
\nabla_{\theta}\mathcal{L}_{\mathrm{DPO}} &= \nabla_{\theta} \left[ -\mathbb{E}[\log \sigma(u)] \right] \\
&= -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \sigma(\hat{r}_{\theta}(x, y_l) - \hat{r}_{\theta}(x, y_w)) \cdot \beta \left( \nabla_{\theta} \log \pi_{\theta}(y_w \mid x) - \nabla_{\theta} \log \pi_{\theta}(y_l \mid x) \right) \right]
\end{align*}
$$

整理一下常数项 $\beta$ 的位置，就得到了幻灯片中展示的最终梯度公式：

$$
\boxed{
\begin{align*}
&\nabla_{\theta}\mathcal{L}_{\mathrm{DPO}}(\pi_{\theta};\pi_{\mathrm{ref}})=\\
&-\beta\mathbb{E}_{(x,y_{w},y_{l})\sim\mathcal{D}}[\underbrace{\sigma(\hat{r}_{\theta}(x,y_{l})-\hat{r}_{\theta}(x,y_{w}))}_{\text{权重项}}[\underbrace{\nabla_{\theta}\log\pi(y_{w}\mid x)}_{\text{增加}y_w\text{似然}}-\underbrace{\nabla_{\theta}\log\pi(y_{l}\mid x)}_{\text{减少}y_l\text{似然}}]]
\end{align*}
}
$$

### 推导总结

这个推导过程的核心步骤是：
1.  **从DPO损失函数出发**：这是已知的结论。
2.  **链式法则**：将损失函数分解为 $log$、$sigmoid$ 和 $u$ 的组合，逐层求导。
3.  **利用sigmoid函数的导数性质**：将梯度简化为 $σ(-u)$ 的形式。
4.  **计算奖励差的梯度**：关键一步是发现隐含奖励的梯度正比于策略的对数梯度。
5.  **合并结果**：将所有项组合，得到清晰直观的最终形式。

## 6.4 Variants

### 一、内容概况

本部分介绍了两种旨在**进一步简化流程或解决特定问题**的DPO变体。

---

### 要点总结

#### 1. 核心趋势：对标准DPO的改进
标准DPO已经大大简化了RLHF，但这些变体试图在其基础上进一步优化，主要围绕两个方向：
*   **简化架构**：能否去掉对参考模型（$π_ref$）的依赖？（SimPO的目标）
*   **解决偏差**：如何消除模型输出中对“长回答”的固有偏好？（长度归一化DPO的目标）

#### 2. 变体一：SimPO - 无参考模型的DPO
SimPO的核心思想是**完全移除对参考模型（π_ref）的依赖**。

*   **动机**：
    *   **节省内存与计算**：在训练过程中不需要同时加载和前向传播参考模型，显著降低了显存占用和计算成本。
    *   **简化流程**：不再需要维护一个固定的参考模型。
*   **方法**：对比标准DPO和SimPO的公式：
    *   **标准DPO**：比较的是**策略概率与参考模型概率的比值**（即隐含奖励）。
    *   **SimPO**：直接比较**策略模型对优选回答和劣选回答的对数概率本身**。
    *   **关键修改**：
        1.  **移除参考模型**：公式中不再有 $π_ref$ 项。
        2.  **长度归一化**：对每个回答的对数概率都除以其长度（$|y_w|$ 和 $|y_l|$），以消除长度偏差。
        3.  **引入目标奖励间隔（γ）**：一个常数项，用于明确区分优选和劣选回答的奖励差值，确保模型学习到足够的区分度。

#### 3. 变体二：长度归一化DPO
该变体专注于解决一个具体问题：**模型可能仅仅因为生成了更长的回答，就在偏好判断中获得优势**，因为这长回答更可能包含关键信息。

*   **动机**：确保偏好判断是基于回答的**质量/单位长度**，而不是单纯的总长度。
*   **方法**：在标准DPO的隐含奖励计算中，对每一项都进行**长度归一化**。即，将 $β log π_θ(y|x)/π_ref(y|x)$ 变为 $(β / |y|) log π_θ(y|x)/π_ref(y|x)$。
*   **效果**：这使模型在学习和判断时，更关注回答的“信息密度”和效率，而不是倾向于机械地生成长篇大论。

---

### 打印公式

#### 1. 标准 DPO 损失函数 (作为对比基准)
$
\mathcal{L}_{\mathrm{DPO}}(\pi_{\theta} ; \pi_{\mathrm{ref}}) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}}\left[\log \sigma\left(\beta \log \frac{\pi_{\theta}(y_{w} \mid x)}{\pi_{\mathrm{ref}}(y_{w} \mid x)} - \beta \log \frac{\pi_{\theta}(y_{l} \mid x)}{\pi_{\mathrm{ref}}(y_{l} \mid x)}\right)\right]
$

#### 2. SimPO 损失函数 (无参考模型)
$
\mathcal{L}_{\mathrm{SimPO}}(\pi_{\theta}) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}}\left[\log \sigma\left(\frac{\beta}{|y_{w}|} \log \pi_{\theta}(y_{w} \mid x) - \frac{\beta}{|y_{l}|} \log \pi_{\theta}(y_{l} \mid x) - \gamma\right)\right]
$

#### 3. 长度归一化 DPO 目标函数
$
\max _{\pi_{\theta}} \mathbb{E}_{(x, y_c, y_r) \sim \mathcal{D}}\left[\log \sigma\left(\frac{\beta}{|y_{c}|} \log \frac{\pi_{\theta}(y_{c} \mid x)}{\pi_{\mathrm{ref}}(y_{c} \mid x)} - \frac{\beta}{|y_{r}|} \log \frac{\pi_{\theta}(y_{r} \mid x)}{\pi_{\mathrm{ref}}(y_{r} \mid x)}\right)\right]
$

**公式中符号说明**：
*   $x$: 输入（提示）
*   $y_w$, $y_c$: 优选回答
*   $y_l$, $y_r$: 劣选回答
*   $π_θ$: 待优化的策略模型
*   $π_ref$: 参考模型（在SimPO中被移除）
*   $β$: 控制奖励最大化与参考模型约束之间平衡的超参数
*   $γ$: SimPO中引入的目标奖励间隔超参数
*   $|y|$: 回答 $y$ 的长度（例如，以token数量计）
*   $σ$: Sigmoid函数

### 总结
这张幻灯片表明，DPO的成功激发了一系列改进研究。**SimPO** 代表了向更简洁、更高效架构的大胆探索，而**长度归一化DPO** 则体现了对模型偏差（如长度偏差）进行精细修正的努力。这些变体共同推动了模型对齐技术向更实用、更鲁棒的方向发展。
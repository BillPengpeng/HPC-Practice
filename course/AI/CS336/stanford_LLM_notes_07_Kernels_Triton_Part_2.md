æœ¬æ–‡ä¸»è¦æ•´ç†CS336 Kernels, Tritonç« èŠ‚çš„ä¸»è¦å†…å®¹ã€‚

## 5 - cuda_gelu

```c
#include <math.h>
#include <torch/extension.h>
#include <c10/cuda/CUDAException.h>

__global__ void gelu_kernel(float* in, float* out, int num_elements) {
    // Get the index into the tensor
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    if (i < num_elements) {  // To handle the case when n < numBlocks * blockDim
        // Do the actual computation
        out[i] = 0.5 * in[i] * (1.0 + tanh(0.79788456 * (in[i] + 0.044715 * in[i] * in[i] * in[i])));
    }
}

inline unsigned int cdiv(unsigned int a, unsigned int b) {
    // Compute ceil(a / b)
    return (a + b - 1) / b;
}

torch::Tensor gelu(torch::Tensor x) {
    TORCH_CHECK(x.device().is_cuda());
    TORCH_CHECK(x.is_contiguous());

    // Allocate empty tensor
    torch::Tensor y = torch::empty_like(x);

    // Determine grid (elements divided into blocks)
    int num_elements = x.numel();
    int block_size = 1024;  // Number of threads
    int num_blocks = cdiv(num_elements, block_size);

    // Launch the kernel
    gelu_kernel<<<num_blocks, block_size>>>(x.data_ptr<float>(), y.data_ptr<float>(), num_elements);
    C10_CUDA_KERNEL_LAUNCH_CHECK();  // Catch errors immediately

    return y;
}
```

```python
text("Set CUDA_LAUNCH_BLOCKING so that if there are errors, CUDA will tell you what went wrong.")
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

text("The `load_inline` function makes it convenient to write CUDA code and bind it to a Python module for immediate use.")

# CUDA code: has the full logic
cuda_gelu_src = open("gelu.cu").read()
text(cuda_gelu_src, verbatim=True)

# C++ code: defines the gelu function
cpp_gelu_src = "torch::Tensor gelu(torch::Tensor x);"

text("Compile the CUDA code and bind it to a Python module.")
ensure_directory_exists("var/cuda_gelu")
if not torch.cuda.is_available():
    return None
module = load_inline(
    cuda_sources=[cuda_gelu_src],
    cpp_sources=[cpp_gelu_src],
    functions=["gelu"],
    extra_cflags=["-O2"],
    verbose=True,
    name="inline_gelu",
    build_directory="var/cuda_gelu",
)

cuda_gelu = getattr(module, "gelu")
return cuda_gelu
```

### æ•´ä½“æ¶æ„

è¿™æ˜¯ä¸€ä¸ªä½¿ç”¨ PyTorch C++/CUDA æ‰©å±•å®ç°çš„ **è‡ªå®šä¹‰ GELU æ¿€æ´»å‡½æ•°**ï¼ŒåŒ…å«ï¼š
1. **CUDA å†…æ ¸** (`gelu_kernel`)
2. **C++ åŒ…è£…å‡½æ•°** (`gelu`)
3. **Python ç¼–è¯‘åŠ è½½ä»£ç **

### æ ¸å¿ƒä»£ç åˆ†æ

#### 1. CUDA å†…æ ¸å®ç°
```cpp
__global__ void gelu_kernel(float* in, float* out, int num_elements) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;  // è®¡ç®—å…¨å±€ç´¢å¼•

    if (i < num_elements) {  // è¾¹ç•Œæ£€æŸ¥
        // GELU è¿‘ä¼¼å…¬å¼è®¡ç®—
        out[i] = 0.5 * in[i] * (1.0 + tanh(0.79788456 * 
                (in[i] + 0.044715 * in[i] * in[i] * in[i])));
    }
}
```

**å…³é”®æŠ€æœ¯ç‚¹**ï¼š
- **`__global__`**: å£°æ˜ä¸ºCUDAæ ¸å‡½æ•°
- **çº¿ç¨‹ç´¢å¼•è®¡ç®—**: æ ‡å‡†CUDAå¹¶è¡Œæ¨¡å¼
- **è¾¹ç•Œæ£€æŸ¥**: é˜²æ­¢è¶Šç•Œè®¿é—®
- **GELUè¿‘ä¼¼å…¬å¼**: ä½¿ç”¨tanhè¿‘ä¼¼ï¼Œæ¯”ç²¾ç¡®è®¡ç®—æ›´é«˜æ•ˆ

#### 2. C++ åŒ…è£…å‡½æ•°
```cpp
torch::Tensor gelu(torch::Tensor x) {
    TORCH_CHECK(x.device().is_cuda());      // æ£€æŸ¥è®¾å¤‡æ˜¯å¦ä¸ºCUDA
    TORCH_CHECK(x.is_contiguous());         // æ£€æŸ¥å†…å­˜è¿ç»­æ€§

    torch::Tensor y = torch::empty_like(x); // åˆ†é…è¾“å‡ºå¼ é‡

    int num_elements = x.numel();
    int block_size = 1024;                  // æ¯ä¸ªblock 1024çº¿ç¨‹
    int num_blocks = cdiv(num_elements, block_size);  // è®¡ç®—blockæ•°é‡

    // å¯åŠ¨æ ¸å‡½æ•°
    gelu_kernel<<<num_blocks, block_size>>>(x.data_ptr<float>(), 
                                           y.data_ptr<float>(), 
                                           num_elements);
    C10_CUDA_KERNEL_LAUNCH_CHECK();         // é”™è¯¯æ£€æŸ¥

    return y;
}
```

**å…³é”®å‡½æ•°**ï¼š
- **`cdiv(a, b)`**: è®¡ç®— `ceil(a/b)`ï¼Œç¡®ä¿è¦†ç›–æ‰€æœ‰å…ƒç´ 
- **`<<<num_blocks, block_size>>>`**: CUDAæ ¸å‡½æ•°å¯åŠ¨è¯­æ³•
- **`C10_CUDA_KERNEL_LAUNCH_CHECK()`**: PyTorchæä¾›çš„CUDAé”™è¯¯æ£€æŸ¥

#### 3. Pythonç«¯ç¼–è¯‘åŠ è½½
```python
# è®¾ç½®ç¯å¢ƒå˜é‡ï¼šåŒæ­¥CUDAé”™è¯¯æŠ¥å‘Š
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

# è¯»å–CUDAå’ŒC++æºç 
cuda_gelu_src = open("gelu.cu").read()
cpp_gelu_src = "torch::Tensor gelu(torch::Tensor x);"

# ä½¿ç”¨PyTorchçš„inlineç¼–è¯‘
module = load_inline(
    cuda_sources=[cuda_gelu_src],
    cpp_sources=[cpp_gelu_src],
    functions=["gelu"],
    extra_cflags=["-O2"],  # ä¼˜åŒ–çº§åˆ«
    verbose=True,
    name="inline_gelu",
    build_directory="var/cuda_gelu",
)
```

### æ€§èƒ½ä¼˜åŒ–ç‰¹æ€§

#### 1. å†…å­˜è®¿é—®ä¼˜åŒ–
```cpp
TORCH_CHECK(x.is_contiguous());  // ç¡®ä¿å†…å­˜è¿ç»­ï¼Œæé«˜è®¿é—®æ•ˆç‡
```

#### 2. å¹¶è¡Œè®¡ç®—ç­–ç•¥
```cpp
int block_size = 1024;  // æœ€ä¼˜çš„blockå¤§å°ï¼ˆç»éªŒå€¼ï¼‰
int num_blocks = cdiv(num_elements, block_size);  // åŠ¨æ€è®¡ç®—blockæ•°é‡
```

#### 3. é”™è¯¯å¤„ç†æœºåˆ¶
```cpp
C10_CUDA_KERNEL_LAUNCH_CHECK();  // ç«‹å³æ•è·CUDAé”™è¯¯
```

### æ•°å­¦å…¬å¼è§£æ

GELU (Gaussian Error Linear Unit) è¿‘ä¼¼å…¬å¼ï¼š
```
0.5 * x * (1 + tanh(âˆš(2/Ï€) * (x + 0.044715 * xÂ³)))
```
å…¶ä¸­ `âˆš(2/Ï€) â‰ˆ 0.79788456`

### ä¸PyTorchå†…ç½®GELUçš„å¯¹æ¯”

| ç‰¹æ€§ | è‡ªå®šä¹‰CUDA GELU | PyTorchå†…ç½®GELU |
|------|----------------|-----------------|
| **æ€§èƒ½** | å¯èƒ½æ›´å¿«ï¼ˆä¼˜åŒ–å®ç°ï¼‰ | é€šç”¨å®ç° |
| **æ§åˆ¶åº¦** | å®Œå…¨æ§åˆ¶ç®—æ³•ç»†èŠ‚ | é»‘ç›’å®ç° |
| **è°ƒè¯•** | å®¹æ˜“è°ƒè¯•å’Œä¿®æ”¹ | éš¾ä»¥è°ƒè¯• |
| **å…¼å®¹æ€§** | éœ€è¦æ‰‹åŠ¨ç¼–è¯‘ | å¼€ç®±å³ç”¨ |

### ç¼–è¯‘æ„å»ºè¿‡ç¨‹

#### 1. **æºç å‡†å¤‡**
- CUDAæºç  (`gelu.cu`)
- C++å¤´å£°æ˜ (`torch::Tensor gelu(torch::Tensor x);`)

#### 2. **ç¼–è¯‘å‘½ä»¤**
```bash
# load_inline å†…éƒ¨æ‰§è¡Œçš„ç±»ä¼¼å‘½ä»¤
nvcc -c gelu.cu -o gelu.o -O2
g++ -shared -o inline_gelu.so gelu.o -lcuda -lcudart
```

#### 3. **Pythonç»‘å®š**
è‡ªåŠ¨ç”ŸæˆPythonå¯è°ƒç”¨çš„å‡½æ•° `cuda_gelu`

### ä½¿ç”¨ç¤ºä¾‹

```python
# ä½¿ç”¨è‡ªå®šä¹‰GELU
x = torch.randn(1000, 1000).cuda()
y_custom = cuda_gelu(x)

# ä¸PyTorchå†…ç½®å¯¹æ¯”
y_official = torch.nn.functional.gelu(x)

# éªŒè¯æ­£ç¡®æ€§
print("æœ€å¤§è¯¯å·®:", torch.max(torch.abs(y_custom - y_official)).item())
```

## 6 - triton_introduction

Tritonæ˜¯OpenAIåœ¨2021å¹´å¼€å‘çš„å¼€æºGPUç¼–ç¨‹è¯­è¨€ï¼Œå®ƒé€šè¿‡**Python-likeçš„è¯­æ³•**è®©æ²¡æœ‰CUDAç»éªŒçš„å¼€å‘è€…ä¹Ÿèƒ½ç¼–å†™é«˜æ•ˆçš„GPUä»£ç ã€‚Tritonçš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†å¤æ‚çš„GPUå†…å­˜ç®¡ç†å’Œè°ƒåº¦ä¼˜åŒ–è‡ªåŠ¨åŒ–ï¼ŒåŒæ—¶è®©å¼€å‘è€…ä¸“æ³¨äº**çº¿ç¨‹å—çº§åˆ«**è€Œéçº¿ç¨‹çº§åˆ«çš„å¹¶è¡Œè®¾è®¡ã€‚

### è¦ç‚¹æ€»ç»“

#### ğŸ¯ æ ¸å¿ƒç›®æ ‡
- **é™ä½GPUç¼–ç¨‹é—¨æ§›**ï¼šè®©éCUDAä¸“å®¶ä¹Ÿèƒ½ç¼–å†™é«˜æ€§èƒ½GPUä»£ç 
- **æé«˜å¼€å‘æ•ˆç‡**ï¼šç”¨Pythonè¯­æ³•ç¼–å†™ï¼Œå‡å°‘ä»£ç é‡
- **ä¿æŒé«˜æ€§èƒ½**ï¼šå¤šæ•°æƒ…å†µä¸‹èƒ½è¾¾åˆ°ä¸“å®¶çº§CUDAä»£ç çš„æ€§èƒ½æ°´å¹³

#### âš¡ æŠ€æœ¯ç‰¹ç‚¹
| ä¼˜åŒ–æ–¹é¢ | CUDA | Triton |
|---------|------|--------|
| **å†…å­˜åˆå¹¶(DRAMä¼ è¾“)** | æ‰‹åŠ¨ | è‡ªåŠ¨ |
| **å…±äº«å†…å­˜ç®¡ç†** | æ‰‹åŠ¨ | è‡ªåŠ¨ |
| **SMå†…éƒ¨è°ƒåº¦** | æ‰‹åŠ¨ | è‡ªåŠ¨ |
| **SMé—´è°ƒåº¦** | æ‰‹åŠ¨ | æ‰‹åŠ¨ |

#### ğŸš€ æ€§èƒ½è¡¨ç°
1. **åŒ¹é…ä¸“ä¸šåº“æ€§èƒ½**ï¼šç”¨ä¸åˆ°25è¡Œä»£ç å®ç°çš„FP16çŸ©é˜µä¹˜æ³•èƒ½è¾¾åˆ°cuBLASçš„æ€§èƒ½
2. **è¶…è¶ŠPyTorch**ï¼šæŸäº›å†…æ ¸æ¯”ç­‰æ•ˆçš„Torchå®ç°æ•ˆç‡é«˜2å€
3. **å†…æ ¸èåˆä¼˜åŠ¿**ï¼šé¿å…äº†ä¸´æ—¶å¼ é‡çš„åˆ›å»ºå’Œç§»åŠ¨ï¼Œå‡å°‘å†…å­˜å¼€é”€

#### ğŸ”§ ç¼–ç¨‹æ¨¡å‹
- **ç±»ä¼¼Numba**ï¼šä½¿ç”¨è£…é¥°å™¨å®šä¹‰å†…æ ¸å‡½æ•°
- **å—çº§æ“ä½œ**ï¼šæ“ä½œçš„æ˜¯å¤šç»´å€¼å—ï¼ˆpower of twoç»´åº¦ï¼‰ï¼Œè€Œéå•ä¸ªçº¿ç¨‹
- **ç®€åŒ–å¹¶å‘**ï¼šæŠ½è±¡äº†CUDAçº¿ç¨‹å—å†…çš„å¹¶å‘é—®é¢˜ï¼ˆå†…å­˜åˆå¹¶ã€å…±äº«å†…å­˜åŒæ­¥ç­‰ï¼‰

#### ğŸª åº”ç”¨æ¡ˆä¾‹
1. **èåˆSoftmax**ï¼šæ¯”PyTorchå®ç°æ›´å¿«ï¼Œé€šè¿‡ä¿æŒæ•°æ®åœ¨SRAMä¸­æœ€å¤§åŒ–é‡ç”¨
2. **çŸ©é˜µä¹˜æ³•**ï¼šç®€æ´ä»£ç å®ç°å³°å€¼æ€§èƒ½ï¼Œæ”¯æŒè‡ªå®šä¹‰èåˆå˜æ¢
3. **ç‰¹æ®Šæ•°æ®ç»“æ„**ï¼šå¦‚å—ç¨€ç–å¼ é‡ç­‰å¤æ‚æ•°æ®ç»“æ„çš„ä¼˜åŒ–å¤„ç†

#### ğŸ—ï¸ ç³»ç»Ÿæ¶æ„
- **Triton-IR**ï¼šåŸºäºLLVMçš„ä¸­é—´è¡¨ç¤ºï¼Œå¤šç»´å€¼å—æ˜¯ä¸€ç­‰å…¬æ°‘
- **è‡ªåŠ¨ä¼˜åŒ–**ï¼šç¼–è¯‘å™¨è‡ªåŠ¨è¿›è¡Œå…±äº«å†…å­˜åˆ†é…ã€åŒæ­¥ã€å¹¶è¡ŒåŒ–ç­‰ä¼˜åŒ–
- **å¤šçº§å¹¶è¡Œ**ï¼šæ”¯æŒSMé—´å’ŒSMå†…çš„è‡ªåŠ¨å¹¶è¡ŒåŒ–

#### ğŸŒŸ æ ¸å¿ƒä»·å€¼
- **ç®€åŒ–å¼€å‘**ï¼šå‡å°‘å¯¹GPUç¡¬ä»¶ç»†èŠ‚çš„å…³æ³¨
- **ä¿æŒçµæ´»æ€§**ï¼šä»æä¾›å¯¹å†…å­˜è®¿é—®çš„ä½çº§æ§åˆ¶
- **ç¤¾åŒºé©±åŠ¨**ï¼šå¼€æºé¡¹ç›®ï¼Œé¼“åŠ±ç¤¾åŒºè´¡çŒ®

Tritonä»£è¡¨äº†GPUç¼–ç¨‹çš„é‡è¦è¿›æ­¥ï¼Œå®ƒé€šè¿‡åœ¨**è‡ªåŠ¨åŒ–ä¼˜åŒ–**å’Œ**ç¼–ç¨‹çµæ´»æ€§**ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ç‚¹ï¼Œä½¿å¾—é«˜æ€§èƒ½GPUä»£ç çš„å¼€å‘å˜å¾—æ›´åŠ  accessibleã€‚

## 7 - triton_gelu_main

```python
def triton_gelu(x: torch.Tensor):
    assert x.is_cuda
    assert x.is_contiguous()

    # Allocate output tensor
    y = torch.empty_like(x)

    # Determine grid (elements divided into blocks)
    num_elements = x.numel()
    block_size = 1024  # Number of threads
    num_blocks = triton.cdiv(num_elements, block_size)

    triton_gelu_kernel[(num_blocks,)](x, y, num_elements, BLOCK_SIZE=block_size)

    return y

@triton.jit
def triton_gelu_kernel(x_ptr, y_ptr, num_elements, BLOCK_SIZE: tl.constexpr):
    # Input is at `x_ptr` and output is at `y_ptr`
    #     |        Block 0            |          Block 1          |      ...      |
    #                            BLOCK_SIZE                                 num_elements

    pid = tl.program_id(axis=0)
    block_start = pid * BLOCK_SIZE

    # Indices where this thread block should operate
    offsets = block_start + tl.arange(0, BLOCK_SIZE)

    # Handle boundary
    mask = offsets < num_elements

    # Read
    x = tl.load(x_ptr + offsets, mask=mask)

    # Approx gelu is 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3)))
    # Compute (tl.tanh doesn't exist, use tanh(a) = (exp(2a) - 1) / (exp(2a) + 1)
    a = 0.79788456 * (x + 0.044715 * x * x * x)
    exp = tl.exp(2 * a)
    tanh = (exp - 1) / (exp + 1)
    y = 0.5 * x * (1 + tanh)

    # Store
    tl.store(y_ptr + offsets, y, mask=mask)
```

### æ ¸å¿ƒä»£ç åˆ†æ

#### 1. Python åŒ…è£…å‡½æ•°
```python
def triton_gelu(x: torch.Tensor):
    assert x.is_cuda          # ç¡®ä¿åœ¨GPUä¸Š
    assert x.is_contiguous()  # ç¡®ä¿å†…å­˜è¿ç»­
    
    y = torch.empty_like(x)   # åˆ†é…è¾“å‡ºå¼ é‡
    
    num_elements = x.numel()
    block_size = 1024         # æ¯ä¸ªblockçš„å¤§å°
    num_blocks = triton.cdiv(num_elements, block_size)  # è®¡ç®—blockæ•°é‡
    
    # å¯åŠ¨Tritonå†…æ ¸
    triton_gelu_kernelx, y, num_elements, BLOCK_SIZE=block_size
    
    return y
```

**å…³é”®ç‰¹æ€§**ï¼š
- **è‡ªåŠ¨å†…å­˜ç®¡ç†**ï¼šä¸éœ€è¦æ‰‹åŠ¨è·å–æ•°æ®æŒ‡é’ˆ
- **ç®€åŒ–ç½‘æ ¼é…ç½®**ï¼š`[(num_blocks,)]` è¯­æ³•æ›´ç®€æ´
- **ç±»å‹å®‰å…¨**ï¼šä½¿ç”¨PyTorchå¼ é‡è€ŒéåŸå§‹æŒ‡é’ˆ

#### 2. Triton å†…æ ¸å‡½æ•°
```python
@triton.jit
def triton_gelu_kernel(x_ptr, y_ptr, num_elements, BLOCK_SIZE: tl.constexpr):
```

##### çº¿ç¨‹ç´¢å¼•è®¡ç®—
```python
pid = tl.program_id(axis=0)        # è·å–ç¨‹åºIDï¼ˆç›¸å½“äºblockIdx.xï¼‰
block_start = pid * BLOCK_SIZE     # è®¡ç®—å½“å‰blockçš„èµ·å§‹ä½ç½®

# ç”Ÿæˆå½“å‰blockè¦å¤„ç†çš„æ‰€æœ‰ç´¢å¼•
offsets = block_start + tl.arange(0, BLOCK_SIZE)

# è¾¹ç•Œæ©ç å¤„ç†
mask = offsets < num_elements
```

**Tritonä¼˜åŠ¿**ï¼š
- **è‡ªåŠ¨å‘é‡åŒ–**ï¼š`tl.arange()` ç”Ÿæˆå‘é‡åŒ–çš„ç´¢å¼•
- **éšå¼çº¿ç¨‹ç®¡ç†**ï¼šä¸éœ€è¦æ‰‹åŠ¨è®¡ç®—threadIdx

##### å†…å­˜è®¿é—®
```python
# å‘é‡åŒ–åŠ è½½
x = tl.load(x_ptr + offsets, mask=mask)

# å‘é‡åŒ–å­˜å‚¨  
tl.store(y_ptr + offsets, y, mask=mask)
```

**å†…å­˜ä¼˜åŒ–**ï¼š
- **è‡ªåŠ¨åˆå¹¶è®¿é—®**ï¼šTritonè‡ªåŠ¨ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼
- **æ©ç æ”¯æŒ**ï¼šå®‰å…¨å¤„ç†è¾¹ç•Œæ¡ä»¶

##### æ•°å­¦è®¡ç®—
```python
# GELUè¿‘ä¼¼å…¬å¼
a = 0.79788456 * (x + 0.044715 * x * x * x)

# æ‰‹åŠ¨å®ç°tanhï¼ˆå› ä¸ºtl.tanhå¯èƒ½ä¸å­˜åœ¨ï¼‰
exp = tl.exp(2 * a)
tanh = (exp - 1) / (exp + 1)

y = 0.5 * x * (1 + tanh)
```

**æ•°å­¦ç‰¹æ€§**ï¼š
- **å‘é‡åŒ–è¿ç®—**ï¼šæ‰€æœ‰æ“ä½œéƒ½æ˜¯å…ƒç´ çº§åˆ«çš„
- **æ•°å€¼ç¨³å®šæ€§**ï¼šåˆç†çš„æ•°å­¦è¿‘ä¼¼

### ä¸ CUDA å®ç°çš„å¯¹æ¯”

#### ä»£ç ç®€æ´æ€§å¯¹æ¯”
| æ–¹é¢ | CUDAå®ç° | Tritonå®ç° |
|------|----------|------------|
| **ä»£ç è¡Œæ•°** | ~30è¡Œ | ~20è¡Œ |
| **çº¿ç¨‹ç®¡ç†** | æ‰‹åŠ¨è®¡ç®—ç´¢å¼• | è‡ªåŠ¨å‘é‡åŒ– |
| **å†…å­˜è®¿é—®** | æ‰‹åŠ¨æŒ‡é’ˆè¿ç®— | é«˜çº§load/store |
| **è¾¹ç•Œå¤„ç†** | æ‰‹åŠ¨ifåˆ¤æ–­ | è‡ªåŠ¨æ©ç  |

#### æ€§èƒ½ä¼˜åŒ–å¯¹æ¯”
| ä¼˜åŒ–æ–¹é¢ | CUDAï¼ˆæ‰‹åŠ¨ï¼‰ | Tritonï¼ˆè‡ªåŠ¨ï¼‰ |
|---------|-------------|---------------|
| **å†…å­˜åˆå¹¶** | éœ€è¦æ‰‹åŠ¨ç¡®ä¿ | ç¼–è¯‘å™¨è‡ªåŠ¨ä¼˜åŒ– |
| **å…±äº«å†…å­˜** | éœ€è¦æ‰‹åŠ¨ç®¡ç† | å¯é€‰è‡ªåŠ¨ä¼˜åŒ– |
| **æŒ‡ä»¤è°ƒåº¦** | éœ€è¦æ‰‹åŠ¨ä¼˜åŒ– | ç¼–è¯‘å™¨è‡ªåŠ¨è°ƒåº¦ |

### Triton çš„æ ¸å¿ƒä¼˜åŠ¿

#### 1. **æŠ½è±¡å±‚æ¬¡æ›´é«˜**
```python
# Tritonï¼ˆå‘é‡åŒ–æ€ç»´ï¼‰
offsets = block_start + tl.arange(0, BLOCK_SIZE)
x = tl.load(x_ptr + offsets, mask=mask)

# CUDAï¼ˆæ ‡é‡æ€ç»´ï¼‰  
int i = blockIdx.x * blockDim.x + threadIdx.x;
if (i < num_elements) {
    out[i] = calculation(in[i]);
}
```

#### 2. **è‡ªåŠ¨ä¼˜åŒ–**
- **å†…å­˜è®¿é—®æ¨¡å¼**ï¼šè‡ªåŠ¨ç¡®ä¿åˆå¹¶è®¿é—®
- **æŒ‡ä»¤é‡æ’**ï¼šç¼–è¯‘å™¨ä¼˜åŒ–æŒ‡ä»¤é¡ºåº
- **å¯„å­˜å™¨åˆ†é…**ï¼šæ™ºèƒ½å¯„å­˜å™¨ç®¡ç†

#### 3. **å¯ç§»æ¤æ€§**
- ç›¸åŒçš„ä»£ç å¯ä»¥åœ¨ä¸åŒæ¶æ„çš„GPUä¸Šè¿è¡Œ
- ç¼–è¯‘å™¨è‡ªåŠ¨é’ˆå¯¹ç‰¹å®šç¡¬ä»¶ä¼˜åŒ–

### æ½œåœ¨æ”¹è¿›æ–¹å‘

#### 1. ä½¿ç”¨å†…ç½®å‡½æ•°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
```python
# å¦‚æœTritonæ”¯æŒç›´æ¥tanh
y = 0.5 * x * (1 + tl.tanh(0.79788456 * (x + 0.044715 * x * x * x)))
```

#### 2. æ”¯æŒæ›´å¤šæ•°æ®ç±»å‹
```python
# æ·»åŠ ç±»å‹æ³¨è§£æ”¯æŒfp16ç­‰
@triton.jit
def triton_gelu_kernel(x_ptr: tl.tensor, y_ptr: tl.tensor, 
                      num_elements: int, BLOCK_SIZE: tl.constexpr):
```

#### 3. æ€§èƒ½è°ƒä¼˜å‚æ•°
```python
# æ·»åŠ è°ƒä¼˜å‚æ•°
@triton.jit
def triton_gelu_kernel(x_ptr, y_ptr, num_elements, 
                      BLOCK_SIZE: tl.constexpr = 1024):
    # å¯ä»¥æ ¹æ®ç¡¬ä»¶è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜BLOCK_SIZE
```

### ä½¿ç”¨ç¤ºä¾‹

```python
# åˆ›å»ºè¾“å…¥æ•°æ®
x = torch.randn(10000).cuda()

# ä½¿ç”¨Triton GELU
y_triton = triton_gelu(x)

# ä¸PyTorchå†…ç½®å¯¹æ¯”
y_pytorch = torch.nn.functional.gelu(x)

# éªŒè¯æ­£ç¡®æ€§
print("æœ€å¤§è¯¯å·®:", torch.max(torch.abs(y_triton - y_pytorch)).item())
```

## 8 - torch.compile

`torch.compile` åœ¨ä»¥ä¸‹åœºæ™¯ä¸­ä¼˜åŠ¿æœ€ä¸ºæ˜æ˜¾ï¼Œæˆ‘æ¥ä¸ºæ‚¨è¯¦ç»†åˆ†æï¼š

### ğŸš€ æ˜¾è‘—ä¼˜åŠ¿åœºæ™¯

#### 1. **è®¡ç®—å¯†é›†å‹æ“ä½œ**
```python
# çŸ©é˜µè¿ç®—å¯†é›†å‹
def matmul_heavy(x, y):
    for _ in range(100):
        x = torch.mm(x, y)  # å¤§é‡çŸ©é˜µä¹˜æ³•
    return x

# ç¼–è¯‘åè·å¾—å·¨å¤§æå‡
compiled_fn = torch.compile(matmul_heavy)
```

#### 2. **å¾ªç¯å¯†é›†å‹ä»£ç **
```python
# åŒ…å«å¤æ‚å¾ªç¯çš„é€»è¾‘
def loop_heavy(x):
    result = torch.zeros_like(x)
    for i in range(x.size(0)):
        for j in range(x.size(1)):
            # å¤æ‚è®¡ç®—é€»è¾‘
            result[i, j] = torch.sin(x[i, j]) + torch.cos(x[i, j]**2)
    return result

# å¾ªç¯ä¼˜åŒ–å¸¦æ¥æ˜¾è‘—åŠ é€Ÿ
compiled_loop = torch.compile(loop_heavy)
```

#### 3. **å°æ“ä½œé¢‘ç¹è°ƒç”¨**
```python
# é¢‘ç¹è°ƒç”¨çš„å°å‡½æ•°
def small_ops(x):
    return x.relu() + x.sigmoid() * x.tanh()

# åœ¨è®­ç»ƒå¾ªç¯ä¸­é‡å¤è°ƒç”¨
for data in dataloader:
    output = small_ops(data)  # ç¼–è¯‘åå†…è”ä¼˜åŒ–
```

#### 4. **è‡ªå®šä¹‰å¤æ‚è®¡ç®—å›¾**
```python
# å¤æ‚è®¡ç®—å›¾
def complex_graph(x, weight1, weight2, weight3):
    x1 = F.conv2d(x, weight1)
    x2 = F.relu(x1)
    x3 = F.conv2d(x2, weight2)
    x4 = F.gelu(x3)
    x5 = F.linear(x4.flatten(1), weight3)
    return F.softmax(x5, dim=1)

# æ•´ä¸ªè®¡ç®—å›¾ä¼˜åŒ–
compiled_graph = torch.compile(complex_graph)
```

### ğŸ“Š æ€§èƒ½æå‡å¯¹æ¯”

#### å…¸å‹åœºæ™¯æ€§èƒ½æå‡
| åœºæ™¯ç±»å‹ | é¢„æœŸåŠ é€Ÿæ¯” | åŸå› åˆ†æ |
|---------|-----------|---------|
| **çŸ©é˜µè¿ç®—å¯†é›†å‹** | 1.5x-3x | ç®—å­èåˆ+å†…å­˜ä¼˜åŒ– |
| **å¾ªç¯å¯†é›†å‹** | 2x-5x | å¾ªç¯å±•å¼€+å‘é‡åŒ– |
| **å°æ“ä½œé¢‘ç¹è°ƒç”¨** | 3x-10x | å‡½æ•°å†…è”+å‡å°‘å¼€é”€ |
| **å¤æ‚è®¡ç®—å›¾** | 1.2x-2x | å›¾ä¼˜åŒ–+è°ƒåº¦ä¼˜åŒ– |

#### å®é™…æµ‹è¯•æ•°æ®
```python
import torch
import time

def benchmark(func, *args, repeats=100):
    # Warmup
    for _ in range(10):
        func(*args)
    
    # Benchmark
    start = time.time()
    for _ in range(repeats):
        func(*args)
    torch.cuda.synchronize()
    return (time.time() - start) / repeats

# æµ‹è¯•ä¸åŒåœºæ™¯
x = torch.randn(1024, 1024).cuda()
```

### ğŸ”§ ç¼–è¯‘æ¨¡å¼é€‰æ‹©

#### ä¸åŒæ¨¡å¼çš„é€‚ç”¨åœºæ™¯
```python
# 1. é»˜è®¤æ¨¡å¼ - å¹³è¡¡ä¼˜åŒ–
torch.compile(func)  # å¤§å¤šæ•°åœºæ™¯

# 2. æœ€å¤§ä¼˜åŒ– - è®¡ç®—å¯†é›†å‹
torch.compile(func, mode="max-autotune")  # çŸ©é˜µè¿ç®—ã€å¾ªç¯

# 3. å‡å°‘å¼€é”€ - å°æ“ä½œé¢‘ç¹è°ƒç”¨  
torch.compile(func, mode="reduce-overhead")  # å°å‡½æ•°é¢‘ç¹è°ƒç”¨

# 4. æ¨ç†ä¼˜åŒ– - é™æ€å½¢çŠ¶
torch.compile(func, mode="max-autotune-no-cudagraphs")  # æ¨ç†åœºæ™¯
```

### ğŸ¯ å…·ä½“ä¼˜åŠ¿ä½“ç°

#### 1. **ç®—å­èåˆ (Kernel Fusion)**
```python
# ç¼–è¯‘å‰ï¼šå¤šä¸ªç‹¬ç«‹kernelè°ƒç”¨
def before_fusion(x):
    a = x.relu()      # kernel launch
    b = a.sigmoid()   # kernel launch  
    c = b * 2.0       # kernel launch
    return c          # 3æ¬¡GPUè°ƒç”¨

# ç¼–è¯‘åï¼šèåˆä¸ºå•ä¸ªkernel
# å•ä¸ªèåˆkernelï¼šrelu + sigmoid + mul
```

#### 2. **å†…å­˜è®¿é—®ä¼˜åŒ–**
```python
# ç¼–è¯‘å‰ï¼šä¸­é—´ç»“æœå­˜å‚¨
def before_optimize(x, weight):
    x1 = torch.mm(x, weight)  # åˆ†é…ä¸´æ—¶å†…å­˜
    x2 = x1.relu()            # åˆ†é…ä¸´æ—¶å†…å­˜
    return x2

# ç¼–è¯‘åï¼šåŸåœ°æ“ä½œæˆ–å†…å­˜å¤ç”¨
# å‡å°‘ä¸´æ—¶å†…å­˜åˆ†é…
```

#### 3. **å¾ªç¯ä¼˜åŒ–**
```python
# ç¼–è¯‘å‰ï¼šPythonå¾ªç¯å¼€é”€
def slow_loop(x):
    result = torch.zeros_like(x)
    for i in range(x.size(0)):  # Pythonå¾ªç¯å¼€é”€
        result[i] = x[i] * 2
    return result

# ç¼–è¯‘åï¼šç”Ÿæˆä¼˜åŒ–GPUä»£ç 
# è‡ªåŠ¨å‘é‡åŒ–å¾ªç¯
```

### âš ï¸ ä¼˜åŠ¿ä¸æ˜æ˜¾åœºæ™¯

#### 1. **æ•°æ®åŠ è½½å¯†é›†å‹**
```python
def data_heavy(data_loader):
    for batch in data_loader:  # æ•°æ®åŠ è½½æ˜¯ç“¶é¢ˆ
        x, y = batch
        output = model(x)      # è®¡ç®—åªå å°éƒ¨åˆ†æ—¶é—´
        loss = criterion(output, y)
        loss.backward()
    # ç¼–è¯‘ä¼˜åŒ–æ•ˆæœæœ‰é™
```

#### 2. **ç®€å•å•ä¸€æ“ä½œ**
```python
def single_op(x):
    return x.relu()  # å·²ç»æ˜¯é«˜åº¦ä¼˜åŒ–çš„kernel

# ç¼–è¯‘æ”¶ç›Šå¾ˆå°ï¼Œå¯èƒ½åè€Œæœ‰å¼€é”€
```

#### 3. **åŠ¨æ€æ§åˆ¶æµå¤æ‚**
```python
def dynamic_flow(x, condition):
    if condition.item() > 0.5:  # è¿è¡Œæ—¶åŠ¨æ€åˆ¤æ–­
        return x.relu()
    else:
        return x.sigmoid()
    # ç¼–è¯‘éš¾ä»¥ä¼˜åŒ–åŠ¨æ€åˆ†æ”¯
```

#### 4. **é¢‘ç¹å½¢çŠ¶å˜åŒ–**
```python
def changing_shapes(x_list):
    results = []
    for x in x_list:  # æ¯æ¬¡è¾“å…¥å½¢çŠ¶ä¸åŒ
        results.append(torch.mm(x, x.t()))
    return results
    # éœ€è¦é‡æ–°ç¼–è¯‘ï¼Œå¼€é”€å¤§
```

### ğŸ› ï¸ æœ€ä½³å®è·µå»ºè®®

#### æ¨èä½¿ç”¨åœºæ™¯
```python
# 1. è®­ç»ƒå¾ªç¯ä¸­çš„å‰å‘ä¼ æ’­
model = torch.compile(model)

# 2. æ•°å€¼è®¡ç®—å¯†é›†å‹å‡½æ•°
def physics_simulation(x):
    # å¤æ‚æ•°å­¦è¿ç®—
    return compiled_sim(x)

# 3. è‡ªå®šä¹‰æŸå¤±å‡½æ•°
custom_loss = torch.compile(complex_loss_fn)

# 4. æ‰¹é‡æ•°æ®å¤„ç†
batch_processor = torch.compile(process_batch)
```

#### é¿å…ä½¿ç”¨åœºæ™¯
```python
# 1. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
# ä¿æŒä¸ºPythonä»£ç 

# 2. ç®€å•çš„å•ä¸€æ“ä½œ
# ç›´æ¥ä½¿ç”¨åŸç”Ÿå‡½æ•°

# 3. åŒ…å«I/Oæ“ä½œçš„å‡½æ•°
def io_heavy(x):
    result = x * 2
    save_to_disk(result)  # I/Oæ“ä½œ
    return result
```

### ğŸ“ˆ æ€§èƒ½æµ‹è¯•ç­–ç•¥

```python
def test_compile_benefit():
    # æµ‹è¯•åŸå§‹ç‰ˆæœ¬
    original_time = benchmark(original_func, test_input)
    
    # æµ‹è¯•ç¼–è¯‘ç‰ˆæœ¬
    compiled_func = torch.compile(original_func)
    compiled_time = benchmark(compiled_func, test_input)
    
    speedup = original_time / compiled_time
    print(f"åŠ é€Ÿæ¯”: {speedup:.2f}x")
    
    return speedup > 1.2  # åªæœ‰æ˜æ˜¾åŠ é€Ÿæ—¶æ‰ä½¿ç”¨
```

### æ€»ç»“

`torch.compile` åœ¨ä»¥ä¸‹æƒ…å†µä¼˜åŠ¿æ˜æ˜¾ï¼š
- âœ… **è®¡ç®—å¯†é›†å‹æ“ä½œ**ï¼ˆçŸ©é˜µè¿ç®—ã€å¤æ‚æ•°å­¦ï¼‰
- âœ… **å¾ªç¯å¯†é›†å‹ä»£ç **ï¼ˆå¤šå±‚å¾ªç¯ã€å‘é‡åŒ–ï¼‰
- âœ… **å°æ“ä½œé¢‘ç¹è°ƒç”¨**ï¼ˆå‡½æ•°å†…è”ä¼˜åŒ–ï¼‰
- âœ… **å¤æ‚é™æ€è®¡ç®—å›¾**ï¼ˆç®—å­èåˆã€å†…å­˜ä¼˜åŒ–ï¼‰

è€Œåœ¨ä»¥ä¸‹æƒ…å†µä¼˜åŠ¿æœ‰é™ï¼š
- âŒ **æ•°æ®åŠ è½½ç“¶é¢ˆ**ï¼ˆI/Oé™åˆ¶ï¼‰
- âŒ **ç®€å•å•ä¸€æ“ä½œ**ï¼ˆå·²é«˜åº¦ä¼˜åŒ–ï¼‰
- âŒ **åŠ¨æ€æ§åˆ¶æµ**ï¼ˆè¿è¡Œæ—¶åˆ†æ”¯ï¼‰
- âŒ **é¢‘ç¹å½¢çŠ¶å˜åŒ–**ï¼ˆé‡å¤ç¼–è¯‘å¼€é”€ï¼‰

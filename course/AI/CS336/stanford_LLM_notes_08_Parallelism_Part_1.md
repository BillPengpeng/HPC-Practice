本文主要整理CS336 Lecture 7 Parallelism 章节的主要内容。

## 1 - Multi-GPU, multi-machine parallelism

![Multi-GPU](https://i-blog.csdnimg.cn/direct/8b892a3dcb6c4f17a40476567e8b7aef.png)

### 核心架构总览

这张图展示了两大并行层次：
1.  **节点内并行（Intra-node Parallelism）**：一台服务器内部，多个GPU之间如何协同工作。
2.  **节点间并行（Inter-node Parallelism）**：多台服务器之间如何通过网络协同工作。

整个系统的目标是：**将巨大的内存和计算需求（如训练大模型）拆分到众多GPU和机器上**。

---

### 术语详解（按图中逻辑顺序）

#### 1. CPU (Central Processing Unit) - 中央处理器
*   **是什么**：服务器的“大脑”和“总指挥”。
*   **作用**：负责运行操作系统、调度任务、管理内存，并作为所有硬件设备（GPU、网卡）的连接枢纽。图中的 `CPU0` 和 `CPU1` 表示这是一台**双路服务器**，拥有两个CPU插槽，以提供更多的PCIe通道和内存容量。

#### 2. PCIe (Peripheral Component Interconnect Express) - 高速串行计算机扩展总线标准
*   **是什么**：连接CPU与所有外部设备（GPU、网卡、SSD等）的**标准高速总线**。
*   **`16x` (x16)**：表示一条由**16条通道（lanes）** 组成的PCIe链路。这是PCIe的最大常规配置，提供最高的单设备带宽。图中用于：
    *   连接两个CPU，使它们能高效共享资源。
    *   连接PLX交换机。
*   **`4x` (x4)**：表示一条由**4条通道**组成的PCIe链路。带宽约为x16的1/4。图中用于CPU到PLX交换机的上行链路。

#### 3. PLX (PCIe Switch) - PCIe交换芯片
*   **是什么**：一个**PCIe交通枢纽**或**扩展器**。
*   **作用**：CPU提供的PCIe通道数量是有限的。PLX芯片的作用是将有限的CPU通道（如图中的`4x`上行链路）**扩展**成多个通道，从而可以连接更多的设备（如图中的4个HCA和1个GPU）。它智能地管理数据流向，避免冲突。

#### 4. HCA (Host Channel Adapter) - 主机通道适配器
*   **是什么**：**InfiniBand网络接口卡（网卡）** 的核心部件。
*   **作用**：这是服务器接入**InfiniBand**高性能网络的“门户”或“网关”。它负责将服务器内部的数据打包并通过InfiniBand线缆发送出去，也负责接收和处理来自网络的数据。图中一台服务器配备了4块HCA卡，以实现极高的对外通信带宽。

#### 5. GPU (Graphics Processing Unit) - 图形处理器
*   **是什么**：现代人工智能和高性能计算的**主力计算单元**。拥有数千个核心，擅长进行高度并行的矩阵和浮点运算。
*   **作用**：承担绝大部分的计算任务（如神经网络训练和推理）。图中一台服务器配备了8块GPU。

#### 6. NVLink
*   **是什么**：由NVIDIA开发的**GPU到GPU**之间的**超高速直连通信协议**。
*   **作用**：它绕开了速度较慢的PCIe总线，允许GPU之间直接、高速地交换数据（例如在模型并行中交换中间计算结果），速度比PCIe快数倍。图中的 `400 GT/s`（吉传输/秒） 显示了其极高的带宽。这是实现**节点内并行**的关键。

#### 7. NVSwitch
*   **是什么**：一个**基于NVLink技术的高速交换芯片**。
*   **作用**：您可以把它想象成一个专为GPU通信设计的**超级网络交换机**。它使服务器内的所有GPU（如图中的8个）都能以**全互联**的方式彼此直接通信，任何两个GPU之间都能同时以最高速交换数据，而不会产生拥堵。这是构建大规模GPU服务器（如NVIDIA DGX）的核心。

#### 8. xGMI (Cross GPU Memory Interconnect) - AMD GPU互连技术
*   **是什么**：**AMD公司**为其GPU开发的、类似于NVLink的**高速直连技术**。
*   **作用**：功能与NVLink类似，用于实现AMD GPU之间的高速直接通信。

#### 9. InfiniBand (IB)
*   **是什么**：一种**专为高性能计算设计**的网络通信协议和硬件标准。
*   **作用**：用于连接**多个计算节点（服务器）**，构成计算集群。相比普通的以太网，它具有**极高的带宽**（如图中 `50 GT/s per lane`）和**极低的延迟**，是实现**节点间并行**、让多台机器高效协同工作的基石。图中的 `HDR` 是InfiniBand的一个版本（High Data Rate）。

---

### 总结：数据如何流动？

1.  **节点内（单台服务器内部）**：
    *   **GPU之间**通过**NVLink**和**NVSwitch**以最高速度（`400 GT/s`）通信。
    *   **CPU管理GPU和网卡**，数据通过**PCIe**总线（`16x`, `4x`）流动，经由**PLX**芯片调度。

2.  **节点间（多台服务器之间）**：
    *   当需要与其他机器交换数据时，数据从GPU或CPU通过PCIe流向**HCA**网卡。
    *   **HCA**网卡通过**InfiniBand**线缆（`50 GT/s`）将数据发送到机架顶部的InfiniBand交换机，再传送到目标服务器。

这个复杂的架构只有一个目的：**最大限度地减少数据在“路上”的时间，让成千上万个计算核心（GPU）能够几乎无等待地协同工作，以解决前所未有的复杂计算问题。**

## 2 - Some basics about collective communication

![collective communication](https://i-blog.csdnimg.cn/direct/823342f29ef148adb46d6595ea87ae59.png)
![all reduce vs reduce-scatter-gather](https://i-blog.csdnimg.cn/direct/c6dda50d17534457ac0dc976ed89b9a1.png)

### 核心概念：集体通信 (Collective Communication)
指在一个**进程组**（例如，一组GPU）中，所有成员都参与的数据交换模式。图中每个小方块代表一个进程（或GPU），通常称为一个 **Rank**。

---

### 第一张图术语详解

这张图介绍了五种最基础的集体通信操作。

#### 1. 广播 (Broadcast)
*   **是什么**：将一个数据源（通常是某个Rank的数据）**复制并分发**到所有其他的Rank。
*   **操作**：如图，Rank 0 将自己的数据 `A` 发送给所有其他Rank。操作完成后，所有Rank都拥有了一份相同的 `A` 数据。
*   **应用场景**：在训练开始时，将主节点的模型参数分发给所有工作节点。

#### 2. 归约 (Reduce)
*   **是什么**：将**所有Rank**的输入数据，通过某种操作（如求和、求最大值等）**合并为一个结果**，并只存放在一个**目标Rank**上。
*   **操作**：图中是所有Rank的数据（A, B, C, D）在目标Rank 0 上被**求和（Sum）**，得到了结果 `A+B+C+D`。其他Rank没有这个结果。
*   **应用场景**：收集所有节点的损失值进行汇总计算。

#### 3. 全归约 (All-Reduce)
*   **是什么**：**归约（Reduce）** 操作的升级版。它将所有Rank的数据进行合并操作后，**将最终结果分发给每一个Rank**。
*   **操作**：图中所有Rank的数据被求和，并且每个Rank最终都得到了完整的结果 `A+B+C+D`。
*   **应用场景**：**这是分布式训练中最核心的操作**。每个GPU计算完梯度后，需要所有GPU的梯度求和得到平均梯度，然后每个GPU都需要得到这个平均梯度来更新自己的模型参数。

#### 4. 全聚集 (All-Gather)
*   **是什么**：**收集所有Rank的数据**，并将其**拼接（Concatenate）** 成一个完整的数组，然后分发给**所有Rank**。
*   **操作**：图中每个Rank提供一块数据（如Rank 0提供 `A`, Rank 1提供 `B`）。操作完成后，所有Rank都得到了一个完整的拼接数组 `[A, B, C, D]`。
*   **应用场景**：收集所有节点的嵌入向量以构建一个全局的嵌入表。

#### 5. 规约散射 (Reduce-Scatter)
*   **是什么**：**归约（Reduce）** 和**散射（Scatter）** 的结合。首先，对所有Rank的输入数据进行归约操作（如求和），但**结果不是形成一个值，而是被切割成若干块，每个Rank获得其中一块**。
*   **操作**：图中，每个Rank都有一个数组 `[A, B, C, D]`。Reduce-Scatter操作会对所有Rank数组的**第一个元素**（A₀, A₁, A₂, A₃）求和，将结果分给Rank 0；对所有**第二个元素**（B₀, B₁, B₂, B₃）求和，将结果分给Rank 1，以此类推。
*   **应用场景**：是高效实现All-Reduce的关键步骤。

---

### 第二张图术语详解

这张图揭示了全归约（All-Reduce）操作的一种**高效实现方式**。

#### 核心观点：All-Reduce = Reduce-Scatter + All-Gather
图中展示了如何将一次昂贵的All-Reduce操作，拆解成两步更高效的操作：

1.  **Reduce-Scatter 阶段**：
    *   如上所述，这个阶段将归约求和后的结果**分散**到各个Rank上。
    *   如图所示，四个GPU（A,B,C,D）经过Reduce-Scatter后，每个GPU只持有总结果的一部分（如GPU A持有 `A₀+B₀+C₀+D₀`）。

2.  **All-Gather 阶段**：
    *   所有Rank将自己在上一步中得到的那**一部分结果**，通过All-Gather操作共享给所有其他Rank。
    *   如图所示，每个GPU将自己持有的一块数据广播出去，最终所有GPU都收集齐了所有部分，拼成了完整的结果 `[A₀+B₀+C₀+D₀, A₁+B₁+C₁+D₁, ...]`。

#### 为什么这种方式更高效？
*   **带宽优化**：在分布式系统中，网络带宽通常是瓶颈。这种两步法巧妙地**均衡了所有链路上的网络流量**，避免了某些链路成为热点。
*   相比于一些朴素的All-Reduce实现（如在一个Rank上做归约再广播出去），`Reduce-Scatter + All-Gather` 的方案能更好地利用所有GPU之间的互联带宽，从而**显著减少整个操作的总完成时间**。
*   图底部的文字 **“in the bandwidth-limited regime, this is the best you can do”** （在带宽受限的情况下，这是你能做到的最佳方案）正是这个原因。

### 总结

这些集体通信原语是构建分布式并行计算算法的基石。其中，**All-Reduce** 最为重要，而通过 **Reduce-Scatter** 和 **All-Gather** 来实现它，是现代深度学习框架（如PyTorch的DDP）进行梯度同步的**标准且最优的方法**。理解这些术语和它们之间的关系，是理解多GPU训练如何工作的关键。

## 3 - TPUs vs GPUs – design differences at the comm level

![TPUs vs GPUs](https://i-blog.csdnimg.cn/direct/b3f942a589444943a6dc4562ae2bab47.png)

### 核心概念：通信层级 (Comm Level)

指芯片（TPU/GPU）之间如何连接和交换数据的硬件和拓扑结构。在分布式计算中，通信效率往往比纯计算能力更能制约整体性能，因此这个层级的设计至关重要。

---

### 第一部分：TPU Networking (谷歌张量处理单元网络)

#### 1. TPU (Tensor Processing Unit)
*   **是什么**：由Google专门为其机器学习工作负载（如TensorFlow）设计的**专用集成电路（ASIC）**。其核心优化方向是矩阵运算。
*   **设计哲学**：为特定的数据中心规模任务进行高度定制化设计。

#### 2. Toroidal Mesh (环面网格)
*   **是什么**：一种**网络拓扑结构**，类似于一个**多维的环**，其中每个节点（此处指TPU芯片）都与相邻节点直接连接，并且边缘的节点也彼此连接，形成一个“环面”（像甜甜圈或轮胎的形状）。
*   **特点**：
    *   **规整有序**：结构非常对称和规整。
    *   **跳数较多**：数据从一个芯片到另一个可能较远的芯片，需要经过多个“跳数”（经过多个中间芯片转发），这会导致**通信延迟（Latency）**。
    *   **对分带宽受限**：（见下文对“对分带宽”的解释）这种拓扑在规模扩大时，对分带宽的增长有瓶颈。

---

### 第二部分：GPU Networking (英伟达图形处理单元网络)

#### 1. DGX A100 / H100 256 SuperPOD
*   **DGX**：NVIDIA推出的**人工智能超级计算机**系列产品。一台DGX是一个服务器单元，内部包含多个GPU、NVLink和NVSwitch。
*   **SuperPOD**：由多台DGX服务器通过**InfiniBand**高速网络连接组成的大规模计算集群。`256` 指的是集群中GPU的总数量（如32台DGX H100服务器 * 每台8个GPU = 256个GPU）。

#### 2. IB HDR (InfiniBand High Data Rate)
*   **是什么**：用于**DGX A100 SuperPOD**的节点间网络技术。**InfiniBand**是一种高性能计算专用网络协议，**HDR**是其一个版本，提供高带宽和低延迟。
*   **Spine Switches (脊柱交换机)**：一种大型网络交换机构建模式（Spine-Leaf架构），“脊柱”交换机是集群的网络骨干，负责在不同“叶子”交换机（连接服务器）之间路由数据。这表明A100集群的通信需要经过外部交换机网络。

#### 3. Fully NVLink-connected / All-to-all (全NVLink连接 / 全对全)
*   **是什么**：用于**DGX H100 SuperPOD**的革命性设计。它意味着集群中的**所有256个GPU**，不仅通过节点内的**NVLink/NVSwitch**连接，节点间也通过**NVLink Switch**系统直接连接，形成了一个巨大的**全连接网络**。
*   **效果**：任何两个GPU之间都能直接通信，无需像传统网络那样经过多次路由。这极大地降低了延迟，并提供了巨大的聚合带宽。

#### 4. NVLink & NVS (NVSwitch)
*   **NVLink**：NVIDIA开发的GPU间高速直连通信协议，带宽远高于PCIe。
*   **NVSwitch (NVS)**：连接多个GPU的交换芯片，使GPU间能实现全互联通信。在H100 SuperPOD中，这个概念从单个服务器扩展到了整个Pod。

---

### 第三部分：性能指标与数据表格

表格对比了不同规模下的性能，并计算了加速比（Speedup）。

#### 1. Dense PFLOP/s (密集浮点运算性能)
*   **是什么**：衡量**纯计算能力**的指标，表示每秒能进行的密集矩阵浮点运算（如FP16）次数，单位为**千万亿次（Peta FLOPs/s）**。
*   **解读**：H100相比A100，由于架构升级，纯算力有显著提升（1.5倍）。但这主要体现的是**计算**层面的改进。

#### 2. Bisection Bandwidth (对分带宽)
*   **是什么**：这是衡量**网络通信能力**的**核心指标**。它指的是将网络对半切成两部分时，这两部分之间所有通信链路的**总带宽**。
*   **解读**：这个值越高，说明网络在处理大规模、所有节点同时通信（All-to-All）时的能力越强。**H100 SuperPOD的对分带宽达到了A100的9倍**，这完全归功于其全NVLink互联的设计，彻底消除了传统网络拓扑的瓶颈。

#### 3. Reduce Bandwidth (归约带宽)
*   **是什么**：衡量执行**归约（Reduce）** 操作（如梯度求和）时的有效带宽。这是分布式训练中最关键的通信操作之一。
*   **解读**：**H100 SuperPOD的归约带宽是A100的4.5倍**。高对分带宽自然也带来了更高的归约带宽，使得梯度同步的速度极大加快，大幅缩短了模型训练时间。

#### 4. Speedup (加速比)
*   **是什么**：H100性能与A100性能的比值，直观地展示了性能提升的倍数。
*   **解读**：关键结论是：**H100带来的通信性能提升（9x, 4.5x）远大于其纯计算性能提升（1.5x）**。这表明，对于大规模分布式训练，**通信网络的架构设计（GPU Networking）是比单纯提升单芯片算力更重要的因素**。

## 4 - Naïve data parallelism

![What’s wrong with naïve data parallelism?  -  Memory](https://i-blog.csdnimg.cn/direct/54b322c9d91e4771b02f702419cf8a4a.png)

### 内容概括

这三张图共同描述了分布式训练中的**数据并行（Data Parallelism）** 方法，并重点揭示了该方法面临的严峻**内存挑战**。

1.  **第一张图：基础方案 (The "What")**
    *   介绍了最基础的分布式训练方法——朴素数据并行。其核心思想是将一个大批次（Batch）的数据平分给多个GPU（或机器）同时计算，然后汇总所有GPU计算出的梯度来更新模型。

2.  **第二张图：核心问题 (The "So What")**
    *   指出了这种朴素方法的一个直观缺陷：**内存占用**。因为它需要在每个GPU上都复制并保存一份完整的模型参数。

3.  **第三张图：问题量化 (The "How Bad")**
    *   这张图至关重要，它深度剖析并量化了内存问题到底有多严重。它指出，在实际训练中，我们保存的远不止模型参数本身，还有更多用于优化算法的中间状态，这导致了惊人的内存开销。

---

### 要点总结

#### 1. 第一张图要点：朴素数据并行的工作原理
*   **目标**：加速随机梯度下降（SGD）的训练过程。
*   **方法**：
    *   **数据分割**：将全局批次（Batch Size = B）平均分割到 M 个GPU上，每个GPU只处理 B/M 个样本。
    *   **并行计算**：每个GPU都有自己的模型副本，独立地计算本地数据的梯度（Gradient）。
    *   **同步更新**：所有GPU计算完成后，将它们计算出的梯度进行汇总（求平均），然后用这个平均梯度来同步更新所有GPU上的模型参数。
*   **优缺点**：
    *   **优点 (计算缩放)**：完美实现了计算负载的并行化，训练速度理论上可以随GPU数量（M）线性提升。
    *   **缺点 (通信开销)**：每个训练步骤（Step）都需要在所有GPU之间传输一次梯度（数据量约为模型参数量的2倍），通信压力大。
    *   **缺点 (内存)**：每个GPU都必须存储一份完整的模型参数。

#### 2. 第二张图要点：指出内存瓶颈
*   **核心问题**：**内存复制**。
*   图示清晰地表明，在数据并行中，**数据集**被分割，但**模型**却被完整地复制到了每一个GPU上。
*   这直接引出了一个关键问题：**如果模型非常大，大到单个GPU的内存都放不下，那么这种“朴素”的数据并行方法就彻底失效了。** 这正是当前训练超大模型（如LLM）时遇到的核心挑战。

#### 3. 第三张图要点：量化内存开销——问题比想象中更严重
*   **惊人结论**：对于每个模型参数，我们实际上需要**16字节**的内存开销，而不仅仅是参数本身的2字节（例如FP16精度下）。这意味着**内存有效利用率极低**。
*   **内存开销的5个来源**（以Adam优化器为例）：
    1.  **模型参数 (2 bytes)**：例如以FP16或BF16格式存储的权重。
    2.  **梯度 (2 bytes)**：反向传播计算出的梯度，通常与参数保持相同精度。
    3.  **主权重 (4 bytes)**：为了保持数值稳定性，优化器（如SGD、Adam）通常会维护一个FP32精度的参数副本用于更新。
    4.  **一阶矩估计 (4 bytes)**：Adam优化器中的动量（Momentum）状态，通常为FP32。
    5.  **二阶矩估计 (4 bytes)**：Adam优化器中的方差（Variance）状态，通常为FP32。
*   **“优化器状态”**：后三项（主权重、一阶/二阶矩估计）统称为优化器状态，它们是内存的**主要消耗者**。

## 5 - ZeRO stage 1. optimizer state sharding

![ZeRO – solving the memory overhead issue of DP](https://i-blog.csdnimg.cn/direct/e1320a9fac8649e2b93b97ebd35a47c6.png)
![ZeRO stage 1. optimizer state sharding](https://i-blog.csdnimg.cn/direct/a69b307fe3354acdace0091d8adf478f.png)
![ZeRO stage 1. how it works](https://i-blog.csdnimg.cn/direct/101b6e657cf14bebba70ace29e0bd5e5.png)
![Comparing ZeRO stage 1 and naïve data parallel](https://i-blog.csdnimg.cn/direct/ab4abf895c334f38bd592509dbf46e84.png)

### 内容概括

这组材料揭示了分布式训练中的一个根本性矛盾：**计算与内存的权衡（Trade-off）**，并提出了ZeRO的解决方案。

1.  **第一张图：总览与核心思想（The "What & Why"）**
    *   定义了问题：朴素数据并行（DP）内存开销极大（`(2+2+K)*Ψ`），并提出了ZeRO的终极目标：通过**分片（Partitioning）** 和**通信原语（Collective Communication）** 将内存开销近乎平均地分配到所有GPU上，从而实现大幅降低。

2.  **第二张图：Stage 1 的具体策略（The "How" - Strategy）**
    *   聚焦于ZeRO的第一阶段：**优化器状态分片（Optimizer State Sharding, P_os）**。解释了其高层思想：将占用内存大头的优化器状态进行分片，每个GPU只保存并更新一部分。

3.  **第三张图：Stage 1 的通信流程（The "How" - Execution）**
    *   详细展示了ZeRO Stage 1 的一个训练步骤中具体的**通信流程**，即如何使用 **Reduce-Scatter** 和 **All-Gather** 这两个操作来协同完成梯度同步和参数更新。

4.  **第四张图：与基线方案的对比（The "Comparison"）**
    *   量化对比了ZeRO Stage 1 和朴素数据并行（Naïve DDP）在**通信成本**和**内存消耗**上的差异，得出了ZeRO“几乎免费”获得内存优势的结论。

---

### 要点总结

#### 1. 核心问题：内存开销的构成
*   训练时，每个GPU上的内存消耗远不止模型参数本身。主要包括：
    *   **模型参数 (Ψ)**：通常占 `2Ψ`（如FP16格式）。
    *   **梯度 (Gradients)**：占 `2Ψ`（与参数同精度）。
    *   **优化器状态 (Optimizer States)**：这是**内存消耗的主力**。例如对于Adam优化器，包括：
        *   主权重（FP32副本）：`4Ψ`
        *   一阶矩（Momentum）：`4Ψ`
        *   二阶矩（Variance）：`4Ψ`
        *   总计：`KΨ`（图中 `K=12`，即 `12Ψ`）。
*   **总内存开销**：`(2 + 2 + K) * Ψ = (4 + K) * Ψ`。对于一个75亿参数（Ψ=7.5B）的模型，`(4+12)*7.5B = 120GB`，远超单个GPU的容量。

#### 2. ZeRO 的核心思想：分片与通信
*   **核心思想**：不再让每个GPU冗余地存储所有状态，而是将**优化器状态（Stage 1）**、**梯度（Stage 2）**、**模型参数（Stage 3）** 进行分片（Partition），平均分布到所有 `N_d` 个GPU上。
*   **协同工作**：通过精心设计的**集体通信操作**（Reduce-Scatter, All-Gather）来保证在分片的情况下，所有GPU能协同完成训练步骤。
*   **内存收益**：
    *   **P_os (Stage 1)**：仅分片优化器状态，内存从 `120GB` 降至 `31.4GB`。
    *   **P_os+g (Stage 2)**：分片优化器状态+梯度，内存降至 `16.6GB`。
    *   **P_os+g+p (Stage 3)**：全部分片，内存降至 `1.9GB`。每个GPU仅存储 `1/N_d` 的状态。

#### 3. ZeRO Stage 1 的工作流程
这是一个关键的“用通信换内存”的典范：
1.  **计算本地梯度**：每个GPU用自己的数据计算**完整的**梯度。
2.  **Reduce-Scatter Gradients**：将所有GPU的完整梯度进行Reduce-Scatter操作。结果是，每个GPU只获得**一部分参数对应的平均梯度**。
3.  **本地更新**：每个GPU用它拥有的那**一部分梯度**和它本地存储的**那部分优化器状态**，来更新它负责的**那部分参数**。
4.  **All-Gather Parameters**：所有GPU通过All-Gather操作，同步更新后的全部参数，使所有GPU都获得最新的、完整的参数副本。

#### 4. 与朴素数据并行的对比（核心结论）
| 对比项 | 朴素数据并行 (Naïve DDP) | ZeRO Stage 1 | 解释 |
| :--- | :--- | :--- | :--- |
| **通信原语** | 一次 All-Reduce（梯度） | 一次 Reduce-Scatter（梯度） + 一次 All-Gather（参数） | 操作不同，但... |
| **通信量** | `2 * #params` | `2 * #params` | **通信总量完全相等！** |
| **内存占用** | `(4 + K) * #params` | `(4 + K/N_gpu) * #params` | **内存占用大幅降低！** |

*   **“免费午餐”**：ZeRO Stage 1 在**通信带宽成本不变**的情况下，换来了**内存占用近乎线性（与GPU数成反比）的下降**。这在网络带宽是主要瓶颈的场景下，是一个巨大的胜利。
*   **启示**：ZeRO 通过更巧妙的通信流程设计，打破了“内存冗余”的桎梏，使得我们能够用有限的GPU内存来训练极其庞大的模型。Stage 2 和 Stage 3 将此思想进一步延伸，实现了更大的内存节省。

## 6 - ZeRO stage 2. the simple extension to gradient sharding

![ZeRO stage 2. the simple extension to gradient sharding](https://i-blog.csdnimg.cn/direct/da9fea936ecc42788c3eeb1393fc2144.png)
![ZeRO stage 2. how it works](https://i-blog.csdnimg.cn/direct/ef50a32bb6ea4580b3eb69b33a0a3180.png)

### 内容概括

这两张图分别从**策略思想**和**执行细节**两个层面，阐述了ZeRO第二阶段的工作原理。

1.  **第一张图：策略与动机（The "What & Why"）**
    *   **标题**：ZeRO stage 2: the simple extension to gradient sharding（梯度分片的简单扩展）。
    *   **核心信息**：在Stage 1成功分片优化器状态（OS）的基础上，受到鼓舞，“让我们分片更多东西”！这一阶段的目标是将**梯度（Gradients）** 也在机器间进行分片。
    *   **核心挑战**：引入了一个复杂性——我们永远无法实例化一个完整的梯度向量，但每个工作节点（Worker）又必须计算完整的梯度。

2.  **第二张图：执行与流程（The "How"）**
    *   **标题**：ZeRO stage 2: how it works（工作原理）。
    *   **核心信息**：详细描述了在一个训练步骤中，如何通过精细化的、增量式的通信和内存管理，来解决上述挑战，并完成训练任务。其流程可分解为三个关键步骤。

---

### 要点总结

#### 1. 核心目标与内存收益
*   **目标**：在Stage 1（分片优化器状态，P_os）的基础上，进一步分片**梯度（Gradients）**，记为 **P_os+g**。
*   **内存收益**：内存占用从Stage 1的 `2Ψ + ((2+K)Ψ)/N_d` 进一步降低到 `((2+K)Ψ)/N_d`。
    *   公式解读：模型参数（`2Ψ`）不再是每个GPU都保存一份完整副本，而是只保存它负责的那一部分（`1/N_d`）。梯度和优化器状态早已被分片。
    *   以图中为例，内存从 **31.4GB** 降至 **16.6GB**。

#### 2. 核心策略与面临的挑战
*   **高层策略**：延续Stage 1的“分而治之”思想。不仅将优化器状态分片，也将梯度张量（Gradient Tensor）进行分片，每个GPU只负责存储和更新其中一部分。
*   **核心矛盾/挑战**：
    *   **不能实例化完整梯度**：因为我们的目标就是节省内存，所以不允许在任何单个GPU上构建完整的梯度张量（否则就失去了分片的意义）。
    *   **必须计算完整梯度**：因为采用数据并行，每个GPU都用自己的一份数据独立进行前向和反向传播，**反向传播的计算过程天然就会产生一份完整的梯度**。

#### 3. 解决方案：精细化的流程设计（How it works）
第二张图提供了完美的解决方案，通过**流水线化**和**即时通信**来化解矛盾：

**Step 1: 增量式反向传播与梯度归约**
*   **1a. 计算即发送**：不再等所有层的梯度都计算完再统一处理。而是**每计算完一层的梯度**，就立即启动一个**Reduce-Scatter**操作，将这层梯度的“碎片”发送给负责该部分参数更新的对应GPU。
*   **1b. 即时释放**：一旦该层的梯度完成了它的使命（即被用于计算更前一层的梯度），并且其分片已经发送出去，就**立即从当前GPU的内存中释放掉**这部分的完整梯度。
*   **效果**：通过这个“计算-发送-释放”的流水线，在整个反向传播过程中，没有任何一个GPU在任何时刻持有**所有层**的完整梯度，从而极大降低了峰值内存消耗。

**Step 2: 本地更新**
*   每个GPU用它收到的**那部分梯度碎片（即全局平均梯度的一部分）** 和它本地存储的**那部分优化器状态**，来更新它负责的**那部分参数**。
*   这一步是本地操作，无需通信。

**Step 3: 全聚集参数**
*   所有GPU通过 **All-Gather** 操作，广播自己更新后的那部分参数，并收集所有其他部分，最终每个GPU都获得完整的、更新后的模型参数副本，为下一个前向传播做准备。

### 总结

ZeRO Stage 2 的核心创新在于其**精细化的内存与通信管理**：

1.  **思想演进**：从只分片优化器状态（Stage 1）到同时分片梯度和优化器状态（Stage 2）。
2.  **解决矛盾**：通过**增量式反向传播**和**计算后即时通信释放**的策略，巧妙地解决了“必须计算完整梯度”与“不能存储完整梯度”之间的矛盾。
3.  **内存优势**：在通信总量与Stage 1和Naïve DDP基本持平的前提下，实现了内存占用的进一步大幅降低，使得能够训练的模型规模变得更大。

ZeRO Stage 2 是通向完全分片（Stage 3）的关键一步，体现了深度学习系统工程中“用通信换内存”的经典设计哲学。

## 7 - ZeRO stage 3 (aka FSDP) shard everything

![ZeRO stage 3 (aka FSDP) shard everything](https://i-blog.csdnimg.cn/direct/dddff4d69d3348c594a005ff4e3c5159.png)
![ZeRO stage 3 (aka FSDP) how it works (baby version)](https://i-blog.csdnimg.cn/direct/58ff2fc484494a13ac02499eb7ff9c91.png)
![Actual picture of how FDSP / ZeRO stage 3 works](https://i-blog.csdnimg.cn/direct/b2c3d05b58b844b2bd2f1f130229883d.png)

### 内容概括

这三张图共同描述了ZeRO的终极形态——Stage 3，其目标是通过极致的“分片”策略，彻底解决训练超大模型时的内存瓶颈。

1.  **第一张图：核心思想与目标（The "What & Ambition"）**
    *   提出了ZeRO Stage 3的宏伟目标：**分片一切（Shard everything）**，包括模型参数、梯度和优化器状态。并点出了实现这一目标的核心技术挑战。

2.  **第二张图：基础执行流程（The "How - Basic Version"）**
    *   展示了FSDP一个简化版（“baby version”）的工作流程，详细描述了在一个前向和反向传播过程中，参数是如何被按需获取、使用并释放的。

3.  **第三张图：高级优化与实现（The "How - Advanced Version"）**
    *   揭示了实际工业级实现中为了提升效率而采用的两个关键技术：**增量通信**和**计算-通信重叠**，这是FSDP能够高效运行而不显著拖慢训练速度的关键。

---

### 要点总结

#### 1. 核心思想：分片一切 (Shard Everything)
*   **目标**：彻底消除单GPU上的所有内存冗余。在Stage 2（分片梯度+优化器状态）的基础上，进一步将**模型参数（Parameters）** 也进行分片。
*   **内存收益**：这是ZeRO内存优化的顶峰。每个GPU**只存储 `1/N_d` 的模型参数、梯度和优化器状态**。内存占用从Stage 2的 `16.6GB` 进一步降至惊人的 `1.9GB`（以75亿参数模型为例）。这使得用极少的GPU内存训练巨型模型成为可能。

#### 2. 核心策略：按需获取与即时释放 (On-Demand Fetch & Immediate Free)
这是FSDP的工作原理，如第二张图所示：
*   **前向传播（Forward）**:
    1.  **All-Gather**：当前需要计算某一部分模型（如一个FSDP封装的模块）时，所有GPU协同操作，**收集（Gather）** 构建该部分完整参数所需的全部碎片。
    2.  **计算**：使用完整的参数执行本地前向计算。
    3.  **释放**：计算完成后，**立即释放**掉这份完整的参数，仅保留其分片。内存被立即回收。
*   **反向传播（Backward）**:
    1.  **All-Gather**：同样，需要计算梯度时，再次通过All-Gather操作重建完整参数。
    2.  **计算**：执行本地反向计算，得到梯度。
    3.  **Reduce-Scatter**：将计算出的梯度进行Reduce-Scatter操作，求平均后并将结果梯度碎片分发到各自负责的GPU上。
    4.  **释放**：再次立即释放完整参数。
*   **通信成本**：每次前向和反向传播，对于每个FSDP模块，都需要进行**2次All-Gather**和**1次Reduce-Scatter**操作。

#### 3. 关键优化：重叠与流水线 (Overlapping & Pipelining)
这是FSDP能实用的关键，如第三张图所示：
*   **计算-通信重叠 (Overlapping Communication and Computation)**：
    *   **理念**：不让GPU等数据。在进行当前层计算的同时，**预先通过通信（All-Gather）去获取下一层计算所需的参数**。
    *   **效果**：将昂贵的通信时间隐藏在高强度的计算时间背后，从而**极大地掩盖了通信开销**，使得总体训练效率接近未分片的情况。
*   **增量式通信/计算 (Incremental Computation / Communication)**：
    *   **理念**：将整个模型分成多个FSDP模块（如按层划分）。参数是按模块为单位进行获取、使用和释放的，而不是为整个模型一次性地进行所有通信。
    *   **效果**：显著降低了**峰值内存占用**（Peak Memory），因为同一时刻只有一部分模型的完整参数驻留在GPU内存中。

### 总结

ZeRO Stage 3 (FSDP) 代表了分布式训练内存优化的最高水平。它通过一种**“用通信换内存”** 的极致设计哲学：

1.  **极致内存节省**：通过分片一切，将内存开销降至理论最低。
2.  **精细流程控制**：通过按需获取和即时释放的策略，化解了“计算需要完整参数”与“内存无法存储完整参数”的矛盾。
3.  **高级性能优化**：通过计算-通信重叠和增量式处理，成功地将额外的通信开销几乎“隐藏”起来，保证了训练效率。

最终，FSDP使得研究人员和工程师能够在有限的硬件资源上，训练之前无法想象的超大规模模型，极大地推动了AI领域的发展。
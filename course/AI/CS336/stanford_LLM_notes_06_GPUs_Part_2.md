本文主要整理CS336 GPUs章节的主要内容。

## 11 - Memory coalescing

![Memory coalescing](https://i-blog.csdnimg.cn/direct/af4e1a127e794dbdb7b4acab66dbd9dd.png)
![Row-major matrices](https://i-blog.csdnimg.cn/direct/d0a5ccf8cffb493593fba6b2ddfe9f5e.png)

### 内容概括

这三张幻灯片系统性地阐述了GPU内存访问中的一个核心性能优化技术——**内存合并（Memory Coalescing）**。其原理根植于现代DRAM（动态随机存取存储器）的硬件工作方式（“突发模式”）。内容从DRAM的物理特性出发，解释了“突发读取”机制如何决定了内存访问的效率，进而定义了“内存合并”的概念，并通过矩阵乘法的具体案例，正反对比了未合并与已合并的访问模式，最终指导开发者如何组织数据访问以最大化内存带宽利用率。

---

### 要点总结

#### **1. DRAM的突发模式访问机制 (Trick (?) 4: Memory coalescing and DRAM)**
*   **核心机制**：DRAM（全局内存）以**“突发模式（Burst Mode）”** 进行读取。每次访问一个内存地址时，DRAM会将该地址所在的一个**连续内存块（称为“突发部分”，Burst Section）** 的全部数据一次性传输给处理器。
*   **地址空间划分**：整个内存地址空间被划分为多个固定大小的“突发部分”（例如，在示意图中为4字节，实际中通常为128字节或更大）。
*   **硬件根源**：这种模式源于DRAM的物理设计。读取数据时，需要先将整行数据从存储阵列复制到**感应放大器（Sense Amplifier）** 这一临时缓冲区，再从该缓冲区进行传输。一次性传输整行数据比多次传输零散数据更高效。
*   **关键启示**：**访问的局部性至关重要**。只要程序访问了某个突发部分内的任何一个地址，该部分的所有数据都会被传输。因此，理想情况是让程序的所有访问都集中在一个或少数几个突发部分内，以避免浪费带宽。

#### **2. 内存合并的概念与条件 (Memory coalescing)**
*   **定义**：内存访问是**合并的（Coalesced）**，当且仅当一个**线程束（Warp）** 中的所有32个线程所访问的内存地址都落在**同一个突发部分**内。
*   **最佳情况**：当Warp执行加载指令时，如果所有线程访问的地址都在同一个突发部分，GPU只需向DRAM发起**1次请求**即可获取所有数据。这种访问被称为**完全合并（Fully Coalesced）**，效率最高。
*   **最差情况**：如果Warp中32个线程访问的地址分散在32个不同的突发部分，GPU就需要发起**32次独立的DRAM请求**，效率极低。

#### **3. 实践案例：矩阵乘法中的内存合并 (Coalescing for matrix multiplication)**
*   **问题场景（未合并）**：在行主序存储的矩阵中，如果让Warp中的每个线程去访问**同一行中相邻的列元素**（即沿行方向移动），由于这些元素在内存中地址相邻，它们有很大概率会落在同一个突发部分内，从而实现**合并访问**。
*   **反面案例（未合并）**：如果让Warp中的线程去访问**同一列中相邻的行元素**（即沿列方向移动），由于矩阵在内存中是按行存储的，相邻行同一列的元素在内存中的地址相距甚远（间隔“宽度”个元素），几乎不可能落在同一个突发部分内。这会导致**访问未合并**，性能急剧下降。
*   **优化策略**：因此，在编写类似矩阵乘法的核函数时，必须精心设计数据的索引方式，确保**每个Warp的访问模式是连续且对齐的**，以充分利用硬件的突发传输特性。

### **整体结论**
**内存合并**是一项至关重要的GPU内存优化技术。其性能收益直接来自于对DRAM硬件特性的理解与利用。编写高效GPU代码的关键原则之一是：**确保同一个Warp中的线程访问连续且对齐的内存地址**，从而将多次零散的内存请求合并为一次大的突发传输，极大提升内存带宽的利用率和程序整体性能。

## 12 - Tiling

### 内容概括

这五张幻灯片系统性地阐述了GPU高性能编程中最为核心的优化技术——**分块（Tiling）**。内容从该技术的基本概念和动机出发，通过矩阵乘法的经典案例，详细展示了其实现方法、巨大的性能收益，并深入探讨了在实际应用中可能遇到的复杂性问题（如尺寸整除、内存对齐）及其解决方案。整套幻灯片揭示了分块技术如何通过将数据暂存于高速的共享内存（Shared Memory）中来最大化数据复用率，从而有效克服“内存墙”瓶颈，是提升GPU计算效能的关键。

---

### 要点总结

#### **1. 分块的核心概念与问题 (Trick 5: tiling)**
*   **核心思想**：**将线程分组和排序，以最小化对慢速全局内存的访问**。
*   **问题背景（以矩阵乘法为例）**：朴素实现的矩阵乘法中，每个输入元素会被多次读取。如图1所示，线程的访问模式是**非合并的（Not Coalesced）** 且存在大量的**重复访问（Repeated）**（例如元素M₀,₀和N₁,₀被多个线程多次读取），导致全局内存带宽成为主要性能瓶颈。

![Tiling](https://i-blog.csdnimg.cn/direct/1961e94f40d947c4b2d215eb30292748.png)

#### **2. 分块的实现方法与优势 (Tiling – store and reuse information in shared memory)**
*   **实现方法**：将大矩阵**切割成更小的“块”（Tiles）**，分阶段计算。
    1.  将当前计算所需的数据块从全局内存**加载到共享内存（SHM）** 中。
    2.  让线程块（Block）内的所有线程**从共享内存中协作读取数据**，计算部分和。
    3.  处理完当前块后，再加载下一个块，循环直至计算完成。
*   **巨大优势**：
    *   **数据复用**：重复的读取操作 now access shared, not global memory。对全局内存的访问次数大幅减少。
    *   **访问合并**：由于加载和读取是在共享内存中有组织地进行，可以确保对全局内存的访问是**合并的（Coalesced）**，极大提高了内存带宽利用率。

![Tiling math](https://i-blog.csdnimg.cn/direct/1a1af54b9c7a488d8494740bdc3939a0.png)

#### **3. 分块的性能收益量化 (Tiling math)**
*   **量化对比**：
    *   **非分块算法**：每个输入元素需要从全局内存中读取 **N次**（N为矩阵维度）。
    *   **分块算法**：每个输入元素仅需从全局内存中读取 **N/T次**（T为块大小），实现了 **T倍的全局内存访问削减**。
*   **结论**：分块通过**用计算（共享内存内的多次访问）换带宽（全局内存的访问）**，从根本上缓解了内存带宽瓶颈。

![Complexities with tiling v1](https://i-blog.csdnimg.cn/direct/4fdb3a5b12e047fc919c1d7f3e527708.png)
![Complexities with tiling v2](https://i-blog.csdnimg.cn/direct/0b8c7a16b02749d7ab9e530b18335a2e.png)

#### **4. 分块的实际挑战与解决方案 (Complexities with tiling)**
*   **挑战一：尺寸整除性问题**
    *   **最佳情况**：矩阵维度能被块尺寸整除，所有线程利用率100%。
    *   **最坏情况**：矩阵维度不能被块尺寸整除，会启动部分“无效”的线程块，导致**利用率低下**（如图4b所示）。
    *   **影响因素**：共享内存大小、合并访问要求、矩阵维度的可整除性。
*   **挑战二：内存对齐问题**
    *   **原理**：DRAM以**突发模式（Burst）** 传输数据，一次传输一个连续块。
    *   **对齐布局（Aligned Layout）**：数据块的起始地址与突发边界对齐，加载速度快。
    *   **未对齐布局（Unaligned Layout）**：一个数据块横跨两个突发部分，需要两次内存请求才能加载，速度慢。
    *   **解决方案**：通过**内存填充（Padding）** 调整矩阵在内存中的布局，确保每个数据块的起始地址都能对齐，从而保证合并访问。

### **整体结论**
**分块（Tiling）** 是释放GPU强大计算潜力的“王牌”技术。它通过巧妙的线程组织和数据调度，将计算任务重构为适合共享内存处理的块，从而实现了：
1.  **极高的数据复用率**，减少对全局内存的访问。
2.  **完美的内存访问合并**，最大化内存带宽利用率。
然而，在实际应用中需谨慎处理**块大小选择、尺寸整除性和内存对齐**等工程细节，才能达到理想的优化效果。这项技术是高性能GPU编程（如cuBLAS、深度神经网络训练）的基石。

## 13 - ​Putting it together: understanding a matrix mystery

![Part1](https://i-blog.csdnimg.cn/direct/8473f9714b424fe98cfd05a6035bd75c.png)
![Part2](https://i-blog.csdnimg.cn/direct/04885b1a3f5c42d187756dfa29039881.png)

### 内容概括

这四张图片共同揭示了高性能计算中一个反直觉的现象：**有时增加矩阵尺寸（甚至增加无用的计算量）反而能大幅提升计算性能**。该现象源于GPU硬件架构（如内存对齐、线程调度）与软件算法（如分块策略）之间的复杂相互作用。内容从Andrej Karpathy的实际优化案例出发，通过性能图表和架构分析，逐步深入解析了导致这一“矩阵之谜”的两个核心原因：**内存布局对齐（Part 1: Tiling）** 和 **GPU流式多处理器（SM）的波量化调度（Part 2: Wave Quantization）**。

---

### 要点总结

#### **1. 核心现象与问题 (Putting it together: understanding a matrix mystery)**
*   **实际案例**：在nanoGPT项目中，将词汇表大小从50257**增加**到50304（最接近的64的倍数），实现了约**25%的速度提升**。
*   **反直觉结论**：增加无用维度（计算量）反而使程序更快。
*   **根本原因**：新的尺寸选择了GPU中一个**占用率（Occupancy）更高**的内核执行路径。
*   **核心问题**：为什么更大的矩阵会更快？

#### **2. 性能波动的表象 (Matrix mystery - FLOPs achieved for square matmuls)**
*   **性能波动**：矩阵乘法的实际性能（TF/s）并非随尺寸（N）增大而平滑上升，而是出现**剧烈的、周期性的波动**。
*   **已知优化技术**：这种波动与**计算强度（Compute Intensity）** 和**分块（Tiling）** 技术有关，但仍有更深层的原因需要探究。

#### **3. 原因一：分块与内存对齐 (Part 1: tiling)**
*   **分块的影响**：性能曲线的颜色编码表明，当矩阵尺寸**能被分块大小（K）整除**时，性能通常更高（如K=32的蓝色线在N=1024, 2048等处出现峰值）。
*   **对齐布局 vs. 非对齐布局**：
    *   **对齐布局（Aligned Layout）**：尺寸合适时，数据块（Tile）的边界与DRAM的突发传输边界对齐，内存访问是**合并的（Coalesced）**，效率极高（“One Nice Tile”）。
    *   **非对齐布局（Unaligned Layout）**：尺寸不合适时，一个数据块可能横跨两个DRAM突发部分，需要两次内存请求，效率低下（“Two Bad Tiles”）。
*   **结论**：分块通过对齐产生重大影响。**能被常见分块大小（如2, 8, 16, 32, 64, 128）整除的矩阵尺寸，其内存访问效率更高，因此性能更好。**

#### **4. 原因二：波量化与硬件调度 (Part 2: wave quantization)**
*   **周期行为的根源**：性能的周期性波动与GPU的**硬件调度单位——“波”（Waves）** 有关。
*   **具体案例**：在尺寸从1792增加到1793时，性能发生突变。
    *   **计算过程**：对于256x128的Tile，1792x1792的矩阵被划分为 (1792/256) * (1792/128) = 7 * 14 = 98个Tiles。
    *   **尺寸变化的影响**：1793x1793的矩阵被划分为 (1793/256) * (1793/128) = 8 * 15 = 120个Tiles（向上取整）。
*   **硬件限制**：NVIDIA A100 GPU只有**108个流式多处理器（SMs）**。120个Tiles无法在一轮调度中（一个“Wave”）被全部执行完毕，必须分成多个波次序列化执行，增加了调度开销，从而导致性能下降。
*   **波量化（Wave Quantization）**：GPU的调度器以“波”为单位分发任务块（Tiles/Blocks）。**任务块的总数相对于SM数量的余数，决定了调度的效率和硬件资源的利用率**，从而产生了周期性的性能变化。

### **整体结论**
“更大的矩阵反而更快”这一神秘现象的背后，是两个硬件层面的核心优化原则：
1.  **对齐原则**：选择能被2的幂次（尤其是64）整除的尺寸，可以确保内存访问模式是**对齐和合并的**，最大化内存带宽利用率。
2.  **占用量化原则**：选择能使得总任务块数恰好是GPU SM数量整数倍的尺寸，可以确保所有硬件计算单元在一轮调度中就被完全占满，**最大化硬件占用率（Occupancy）**，最小化调度开销。

因此，在深度学习和高性能计算中，将张量尺寸填充（Padding）到最近的64或128的倍数，是一种用微小的计算量代价换取巨大的内存带宽和硬件利用率提升的经典优化手段。

## 14 - Recap of attention computation

![Flash attention](https://pic1.zhimg.com/v2-a4b6cb732fc5b93b618f98b4815ec1b4_1440w.jpg)
![Attention computation](https://picx.zhimg.com/v2-65f2f3684687ca0ef024634eaa257791_1440w.jpg)

### 内容概括

这两张幻灯片系统地解释了**FlashAttention算法**的核心原理与巨大优势。内容从回顾标准注意力机制的计算瓶颈出发，通过性能对比数据（时间、HBM访问量）凸显了FlashAttention的显著加速效果，并最终揭示其技术本质：**将分块（Tiling）和重计算（Recomputation）这两种经典的GPU优化技术，创造性地应用于注意力计算这一特定领域**，从而在保证计算结果精确的前提下，极大地减少了对慢速全局内存（HBM）的访问次数，突破了传统实现的内存墙限制。

---

### 要点总结

#### **1. 标准注意力机制的瓶颈 (Recap of attention computation)**
*   **计算过程**：注意力机制包含**3次矩阵乘法**，中间插入一个Softmax函数。具体流程为：
    1.  计算 Q（查询）、K（键）、V（值）矩阵：$Q = XW_q$, $K = XW_k$, $V = XW_v$。
    2.  计算注意力分数矩阵：$S = QK^T$ （一个 $[n x n]$ 的矩阵）。
    3.  对S应用 Softmax 函数：$P = softmax(S)$。
    4.  计算输出：$O = PV$ （输出维度为 $[n x d]$）。
*   **核心问题**：中间产生的 $S$ 和 $P$ 这两个 $[n x n]$ 的矩阵非常巨大（尤其是当序列长度n很大时），需要频繁读写慢速的HBM，成为主要性能瓶颈。

#### **2. FlashAttention的卓越性能 (Flash attention dramatically accelerates attention)**
*   **性能对比**：左侧柱状图显示，在GPT-2模型上，FlashAttention相比PyTorch标准实现，**注意力计算部分耗时大幅降低**。
*   **数据量化**：中间表格的数据表明，FlashAttention在运行时间、HBM访问量等关键指标上全面优于标准实现。
*   **优化目标**：FlashAttention的核心目标是**减少对HBM的访问次数**，从而突破内存带宽限制。

#### **3. FlashAttention的技术本质 (Technique from paper)**
*   **两大核心技术**：Flash**并非**发明了全新的数学运算，而是将GPU编程中两项成熟的技术——**分块（Tiling）** 和**重计算（Recomputation）**——巧妙地应用于注意力计算。
    1.  **分块（Tiling）**：将巨大的Q、K、V矩阵**分割成小块**，每次只将一个小块从HBM加载到高速SRAM（共享内存）中进行计算。计算完一个小块的结果后，再处理下一个块。这避免了在HBM中存储庞大的中间矩阵 $S$ 和 $P$。
    2.  **重计算（Recomputation）**：在前向传播中，**不保存**注意力分数矩阵 $S$ 和 softmax输出 $P$。在反向传播需要时，**根据存储的Q、K、V小块重新计算**这些中间结果。这是一种经典的“用计算换内存”的策略。
*   **最终效果**：通过这两项技术，FlashAttention将注意力计算对HBM的访问复杂度从**O(n²) 降低到了 O(n)**（子二次访问），从而实现了数量和速度上的巨大提升。

#### **4. 工程实现与调优 (Block size matters)**
*   **块大小（Block Size）的影响**：右侧折线图表明，FlashAttention的性能（运行时间和HBM访问量）**对块大小（SRAM容量）非常敏感**。需要根据GPU硬件的SRAM大小精心选择和调整块大小，以取得最佳性能。
*   **算法保证**：尽管使用了分块和重计算，FlashAttention通过精细的算法设计，确保了最终的计算结果与标准注意力机制**在数学上是完全等价的（exact attention）**，没有任何近似。

### **整体结论**
FlashAttention的成功是**将通用优化思想（分块、重计算）与特定领域问题（注意力计算）完美结合的典范**。它深刻理解了GPU的内存层次结构（SRAM vs. HBM）和注意力计算的数据流特性，通过减少对慢速内存的访问，而非单纯减少计算量（FLOPs），从根本上解决了Transformer模型训练和推理中的内存瓶颈问题。

## 15 - the forward pass of flash attention

![Tiling part 1](https://pic3.zhimg.com/v2-f58a2e7c2dd0b29ed8444951c415d85a_1440w.jpg)
![Tiling part 2](https://pic2.zhimg.com/v2-bf53c47d0f9de90ec958450970cad047_1440w.jpg)
![Tiling part 3](https://pic3.zhimg.com/v2-039d606b962f3f8606e1005aa8319df0_1440w.jpg)

### 内容概括

这三张幻灯片系统地揭示了**FlashAttention算法**的核心实现机制，特别是其如何通过**分块（Tiling）技术**和**在线Softmax计算**来克服标准注意力机制中的内存瓶颈。内容从FlashAttention的整体计算流程图出发，首先展示了如何对Q、K、V矩阵乘法进行分块以适配GPU的内存层次结构（SRAM vs. HBM），然后深入探讨了实现分块Softmax的关键数学技巧——“在线Softmax”算法，最终将两者结合，完整阐述了FlashAttention前向传播的高效工作流程。

---

### 要点总结

#### **1. 分块策略与内存层次结构 (Tiling part 1: tiling for the KQV matrix multiply)**
*   **核心问题**：标准注意力机制需要计算并存储巨大的 $[n x n]$ 中间矩阵（S和P），远超GPU高速SRAM的容量，导致必须频繁访问慢速的HBM，形成性能瓶颈。
*   **解决方案**：采用**分块（Tiling）** 策略。将大的Q、K、V矩阵**切割成小块（Tiles）**。
*   **执行流程**：
    1.  将计算所需的小块数据从HBM**加载到高速的SRAM**中。
    2.  在SRAM上进行**高效的矩阵乘法**计算，得到部分结果。
    3.  将部分结果**写回HBM**。
    4.  循环此过程，直至处理完所有数据块。
*   **遗留挑战**：Softmax操作需要全局的归一化信息（分母中的求和项），而分块计算无法直接获得全局信息。

#### **2. 在线Softmax：分块计算的关键数学技巧 (Tiling part 2: incremental computation of the softmax)**
*   **核心问题**：如何在分块计算的情况下，正确且高效地计算Softmax？
*   **解决方案**：采用**“在线Softmax”（Online Softmax）** 算法（源自Mikailov和Gimelshein, 2018）。
*   **算法精髓**：通过增量式地跟踪两个关键统计量，即可分块计算Softmax：
    1.  **最大值（m）**：动态更新当前已处理数据块中的最大值。
    2.  **伸缩和（d）**：一个经过巧妙设计的、可增量更新的求和项。其更新公式为：$d_j = d_{j-1} * e^{m_{j-1} - m_j} + e^{x_j - m_j}$。这个公式确保了求和项在不同数据块之间能够正确地进行“伸缩”和合并。
*   **最终计算**：处理完所有数据块后，即可得到全局的最大值 $m_V$ 和求和项 $d_V$。每个元素的Softmax输出为：$y_i = e^{x_i - m_V} / d_V$。

#### **3. FlashAttention前向传播的整体流程 (Putting it all together – the forward pass of flash attention)**
*   **技术融合**：FlashAttention的前向传播是**分块矩阵乘法**和**在线Softmax**技术的完美结合。
*   **工作流程**（根据Dao, 2023）：
    1.  **分块计算内积**：将Q、K分块，在SRAM上计算块间的注意力分数矩阵 $S_ij$。
    2.  **融合指数运算**：对 $S_ij$ 应用指数运算，并利用**在线Softmax算法**增量更新全局最大值和求和项。
    3.  **分块计算输出**：结合V矩阵的分块，逐步计算并累加最终的输出矩阵 $O$。
*   **内存优化**：在整个前向过程中，巨大的中间矩阵 $S$ 和 $P$ **从未在HBM中存在过**，极大地减少了内存读写次数。
*   **反向传播提示**：反向传播同样采用分块策略，需要时会根据存储的Q、K、V小块**重新计算（Recompute）** 中间结果，这也是“以计算换内存”思想的体现。

### **整体结论**
FlashAttention的成功并非源于全新的数学发现，而是对**分块（Tiling）** 和**重计算（Recomputation）** 这两项经典GPU优化技术的创造性应用。它通过：
1.  **分块处理**：将大数据分解为小块，适配SRAM容量。
2.  **在线Softmax**：巧妙的数学方法使Softmax能够分块计算。
3.  **数据流重构**：彻底避免在HBM中存储大型中间矩阵。

从而将注意力计算对HBM的访问复杂度从O(n²)降至O(n)，从根本上解决了Transformer模型的内存瓶颈，实现了巨大的性能提升。这是一种将底层硬件特性与上层算法设计深度融合的典范。
本文主要整理CS336 Lecture 15 RLHF / ALIGNMENT章节的主要内容。

## 1.0 The class thus far

### 核心主题理解

本部分**核心主题**是展示大型语言模型从**基础能力（GPT-3）** 到**实用化、人性化（instructGPT/ChatGPT）** 的关键演进。它通过两个具体的商业应用案例，生动地说明了这种技术进步带来的巨大影响。

*   **起点（左侧）：** GPT-3 作为强大的文本生成基础模型，展示了其商业应用潜力（如 Copy.ai 用于营销文案生成）。
*   **演进目标（右侧）：** 提出了核心问题——如何从有时不可控的GPT-3，发展到能更好理解人类指令、进行对话的模型（instructGPT/ChatGPT），并引出了BuzzFeed使用此类技术的新闻作为证据。
*   **最终落脚点（右下角）：** 明确指出了这一演进方向的成果——**ChatGPT**，一个为对话优化的语言模型。

### 要点总结

**1. 技术演进路径**
*   **基础：** 预训练模型（如GPT-3）具备了强大的语言生成能力。
*   **目标：** 演进至能更好遵循指令、更安全、更符合人类期望的模型（instructGPT）。
*   **成果：** ChatGPT 是这一优化路径上的成功代表，专为对话交互而设计。

**2. 商业应用案例（左 vs 右）**
*   **案例一（Copy.ai & GPT-3）：**
    *   **领域：** 营销文案生成。
    *   **展示方式：** 产品界面截图，包含具体的输入（品牌介绍）和输出（生成的圣诞促销文案）。
    *   **意义：** 证明了基础大模型（GPT-3）在内容创作领域的直接应用价值，展示了其生成连贯、有创意文本的能力。

*   **案例二（BuzzFeed & ChatGPT）：**
    *   **领域：** 媒体内容创作。
    *   **展示方式：** 新闻标题截图，指出BuzzFeed计划使用ChatGPT的创建者的技术来生成内容。
    *   **意义：** 预示着新一代对话优化模型正在改变主流媒体的内容生产模式，其影响更为深远。

**3. 核心问题与转折点**
*   幻灯片中间用蓝色大字提出了关键问题：**“But how do we get to instructGPT?”**
*   这个问题是连接左右两部分的核心，它引导听众思考从“能生成文本”的模型到“能理解意图、安全对话”的模型之间需要什么样的技术突破（例如：基于人类反馈的强化学习-RLHF）。

## 1.1 instruct理解

### 核心含义：遵循指令

这是最直接的含义。**Instruct** 指的是模型能够**理解并执行**用户给出的明确指令。

*   **GPT-3 之前更像什么？** 像一个知识渊博但缺乏专注的“万事通”。你给它一个开头，它就会基于这个开头自顾自地继续“续写”下去，但结果可能偏离你的本意。
    *   *例如：* 你输入“中国的首都是”，它会正确续写“北京”。但如果你输入“写一首关于月亮的诗”，它可能会写出一首诗，也可能开始论述月球的天文知识，因为它只是在“续写”你的句子。
*   **InstructGPT 的核心能力**：它被训练来识别你的句子是一个需要被执行的“任务”或“指令”，而不仅仅是一个需要被“续写”的开头。
    *   *例如：* 当你给出指令“**写一首**关于月亮的**七言绝句**”时，它能理解到关键动作是“写”，对象是“诗”，且有特定格式要求“七言绝句”，然后生成符合要求的文本。

所以，**instruct** 首先意味着模型从被动的“续写者”变成了主动的“任务执行者”。

### 技术层面的含义：指令微调

在技术实现上，**instruct** 特指一种关键的训练方法——**指令微调**。

*   **基础模型（如GPT-3）**：通过海量文本进行“预训练”，学会了语言的规律和世界的知识，但不知道如何与人类“对话”。
*   **InstructGPT**：在GPT-3的基础上，使用大量由人类标注员编写的 **“指令-期望回复”** 配对数据进行**微调**。
    *   **指令范例**： “用简单的语言解释量子力学”
    *   **期望回复**： 标注员撰写的一个简单、准确的解释。
*   通过反复学习成千上万这样的配对，模型逐渐掌握了“当人类这样说时，我应该这样回答”的模式。这个过程就是**指令微调**，它让模型学会了“遵从指令”（To Follow the Instruction）。

### 引申含义：对齐 - 理解意图，符合期望

这是 **instruct** 更深层、更重要的含义，即 **对齐**。它不仅仅是遵循字面指令，更是要**理解人类的真实意图**，并生成**有用、真实、无害**的回复。

*   **理解意图**：人类指令可能是模糊或不完整的。一个好的模型需要揣测背后的意图。
    *   *例如：* 指令是“我很热”。字面意思是陈述一个事实，但模型的“指令”是理解到用户的意图可能是“请帮我降温”，从而给出“要我把空调打开吗？”或“喝点冰水怎么样？”这样的有用建议。
*   **符合人类期望**：通过一种叫做“基于人类反馈的强化学习”的技术，模型被训练成更倾向于生成人类标注员认为“好”的答案（如清晰、有帮助、安全的答案），而不是虽然通顺但有害或无用的答案。

## 2.0 What are the ingredients in SFT?

### 内容概况

本部分系统地介绍了**监督微调** 的关键组成部分和实施方法。它采用上下结构，清晰地回答了SFT的两个核心问题：**“用什么数据？”** 和 **“如何训练？”**。

*   **上半部分** 详细展示了SFT所使用的**训练数据**，包括不同任务类型、对应的数据集、类别和任务数量，体现了数据构成的多样性和规模。
*   **下半部分** 通过对比两个饼图，直观地展示了训练过程中**数据混合策略** 在不同阶段（稳定阶段 vs. 衰减阶段）的动态变化。
*   **右侧信息框** 介绍了开展此工作的项目——**Open Assistant**，阐明了其打造革命性对话AI的愿景。

### 要点总结

#### 1. SFT的训练数据构成
本部分上半部分表明，一个有效的SFT需要**大规模、多任务、高质量**的数据集。这些数据通常包括：
*   **多种任务类型**：如常识推理、闭卷问答、对话生成等，旨在让模型掌握多种技能。
*   **丰富的数据来源**：每个任务类型下都包含多个知名数据集，保证了数据的广泛性。
*   **巨大的任务数量**：图中列出的任务总数（如207, 761, 500等）表明，需要海量的训练实例才能使模型很好地遵循指令。

**核心要点**：SFT的成功严重依赖于训练数据的广度、深度和质量。

#### 2. SFT的训练方法
本部分下半部分的两个饼图揭示了SFT训练的一个关键技巧：**动态数据混合**。
*   **阶段化策略**：训练分为 **“稳定阶段”** 和 **“衰减阶段”**。
*   **不同混合比例**：
    *   **稳定阶段**：训练数据由几种主要类型（如图中几种不同颜色的饼图区块）按一定比例混合。这有助于模型稳定地学习各项基本技能。
    *   **衰减阶段**：数据混合比例发生变化，某些类型的数据权重可能会被“衰减”，而另一些则被加强。这通常是为了针对性地提升模型的特定能力或平衡学习效果。

**核心要点**：SFT并非简单地将所有数据一次性输入，而是采用精心设计的、动态调整的数据采样策略，以优化训练效果。

## 2.1 Training data

We’ve already seen some of the major types of instruction data..
Let’s talk about two more details about instruction tuning datasets
1. What’s actually inside these datasets?
2. What matters in building ‘high performance’ instruction tuning data?

## 2.2 Looking inside some instruction-tuning data

![Looking inside some instruction-tuning data](https://pic4.zhimg.com/v2-08e9df5624c696234c684aa0e2023cbb_1440w.jpg)

## 2.3 FLAN / Alpaca / OpenAssistant

### 内容概况

1.  **FLAN数据集示例**：展示了**多样化、任务型**的样本。包含为邮件写主题、新闻分类、文章摘要、根据结构化数据生成句子等，更像是在测试模型完成特定、定义明确的NLP任务的能力。
2.  **Alpaca数据集示例**：展示了**简洁、通用、面向日常助手**的样本。问题更贴近普通用户的日常提问，如健康建议、概念解释、编写代码等，回答直接、实用。
3.  **OpenAssistant数据集示例**：展示了**复杂、深入、对话式**的样本。问题更具深度和专业性，要求模型进行论述和引用，回答则详尽、结构化，更像一个知识渊博的人类专家在提供帮助。

### 要点总结

| 特征 | FLAN | Alpaca | OpenAssistant |
| :--- | :--- | :--- | :--- |
| **核心目标** | **提高模型泛化能力**，通过多任务指令学习，让模型“学会如何遵循指令”。 | **创建一个低成本、高质量的指令跟随模型**，旨在复现类似ChatGPT的能力。 | **通过社区众包，构建一个完全开源、透明、高质量的人类标注对话数据集**。 |
| **任务/指令风格** | **格式化工任务**。指令更像标准的NLP任务描述（如“分类”、“总结”、“生成句子”）。 | **通用助手任务**。指令是用户可能向AI助手提出的典型问题，简单直接。 | **开放域深度对话**。指令更复杂，常要求模型进行解释、论述、提供创意，具有多轮对话的潜力。 |
| **回答风格** | **任务导向型**。回答精准对应任务要求，相对客观、简洁。 | **实用简洁型**。回答条理清晰（如分点列举）、实用，包含可执行代码。 | **详尽专业型**。回答内容深入、结构完整（如包含参考文献），语言更像一篇微型文章或专业回复。 |
| **数据来源** | 基于已有的**学术NLP数据集**进行重构和转化。 | 使用**self-instruct**方法，以更强大的模型（如GPT-3.5/4）为教师，自动生成指令-回答对。 | 由**全球志愿者社区**人工标注和评分产生，是真正的人类反馈数据。 |
| **代表性意义** | 指令微调方向的**开创性工作之一**，验证了“指令调优”能显著提升模型的零样本泛化能力。 | 证明了用**更少、更便宜的方法**也能训练出强大的指令跟随模型，降低了该领域的技术门槛。 | 代表了**开源、透明、众包**的AI发展路径，是社区推动AI发展的重要实践。 |

### 数据集介绍

#### 1. FLAN (Finetuned Language Net)
- **核心介绍**：FLAN 是Google的研究项目，其核心思想是**指令调优**。它不是创建一个新的数据集，而是将数十个已有的经典NLP数据集（如文本分类、摘要、问答等）重新格式化为统一的“指令”形式。
- **设计理念**：通过让模型在五花八门的任务上学习“如何理解指令”，从而激发模型在未见过的任务上的**零样本泛化能力**。例如，模型学会了“总结这篇文章”的指令模式后，即使遇到一篇全新的文章，它也知道该怎么做。
- **从示例看特点**：图片中的示例（邮件主题、新闻分类、数据到文本生成）正是这种“多任务集成”的体现，旨在训练一个“多面手”模型。

#### 2. Alpaca
- **核心介绍**：Alpaca是斯坦福大学发布的一个数据集及模型。其目标是**低成本地复现一个类似ChatGPT性能的模型**。它包含5.2万条指令-输出样本。
- **设计理念**：采用 **“Self-Instruct”** 方法。具体来说，是使用OpenAI的`text-davinci-003`模型作为“教师机”，自动生成大量的指令-回答对，然后用这些数据来微调一个开源的基础模型（如LLaMA），得到Alpaca模型。
- **从示例看特点**：示例中的问题（健康建议、算法含义、求平均值代码）都是非常典型的、用户会向ChatGPT提问的问题。回答简洁实用，体现了其目标是打造一个“平价版”的通用AI助手。

#### 3. OpenAssistant (OASST)
- **核心介绍**：OpenAssistant是一个由全球超过1.3万名志愿者参与、通过众包方式构建的**大规模、高质量的人工标注对话数据集**。其目标是创建一个真正开源、透明的能与ChatGPT媲美的聊天助手。
- **设计理念**：强调**人类反馈**和**对话质量**。数据收集过程模拟了真实的对话交互，志愿者既扮演用户提出高质量问题，也扮演助手生成回复，并对其他回复进行排名，从而收集到大量的人类偏好数据。
- **从示例看特点**：示例中的问题（解释“买方垄断”并引用研究、为孩子推荐科学项目）明显更具深度和复杂性，要求助手具备知识整合与创造性思维的能力。回答也非常详尽、专业，并包含引用，展示了其对高质量、长文本对话能力的追求。

## 2.4 What did we notice across the datasets?

### 整体内容概况

这组材料系统性地探讨了一个核心问题：**不同的指令微调数据集如何塑造最终模型的行为和性能？** 其逻辑脉络如下：

1.  **提出问题（图1）**：首先指出不同数据集在**格式风格、知识复杂度、规模、安全性**等方面存在显著差异，并引出核心问题：“这些因素如何影响模型？”

![What about benchmarks?](https://pica.zhimg.com/v2-9a8f182e1788618b88f4b0910e601d4a_1440w.jpg)

2.  **量化风格差异（图2）**：通过详尽的表格数据，具体展示了不同数据集在**提示词长度、输出长度、对话轮数**等“风格”维度上的巨大差异。

![When evaluating by preferences, style matters.](https://picx.zhimg.com/v2-9529c2d141b3fac13c76fcb355b23dc5_1440w.jpg)

3.  **分析风格偏好的影响（图3）**：通过偏好评估实验，揭示了模型的输出风格（尤其是**长度**和**列表格式**）会**强烈影响**人类和AI评估者的判断，即“风格本身会影响优劣评价”。
4.  **评估对基准测试的影响（图4）**：最后，通过多维度基准测试，检验了不同数据集在提升模型**事实性、推理、编程**等硬实力方面的效果，并指出风格等因素对这些传统基准影响不大。

---

### 综合要点总结

#### 1. 核心结论：数据集特征深刻影响模型，但影响层面不同
这四张图共同论证了一个核心观点：**用于指令微调的数据集并非中性，其内在特征会直接“烙印”在模型上，主要影响两个层面：**
*   **表层行为与风格（图2、图3的核心发现）**：模型会直接模仿其训练数据的**输出风格**，如回答长度、是否使用项目符号等。更重要的是，这种风格会显著影响用户对模型能力的主观偏好和评价。
*   **深层核心能力（图4的核心发现）**：数据集的内容和质量决定了模型在**事实知识、逻辑推理、代码生成**等核心能力上的上限。不同来源的数据集在不同能力上各有专长。

#### 2. 关键洞察：警惕“风格偏好”带来的评估陷阱
图3是其中最具启发性的部分，它揭示了一个重要现象：**在评估模型时，人类和AI都表现出对“更长、更有结构（如列表）回答”的强烈偏好。** 这意味着：
*   一个回答更长、格式更漂亮的模型，可能仅仅因为其**风格**就在评估中胜出，即便其内容质量与较短的对手相当。
*   这提醒我们，不能仅凭主观偏好判断模型优劣，需要像图4那样通过**多样化、去风格化的基准测试**来客观衡量其真实能力。

#### 3. 实践指导：没有“万能”数据集，混合策略是最优解
图4的表格提供了至关重要的实践指导：
*   **专才 vs 通才**：不同数据集训练的模型在不同任务上表现各异。例如，“Open Assistant 1”和“GPT4-Alpaca”在开放式对话上表现出色，而“CoT”在推理任务上优势明显。
*   **混合策略为王**：表格最后两行明确显示，**结合了人类数据和GPT数据的混合数据集**在平均表现上最优。这证明了为了打造一个全面强大的模型，采用**多元化、互补的数据混合策略**比依赖单一类型的数据集更有效。

## 2.5 References, complex knowledge, and factuality

### 内容概况

#### 图1：参考文献、复杂知识与事实性
*   **主题**：通过一个具体案例（要求模型解释“买方垄断”并引用研究），探讨如何训练模型输出有依据、可信的内容。
*   **结构**：
    *   **左侧（输入）**：一个高质量的指令，要求模型结合例子和引用来解释一个专业术语。
    *   **右侧（输出）**：理想的回答，包含了术语定义、实例分析以及格式规范的学术引用。
    *   **底部（分析）**：提出了两个教学目标，并抛出一个关键疑问：模型究竟是通过什么机制来“知道”并输出这些引文的？

![Knowledge extraction and alignment](https://pic1.zhimg.com/v2-4f28cadfe2c3906c34af76dacd571d24_1440w.jpg)

#### 图2：知识提取与对齐
*   **主题**：通过实验数据，科学地验证并挑战了一个关于微调的“传统观点”（Folklore）。
*   **结构**：
    *   **左侧（理论与问题）**：提出了一个普遍担忧——在模型不知道的事实上微调会导致其“幻觉”（胡编乱造）。
    *   **右侧（实验数据）**：一张关键的折线图，展示了模型在“已知事实”和“未知事实”上随着训练周期（Epochs）增加的准确率变化，为左侧的问题提供了答案。

---

### 要点总结

#### 1. 目标：训练模型成为“严谨的学者”而非“信口开河的讲故事者”
**图1** 展示了指令微调的一个高级目标：我们希望模型不仅能生成文本，还能像严谨的学者一样，**为其陈述提供证据（引用），处理复杂知识，并保证事实准确性**。这超越了基本的对话能力，指向了可靠、可信的AI助手。

#### 2. 挑战与迷思：对“知识微调”的普遍担忧
**图2的开头**指出了一个广泛存在的担忧（Folklore）：如果我们在微调时教给模型一些它**预训练阶段从未见过的新知识**，可能会迫使模型“捏造”答案，从而导致**幻觉（Hallucination）**。这是一种风险。

#### 3. 关键发现（核心结论）：模型可以有效学习新知识，但需要正确方法
**图2的折线图是答案所在**，它通过实验驳斥了上述迷思，并揭示了机制：
*   **模型可以学习新知识**：图表显示，即使在训练前“未知”的事实上，模型的开发集准确率（Dev Accuracy）也随着训练稳步上升。这表明**微调确实能将新知识有效地“注入”模型**。
*   **学习模式与风险**：图表揭示了两种学习模式：
    *   **已知事实**：模型快速学习并达到高准确率（“Train Known”线）。
    *   **未知事实**：模型学习速度较慢，但最终也能很好地掌握（“Train Unknown”线）。关键在于，如果训练过度（周期过多），模型对“未知事实”的掌握会先于“已知事实”开始下降，这提示了**过拟合的风险**——即模型死记硬背了训练数据，反而丧失了泛化能力。

*   **对图1疑问的解答**：这个机制回答了图1的疑问“模型通过什么机制知道引用？”——答案就是**通过指令微调**。模型在包含引文的优质数据上训练，学会了“当被要求引用时，应该从参数记忆中提取并输出特定的文献信息”这种模式和行为。

### 总结

总而言之，这两张图片连贯地说明了：**通过精心设计的、包含可靠知识和引用范例的指令数据对模型进行微调，是一种有效且必要的“知识对齐”手段。它不仅能教会模型新的具体事实，更能培养其输出有据可查、负责任的答案的行为习惯。** 成功的关键在于平衡数据质量和训练强度，以避免过拟合，确保模型真正理解而非机械记忆。

## 2.6 Safety-tuning

### 内容概况

1.  **图1：安全微调的效果演示（概念与效果）**
    *   通过两个指令的对比，直观展示了安全微调的前后差异。一个未经安全微调的模型会直接响应有害指令（如“列出有趣的事”），而经过安全微调的模型学会了**拒绝回答有害指令**（如“如何杀人”）。

![Safety-tuning](https://pic4.zhimg.com/v2-91dc654d561a626d196a23b186d7c371_1440w.jpg)

2.  **图2：安全微调的平衡挑战（挑战与风险）**
    *   指出了安全微调中的一个核心难题：**平衡“安全”与“过度拒绝”**。如果使用过多安全数据，模型会变得过于敏感，甚至拒绝回答原本无害的指令（如“如何终止Python进程”），这被称为“夸大安全响应”。

3.  **图3：安全微调的数据效率（方法与效能）**
    *   通过柱状图数据证明，**仅需少量高质量的安全数据（约500个样本）**，就能显著提升模型在各种安全测试集上的表现，实现高效的安全对齐。

---

### 要点总结

#### 1. **核心价值：安全微调是塑造模型安全边界的关键手段**
    安全微调是一种专门的指令微调，其目的不是教会模型新知识，而是**教会模型“什么不该做”**。它通过在包含有害指令和恰当拒绝回复的数据上进行训练，为模型建立安全护栏，使其从“有能力但可能有害”变得“既能力强又安全可靠”。

#### 2. **关键挑战：必须在“安全”和“可用性”之间取得平衡**
    安全微调并非越严格越好。**过度调优** 会导致模型出现 **“过度拒绝”** 的问题：
    *   **表现**：模型变得“草木皆兵”，甚至会拒绝回答完全无害、合法的请求（例如正常的编程问题）。
    *   **后果**：这严重损害了模型的可用性和帮助性，从一个极端（有问必答，包括有害内容）走向另一个极端（拒绝回答，包括无害内容）。

#### 3. **重要发现：“少而精”的数据策略即可有效提升安全性**
    安全微调并不需要海量数据，这在实际应用中极具价值：
    *   **高效性**：如图3所示，**仅仅添加约500个精心构建的安全示例**，就能让模型在遵循安全准则方面取得显著进步。
    *   **实用性**：这意味着以较低的成本和较快的速度，就能为一个强大的基础模型注入重要的安全属性，大大降低了安全对齐的门槛。

### 总结

总而言之，这三张图片清晰地表明：**安全微调是通过“教学”让模型学会拒绝有害请求的有效方法。成功的实践关键在于使用“少而精”的数据，并精准拿捏尺度，在构筑坚实安全护栏的同时，避免因过度敏感而损害模型应有的实用价值。** 这是一个在“安全”与“可用”之间寻找最佳平衡点的精细工艺。

## 2.7 Putting it together – SFT Data

### 要点总结

#### 1. 核心原则：SFT 旨在“激发”而非“灌输”
- Instruction fine-tuning (SFT) works works best when we are just extracting pre-
training behaviors, not adding new ones
- **要点**：指令微调（SFT）在最有效的情况下，是**提取和激发模型在预训练阶段已经学会的行为和能力**，而不是试图教给模型全新的、它从未接触过的知识或技能。
- **解读**：这好比是引导一个已经博览群书的人如何更好地回答问题，而不是从零开始教一个小学生。SFT的成功很大程度上依赖于预训练模型本身已经具备的丰富知识基础和语言能力。SFT的作用是教会模型如何按照指令来调用这些已有的能力。

#### 2. 重要警示：即使是“正确”的数据也可能带来风险
- Adding (factually correct!) data can sometimes hurt
- **要点**：添加（在事实上正确的！）数据有时**可能产生负面影响（有害）**。
- **解读**：这是一个反直觉但至关重要的洞察。盲目增加数据并不总是带来性能提升。如果新加入的SFT数据与模型在预训练阶段建立的知识分布或推理模式存在冲突，可能会导致：
    - **性能下降**：干扰模型原有的正确判断，使其表现不稳定。
    - **过拟合**：模型只是机械记忆了新的数据点，但丧失了泛化能力。

#### 3. 实践指导：聚焦“关键行为”，接受“长尾效应”
- Small amounts of the right kinds of behavior (safety, instruction-following, style)
make a big difference, but there is a long-tail that benefits from more data
- **要点**：少量**正确类型的行为数据**（如安全性、指令遵循、特定风格）能产生巨大影响；但同时存在一个**长尾问题**，需要更多数据来覆盖。
- **解读**：这条要点提供了极具操作性的建议：
    - **高效率区**：对于模型行为的核心方面（如**学会拒绝有害指令、理解基本指令格式、模仿特定口吻**），通常只需要相对少量的高质量数据进行微调，就能看到显著效果。
    - **长尾挑战**：要处理各种各样、千奇百怪的复杂指令或细分任务（即“长尾”部分），则需要更大规模、更多样化的数据来持续改善模型表现。不能指望用少量数据解决所有问题。

## 3.0 Turning instruction tuning into pretraining

The following (increasingly popular) idea says yes:
1. Pre-train on web/pretraining data
2. Mix in instruction-tuning data into pre-training
3. Do an actual (but short) instruction-tuning round.

## 3.1 From imitation to optimization

### 内容概况

本部分的核心主题是**大型语言模型训练范式的演进**，即**从“模仿”到“优化”**。它通过对比的方式，清晰地阐述了两种关键方法的核心思想、数学本质及其局限性：

*   **模仿：** 即**监督微调（SFT）**，其目标是让模型尽可能逼真地“模仿”一个高质量的参考数据分布。
*   **优化：** 即**基于人类反馈的强化学习（RLHF）**，其目标是直接“优化”一个可量化的奖励函数，寻找最优策略。

---

### 要点总结

1.  **两大范式对比：**
    *   **SFT（模仿）：** 视角是**生成式建模**。模型的任务是学习并拟合一个给定的、高质量的参考数据分布（通常由人类标注者提供）。它是一个相对被动和保守的过程。
    *   **RLHF（优化）：** 视角是**策略优化**。模型被看作一个“策略”，其任务不再是模仿，而是主动探索和生成能最大化奖励函数（通常基于人类偏好）的回应。这是一个更主动和寻求突破的过程。

2.  **核心依赖不同：**
    *   **SFT** 严重依赖于**高质量的参考样本**。模型的能力上限很大程度上受限于所提供示范数据的质量和多样性。
    *   **RLHF** 的核心是定义一个良好的**奖励函数**。这个函数如同“指挥棒”，直接引导模型优化的方向，其设计至关重要。

3.  **哲学思想的演进：**
    *   幻灯片标题暗示了一个发展路径：先从**模仿人类（SFT）** 开始，打下基础，再转向**优化特定目标（RLHF）**，以超越单纯模仿，生成更符合人类复杂偏好的内容。

---

### 打印公式

#### 1. 模仿 - 监督微调（SFT）的目标公式

**公式：**
$$ \hat{p}(y|x) \approx p^*(y|x) $$

**公式解读：**
*   $ p^*(y|x) $：代表理想的、高质量的**参考条件分布**。即给定输入 $ x $（如一个问题），人类专家或高质量数据所对应的最优输出 $ y $（如回答）的分布。
*   $ \hat{p}(y|x) $：代表我们**要训练的模型**。
*   $ \approx $：表示“近似于”或“拟合”。
*   **整体含义：** 训练模型 $ \hat{p} $，使其输出的条件概率分布 $ \hat{p}(y|x) $ 尽可能接近理想的参考分布 $ p^*(y|x) $。

#### 2. 优化 - 基于人类反馈的强化学习（RLHF）的目标公式

**公式：**
$$ \max_{p} \mathbb{E}_{y \sim p(\cdot|x)}[R(y, x)] $$

**公式解读：**
*   $ R(y, x) $：代表**奖励函数**。它为一个给定的“输入-输出”对 $ (x, y) $ 打分，分数越高表示输出越好（越符合人类偏好）。
*   $ p(y|x) $：代表模型的**策略**，即给定输入 $ x $ 时，模型选择输出 $ y $ 的概率。
*   $ \mathbb{E}_{y \sim p(\cdot|x)}[\cdot] $：表示在模型策略 $ p $ 下，对所有可能输出 $ y $ 的**期望奖励**。
*   $ \max_{p} $：表示要**最大化**这个期望奖励。
*   **整体含义：** 寻找一个最优的策略 $ p $（即最优的模型），使得它生成的回答能获得最高的**期望奖励**。

## 3.2 Why optimize? Costs

![RLHF](https://pic1.zhimg.com/v2-4b0271e0a8af60e5f42bfe6b37f84fd2_1440w.jpg)


### 内容概况

这张幻灯片从一个非常实际的角度——**成本**——来论证为什么在大型语言模型训练中，从单纯的“模仿”（监督微调，SFT）转向“优化”（如基于人类反馈的强化学习，RLHF）是必要且高效的。它通过一个针对7B参数模型的**成本分解表格**，直观地对比了不同训练阶段的计算成本和数据标注成本。

---

### 要点总结

#### 1. 核心论点：获取“偏好反馈”比提供“完美答案”成本更低
*   Might be easier to get scalar feedback rather than optimal policy.
*   **深层含义**：对于许多复杂任务，让人类专家**验证**一个答案的质量（打分或比较），远比让他们从头**生成**一个高质量答案要省时省力。这降低了高质量训练数据的获取门槛。

#### 2. 关键证据：成本表格揭示SFT数据标注是最大开销
成本对比表格提供了极具说服力的证据：

| 成本类型 | 基础模型（一次性） | 监督学习 |  pairwise反馈  |   RL   | 评估 |
| :--- | :---: | :---: | :---: | :---: | :---: |
| **计算成本** | 30万美元 | 100美元 | 100美元 | 100美元 | 0美元 |
| **标注成本** | **0美元** | **2.5万美元** | **4000美元** | **0美元** | 50美元 |

*   **SFT数据极其昂贵**：监督微调阶段的**标注成本（$25k）** 远高于其他所有后续阶段（如RLHF中的偏好标注仅需$4k）的成本总和。这是因为撰写高质量的示范答案需要很高的专业知识和时间成本。
*   **优化阶段成本优势明显**：RLHF阶段的计算成本很低，其标注成本（偏好对比）也远低于SFT。这表明，**通过相对廉价的“优化”方法，可以显著提升模型性能，性价比很高。**

#### 3. 最终结论：前沿模型实验室的投资重点已转向后训练数据
幻灯片最后总结道：**大多数前沿模型实验室在“后训练数据”（即SFT和RLHF所用的数据）上投入了数百万美元。**
*   这强调了在基础模型趋同的背景下，**高质量的后训练数据已成为核心竞争力**。而其中，如何高效地利用成本（即更多采用“优化”思路而非纯粹“模仿”）是降低成本、提升效果的关键。

## 3.3 Why optimize? G-V gap

### 内容概况

这张幻灯片揭示了一个关键洞察：**人类标注者所“书写”的示范答案，并不总能代表他们内心真正“偏好”的答案。** 

---

### 要点总结

#### 1. 核心问题：揭示“所言”与“所好”之间的差距
幻灯片开篇即点明核心论点：**人们并不总是会写下他们自己在LLM输出结果中真正偏好的内容。**
*   **G-V Gap** 指的是 **“所说即所信”** 的假设与**真实偏好**之间的差距。在这里具体表现为：当要求标注者为SFT**编写**一个高质量的示范摘要时，他们写下的是一种风格（例如，更像人类专家的写法）；但当让他们**评估**模型生成的摘要时，他们内心可能更偏好另一种风格的输出（例如，更简洁、重点更突出的模型摘要）。

#### 2. 关键证据：偏好高度分散且不一致
条形图提供了强有力的证据来支持上述论点：
*   **总体偏好近乎平分**：总体上，对自由撰稿人摘要的偏好（50.4%）和对Instruct Davinci摘要的偏好（49.6%）几乎各占一半，且一致性系数α低至0.07，表明**群体内部没有达成明确共识**。
*   **个体偏好存在巨大分歧**：不同标注者的偏好差异极大。例如，标注者1明显偏好模型摘要（57.0%），而标注者6则强烈偏好人类摘要（56.9%）。这证明**“何为高质量”的标准是主观且多元的**。

#### 3. 最终结论：单纯“模仿”SFT存在局限，必须引入“优化”
这张幻灯片有力地论证了从“模仿”（SFT）转向“优化”（如RLHF）的必要性：
*   **SFT的局限性**：如果只做SFT，相当于强迫模型去“模仿”某一位标注者写下的特定风格的答案。但数据表明，这个被模仿的答案可能并非大多数人（甚至包括撰写者自己）的真正偏好。
*   **优化的优势**：而RLHF等优化方法，通过收集人类对模型输出的**偏好判断**（例如，“回答A比回答B好”），能够直接学习到人类模糊但真实的“好感度”函数。这允许模型探索和生成可能比有限的SFT示范答案**更优**的输出，从而绕过G-V Gap问题。

### 总结

总而言之，这张幻灯片通过令人信服的实验数据揭示：**由于人类偏好本身是主观和多样的，仅依靠模仿有限的人类示范（SFT）来训练模型存在天花板。必须引入基于偏好的优化方法（如RLHF），才能直接对齐人类复杂且内隐的评判标准，从而训练出真正符合人类喜好的模型。** 

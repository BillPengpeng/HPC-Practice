本文主要整理CS336 Lecture 10 Inference章节的主要内容。

## 6. reduce_kv_cache_size

![gmqa](https://jax-ml.github.io/scaling-book/assets/img/gmqa.png)
![Local attention](https://research.character.ai/content/images/2024/06/figure1-2-1.png)

### 内容概况

*  从宏观上引入**分组查询注意力** 这一核心优化技术。通过对比MHA、MQA和GQA，并以Llama 2为例，展示了GQA如何在减少KV缓存大小、提升吞吐量的同时，保持模型准确性。
*  进一步介绍了三种更前沿或更复杂的注意力优化技术，包括**多头潜在注意力**、**跨层注意力** 和**局部注意力**。这些方法从不同维度（降低维度、共享缓存、限制上下文范围）对注意力机制进行革新，旨在实现准确性、缓存大小和推理速度的最佳权衡。

---

### 要点总结

#### 1. 核心问题与通用思路
*   **根本瓶颈**：LLM推理是**内存受限**的，巨大的KV缓存是主要瓶颈。
*   **优化思路**：改造注意力机制，**减少需要存储和读取的KV缓存总量**，同时尽可能保持甚至提升模型能力。

#### 2. 主要技术方案对比

| 技术方案 | 核心思想 | 优势 | 备注 |
| :--- | :--- | :--- | :--- |
| **分组查询注意力（GQA）** | 让 **K** 个键值头被 **G** 个查询头共享（`G = H / K`）。 | **效果显著且应用广泛**（如Llama 2）。在MHA（K=H）和MQA（K=1）间取得平衡，**有效减小KV缓存，对准确性影响小**。 | 工业界主流选择，易于实现。 |
| **多头潜在注意力（MLA）** | 将每个键/值向量的**维度投影到一个较低的潜在空间**（如从16384维降至512维）。 | **成本更低，且据报道准确性优于MHA**（如DeepSeek-V2）。 | 一种更激进的“压缩”方式，可能需额外处理（如适配RoPE）。 |
| **跨层注意力（CLA）** | 在不同网络**层之间共享键值缓存**（类比GQA在头之间共享）。 | 改善准确性与缓存大小的帕累托前沿。 | 与GQA思路类似，但共享维度不同。 |
| **局部注意力** | 让注意力仅作用于**一个局部窗口**内的上下文。 | **KV缓存大小与序列长度无关**，极适合长文本。 | 可能损害全局建模能力，常与全局注意力**交替使用**（混合层）以平衡效果。 |

#### 3. 关键效益
*   **直接收益**：
    1.  **减小内存占用**：KV缓存大小可减少为原来的 `1/K`（GQA）或更多（MLA）。
    2.  **提升吞吐量**：在相同内存下可支持**更大的批处理大小**，从而大幅提高吞吐量。
    3.  **改善延迟**：减少需要搬运的数据量，有助于降低延迟。
*   **核心权衡**：所有这些优化的成功，都建立在**能够保持模型准确性不大幅下降**的基础上。图片通过引用论文中的准确率图表（如MMLU基准）证明了这一点。

## 7. alternatives_to_the_transformer

### 要点总结

#### 1. 核心动机：突破Transformer的根本瓶颈
*   **问题**：Transformer的注意力机制在自回归生成时需要进行O(T)的KV缓存（T为序列长度），导致**严重的内存瓶颈**。
*   **目标**：寻找**次平方复杂度**或**恒定状态**的架构，从根本上提升长上下文推理效率。

#### 2. 主要方向一：状态空间模型
这类模型源自信号处理，核心优势是能高效处理长序列。

*   **S4**：based on classic state space models, good at synthetic long-context tasks
*   **Mamba**：allow SSM parameters to be input-dependent, match Transformers at 1B scale
*   **Jamba**：interleave Transformer-Mamba layers (1:7 ratio) with a 52B MoE
*   **BASED**：use linear attention + local attention。
*   **MiniMax-01**: use linear attention + full attention (456B parameter MoE)
*   **关键收益**：状态空间模型的核心优势在于将KV缓存**从O(T)减少到O(1)的恒定状态**，极大减轻了内存压力。

#### 3. 主要方向二：扩散模型
这类模型为文本生成提供了全新的非自回归范式。

*   **原理**：从随机噪声开始，**并行地**对整个序列进行多轮迭代去噪和 refinement，而非逐个token自回归生成。
*   **挑战**：将其成功应用于离散的文本生成比图像生成更困难。
*   **最新进展**：示例显示，扩散模型在代码生成等基准测试上**速度远超自回归模型**，展示了巨大潜力。

#### 4. 核心结论与启示
*   **架构创新潜力巨大**：通过**线性/局部注意力混合架构**或**扩散模型**等根本性变革，可以在推理效率上获得远超Transformer微调方案的提升。
*   **混合架构是实用路径**：Jamba的成功表明，**结合传统Transformer的优势和新架构的效率**（而非完全取代）是一条非常有效的实践路径。

总而言之，这张图清晰地指出，下一代高效LLM的架构正在向**状态空间模型**和**扩散模型**等方向演进，这些变革有望彻底解决当前Transformer模型在推理时面临的内存瓶颈问题。

## 8. quantization

![quantization](https://www.datocms-assets.com/104802/1709770809-twitter-post-20.png)

### 要点总结

#### 1. **核心思想与收益**
*   **目标**：降低数值表示精度（如从16位浮点数降至8位整数），减少模型存储大小和内存占用。
*   **根本收益**：由于LLM推理是**内存受限**的，减少内存占用意味着可以**提高吞吐量、降低延迟**。
*   **核心权衡**：必须在**内存效率**和**模型准确性**之间取得平衡。

#### 2. **主要的数值精度格式**
图片对比了不同精度格式的优缺点和应用场景：
*   **FP32 (4字节)**：训练阶段用于参数和优化器状态的标准格式。
*   **BF16/FP16 (2字节)**：推理的默认格式，在性能和精度间取得良好平衡。
*   **FP8 (1字节)**：新一代格式（如H100支持的E4M3），可用于训练，动态范围较大。
*   **INT8 (1字节)**：主要用于推理，精度不如FP8，但硬件支持广泛，计算成本更低。
*   **INT4 (0.5字节)**：极致的压缩，精度损失较大，但能显著提升效率。

#### 3. **两种量化实施方法**
*   **量化感知训练（QAT）**：在训练过程中模拟量化效应，让模型适应低精度。**优点**是精度保持较好；**缺点**是成本高，难以扩展到大型模型。
*   **训练后量化（PTQ）**：对已训练好的模型，通过校准数据（计算缩放因子和零点）进行量化。**优点**是简单快捷，是当前的主流方法。

![quant-freeze](https://huggingface.co/blog/assets/96_hf_bitsandbytes_integration/quant-freeze.png)
![Mixed-int8](ttps://huggingface.co/blog/assets/96_hf_bitsandbytes_integration/Mixed-int8.gif)

#### 4. **两种先进的PTQ技术（解决核心挑战）**
1.  **LLM.int8()**：
    *   **问题**：大模型中存在数值**异常值**，直接量化会严重损害性能。
    *   **解决方案**：将异常值（如大于阈值的激活值）分离出来，用FP16精度处理，其余大部分值用INT8处理。
    *   **效果**：能很好地保持精度，但比FP16(**此处是否应该为INT8?**)推理慢15-23%。由于需要分离异常值、进行混合精度计算和反量化，LLM.int8() 在推理速度上通常比 FP16 基准慢 10% 到 30%。它主要是一种内存压缩技术，而非纯粹的速度优化技术。

2.  **激活感知量化（AWQ）**：
    *   **洞察**：并非所有权重都同等重要。保护对激活值影响大的少量（0.1%-1%）权重，使其保持高精度，可以最大程度减少精度损失。
    *   **效果**：可实现极致的INT3量化，带来**4倍内存降低和3.2倍加速**，同时保持模型质量。

### 总结

**量化是通过降低数值精度来突破LLM推理内存瓶颈的关键技术**。成功的关键在于巧妙地处理**异常值**和**重要权重**，从而在效率与精度之间找到最佳平衡。LLM.int8()和AWQ等先进技术使得在仅产生极小精度损失的情况下，实现大幅的性能提升成为可能。

## 9. model_pruning

### 要点总结

#### 1. 核心思想
方法的核心可概括为：**“先剪枝，后修复”**。
*   **目标**：从一个昂贵的大模型（如Llama 3 70B）中“剔除”不重要的部分，得到一个更小、更便宜的模型。
*   **关键**：剪枝后需要一个“修复”过程来恢复模型性能，而非简单删除。

#### 2. 算法三个关键步骤
该方法是一个迭代优化过程：

1.  **识别重要组件**：
    *   在一个**极小的校准数据集**（仅1024个样本）上，评估并识别出模型中重要的层、注意力头和隐藏维度。

2.  **移除不重要部分**：
    *   根据上一步的评估，直接移除那些被判定为“不重要”的组件，从而得到一个结构更小、参数更少的模型。

3.  **知识蒸馏**：
    *   这是“修复”阶段。将原始大模型（教师模型）的知识蒸馏到被剪枝后的小模型（学生模型）中，以恢复其性能。

#### 3. 方法优势与成果
*   **数据高效**：仅需少量校准数据，避免了重新训练的巨大成本。
*   **效果显著**：根据结果图显示，该方法能够从70B模型剪枝得到38B和20B等不同规模的模型，这些模型在性能上显著优于同规模从头训练的模型，甚至在某些基准上逼近或超越了原始的70B模型，实现了**性能、规模和成本之间的帕累托改进**。

### 总结
不同于量化（降低权重精度），而是通过**结构化剪枝**来直接改变模型架构。其成功关键在于将**剪枝**与**知识蒸馏**紧密结合，先激进地裁剪模型，再通过蒸馏巧妙地“修复”模型能力，最终获得在性能和效率上都极具竞争力的小型模型。这项技术为实际部署大型模型提供了一条可行的低成本路径。

## 10. speculative_sampling

### 要点总结

#### 1. 核心思想：利用速度不对称性
*   **洞察**：LLM推理中，**并行处理多个token（验证阶段）的速度远快于串行生成一个token（生成阶段）**。
*   **方法**：使用一个小的、快的**草稿模型(draft model)** 先连续“猜测”生成多个token（如4个），然后由大的、慢的**目标模型(target model)** 一次性并行验证这些猜测。通过巧妙的算法设计，在保证输出质量完全不变的前提下，大幅提升生成速度。

#### 2. 工作流程：推测、验证、接受
1.  **推测**：草稿模型自回归地生成一个短的候选token序列（γ个token）。
2.  **验证**：目标模型并行处理整个序列（输入提示+候选序列），得到每个位置的真实概率分布。
3.  **接受**：
    *   将草稿模型生成的token与目标模型计算出的概率进行比较。
    *   如果草稿的token被目标模型以更高或相等的概率接受，则直接采纳。
    *   一旦出现不匹配，则拒绝后续所有token，并从目标模型的修正分布中采样一个新token作为下一轮的开始。
    *   **关键**：算法保证至少接受第一个候选token。

#### 3. 关键特性：精确采样
*   该方法并非近似，而是经过数学证明的**精确采样**。这意味着，加速后的模型产生的输出分布与原始目标模型**完全一致**，没有任何质量损失。这是该技术最强大的优势。

#### 4. 实践效果与扩展
*   **典型配置**：如70B参数的目标模型配一个7B参数的草稿模型，实测可带来2-3倍的**吞吐量提升**。
*   **进阶技术**：
    *   **Medusa**：让草稿模型一次性并行预测多个未来的token，而不是串行预测。
    *   **EAGLE**：让草稿模型利用目标模型的中间特征来做出更准确的预测。
    *   这些扩展进一步提升了草稿的准确性和加速比。

### 总结

推测性解码是一种极其巧妙的推理加速技术。它通过**“以小博大”**的策略，利用快速验证和慢速生成之间的效率差，实现了2-3倍的吞吐量提升，且**严格保证输出质量不变**。它代表了LLM推理优化从“如何算得更快”到“如何减少计算次数”的范式转变，是当前高性能推理服务不可或缺的核心技术之一。

## 11. continuous_batching

![continuous_batching](https://images.ctfassets.net/xjan103pcp94/1LJioEsEdQQpDCxYNWirU6/82b9fbfc5b78b10c1d4508b60e72fdcf/cb_02_diagram-static-batching.png)


### 要点总结

#### 1. 核心问题：推理时的“不规则数组”
*   **训练时**：数据是规整的张量，形状为 `[批次大小, 序列长度]`，便于高效并行计算。
*   **推理时**：用户请求随机到达、生成速度不同、序列长度各异，形成一个**不规则数组**。如果等待一个批次同时开始、同时结束，会造成严重的计算资源闲置。

#### 2. 根本解决方案：迭代级调度
*   **核心思想**：**以解码步（迭代）为单位进行调度，而不是以整个请求为单位**。
*   **工作方式**：
    *   每一步，系统都解码当前批次中所有未完成请求的下一个token。
    *   新请求可以随时加入正在进行的批次，无需等待当前批次全部完成。
    *   已完成生成的请求会立刻离开批次，释放资源。
*   **巨大优势**：极大地提升了GPU的利用率，从而显著提高**系统吞吐量**。

#### 3. 关键技术：选择性批处理
迭代级调度带来了一个技术挑战：如何高效处理不同长度的序列？选择性批处理提供了答案。
*   **针对非注意力计算（如MLP层）**：
    *   **策略**：将所有序列的token**拼接**成一个长的、连续的张量（如形状为`[总token数, 隐藏层维度]`）进行计算。
    *   **原因**：MLP层对每个token的计算是独立的，拼接后形成一个大的矩阵乘法，能最大化GPU计算效率。
    *   Non-attention computation: concatenate all the sequences together to [3 + 9 + 5, H]
*   **针对注意力计算**：
    *   **策略**：由于每个序列的注意力掩码不同，需要**为每个序列单独计算注意力**。
    *   **原因**：注意力机制要求一个序列内的token只能关注到它前面的token。不同序列无法在注意力层直接拼接。
    *   **实现**：通常使用填充或高效的分发/收集操作来处理多个独立序列。
    *   Attention computation: process each sequence separately

## 12. paged_attention

### 要点总结

#### 1. 核心问题：KV缓存的内存碎片化
在vLLM之前，为每个请求预先分配一块连续的KV缓存空间会导致：
*   **内部碎片**：由于无法预知每个请求最终会生成多长的序列，为避免溢出，必须按最大可能长度分配空间。但实际生成长度通常短得多，导致分配的空间大部分被浪费。
*   **外部碎片**：不同请求的缓存块之间会产生无法利用的小块空闲内存。
这些碎片严重限制了批处理大小，从而降低了GPU利用率和系统吞吐量。

#### 2. 革命性解决方案：PagedAttention
其核心思想完全借鉴了操作系统的虚拟内存和分页机制。

*   **分块管理**：将每个序列的KV缓存划分为一系列固定大小的**块**。这些块在物理显存中**无需连续存放**。
*   **逻辑到物理的映射**：系统为每个序列维护一个“块表”，记录其各个块在物理显存中的实际位置。
*   **巨大优势**：
    *   **消除外部碎片**：所有块大小相同，可以像操作系统管理内存页一样高效分配和回收，完全避免了外部碎片。
    *   **高效利用内存**：只需为序列实际使用的部分分配块，几乎消除了内部碎片。这使得在**固定显存下，批处理大小可提升数倍**，从而极大提升吞吐量。

#### 3. 关键技术延伸：KV缓存的块级共享
PagedAttention的分块机制进一步实现了更强大的功能：**跨序列的KV缓存共享**。
*   **应用场景**：
    1.  **共享系统提示词**：在多轮对话中，所有对话副本可以共享长长的系统提示词的KV缓存。
    2.  **为同一提示词生成多个输出**。
*   **实现机制**：**写时复制**。多个序列可以逻辑上共享指向相同物理块的映射。只有当某个序列需要修改共享块的内容时（即生成的分支点不同），系统才会真正复制该物理块。这进一步大幅减少了内存重复占用。

#### 4. 其他vLLM优化
除了PagedAttention，vLLM还集成了多项优化以最大化性能：
*   **内核融合**：将分块读取和注意力计算融合到一个内核中，减少启动开销。
*   **集成最新内核**：如FlashAttention、FlashDecoding，优化注意力计算本身。
*   **使用CUDA图**：避免重复的内核启动开销。

### 总结

PagedAttention是LLM服务端推理领域的一项**里程碑式创新**。它通过将操作系统中成熟的分页管理机制引入LLM的KV缓存管理，**从根本上解决了内存碎片化问题**，使得GPU显存得以被极致利用。这不仅直接带来了吞吐量的数量级提升，还通过**块级共享**机制开启了新的优化可能。vLLM也因此成为当前高性能LLM推理服务的事实标准引擎。


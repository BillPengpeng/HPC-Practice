æœ¬æ–‡ä¸»è¦æ•´ç†CS336 Overview, tokenizationç« èŠ‚çš„ä¸»è¦å†…å®¹ã€‚

## 1 - why_this_course_exists

### **Why did we make this course?**
- **ç ”ç©¶è„±èŠ‚é—®é¢˜**  
  - `8å¹´å‰`ï¼šç ”ç©¶è€…è‡ªè¡Œå®ç°å’Œè®­ç»ƒæ¨¡å‹  
  - `6å¹´å‰`ï¼šç ”ç©¶è€…ä¸‹è½½æ¨¡å‹å¾®è°ƒï¼ˆå¦‚BERTï¼‰  
  - `ç°åœ¨`ï¼šä»…è°ƒç”¨APIæç¤ºç§æœ‰æ¨¡å‹ï¼ˆGPT-4/Claude/Geminiï¼‰  
- **æŠ½è±¡å±‚çš„æ¼æ´**  
  - ç°æœ‰æŠ€æœ¯æŠ½è±¡ä¸å®Œå–„ï¼ˆç±»æ¯”æ“ä½œç³»ç»Ÿ/ç¼–ç¨‹è¯­è¨€ï¼‰  
  - åŸºç¡€ç ”ç©¶ä»éœ€æ·±å…¥åº•å±‚æŠ€æœ¯æ ˆ  
- **æ ¸å¿ƒç›®æ ‡**ï¼šé€šè¿‡**å®è·µæ„å»º**å®ç°å…¨æ ˆç†è§£  

> GPT-4çš„å›ç­”æ‘˜è¦ï¼š  
> "Teaching hands-on implementation fosters deeper understanding of fundamental principles for language model innovation."

---

### **The industrialization of language models**
- **è¶…å¤§è§„æ¨¡æ¨¡å‹ä¸å¯åŠ**  
  - GPT-4ï¼š1.8Tå‚æ•° / $100Mè®­ç»ƒæˆæœ¬ï¼ˆhttps://www.hpcwire.com/2024/03/19/the-generative-ai-future-is-now-nvidias-huang-saysï¼‰  
  - xAIï¼š20ä¸‡H100 GPUé›†ç¾¤è®­ç»ƒGrokï¼ˆhttps://www.tomshardware.com/pc-components/gpus/elon-musk-is-doubling-the-worlds-largest-ai-gpu-cluster-expanding-colossus-gpu-cluster-to-200-000-soon-has-floated-300-000-in-the-pastï¼‰  
  - Stargateè®¡åˆ’ï¼š$500BæŠ•èµ„ï¼ˆhttps://openai.com/index/announcing-the-stargate-project/ï¼‰  
- **æŠ€æœ¯é»‘ç®±åŒ–**  
  > GPT-4æŠ€æœ¯æŠ¥å‘Šå£°æ˜ï¼š  
  > "No details on architecture, hardware, training methods..."  
  > !images/gpt4-no-details.png  

---

### **More is different**
1. **è®¡ç®—ç»“æ„å˜åŒ–**  
   - å°æ¨¡å‹ä¸å¤§æ¨¡å‹çš„è®¡ç®—åˆ†é…å·®å¼‚ï¼š  
     !images/roller-flops.png  
     ï¼ˆæ¥æºï¼šhttps://x.com/stephenroller/status/1579993017234382849ï¼‰  
2. **æ¶Œç°ç°è±¡**  
   - å¤§æ¨¡å‹ç‹¬æœ‰çš„çªç°èƒ½åŠ›ï¼ˆå¦‚æ¨ç†/æ¦‚å¿µç»„åˆï¼‰  
   !images/wei-emergence-plot.png  
   ï¼ˆæ¥æºï¼šhttps://arxiv.org/pdf/2206.07682ï¼‰

---

### **What can we learn in this class that transfers to frontier models?**
| **çŸ¥è¯†ç±»å‹** | **æ˜¯å¦å¯è¿ç§»** | **è¯´æ˜**                     |
|--------------|----------------|------------------------------|
| Mechanicsæœºæ¢°åŸç†  âœ… | å®Œå…¨å¯è¿ç§»     | Transformerç»“æ„/æ¨¡å‹å¹¶è¡Œç­‰  |
| Mindsetæ€ç»´æ–¹å¼    âœ… | å®Œå…¨å¯è¿ç§»     | ç¡¬ä»¶æè‡´ä¼˜åŒ–/è§„æ¨¡æ³•åˆ™        |
| Intuitionså·¥ç¨‹ç›´è§‰ â“ | éƒ¨åˆ†å¯è¿ç§»     | æ•°æ®é€‰æ‹©/è¶…å‚å†³ç­–å—è§„æ¨¡å½±å“ |

---

### **Intuitionsçš„å±€é™æ€§**
- **å®è·µé©±åŠ¨è®¾è®¡**ï¼š  
  è®¸å¤šå†³ç­–ä»…é€šè¿‡å®éªŒéªŒè¯ï¼ˆæ— æ³•ç†è®ºæ¨å¯¼ï¼‰  
  > ä¾‹ï¼šSwiGLUæ¿€æ´»å‡½æ•°çš„è®ºæ–‡è‡ªå˜²ï¼š  
  > "Divine benevolence... we choose this form"  
  > !images/divine-benevolence.png  
  > ï¼ˆæ¥æºï¼šhttps://arxiv.org/abs/2002.05202ï¼‰

---

### **The bitter lesson**
- **æ ¸å¿ƒå…¬å¼**ï¼š  
  $$\text{å‡†ç¡®ç‡} = \text{æ•ˆç‡} \times \text{èµ„æº}$$  
- **å…³é”®æ´å¯Ÿ**ï¼š  
  - ç®—æ³•æ•ˆç‡æ¯”ç»å¯¹è§„æ¨¡æ›´é‡è¦ï¼ˆå¤§æ¨¡å‹æ— æ³•å®¹å¿ä½æ•ˆï¼‰  
  - 2012-2019å¹´ImageNetæ•ˆç‡æå‡44å€ï¼ˆhttps://arxiv.org/abs/2005.04305ï¼‰  
- **è¯¾ç¨‹ç›®æ ‡æ¡†æ¶**ï¼š  
  **åœ¨æœ‰é™ç®—åŠ›/æ•°æ®ä¸‹æœ€å¤§åŒ–æ¨¡å‹æ•ˆç‡**

---

### **æ€»ç»“**
> è¯­è¨€æ¨¡å‹ç ”ç©¶çš„æœªæ¥å±äºï¼š  
> **æŒæ¡åº•å±‚åŸç† + æè‡´æ•ˆç‡ä¼˜åŒ– + æœ‰é™èµ„æºåˆ›æ–°**  
> è€Œéä»…ä¾èµ–è§„æ¨¡æ‰©å¼ æˆ–é»‘ç®±APIè°ƒç”¨

## 2 - current_landscape

### **â… . Pre-neural (before 2010s) **
1. **è¯­è¨€æ¨¡å‹é›å½¢**  
   - ä¿¡æ¯è®ºåŸºç¡€ï¼šé¦™å†œé€šè¿‡è¯­è¨€æ¨¡å‹æµ‹é‡è‹±è¯­ç†µï¼ˆhttps://ieeexplore.ieee.org/document/6773024ï¼‰
2. **ç»Ÿè®¡æ–¹æ³•ä¸»å¯¼**  
   - N-gramæ¨¡å‹å¹¿æ³›åº”ç”¨äºæœºå™¨ç¿»è¯‘ä¸è¯­éŸ³è¯†åˆ«ï¼ˆhttps://aclanthology.org/D07-1090/ï¼‰

---

### **â…¡. Neural ingredients (2010s) **
| **å…³é”®æŠ€æœ¯çªç ´**              | **æ ¸å¿ƒè´¡çŒ®**                                  | **ä»£è¡¨è®ºæ–‡**                                                                 |
|-------------------------------|---------------------------------------------|----------------------------------------------------------------------------|
| é¦–ä¸ªäººå·¥ç¥ç»ç½‘ç»œè¯­è¨€æ¨¡å‹       | å¼•å…¥è¯åµŒå…¥ä¸åˆ†å¸ƒå¼è¡¨ç¤º                      | https://www.jmlr.org/papers/v3/bengio03a            |
| Seq2Seqæ¶æ„                   | å®ç°ç«¯åˆ°ç«¯åºåˆ—è½¬æ¢ï¼ˆæœºå™¨ç¿»è¯‘ï¼‰               | https://arxiv.org/abs/1409.3215                  |
| Adamä¼˜åŒ–å™¨                    | é«˜æ•ˆéšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•                         | https://arxiv.org/abs/1412.6980                      |
| æ³¨æ„åŠ›æœºåˆ¶                    | è§£å†³é•¿ç¨‹ä¾èµ–é—®é¢˜                            | https://arxiv.org/abs/1409.0473                   |
| **Transformeræ¶æ„**          | å¥ å®šç°ä»£å¤§æ¨¡å‹åŸºç¡€ï¼ˆå¹¶è¡Œè®¡ç®—/è‡ªæ³¨æ„åŠ›ï¼‰       | https://arxiv.org/abs/1706.03762                   |
| æ··åˆä¸“å®¶(MoE)                 | åŠ¨æ€è·¯ç”±æå‡æ¨¡å‹å®¹é‡                         | https://arxiv.org/abs/1701.06538                   |
| æ¨¡å‹å¹¶è¡ŒæŠ€æœ¯                  | æ”¯æŒè¶…å¤§è§„æ¨¡è®­ç»ƒï¼ˆZero/GPipe/Megatronï¼‰      | https://arxiv.org/abs/1811.06965, https://arxiv.org/abs/1910.02054, https://arxiv.org/abs/1909.08053 |

---

### **â…¢. Early foundation models (late 2010s)**
1. **åŠ¨æ€è¯å‘é‡æ¼”è¿›**  
   - ELMoï¼šåŸºäºLSTMçš„é¢„è®­ç»ƒ+å¾®è°ƒèŒƒå¼ï¼ˆhttps://arxiv.org/abs/1802.05365ï¼‰
2. **Transformerå®è·µçªç ´**  
   - BERTï¼šåŒå‘é¢„è®­ç»ƒæ¶æ„é©æ–°NLPä»»åŠ¡ï¼ˆhttps://arxiv.org/abs/1810.04805ï¼‰
   - T5ï¼šç»Ÿä¸€æ–‡æœ¬åˆ°æ–‡æœ¬æ¡†æ¶ï¼ˆ110äº¿å‚æ•°ï¼‰ï¼ˆhttps://arxiv.org/abs/1910.10683ï¼‰

---

### **â…£. Embracing scaling, more closed **
| **é‡Œç¨‹ç¢‘æ¨¡å‹**      | **å…³é”®åˆ›æ–°**                              | **æŠ€æœ¯å½±å“**                                                               |
|--------------------|------------------------------------------|--------------------------------------------------------------------------|
| GPT-2 (15äº¿å‚æ•°)    | å±•ç°é›¶æ ·æœ¬èƒ½åŠ›ï¼Œåˆ†çº§å‘å¸ƒç­–ç•¥              | æ­ç¤ºå¤§è§„æ¨¡æ–‡æœ¬ç”Ÿæˆæ½œåŠ›ï¼ˆhttps://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdfï¼‰ |
| **GPT-3 (1750äº¿)** | ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œé—­æºå•†ä¸šæ¨¡å‹èµ·ç‚¹              | é¦–æ¬¡éªŒè¯å¤§æ¨¡å‹æ³›åŒ–èƒ½åŠ›ï¼ˆhttps://arxiv.org/abs/2005.14165ï¼‰ |
| Scaling laws        | é¢„æµ‹æ¨¡å‹æ€§èƒ½ä¸è§„æ¨¡å…³ç³»                    | æŒ‡å¯¼é«˜æ•ˆè®­ç»ƒï¼ˆhttps://arxiv.org/abs/2001.08361ï¼‰       |
| PaLM (5400äº¿)       | éªŒè¯æè‡´è§„æ¨¡å¯è¡Œæ€§                        | æ­ç¤ºæ¬ è®­ç»ƒé—®é¢˜ï¼ˆhttps://arxiv.org/abs/2204.02311ï¼‰ |
| Chinchilla (700äº¿)  | è®¡ç®—æœ€ä¼˜ç¼©æ”¾å®šå¾‹ (æ•°æ®vså‚æ•°å¹³è¡¡)         | é‡æ–°å®šä¹‰è®­ç»ƒèŒƒå¼ï¼ˆhttps://arxiv.org/abs/2203.15556ï¼‰  |

---

### **â…¤. Open models**
| **é¡¹ç›®/æ¨¡å‹**              | **æ ¸å¿ƒè´¡çŒ®**                                 | **æŠ€æœ¯ç‰¹æ€§**                                                                 |
|---------------------------|-------------------------------------------|----------------------------------------------------------------------------|
| EleutherAI                | å¼€æºæ•°æ®é›†The Pile + GPT-Jæ¨¡å‹             | ç¤¾åŒºé©±åŠ¨å¼€æ”¾ç”Ÿæ€ï¼ˆhttps://arxiv.org/abs/2101.00027, https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/ï¼‰ |
| Meta OPT (1750äº¿)         | GPT-3å¤ç°é¡¹ç›®                             | ç¡¬ä»¶æŒ‘æˆ˜æš´éœ²ï¼ˆhttps://arxiv.org/abs/2205.01068ï¼‰                  |
| BLOOM (1760äº¿)            | å¤šè¯­è¨€æ•°æ®ä¸ä¼¦ç†è€ƒé‡ä¼˜å…ˆ                   | å…¨çƒåä½œå…¸èŒƒï¼ˆhttps://huggingface.co/bigscience/bloomï¼‰             |
| **Llamaç³»åˆ—** (Meta)      | å¼€æºæ¨¡å‹äº§ä¸šåŒ–æ ‡æ†                         | LLaMA 1/2/3 æŒç»­è¿­ä»£ï¼ˆhttps://arxiv.org/abs/2302.13971, https://arxiv.org/abs/2307.09288, https://ai.meta.com/blog/meta-llama-3/ï¼‰ |
| ä¸­å›½æ¨¡å‹é˜µè¥               | æ·±åº¦æ±‚ç´¢/é€šä¹‰åƒé—®/è…¾è®¯æ··å…ƒ                  | DeepSeek V2/V3/R1ï¼ˆhttps://github.com/deepseek-ai/DeepSeek-VLï¼‰, Qwen 2.5ï¼ˆhttps://qwenlm.github.io/blog/qwen2.5-max/ï¼‰, Hunyuan-T1ï¼ˆhttps://arxiv.org/abs/2405.16773ï¼‰ |
| OLMo (AI2)                | å®Œå…¨é€æ˜æ¨¡å‹ï¼ˆæƒé‡/æ•°æ®/è®­ç»ƒç»†èŠ‚ï¼‰          | å¯å¤ç°æ€§æ ‡æ†ï¼ˆhttps://allenai.org/olmo, https://blog.allenai.org/olmo-2-8b-and-7b-instruct-available-now-7d77bd8f31eeï¼‰ |

---

### **â…¥. Levels of openness**
| **å¼€æ”¾å±‚çº§**       | **ä»£è¡¨æ€§æ¨¡å‹**        | **ç‰¹ç‚¹**                                                                 |
|-------------------|---------------------|-------------------------------------------------------------------------|
| é—­æºæ¨¡å‹           | GPT-4o, Claude 3.7  | ä»…æä¾›APIè®¿é—®ï¼ˆhttps://openai.com/index/openai-o3-mini/, https://www.anthropic.com/news/claude-3-7-sonnetï¼‰ |
| **å¼€æ”¾æƒé‡**       | DeepSeek V3         | å…¬å¼€æ¨¡å‹æƒé‡+æ¶æ„ç»†èŠ‚ï¼ˆç¼ºå¤±è®­ç»ƒæ•°æ®ï¼‰ï¼ˆhttps://github.com/deepseek-ai/DeepSeek-V3ï¼‰ |
| å¼€æºæ¨¡å‹           | OLMo 2              | å…¨æ ˆå¼€æºï¼ˆæƒé‡/æ•°æ®/ä»£ç ï¼‰+å®éªŒç»†èŠ‚æŠ«éœ²ï¼ˆhttps://allenai.org/olmoï¼‰ |

---

### **â…¦. Today's frontier models**
| **æœºæ„**       | **æ——èˆ°æ¨¡å‹**       | **æŠ€æœ¯äº®ç‚¹**                                  |
|---------------|-------------------|--------------------------------------------|
| OpenAI        | o3                | å¤šæ¨¡æ€ä¼˜åŒ–ï¼ˆhttps://openai.com/index/openai-o3-mini/ï¼‰ |
| Anthropic     | Claude 3.7 Sonnet | é•¿æ–‡æœ¬æ¨ç†å¢å¼º                               |
| xAI           | Grok 3            | 20ä¸‡H100é›†ç¾¤è®­ç»ƒï¼ˆhttps://x.ai/news/grok-3ï¼‰     |
| Google        | Gemini 2.5        | 10M tokenä¸Šä¸‹æ–‡ï¼ˆhttps://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/ï¼‰ |
| **Meta**      | **Llama 3.3**     | å¼€æºæ¨¡å‹æ€§èƒ½æ ‡æ†                             |
| **ä¸­å›½é˜µè¥**   | DeepSeek R1       | ä¸­è‹±åŒè¯­ä¼˜åŒ–ï¼ˆhttps://github.com/deepseek-ai/DeepSeek-R1ï¼‰ |
|               | Qwen 2.5 Max      | ä¸­æ–‡é¢†åŸŸå¼ºåŒ–                                |
| Tencent       | Hunyuan-T1        | ä¸‡äº¿å‚æ•°çº§å›½äº§æ¨¡å‹ï¼ˆhttps://tencent.github.io/llm.hunyuan.T1/README_EN.htmlï¼‰ |

> æ ¸å¿ƒè¶‹åŠ¿ï¼š**å¼€æºä¸é—­æºå¹¶è¡Œå‘å±•**ï¼Œä¸­å›½æ¨¡å‹é˜µè¥å¿«é€Ÿå´›èµ·ï¼Œ**é€æ˜å¯å¤ç°æ€§**æˆä¸ºå…³é”®æŠ€æœ¯ä¼¦ç†è®®é¢˜ã€‚

## 3 - course_logistics

### **è¯¾ç¨‹åŸºæœ¬ä¿¡æ¯**
- **å®˜æ–¹å¹³å°**ï¼š  
  æ‰€æœ‰è¯¾ç¨‹ä¿¡æ¯åœ¨çº¿å…¬å¼€ â†’ https://stanford-cs336.github.io/spring2025/
- **å­¦åˆ†å¼ºåº¦**ï¼š  
  `5å­¦åˆ†`ï¼ˆå®é™…å·¥ä½œé‡æå¤§ï¼‰  
  > ğŸ“Œ **2024å±Šå­¦ç”Ÿè¯„ä»·è­¦ç¤º**ï¼š  
  > *"å•æ¬¡ä½œä¸š â‰ˆ CS 224nå…¨å­¦æœŸ5æ¬¡ä½œä¸š+æœŸæœ«é¡¹ç›®æ€»å’Œ"*

---

### **ç›®æ ‡äººç¾¤é€‚é…æŒ‡å—**
| **é€‚åˆäººç¾¤ âœ…**               | **ä¸é€‚åˆäººç¾¤ âŒ**                     |
|-----------------------------|--------------------------------------|
| **æ·±åº¦åŸç†æ¢ç©¶è€…**ï¼šç—´è¿·æŠ€æœ¯åº•å±‚å®ç°é€»è¾‘ | **çŸ­æœŸç ”ç©¶è€…**ï¼šéœ€ä¸å¯¼å¸ˆç¡®è®¤æ—¶é—´å†²çª     |
| **å·¥ç¨‹èƒ½åŠ›å»ºè®¾è€…**ï¼šå¼ºåŒ–ç ”ç©¶çº§å·¥ç¨‹èƒ½åŠ› | **æŠ€æœ¯è¶‹åŠ¿è¿½é€è€…**ï¼šå»ºè®®é€‰ä¿®ä¸“é¢˜è¯¾ï¼ˆå¦‚å¤šæ¨¡æ€/RAGï¼‰ |
|                              | **åº”ç”¨ä¼˜åŒ–è€…**ï¼šå»ºè®®ç›´æ¥å¾®è°ƒç°æœ‰APIæ¨¡å‹      |

---

### **è‡ªå­¦å‚ä¸è·¯å¾„**
1. **èµ„æºå¼€æ”¾**ï¼š  
   - æ‰€æœ‰è®²ä¹‰/ä½œä¸šåœ¨çº¿å…¬å¼€ï¼Œæ”¯æŒå®Œå…¨è‡ªå­¦
2. **è¯¾ç¨‹å½•æ’­**ï¼š  
   - é€šè¿‡https://cgoe.stanford.edu/å½•åˆ¶  
   - YouTubeå»¶è¿Ÿå‘å¸ƒï¼ˆçº¦1-2å‘¨ï¼‰
3. **å¼€è¯¾è®¡åˆ’**ï¼š  
   - 2026å¹´å°†é‡æ–°å¼€è¯¾ï¼ˆå¯è§„åˆ’æ˜å¹´å‚ä¸ï¼‰

---

### **Assignments**
| **ç‰¹å¾**                  | **è¯´æ˜**                                                                 |
|---------------------------|-------------------------------------------------------------------------|
| **5å¤§ä¸»é¢˜ä½œä¸š**           | basics, systems, scaling laws, data, alignment                              |
| **é›¶è„šæ‰‹æ¶ä»£ç **          | æä¾›å•å…ƒæµ‹è¯•+é€‚é…æ¥å£ï¼Œä½†éœ€ä»é›¶å®ç°æ ¸å¿ƒé€»è¾‘                                |
| **åŒé˜¶æ®µéªŒè¯**            | æœ¬åœ°æµ‹è¯•æ­£ç¡®æ€§ â†’ é›†ç¾¤è¿è¡Œæµ‹æ€§èƒ½ï¼ˆç²¾åº¦/é€Ÿåº¦ï¼‰                               |
| **ç«äº‰æœºåˆ¶**              | éƒ¨åˆ†ä½œä¸šè®¾æ’è¡Œæ¦œï¼ˆé™å®šç®—åŠ›ä¸‹æœ€å°åŒ–å›°æƒ‘åº¦ï¼‰                                 |
| **AIå·¥å…·è­¦ç¤º**            | Copilot/Cursorç­‰å·¥å…·å¯èƒ½å‰Šå¼±å­¦ä¹ æ•ˆæœï¼Œä½¿ç”¨éœ€è‡ªæ‹…é£é™©                        |

---

### **è®¡ç®—é›†ç¾¤ä½¿ç”¨é¡»çŸ¥**
1. **ç®—åŠ›æ”¯æŒ**ï¼š  
   - ç”± **Together AI** æèµ ç®—åŠ›é›†ç¾¤ ğŸ™
2. **æ“ä½œæŒ‡å—**ï¼š  
   - è¯¦ç»†æ–‡æ¡£ â†’ https://docs.google.com/document/d/1BSSig7zInyjDKcbNGftVxubiHlwJ-ZqahQewIzBmBOo/edit
3. **å…³é”®æé†’**ï¼š  
   > âš ï¸ **æˆªæ­¢æ—¥å‰é›†ç¾¤å¿…çˆ†æ»¡ï¼**  
   > å¼ºçƒˆå»ºè®®æå‰2å‘¨å¯åŠ¨ä½œä¸š  

---

### **è¯¾ç¨‹æ ¸å¿ƒå®šä½**
> ğŸ’¡ **ç¡¬æ ¸å¼€å‘è€…è®­ç»ƒè¥**ï¼š  
> é€šè¿‡ä»é›¶æ„å»ºè¯­è¨€æ¨¡å‹ï¼Œ**æ·±åº¦æŒæ¡ï¼š**  
> - å¤§æ¨¡å‹æ ¸å¿ƒç®—æ³•åŸç†  
> - åˆ†å¸ƒå¼ç³»ç»Ÿä¼˜åŒ–èƒ½åŠ›  
> - æé™èµ„æºä¸‹çš„å·¥ç¨‹å†³ç­–  
> è€Œéè¿½æ±‚çŸ­æœŸç ”ç©¶äº§å‡ºæˆ–åº”ç”¨é€Ÿæˆ

## 4 - course_components

### **It's all about efficiency**
Resources: data + hardware (compute, memory, communication bandwidth)
**å…³é”®çº¦æŸ**ï¼š  
- **æœ‰é™èµ„æºæ¡†æ¶**ï¼šç»™å®šæ•°æ®é›†ï¼ˆå¦‚Common Crawlï¼‰+ ç¡¬ä»¶ï¼ˆ32å¼ H100 Ã— 2å‘¨ï¼‰  
- **ç»ˆæç›®æ ‡**ï¼šåœ¨èµ„æºè¾¹ç•Œå†…å®ç°æœ€ä¼˜æ¨¡å‹æ€§èƒ½  

!images/design-decisions.png  
*æ³¨ï¼šè®¾è®¡å†³ç­–å›¾å±•ç¤ºæ•°æ®/æ¶æ„/è®­ç»ƒç­–ç•¥çš„ç›¸äº’åˆ¶è¡¡*

---

### **è¯¾ç¨‹æ¨¡å—æ¶æ„**
```mermaid
graph LR
A[åŸºç¡€æ¶æ„] --> B[åˆ†å¸ƒå¼ç³»ç»Ÿ]
B --> C[scaling_laws]
C --> D[æ•°æ®å¤„ç†]
D --> E[alignment]
```

| **æ¨¡å—**      | **æ ¸å¿ƒå…³æ³¨ç‚¹**                  | **æ•ˆç‡å®ç°è·¯å¾„**                  |
|--------------|------------------------------|---------------------------------|
| åŸºç¡€å®ç°      | Transformeræ ¸å¿ƒç»„ä»¶            | æœ€å°åŒ–å†…å­˜å ç”¨çš„ç®—æ³•è®¾è®¡            |
| ç³»ç»Ÿä¼˜åŒ–      | GPUé›†ç¾¤ååŒè®¡ç®—                | é€šä¿¡å¸¦å®½ä¼˜åŒ–+è®¡ç®—è´Ÿè½½å‡è¡¡           |
| scaling_laws | è®¡ç®—-æ•°æ®-å‚æ•°ä¸‰è§’å…³ç³»           | è¶…å‚è°ƒä¼˜çš„å°æ¨¡å‹ä»£ç†ç­–ç•¥            |
| æ•°æ®å¤„ç†      | è®­ç»ƒé›†è´¨é‡ç®¡æ§                 | è¿‡æ»¤ä½ä»·å€¼æ ·æœ¬çš„é¢„å¤„ç†æµæ°´çº¿         |
| alignment    | äººç±»åå¥½å­¦ä¹                   | å°æ¨¡å‹+ç²¾å‡†å¾®è°ƒçš„å¸•ç´¯æ‰˜æœ€ä¼˜          |

---

### **Efficiency drives design decisions**
1. **Data processing**  
   - **æ ¸å¿ƒçŸ›ç›¾**ï¼šåƒåœ¾æ•°æ® = è®¡ç®—èµ„æºæµªè´¹  
   - **è§£å†³æ–¹æ¡ˆ**ï¼šåŠ¨æ€æ•°æ®æ¸…æ´—ï¼ˆè¿‡æ»¤é‡å¤/ä½è´¨å†…å®¹ï¼‰

2. **Tokenization**  
   - **ç†æƒ³**ï¼šå­—èŠ‚çº§å¤„ç†ï¼ˆæœ€å¤§çµæ´»æ€§ï¼‰  
   - **ç°å®**ï¼šå­è¯åˆ†è¯ï¼ˆBPE/WordPieceï¼‰å¹³è¡¡è¯è¡¨ä¸è®¡ç®—å¼€é”€  
   > *å­—èŠ‚çº§FLOPså¼€é”€ â†‘300%*

3. **æ¶æ„åˆ›æ–°**  
   | **æŠ€æœ¯**               | **æ•ˆç‡å¢ç›Šæ¥æº**              | **æ¡ˆä¾‹**                    |
   |-----------------------|----------------------------|----------------------------|
   | KVç¼“å­˜å…±äº«            | å‡å°‘Attentioné‡å¤è®¡ç®—        | Mistralæ»‘åŠ¨çª—å£Attention    |
   | ç¨€ç–æ¿€æ´»(MoE)         | åŠ¨æ€è·¯ç”±èŠ‚çœ95%è®¡ç®—é‡         | Mixtral 8Ã—7B               |
   | é‡åŒ–æ³¨æ„åŠ›             | å†…å­˜å¸¦å®½ä¼˜åŒ–                 | FlashAttention-3           |

4. **è®­ç»ƒèŒƒå¼**  
   - **å•epochè®­ç»ƒå¯è¡Œæ€§**ï¼šChinchillaè¯æ˜æ•°æ®é«˜è´¨é‡æ—¶å•è½®è®­ç»ƒè¶³å¤Ÿ  
   - **èµ„æºé‡åˆ†é…ç­–ç•¥**ï¼šèŠ‚çœçš„epochè½®æ¬¡ â†’ æ›´å¤šæ¶ˆèå®éªŒ

5. **Scaling laws**  
   ```python
   # è¶…å‚æœç´¢çš„å»‰ä»·ä»£ç†
   small_model = train(0.1% data, 1% params)  # æˆæœ¬â†“99%
   opt_hyper = hyper_tune(small_model)         # å‚æ•°ç§»æ¤
   large_model = train(full_data, params, opt_hyper)
   ```

6. **Alignment**  
   - **åå¸¸è¯†æ´è§**ï¼šif tune model more to desired use cases, require smaller base model
     $$\text{åŸºç¡€æ¨¡å‹è´¨é‡} \propto \frac{1}{\text{å¯¹é½æ‰€éœ€è®¡ç®—é‡}}$$  
   - **å®è·µ**ï¼š7Bç²¾è°ƒæ¨¡å‹ > æœªç»å¯¹é½çš„70BåŸºç¡€æ¨¡å‹ï¼ˆç›¸åŒè®¡ç®—é¢„ç®—ï¼‰

---

### **æœªæ¥èŒƒå¼è¿ç§»**
| **å½“å‰é˜¶æ®µ**         | **æœªæ¥æŒ‘æˆ˜**          | **æŠ€æœ¯å‡†å¤‡**                  |
|---------------------|---------------------|-----------------------------|
| è®¡ç®—çº¦æŸï¼ˆCompute-Boundï¼‰ | æ•°æ®çº¦æŸï¼ˆData-Boundï¼‰ | åˆæˆæ•°æ®ç”Ÿæˆ+æ•°æ®è’¸é¦æŠ€æœ¯       |
| ç¡¬ä»¶æ•ˆç‡ä¸»å¯¼          | æ•°æ®è´¨é‡ä¸»å¯¼          | è‡ªæˆ‘æ¼”è¿›è®­ç»ƒï¼ˆSelf-Playï¼‰      |
| å•æ¨¡æ€ä¼˜åŒ–            | å¤šæ¨¡æ€ååŒä¼˜åŒ–        | è·¨æ¨¡æ€å…±äº«è¡¨ç¤ºæ¶æ„             |

> **æ•ˆç‡æ¼”è¿›æœ¬è´¨**ï¼šå½“ç®—åŠ›ä¸å†ç¨€ç¼ºæ—¶ï¼Œ**æ•°æ®ä»·å€¼å¯†åº¦**å°†æˆä¸ºæ–°çš„æˆ˜åœºï¼Œé©±åŠ¨æ¨¡å‹è®¾è®¡èŒƒå¼é©å‘½ã€‚

## 5 - tokenization

### 5.1 - intro_to_tokenization

#### **è¯­è¨€æ¨¡å‹è¾“å…¥è¾“å‡ºæœ¬è´¨**
| **æ¦‚å¿µ**               | **è¯´æ˜**                                                                 | **ç¤ºä¾‹**                     |
|------------------------|-------------------------------------------------------------------------|-----------------------------|
| **åŸå§‹æ–‡æœ¬**           | Unicodeå­—ç¬¦ä¸²ï¼ˆæ”¯æŒå¤šè¯­è¨€/ç¬¦å·ï¼‰                                          | `"Hello, ğŸŒ! ä½ å¥½!"`         |
| **æ¨¡å‹è¾“å…¥**           | æ•´æ•°ç´¢å¼•åºåˆ—ï¼ˆToken IDï¼‰                                                  | `[15496, 11, 995, 0]`       |
| **æ¦‚ç‡åˆ†å¸ƒå¯¹è±¡**       | æ¨¡å‹å¯¹Tokenåºåˆ—çš„æ¦‚ç‡å»ºæ¨¡                                                | $P(\text{token}_n \mid \text{token}_1,...,\text{token}_{n-1})$ |

---

#### **åˆ†è¯å™¨ï¼ˆTokenizerï¼‰çš„æ ¸å¿ƒèŒèƒ½**
```mermaid
graph LR
A[å­—ç¬¦ä¸²] -- ç¼–ç  --> B[Token IDåºåˆ—]
B -- è§£ç  --> A
```

1. **ç¼–ç ï¼ˆEncodeï¼‰**  
   - å°†å­—ç¬¦ä¸² â†’ Token IDåºåˆ—  
   - *æŠ€æœ¯æŒ‘æˆ˜*ï¼š  
     - å¤„ç†å¤šè¯­è¨€æ··åˆæ–‡æœ¬ï¼ˆå¦‚ğŸŒ+ä¸­æ–‡ï¼‰  
     - ç”Ÿåƒ»è¯/æ–°ç¬¦å·çš„åˆ†è§£ç­–ç•¥ï¼ˆOOVé—®é¢˜ï¼‰  

2. **è§£ç ï¼ˆDecodeï¼‰**  
   - å°†Token IDåºåˆ— â†’ å¯è¯»å­—ç¬¦ä¸²  
   - *æŠ€æœ¯æŒ‘æˆ˜*ï¼š  
     - åˆå¹¶å­è¯æ¢å¤åŸè¯ï¼ˆå¦‚`"un"+"happi"+"ness"â†’"unhappiness"`ï¼‰  
     - å¤„ç†å­—èŠ‚çº§æ‹¼æ¥çš„è¾¹ç•Œå¯¹é½ï¼ˆUTF-8å¤šå­—èŠ‚å­—ç¬¦ï¼‰  

---

#### **è¯è¡¨è§„æ¨¡ï¼ˆVocabulary Sizeï¼‰**
- **å®šä¹‰**ï¼šåˆ†è¯å™¨æ”¯æŒçš„æœ€å¤§Token IDæ•°é‡  
- **å½±å“**ï¼š  
  | **å°è¯è¡¨** (e.g., 1K)       | **å¤§è¯è¡¨** (e.g., 100K)        |  
  |----------------------------|--------------------------------|  
  | âœ… æ¨¡å‹å‚æ•°é‡å°‘              | âŒ æ¨¡å‹å‚æ•°é‡å¤§                 |  
  | âŒ é•¿åºåˆ—ï¼ˆæ›´å¤šå­è¯åˆå¹¶ï¼‰     | âœ… çŸ­åºåˆ—ï¼ˆæ›´å¤šæ•´è¯ä¿ç•™ï¼‰        |  
  | âŒ ç”Ÿåƒ»è¯åˆ†è§£ç²’åº¦ç²—           | âœ… è¯­ä¹‰ä¿ç•™æ›´å®Œæ•´               |  

---

#### **å…³é”®æŠ€æœ¯æŒ‘æˆ˜**
1. **æ··åˆå­—ç¬¦é›†å¤„ç†**  
   - ç¤ºä¾‹ `"ğŸŒ"`ï¼ˆU+1F30Dï¼‰ï¼š  
     - UTF-8ç¼–ç ï¼š`0xF0 0x9F 0x8C 0x8D` â†’ å¯èƒ½è¢«æ‹†ä¸º4å­—èŠ‚Token  
   - ä¸­è‹±æ··åˆ `"ä½ å¥½!"`ï¼š  
     - ä¸­æ–‡éœ€æŒ‰è¯è¯­è€Œéå•å­—æ‹†åˆ†ï¼ˆå¦‚`["ä½ ","å¥½","!"]` vs `["ä½ å¥½","!"]`ï¼‰

2. **è§£ç ä¸€è‡´æ€§**  
   - è¦æ±‚ï¼š`decode(encode(s)) == s`  
   - éš¾ç‚¹ï¼š  
     - å¤§å°å†™/æ ‡ç‚¹è¿˜åŸï¼ˆå¦‚`"Hello"`ç¼–ç åä¸åº”è§£ç ä¸º`"hello"`ï¼‰  
     - ç©ºæ ¼å¤„ç†ï¼ˆè‹±æ–‡éœ€ä¿ç•™ï¼Œä¸­æ–‡éœ€å¿½ç•¥ï¼‰

> **å…³é”®è®¾è®¡åŸåˆ™**ï¼šç¼–ç -è§£ç çš„**æ— æŸå¾€è¿”**ï¼ˆRound-trip Safetyï¼‰ç¡®ä¿ä¿¡æ¯ä¸ä¸¢å¤±ã€‚

### 5.2 - intro_to_tokenization

#### **åˆ†è¯å™¨äº¤äº’æ¢ç´¢å·¥å…·**
- **æ¨èå¹³å°**ï¼šhttps://tiktokenizer.vercel.app/?encoder=gpt2  
  *åŠŸèƒ½*ï¼šå®æ—¶å¯è§†åŒ–ä¸»æµåˆ†è¯å™¨ï¼ˆGPT-2/GPT-4ç­‰ï¼‰å¤„ç†é€»è¾‘

---

#### **Observations**
| **ç°è±¡**                          | **ç¤ºä¾‹**                     | **æŠ€æœ¯åŸå› **                     | **å½±å“**                     |
|-----------------------------------|----------------------------|--------------------------------|-----------------------------|
| **ç©ºæ ¼ä¸è¯ç»‘å®š**                  | `" world"` â†’ å•token        | è‹±æ–‡ä¸­ç©ºæ ¼æ‰¿è½½è¯­ä¹‰ä¿¡æ¯            | é¿å…å¥é¦–/å¥ä¸­åŒè¯ä¸åŒè¡¨å¾      |
| **ä½ç½®ä¾èµ–æ€§**                    | å¥é¦–`"hello"`â‰ å¥ä¸­`"hello"` | åˆ†è¯å™¨åŒºåˆ†è¯è¾¹ç•Œä¸Šä¸‹æ–‡            | åŒè¯ä¸åŒIDå¢åŠ è¯è¡¨å†—ä½™         |
| **æ•°å­—åˆ†æ®µå¤„ç†**                  | `123456` â†’ `[12, 3456]`     | æ•°å­—ç»„åˆæ¨¡å¼æ— é™ï¼Œåˆ†æ®µå‹ç¼©         | æå‡é«˜é¢‘æ•°å­—åºåˆ—å¤„ç†æ•ˆç‡        |

---

#### **GPT-2åˆ†è¯å™¨ï¼ˆtiktokenï¼‰éªŒè¯å®éªŒ**
```python
# 1. åˆå§‹åŒ–åˆ†è¯å™¨
tokenizer = tiktoken.get_encoding("gpt2")  # @inspect

# 2. å¤šè¯­è¨€æ··åˆè¾“å…¥
text = "Hello, ğŸŒ! ä½ å¥½!"  # å«è‹±æ–‡/è¡¨æƒ…/ä¸­æ–‡

# 3. ç¼–ç -è§£ç å¾€è¿”éªŒè¯
indices = tokenizer.encode(text)    # è¾“å‡º: [15496, 11, 50118, 0, ...]
recon_text = tokenizer.decode(indices)

assert text == recon_text  # âœ… æ— æŸå¾€è¿”éªŒè¯é€šè¿‡
```

##### **å…³é”®æŠ€æœ¯æŒ‡æ ‡**
```python
# å‹ç¼©æ¯”è®¡ç®—
def get_compression_ratio(text, indices):
    orig_bytes = len(text.encode('utf-8'))
    token_bytes = len(indices) * 4  # å‡è®¾æ¯ä¸ªIDå 4å­—èŠ‚
    return orig_bytes / token_bytes

# ç¤ºä¾‹ç»“æœï¼š 
#  åŸå§‹å­—èŠ‚: 17å­—èŠ‚ (Hello,ğŸŒ!ä½ å¥½!)
#  Tokenå­—èŠ‚: 6 token Ã— 4B = 24B 
#  å‹ç¼©æ¯” = 17/24 â‰ˆ 0.71 (<1è¡¨ç¤ºä¿¡æ¯è†¨èƒ€)
```

---

#### **å‹ç¼©æ¯”åˆ†æï¼ˆå¤šè¯­è¨€åœºæ™¯ï¼‰**
| **æ–‡æœ¬ç±»å‹**       | **åŸå§‹å­—èŠ‚** | **Tokenæ•°** | **å‹ç¼©æ¯”** | **ç°è±¡è§£é‡Š**                 |
|-------------------|------------|------------|-----------|---------------------------|
| çº¯è‹±æ–‡             | 6B ("Hello") | 1          | 6.0       | æ•´è¯é«˜æ•ˆè¡¨ç¤º                 |
| è¡¨æƒ…ç¬¦å·ğŸŒ         | 4B         | 1          | 4.0       | ç›´æ¥æ˜ å°„                    |
| ä¸­æ–‡å­—ç¬¦"ä½ å¥½"      | 6B         | 2          | 1.5       | å­—çº§æ‹†åˆ†é™ä½æ•ˆç‡             |
| æ··åˆæ–‡æœ¬           | 17B        | 6          | 0.71      | **å¤šè¯­è¨€æ··è¾“æ—¶æ•ˆç‡å€’æŒ‚**      |

> ğŸ’¡ **æ ¸å¿ƒå‘ç°**ï¼š  
> - è‹±æ–‡/ç¬¦å·å¤„ç†é«˜æ•ˆï¼ˆå‹ç¼©æ¯”>1ï¼‰  
> - **ä¸­æ–‡ç­‰éæ‹‰ä¸å­—ç¬¦æ•ˆç‡éª¤é™**ï¼ˆå› UTF-8å­—èŠ‚æ‹†åˆ†+å­—çº§åˆ†è¯ï¼‰  
> - æ··åˆæ–‡æœ¬æ•´ä½“å¯èƒ½ä¿¡æ¯è†¨èƒ€  

### 5.3 character_tokenizer

#### **å­—ç¬¦åˆ†è¯å™¨åŸºç¡€åŸç†**
```mermaid
flowchart LR
A[å­—ç¬¦] -->|ord| B[Unicodeç ç‚¹]
B -->|chr| A
```

1. **Unicode è¡¨ç¤º**
   - æ–‡æœ¬ = Unicodeå­—ç¬¦åºåˆ—
   - æ¯ä¸ªå­—ç¬¦ â†” å”¯ä¸€æ•´æ•°ï¼ˆç ç‚¹ï¼‰  
     **æ ¸å¿ƒå‡½æ•°**ï¼š
     - `ord()`ï¼šå­—ç¬¦ â†’ ç ç‚¹  
       ```python
       ord("a") = 97
       ord("ğŸŒ") = 127757  # åœ°çƒè¡¨æƒ…
       ```
     - `chr()`ï¼šç ç‚¹ â†’ å­—ç¬¦  
       ```python
       chr(97) = "a"
       chr(127757) = "ğŸŒ"
       ```

2. **åˆ†è¯å™¨å®ç°**
   ```python
   class CharacterTokenizer:
       def encode(self, text: str) -> list[int]:
           return [ord(char) for char in text]
           
       def decode(self, ids: list[int]) -> str:
           return "".join(chr(i) for i in ids)
   ```

---

#### **å·¥ä½œæµç¨‹éªŒè¯**
**è¾“å…¥æ–‡æœ¬**ï¼š`"Hello, ğŸŒ! ä½ å¥½!"`  
```python
# ç¼–ç è¿‡ç¨‹
indices = [72, 101, 108, 108, 111, 44, 32, 127757, 33, 32, 20320, 22909, 33]

# è§£ç è¿˜åŸ
recon_str = "H e l l o ,   ğŸŒ !   ä½  å¥½ !" â†’ æ‹¼æ¥æ¢å¤åŸæ–‡æœ¬

# æ— æŸå¾€è¿”éªŒè¯
assert "Hello, ğŸŒ! ä½ å¥½!" == recon_str  # âœ…
```

---

#### **æ ¸å¿ƒæ€§èƒ½é—®é¢˜**

| **ç»´åº¦**        | **æ•°æ®**                | **é—®é¢˜æœ¬è´¨**                  |
|-----------------|------------------------|------------------------------|
| **è¯è¡¨è§„æ¨¡**     | â‰ˆ150,000 Unicodeå­—ç¬¦   | è¯è¡¨çˆ†ç‚¸ï¼ˆéœ€150K+çš„åµŒå…¥çŸ©é˜µï¼‰  |
| **ç¨€ç–åˆ†å¸ƒ**     | 80%å­—ç¬¦ä½¿ç”¨ç‡<0.001%    | èµ„æºæµªè´¹ï¼ˆğŸŒç­‰ç½•è§å­—ç¬¦å ä½ï¼‰    |
| **å‹ç¼©æ•ˆç‡**     | **å‹ç¼©æ¯”â‰ˆ0.33**         | ä¿¡æ¯è†¨èƒ€ï¼ˆæ¯”åŸå§‹æ–‡æœ¬å¤§3å€ï¼‰     |

##### å‹ç¼©æ¯”è®¡ç®—è¯¦è§£ï¼š
```python
åŸå§‹å­—èŠ‚æ•° = len("Hello, ğŸŒ! ä½ å¥½!".encode('utf-8')) = 17å­—èŠ‚
Tokenå­—èŠ‚æ•° = 13 token Ã— 4å­—èŠ‚/ID = 52å­—èŠ‚  # å‡è®¾æ¯ä¸ªIDç”¨int32å­˜å‚¨
å‹ç¼©æ¯” = 17/52 â‰ˆ 0.33  # <<1 è¡¨ç¤ºä¸¥é‡ä½æ•ˆ
```

---

#### **å­—ç¬¦åˆ†è¯çš„è‡´å‘½ç¼ºé™·**

1. **æ¨¡å‹å‚æ•°ç¾éš¾**  
   - 150Kè¯è¡¨ â†’ åµŒå…¥å±‚å  **600MBå†…å­˜**ï¼ˆå‡è®¾4å­—èŠ‚/ID Ã— 150Kï¼‰
   - å¯¹æ¯”ï¼šBPEåˆ†è¯å™¨(50Kè¯è¡¨)ä»…éœ€ **200KB**

2. **è®¡ç®—æ•ˆç‡ä½ä¸‹**  
   | **æ“ä½œ**      | å­—ç¬¦åˆ†è¯ | BPEåˆ†è¯ | å·®è·   |
   |--------------|---------|---------|-------|
   | åºåˆ—é•¿åº¦      | 13      | 6       | 2.2å€ |
   | æ³¨æ„åŠ›è®¡ç®—é‡   | O(13Â²)  | O(6Â²)   | 4.7å€ |

3. **è¯­ä¹‰ç ´ç¢**  
   ```python
   # ä¸­æ–‡å­—ç¬¦"ä½ å¥½"è¢«æ‹†è§£
   encode("ä½ å¥½") = [20320, 22909]  # å®Œå…¨ä¸¢å¤±è¯è¯­å…³è”
   ```

---

#### **ç°å®åœºæ™¯å¯¹æ¯”**
| **æ–‡æœ¬ç±»å‹**   | **å­—ç¬¦åˆ†è¯Tokenæ•°** | **BPEåˆ†è¯Tokenæ•°** | **æ•ˆç‡æ¯”** |
|---------------|--------------------|-------------------|-----------|
| è‹±æ–‡æ®µè½       | 1200               | 900               | 1.33x     |
| ä¸­æ–‡æ–‡ç«        | 2500               | 600               | 4.17x     |
| æ··åˆæ–‡æœ¬       | 1800               | 800               | 2.25x     |
| å«è¡¨æƒ…ç¬¦å·     | 2000               | 450               | 4.44x     |

> ğŸ’¡ **ç»“è®º**ï¼šå­—ç¬¦åˆ†è¯ä»…åœ¨ç†è®ºéªŒè¯ä¸­æœ‰ä»·å€¼ï¼Œ**å®é™…å·¥ç¨‹ä¸­å› æ•ˆç‡ç¾éš¾å·²è¢«å®Œå…¨æ·˜æ±°**ã€‚ç°ä»£æ–¹æ¡ˆï¼ˆBPE/WordPieceï¼‰é€šè¿‡å­è¯æ‹†åˆ†å¹³è¡¡è¯è¡¨ä¸åºåˆ—é•¿åº¦ã€‚

### 5.4 byte_tokenizer

#### **å­—èŠ‚åˆ†è¯æ ¸å¿ƒåŸç†**
```mermaid
flowchart LR
A[å­—ç¬¦ä¸²] --> B[UTF-8å­—èŠ‚åºåˆ—]
B --> C[0-255æ•´æ•°]
C --> B
```

1. **Unicodeå­—èŠ‚è¡¨ç¤º**  
   - **åŸºç¡€å•ä½**ï¼š0-255çš„æ•´æ•°å­—èŠ‚å€¼  
   - **è¯è¡¨å¤§å°å›ºå®š**ï¼š256ï¼ˆå®Œç¾è¦†ç›–æ‰€æœ‰å­—èŠ‚ç»„åˆï¼‰

2. **ç¼–ç -è§£ç æœºåˆ¶**  
   - ç¼–ç ï¼š`å­—ç¬¦ä¸² â†’ UTF-8å­—èŠ‚åºåˆ— â†’ æ•´æ•°åºåˆ—`  
     ```python
     "a" â†’ b'a' â†’ [97]
     "ğŸŒ" â†’ b'\xf0\x9f\x8c\x8d' â†’ [240, 159, 140, 141]
     ```
   - è§£ç ï¼š`æ•´æ•°åºåˆ— â†’ å­—èŠ‚åºåˆ— â†’ UTF-8è§£ç  â†’ å­—ç¬¦ä¸²`

---

#### **å®ç°éªŒè¯**
```python
class ByteTokenizer:
    def encode(self, text: str) -> list[int]:
        return list(text.encode("utf-8"))  # å­—ç¬¦ä¸²â†’UTF-8å­—èŠ‚â†’æ•´æ•°åˆ—è¡¨
        
    def decode(self, ids: list[int]) -> str:
        return bytes(ids).decode("utf-8")  # æ•´æ•°â†’å­—èŠ‚â†’å­—ç¬¦ä¸²
```

**è¾“å…¥æ–‡æœ¬**ï¼š`"Hello, ğŸŒ! ä½ å¥½!"`  
```python
# ç¼–ç ç»“æœ
indices = [72, 101, 108, 108, 111, 44, 32, 
           240, 159, 140, 141,  # "ğŸŒ"çš„4å­—èŠ‚
           33, 32, 
           228, 189, 160,     # "ä½ "çš„3å­—èŠ‚
           229, 165, 189, 33]  # "å¥½!"çš„3+1å­—èŠ‚

# æ— æŸå¾€è¿”éªŒè¯
assert text == decode(indices)  # âœ…
```

---

#### **æ€§èƒ½å…³é”®æŒ‡æ ‡**
| **ç»´åº¦**         | **æ•°å€¼**         | **åˆ†æ**                     |
|------------------|-----------------|-----------------------------|
| **è¯è¡¨å¤§å°**      | 256             | âœ… æç®€è¯è¡¨ï¼ˆæ— OOVé—®é¢˜ï¼‰       |
| **å‹ç¼©æ¯”**        | 1.0             | âŒ é›¶å‹ç¼©ï¼ˆä¿¡æ¯é‡ä¿æŒåŸæ ·ï¼‰     |
| **åºåˆ—é•¿åº¦**      | åŸå§‹å­—èŠ‚æ•°        | âš ï¸ è‡´å‘½ç¼ºé™·                   |

---

#### **åºåˆ—é•¿åº¦ç¾éš¾è¯¦è§£**
##### æ–‡æœ¬åˆ†è§£ç¤ºä¾‹
```
"Hello, ğŸŒ! ä½ å¥½!" â†’ åŸå§‹å­—ç¬¦ä¸²é•¿åº¦ï¼š11å­—ç¬¦
                  â†’ å­—èŠ‚åºåˆ—é•¿åº¦ï¼š19ä¸ªtoken
```

##### é•¿åº¦è†¨èƒ€åŸå› 
| **å­—ç¬¦ç±»å‹** | **å­—ç¬¦æ•°** | **å­—èŠ‚/Tokensæ•°** | **è†¨èƒ€å€æ•°** |
|-------------|-----------|------------------|------------|
| ASCIIå­—æ¯   | 6         | 6                | 1x         |
| ASCIIæ ‡ç‚¹   | 3         | 3                | 1x         |
| åœ°çƒè¡¨æƒ…ğŸŒ   | 1         | 4                | 4x         |
| ä¸­æ–‡å­—ç¬¦     | 2         | 6                | 3x         |

> ğŸ’¡ ç»¼åˆè†¨èƒ€ï¼š**1.73å€**ï¼ˆ11å­—ç¬¦ â†’ 19 tokensï¼‰

### 5.5 word_tokenizer

#### **å•è¯åˆ†è¯åŸºæœ¬åŸç†**
```mermaid
flowchart TD
A[å­—ç¬¦ä¸²] --> B[æ­£åˆ™åˆ†å‰²] 
B --> C[å­—æ¯æ•°å­—å•å…ƒ] 
B --> D[éå­—æ¯ç¬¦å·å•å…ƒ]
```

1. **ç»å…¸NLPåˆ†è¯æ–¹å¼**  
   - æ ¸å¿ƒé€»è¾‘ï¼šæŒ‰è¯è¾¹ç•Œåˆ†å‰²  
   - æ­£åˆ™è¡¨è¾¾å¼ç¤ºä¾‹ï¼š  
     ```python
     # åŸºç¡€ç‰ˆï¼šä¿ç•™å­—æ¯æ•°å­—åºåˆ—
     segments = regex.findall(r"\w+|.", "I'll say supercalifragilisticexpialidocious!")
     # ç»“æœï¼š["I", "'", "ll", "say", "supercalifragilisticexpialidocious", "!"]
     
     # GPT-2å¢å¼ºç‰ˆï¼ˆå¤„ç†ç¼©å†™ï¼‰ï¼š
     pattern = r"""'(?:[sdmt]|ll|ve|re)| ?\w\w+|\w|\S\s*"""
     segments = regex.findall(pattern, "I'll")  # ç»“æœï¼š["I", "'ll"]
     ```

---

#### **æ ¸å¿ƒå®ç°æŒ‘æˆ˜**
| **æ­¥éª¤**          | **é—®é¢˜**                  | **åæœ**                     |
|-------------------|--------------------------|-----------------------------|
| **ç‰‡æ®µæ˜ å°„**       | éœ€è¦æ„å»ºç‰‡æ®µâ†’IDçš„æ˜ å°„       | è¯è¡¨è§„æ¨¡å¤±æ§                  |
| **ç½•è§è¯å¤„ç†**     | é•¿å°¾è¯æ±‡å­¦ä¹ ä¸è¶³            | æ¨¡å‹æ— æ³•ç†è§£ç”Ÿåƒ»è¯             |
| **æ–°è¯å¤„ç†**       | ä¾èµ–UNKå ä½ç¬¦             | ç ´åæ–‡æœ¬å®Œæ•´æ€§                 |

---

#### **è‡´å‘½ç¼ºé™·åˆ†æ**

##### 1. **è¯è¡¨è§„æ¨¡çˆ†ç‚¸**
| **è¯­æ–™è§„æ¨¡**       | **é¢„ä¼°è¯è¡¨å¤§å°**     | **æ¨¡å‹å†…å­˜éœ€æ±‚**   |
|-------------------|---------------------|-------------------|
| å°å‹æ•°æ®é›†(1GB)    | 50,000~200,000      | 200~800MB         |
| å¤§å‹è¯­æ–™(100GB)   | 500,000~2,000,000   | 2GB~8GB           |
| å¤šè¯­è¨€æ··åˆè¯­æ–™     | 5,000,000+          | >20GB             |

> ç¤ºä¾‹é•¿è¯ï¼š`"supercalifragilisticexpialidocious"`ï¼ˆ34å­—æ¯ï¼‰éœ€ç‹¬å ä¸€ä¸ªID

##### 2. **æ•°æ®ç¨€ç–ç¾éš¾**
- **é½å¤«å®šå¾‹ä½“ç°**ï¼š  
  è¯­æ–™ä¸­50%å•è¯ä»…å‡ºç°1æ¬¡ â†’ æ¨¡å‹æ— æ³•å­¦ä¹ æœ‰æ•ˆè¡¨ç¤º
- **å®é™…å½±å“**ï¼š  
  ```python
  # è®­ç»ƒæ–‡æœ¬ä¸­å‡ºç°1æ¬¡çš„è¯
  rare_words = [w for w in vocab if counts[w]==1] 
  # å æ¯”é€šå¸¸ >40%
  ```

##### 3. **UNKé™·é˜±**
```python
è¾“å…¥ï¼š"ChatGPT is revolutionary!"
åˆ†è¯ï¼š["Chat", "GPT", "is", "revolutionary", "!"] â†’ è‹¥"GPT"æœªç™»å½•
è¾“å‡ºï¼š["Chat", UNK, "is", "revolutionary", "!"]
```
- **è§£ç ç ´å**ï¼šåŸå§‹è¯­ä¹‰ä¸¢å¤±ï¼ˆGPTâ†’UNKï¼‰
- **å›°æƒ‘åº¦å¤±çœŸ**ï¼š`P(text) = P(Chat) * P(UNK) * ...` æ— æ³•åæ˜ çœŸå®æ¦‚ç‡

---

#### **å‹ç¼©æ•ˆç‡å‡è±¡**
```python
text = "I'll say supercalifragilisticexpialidocious!"
bytes_orig = len(text.encode())  # 45å­—èŠ‚
token_count = len(segments)      # 6ä¸ªtokenï¼ˆå‡è®¾ï¼‰

# è®¡ç®—å‹ç¼©æ¯”
compression_ratio = 45 / (6 * 4) = 1.875  # è¡¨é¢é«˜æ•ˆ
```

##### çœŸå®æ•ˆç‡æ‹†è§£ï¼š
| **token**             | **å­˜å‚¨æˆæœ¬** | **ä¿¡æ¯ä»·å€¼**       |
|-----------------------|-------------|-------------------|
| `"I"`                 | 4å­—èŠ‚        | é«˜ï¼ˆå¸¸è§è¯ï¼‰        |
| `"'ll"`               | 4å­—èŠ‚        | ä¸­ï¼ˆä¸­ç­‰é¢‘ç‡ï¼‰      |
| `"say"`               | 4å­—èŠ‚        | é«˜                |
| `"supercali...ocious"`| 4å­—èŠ‚        | **æä½**ï¼ˆç½•è§è¯ï¼‰  |
| `"!"`                 | 4å­—èŠ‚        | ä½                |

> ğŸ’¡ æ ¸å¿ƒé—®é¢˜ï¼š**ç½•è§è¯å ç”¨ç›¸åŒå­˜å‚¨ä½†è´¡çŒ®æå°ä»·å€¼**ï¼Œå®é™…ä¿¡æ¯å¯†åº¦ä½ä¸‹

---

#### **ä¸å­è¯åˆ†è¯å¯¹æ¯”**
| **æŒ‡æ ‡**         | å•è¯åˆ†è¯          | å­è¯åˆ†è¯ï¼ˆBPEï¼‰   |
|------------------|------------------|------------------|
| è¯è¡¨è§„æ¨¡         | 10âµ~10â¶          | 10â´~10âµ          |
| UNKå‡ºç°ç‡        | >5%              | <0.1%            |
| åºåˆ—é•¿åº¦         | è¾ƒçŸ­             | é€‚ä¸­             |
| æ–°è¯å¤„ç†         | å®Œå…¨å¤±è´¥          | å­è¯ç»„åˆæˆåŠŸ      |
| åµŒå…¥å±‚å†…å­˜       | è¶…å¤§è§„æ¨¡          | ç´§å‡‘é«˜æ•ˆ          |

---

#### **å†å²åœ°ä½**
- **2010å¹´å‰ä¸»æµ**ï¼šN-gramè¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒæŠ€æœ¯
- **è¢«æ·˜æ±°åŸå› **ï¼š  
  2013å¹´https://arxiv.org/abs/1301.3781æå‡ºå­è¯å‘é‡åŒ–  
  2015å¹´https://arxiv.org/abs/1508.07909å…¨é¢å–ä»£å•è¯åˆ†è¯

### 5.6 bpe_tokenizer

#### **BPEæŠ€æœ¯æ¼”è¿›å²**
| **æ—¶é—´** | **é‡Œç¨‹ç¢‘**                 | **è´¡çŒ®**                                 | **æ ¸å¿ƒæ–‡çŒ®**                                                                 |
|---------|---------------------------|----------------------------------------|----------------------------------------------------------------------------|
| 1994     | Philip Gage               | é¦–æ¬¡æå‡ºBPEç”¨äºæ•°æ®å‹ç¼©                 | http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM |
| 2016     | Sennrich et al.           | å°†BPEå¼•å…¥NLPé¢†åŸŸæ›¿ä»£å•è¯åˆ†è¯             | https://arxiv.org/abs/1508.07909             |
| 2019     | OpenAI GPT-2              | BPEå·¥ä¸šçº§å®è·µï¼Œç»“åˆæ­£åˆ™é¢„åˆ†è¯            | https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf |

---

#### **Byte Pair Encoding (BPE) æ ¸å¿ƒåŸç†**
```mermaid
graph LR
A[åŸå§‹æ–‡æœ¬] --> B(å­—ç¬¦çº§æ‹†åˆ†)
B --> C{ç»Ÿè®¡ç›¸é‚»ç¬¦å·å¯¹é¢‘ç‡}
C --> D[åˆå¹¶æœ€é«˜é¢‘ç¬¦å·å¯¹]
D --> C
C --> E[é‡å¤ç›´åˆ°è¯è¡¨è¾¾æ ‡]
```

**æ ¸å¿ƒç›´è§‰**ï¼š
- **é«˜é¢‘åºåˆ—**ï¼šåˆå¹¶æˆå•tokenï¼ˆå¦‚"ing"â†’1 tokenï¼‰
- **ç½•è§åºåˆ—**ï¼šä¿ç•™å¤štokenï¼ˆå¦‚"ğŸŒ"â†’4å­—èŠ‚tokenï¼‰

---

#### **BPEè®­ç»ƒå®æˆ˜**
**è¾“å…¥æ–‡æœ¬**ï¼š`"the cat in the hat"`  
```python
# åˆå§‹çŠ¶æ€
è¯æ±‡: ['t','h','e',' ','c','a','i','n','h','a','t']
é¢‘ç‡: {('t','h'):2, ('h','e'):2, ...}

# æ‰§è¡Œ3æ¬¡åˆå¹¶ï¼š
1. åˆå¹¶æœ€é«˜é¢‘å¯¹('t','h')â†’'th': 
  æ–°åºåˆ—: ["th", "e", " ", "c", "a", "t", " ", "i", "n", " ", "th", "e", " ", "h", "a", "t"]
2. åˆå¹¶('th','e')â†’'the':
  æ–°åºåˆ—: ["the", " ", "c", "a", "t", " ", "i", "n", " ", "the", " ", "h", "a", "t"]
3. åˆå¹¶('h','a')â†’'ha':
  æœ€ç»ˆåºåˆ—: ["the", " ", "c", "a", "t", " ", "i", "n", " ", "the", " ", "ha", "t"]

# è¾“å‡ºå‚æ•°ï¼š
merges = {('t','h'):256, ('th','e'):257, ('h','a'):258}
vocab = {0-255å­—èŠ‚ + 256:'th', 257:'the', 258:'ha'}
```

---

#### **BPEç¼–è§£ç éªŒè¯**
```python
class BPETokenizer:
    def __init__(self, params):
        self.merges = params.merges  # åˆå¹¶è§„åˆ™å­—å…¸
        self.vocab = params.vocab    # è¯æ±‡è¡¨

    def encode(self, text):
        # åˆå§‹å­—èŠ‚åºåˆ—
        indices = list(text.encode('utf-8'))
        # æŒ‰åºåº”ç”¨åˆå¹¶è§„åˆ™
        for pair, new_id in self.merges.items():
            indices = merge(indices, pair, new_id)
        return indices

    def decode(self, ids):
        return b''.join(self.vocab[i] for i in ids).decode('utf-8')

# æµ‹è¯•ç”¨ä¾‹
text = "the quick brown fox"
ids = tokenizer.encode(text)  # ç¤ºä¾‹: [257, 32, 113, 258, ...]
assert tokenizer.decode(ids) == text  # âœ… æ— æŸå¾€è¿”
```

---

#### **BPEç”Ÿäº§çº§ä¼˜åŒ–æ–¹å‘**
| **åŸºç¡€å®ç°å±€é™**       | **å·¥ä¸šçº§ä¼˜åŒ–æ–¹æ¡ˆ**                     |
|------------------------|----------------------------------------|
| å…¨é‡éå†åˆå¹¶è§„åˆ™         | **å¢é‡åˆå¹¶**ï¼šåªæ›´æ–°å—å½±å“çš„ç¬¦å·å¯¹         |
| æ— ç‰¹æ®Šæ ‡è®°æ”¯æŒ           | **ä¿ç•™å…ƒæ ‡è®°**ï¼š`<\|endoftext\|>`ç­‰       |
| æœªå¤„ç†å¤šè¯­è¨€æ··åˆ         | **é¢„åˆ†è¯ä¼˜åŒ–**ï¼šé›†æˆGPT-2æ­£åˆ™è¡¨è¾¾å¼        |
| O(nÂ²)æ—¶é—´å¤æ‚åº¦        | **å“ˆå¸ŒåŠ é€Ÿ**ï¼šåŒæ•°ç»„Trieæ ‘å®ç°å¿«é€ŸåŒ¹é…     |

**GPT-2é¢„åˆ†è¯æ­£åˆ™**ï¼š
```python
GPT2_REGEX = r"""'(?:[sdmt]|ll|ve|re)| ?\w\w+|\w|\S\s*"""
# åŠŸèƒ½ï¼šæ™ºèƒ½åˆ†å‰²ç¼©å†™ï¼ˆå¦‚"I'll"â†’["I", "'ll"]ï¼‰
```

---

#### **BPEä¼˜åŠ¿è§£æ**
1. **è¯è¡¨è§„æ¨¡å¯æ§**  
   - ç”¨æˆ·è‡ªå®šä¹‰åˆå¹¶æ¬¡æ•°ï¼ˆå¦‚50Kæ¬¡â†’50Kè¯è¡¨ï¼‰
2. **é›¶OOVé—®é¢˜**  
   OOV: å®Œå…¨æ¶ˆé™¤æœªç™»å½•è¯ï¼ˆOut-Of-Vocabularyï¼‰â€‹â€‹

   ```python
   # æ–°è¯å¤„ç†ç¤ºä¾‹
   encode("ChatGPT") = encode("Chat") + encode("G") + encode("PT")
   # å³ä½¿æœªè®­ç»ƒè¿‡"ChatGPT"ä¹Ÿèƒ½åˆç†æ‹†åˆ†
   ```
3. **å¤šè¯­è¨€å‹å¥½**  
   !https://tiktokenizer.vercel.app/example-zh-en.png

4. **æ•ˆç‡å¹³è¡¡**  
   | **æ–‡æœ¬ç±»å‹** | å­—ç¬¦åˆ†è¯é•¿åº¦ | BPEåˆ†è¯é•¿åº¦ | å‹ç¼©æ¯” |
   |-------------|-------------|------------|--------|
   | çº¯è‹±æ–‡       | 1200        | 850        | 1.41x  |
   | ä¸­è‹±æ··åˆ     | 1800        | 700        | 2.57x  |

---

#### **å†å²æ„ä¹‰**
> BPEçš„å‘æ˜æ ‡å¿—ç€NLPä»**åŸºäºè§„åˆ™çš„åˆ†è¯**è½¬å‘**æ•°æ®é©±åŠ¨çš„å­è¯è¡¨ç¤º**ï¼Œæˆä¸ºTransformeræ—¶ä»£çš„æ ¸å¿ƒåŸºç¡€è®¾æ–½ã€‚å…¶å¹³è¡¡è¯è¡¨æ•ˆç‡ä¸åºåˆ—é•¿åº¦çš„è®¾è®¡å“²å­¦ï¼Œè‡³ä»Šä»åœ¨Llamaã€DeepSeekç­‰å¼€æ”¾æ¨¡å‹ä¸­å»¶ç»­ã€‚
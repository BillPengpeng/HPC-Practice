本文主要整理CS336 Lecture 12 Evaluation章节的主要内容。

## 1. what_you_see

Recent language models are evaluated on similar, but not entirely identical, benchmarks (MMLU, MATH, etc.).
- What are these benchmarks?
- What do these numbers mean?

### 内容概况

核心论点是**行业正面临一场“评估危机”**：传统的公共评估基准或因过时、或因范围狭窄、或因被过度拟合而逐渐失效，导致难以准确、全面地衡量LLM的真实能力。作者表达了一种普遍的困惑：在缺乏可靠指标的情况下，甚至无法判断这些模型究竟有多好。

### 要点总结

1.  **核心论断：评估危机**
    *   开篇明义，指出当前LLM领域存在“评估危机”。作者坦言自己目前不知道应该关注哪些指标来有效评判模型优劣。

2.  **对现有主流评估基准的批判**
    *   **MMLU（大规模多任务语言理解）**：承认它过去几年很有用，但认为其有效性“早已过去”，暗示已无法充分衡量前沿模型的能力。
    *   **SWE-Bench Verified（软件工程基准验证版）**：肯定其价值（基于真实、实用、已验证的问题），但指出其评估范围**过于狭窄**，仅专注于编码能力，无法代表模型的整体水平。
    *   **Chatbot Arena（聊天机器人竞技场）**：认为其信号质量**正在下降**。原因在于模型开发方开始系统性地“过度拟合”这个基于人类反馈的排行榜，手段包括：
        *   **提示挖掘**：从API请求中分析用户偏好。
        *   **私有评估轰炸**：在私有测试集上反复调试。
        *   **将排名用作训练监督**：直接优化模型以提升排名。

3.  **对替代方案的看法**
    *   **私有评估集**：注意到大量私有评估集出现，认为将多个私有评估组合起来可能是未来一个有希望的方向。
    *   **主观“氛围检查”**：在缺乏全面评估的情况下，曾尝试依赖主观感受，但现在认为这**容易产生误导**，存在确认偏差和样本量过小等问题，并不可靠。

4.  **最终结论：不确定性**
    *   总结再次强调，由于上述评估困境，作者无法确切知道当前这些LLM的真实能力水平如何。这反映了一种普遍的行业焦虑。

**总结而言，这段评论揭示了LLM领域一个关键挑战：模型的发展速度已经超过了公共评估体系的发展，导致衡量标准模糊不清，这对技术进步、学术研究和商业应用都构成了障碍。**

## 2. how_to_think_about_evaluation

### 内容概况

内容首先驳斥了“评估是机械性打分(evaluation is a mechanical process)”的浅显观点，转而将其提升到一个**深刻、丰富且能决定技术未来走向的战略高度**。其核心论点是：**不存在唯一的“正确”评估方法，评估的目的完全取决于你想回答什么问题**。为此，内容为读者提供了一个完整的评估框架，该框架将评估过程分解为四个关键环节（输入、模型调用、输出评估、结果解读），并在每个环节提出了一系列需要深思熟虑的问题，旨在引导读者构建一个严谨、可靠且有针对性的评估体系。

### 要点总结

**1. 核心观点：评估是目的驱动的，而非机械化的**
*   **驳斥误区**：评估不是简单地用提示词测试模型然后取平均分。
*   **正确定位**：评估是一个深刻、丰富的话题，它决定着语言模型的未来发展方向。
*   **核心原则**：不存在“唯一真评估”。评估方法完全取决于评估者想要解答的核心问题。

**2. 评估的四大目的（取决于角色和需求）**
*   **使用者/公司**：为了特定用例（如客服聊天机器人）做出采购决策（选模型A还是B）。
*   **研究者**：衡量模型的原始能力（如智力水平）。
*   **政策/商业决策者**：理解模型的益处和潜在危害。
*   **模型开发者**：获取反馈以迭代和改进模型。
    *   **总结**：每种情况都需要将一个**抽象目标**转化为一个**具体的评估方案**。

**3. 系统性的评估框架（四个关键环节）**
框架提出了进行评估时必须系统化思考的四个步骤及其核心问题：

*   **环节一：定义输入**
    *   评估覆盖了哪些**用例**？
    *   是否包含了长尾分布中的**困难输入**样本？
    *   输入是否针对模型做了**适配**（例如，多轮对话）？

*   **环节二：如何调用模型**
    *   使用什么**提示词**？
    *   模型是否使用了**思维链、工具调用、RAG**等技术？
    *   评估对象是**模型本身**还是一个**智能体系统**？（开发者关心前者，用户更关心后者）

*   **环节三：如何评估输出**
    *   用于评估的**参考答案**本身是否准确无误？
    *   使用什么**评估指标**（如pass@k）？
    *   如何将**成本**（推理、训练）因素考虑在内？
    *   如何处理**非对称错误**（e.g., hallucinations in a medical setting）？
    *   如何评估**开放式生成**任务（no ground truth）？

*   **环节四：如何解读结果**
    *   如何解读一个具体数字（如91%）——是否意味着可以部署上线？
    *   在训练数据和测试数据可能存在重叠的情况下，如何评估其**泛化能力**？
    *   我们评估的究竟是**最终的模型**，还是训练模型的**方法**？

**总结而言，这段内容提供了一个极具价值的评估思维框架。它强调评估是一个需要精心设计的、问题导向的探究过程，而非简单的技术操作，引导读者全面而批判性地思考模型评估的每一个细节。**

## 3. perplexity

### 内容概况

本部分系统性地阐述了评估语言模型的核心指标——**困惑度**。内容涵盖了从困惑度的基本定义、计算方法、在预训练和测试中的应用，到其历史演变、优缺点分析，以及与下游任务评估的关系。最后还引入了“困惑度最大主义”的理论观点和与困惑度精神相似的其他评估任务。整体结构清晰，由浅入深，兼具理论深度和实际考量。

---

### 要点总结

**1. 核心定义与作用**
*   **定义**：困惑度是衡量一个概率分布模型 `p(x)` 为某个数据集 `D` 分配概率高低的指标。值越低，说明模型认为这个数据集出现的可能性越高，即模型越好。
*   **核心作用**：
    *   **训练**：预训练的目标就是最小化模型在训练集上的困惑度。
    *   **测试**：最直接的方法是衡量模型在测试集上的困惑度。

**2. 历史、标准数据集与演变**
*   **标准数据集**：提到了经典的语言模型评估数据集，如Penn Treebank、WikiText-103和One Billion Word Benchmark。
*   **评估方式的演变**：
    *   **早期**：模型在同一数据集的不同划分（训练集/测试集）上进行训练和评估。
    *   **GPT-2/GPT-3之后**：研究重点更多转向**下游任务的准确率**。GPT-2开创了在庞大网络数据（WebText）上训练，然后在标准数据集上进行**零样本、分布外评估**的模式。

**3. 困惑度的优势与局限**
*   **为什么仍然有用**：
    *   **平滑性**：相比离散的下游任务准确率，困惑度变化更平滑，更适合用于拟合**缩放定律**。
    *   **通用性**：是训练模型的通用目标，能捕捉到任务准确率可能忽略的细微差别。
    *   **可扩展**：也可以在下游任务上测量**条件困惑度**。
*   **重要警告**：
    *   测量困惑度需要**信任模型**本身，因为它依赖于模型提供的概率值。而对于任务准确率，只需将模型视为黑箱，分析其输出即可。

**4. 理论视角：困惑度最大主义**
*   **核心观点**：如果真实的数据分布是 `t`，模型是 `p`，那么最好的可能困惑度是 `t` 的熵 `H(t)`，且当且仅当 `p = t` 时达到。因此，持续降低困惑度，最终让模型无限接近真实分布，就能实现AGI。
*   **潜在问题**：这条路径可能不是最高效的，因为优化可能会集中在数据分布中不重要的部分。

**5. 精神相似的评估任务**
*   指出了一些在理念上与困惑度相似的任务，它们都侧重于评估模型对概率分布的掌握程度：
    *   **LAMBADA**：完形填空任务，测试模型理解长距离依赖的能力。
    *   **HellaSwag**：评估模型的常识推理能力。

## 4. knowledge_benchmarks

### 内容概况

本部分系统地介绍了四个用于评估大型语言模型**知识广度与深度**的核心基准测试。内容从经典的MMLU开始，逐步过渡到其升级版MMLU-Pro、更具挑战性的专家级测试GPQA，最终到最新、最全面的“人类终极考试”。每个测试都详细说明了其科目范围、问题来源、评估方式、前沿模型的表现，并提供了可视化的排行榜链接。整体呈现了LLM知识评估从基础到前沿的演进路径。

---

### 要点总结

**1. Massive Multitask Language Understanding (MMLU) - 经典基准**
*   **范围**：涵盖57个学科（如数学、美国历史、法律、道德），均为选择题。
*   **来源**：问题由学生从网上免费资源收集。
*   **核心**：本质是测试**知识储备**，而非纯粹的语言理解。
*   **评估方式**：使用小样本提示进行评测。
*   **现状**：虽经典但已趋于饱和，需要更优的测试。

**2. MMLU-Pro - MMLU的优化与强化版**
*   **改进点**：
    *   **题目质量**：删除了原始MMLU中嘈杂/简单的问题。
    *   **选项数量**：将选项从4个增加到10个，大幅提高猜测难度。
    *   **评估方式**：使用思维链进行评测，给模型更多推理机会。
*   **效果**：模型准确率下降16%至33%，表明其能更好地区分模型能力，尚未饱和。

**3. Graduate-Level Google-Proof Q&A (GPQA) - 专家级难题**
*   **来源**：题目由61位拥有博士学位的专家编写，确保专业性和深度。
*   **难度标杆**：
    *   博士专家本人答题准确率为65%。
    *   非专业人士在允许使用谷歌搜索30分钟的情况下，准确率仅为34%。
    *   GPT-4的准确率为39%，表明其难度极高。

**4. Humanity‘s Last Exam (HLE) - “终极考试”**
*   **规模与形式**：包含2500道题目，**多模态**（不限于文本），涵盖多学科，题型包括选择题和简答题。
*   **题目质量**：通过前沿LLM筛选和多轮专家评审，并设立了高额奖金池激励题目创作。
*   **目标**：旨在创建一個极其困难、全面且高质量的评估基准，以测试最前沿模型的能力极限。

**总结而言，这四个基准测试展示了LLM知识评估的演进方向：从广度的覆盖到深度的挖掘，从纯文本到多模态，不断抬高评估的天花板，以推动模型在知识能力上持续进步。**

## 5. instruction_following_benchmarks

### 内容概况

本部分系统性地介绍了评估大语言模型“指令遵循”能力的几种主流方法。指令遵循是像ChatGPT这样的模型的核心能力，即理解并执行用户的开放式指令。其核心挑战在于如何评估一个开放式的、没有标准答案的回复。介绍从最具代表性的**人类反馈评估**（Chatbot Arena）开始，逐步深入到其他各具特色的自动或半自动评估基准，展示了当前业界如何应对这一挑战。

---

### 要点总结

**1. 核心挑战：如何评估开放式生成结果？**
*   与有标准答案的测试（如选择题）不同，指令遵循的回复是开放性的，难以用简单的对错来衡量。因此，评估方法主要围绕**比较模型回复的优劣**展开。

**2. 四大主流评估基准及其特点：**

| 评估基准 | 核心机制 | 主要特点与优缺点 |
| :--- | :--- | :--- |
| **Chatbot Arena** | **人类偏好投票**：真实用户匿名比较两个模型的回复，并投票选择更好的一个，最终计算ELO排名。 | **优点**：基于真实、动态的用户输入；能容纳新模型；反映了人类最直接的偏好。<br>**地位**：常被用作检验其他基准有效性的“事实标准”（如WildBench与之相关性达0.95）。 |
| **Instruction-Following Eval (IFEval)** | **可验证约束**：在指令中加入可自动验证的简单、合成性约束（如“必须包含关键词X”、“回答不超过50字”），检查回复是否满足这些形式要求。 | **优点**：评估客观、自动化。<br>**缺点**：只能评估形式，无法评估语义质量；指令和约束较为人工化，不够自然。 |
| **AlpacaEval** | **强模型作为裁判**：使用805条指令，让评估模型与基线模型（如GPT-4）分别作答，然后用一个强大的LLM（如GPT-4本身）作为裁判来评判哪个回复更好，计算胜率。 | **优点**：快速、廉价、可重复。<br>**缺点**：存在裁判模型的**偏见风险**（可能偏向于特定风格或它自己生成的答案）。 |
| **WildBench** | **真实对话+结构化评判**：从互联网收集1024个真实的人机对话记录，使用GPT-4作为裁判，并提供一个**评判清单**来引导裁判进行更细致、更可靠的评估。 | **优点**：基于真实交互数据；通过评判清单（类似思维链）提高了评估的可靠性；与Chatbot Arena相关性极高。 |

**总结而言，这些基准代表了两种主要的评估范式：基于人类偏好的“黄金标准”和基于强模型自动评判的“高效替代方案”。它们从不同角度（真实性、可验证性、效率、可靠性）出发，共同构成了评估模型指令遵循能力的多维视角。**

## 6. agent_benchmarks

### 内容概况
本部分内容聚焦于**评估AI智能体（Agent）在需要工具使用和迭代执行的任务上的性能**。智能体被定义为“语言模型+智能体框架”（即决定如何使用语言模型的逻辑）。图片详细介绍了三个针对不同领域的智能体基准测试：**SWE Bench**（软件工程）、**CyBench**（网络安全）和**MLEBench**（机器学习工程）。每个基准测试都提供了任务描述、规模、评估指标以及结果的可视化图表链接。

---

### 要点总结

1.  **核心评估对象：AI智能体**
    *   评估对象不是基础的语言模型，而是**智能体**，即具备工具使用（如运行代码）和迭代执行复杂任务能力的AI系统。

2.  **三大基准测试概览**
    *   **SWE Bench（软件工程）**：
        *   **任务**：基于真实的代码库和问题描述，生成能通过单元测试的拉取请求。
        *   **规模**：包含来自12个Python仓库的2294项任务。
        *   **评估方式**：以单元测试是否通过作为核心评估指标。

    *   **CyBench（网络安全）**：
        *   **任务**：完成40项夺旗任务。
        *   **评估方式**：采用**首次解决时间**作为衡量任务难度和智能体性能的关键指标。

    *   **MLEBench（机器学习工程）**：
        *   **任务**：模拟75个Kaggle竞赛场景，要求智能体完成数据预处理、模型训练等端到端的机器学习流水线任务。

3.  **共同特点**
    *   **注重实际应用**：所有基准测试都基于真实世界的复杂场景，测试的是智能体解决实际问题的综合能力。
    *   **评估指标明确**：均采用客观、可量化的指标（如测试通过率、解决时间）进行评估。
    *   **超越纯文本生成**：这些基准共同指向了对AI能力更高的要求——不再仅仅是对话或内容生成，而是在特定领域中**使用工具、执行多步计划并解决专业问题**。

**总结**：这张图片呈现了当前评估前沿AI智能体的几个关键基准，它们标志着对大模型能力的评估正从“知识”和“对话”转向在专业领域内“行动”和“解决问题”的能力。

## 7. pure_reasoning_benchmarks

![arc-task-grids]("https://arcprize.org/media/images/arc-task-grids.jpg")
![oseriesleaderboard]("https://arcprize.org/media/images/oseriesleaderboard.png")
![arc-agi-2-unsolved-1]("https://arcprize.org/media/images/blog/arc-agi-2-unsolved-1.png")

### 内容概况
本部分重点探讨了如何评估AI的**纯推理能力**——即剥离语言知识和事实记忆后，衡量其核心智能水平。内容围绕**ARC-AGI**这一基准测试展开，强调了推理作为“更纯粹的智力形式”的价值，并通过任务网格图、排行榜和未解难题示例，直观展示了该测试的设计理念与挑战性。

---

### 要点总结

1. **核心问题：隔离推理与知识**
   - 指出此前任务均依赖语言和世界知识，而ARC-AGI旨在**剥离这些外部知识**，单独评估模型的抽象推理能力。
   - 强调推理代表一种“更纯粹的智力形式”，其价值超越单纯的事实记忆。

2. **ARC-AGI基准测试**
   - **提出者与时间**：由François Chollet于2019年提出。
   - **核心任务**：要求模型从少量示例中归纳出抽象规则，并应用于新场景（核心是**模式识别与推理**）。
   - **可视化展示**：
     - **任务网格图**：呈现多种推理任务类型，体现测试的多样性。
     - **排行榜图**：显示当前模型在该测试中的性能排名。
     - **未解难题示例**（ARC-AGI-2）：突出第二代测试更难，多数问题尚未被解决。

3. **意义与挑战**
   - **评估目标**：直接指向AGI（通用人工智能）的核心能力——无需依赖预训练知识的基础推理。
   - **现实难度**：ARC-AGI-2的“未解难题”表明，当前AI在纯粹推理方面仍面临显著挑战。

**总结**：该部分内容通过ARC-AGI基准，强调了评估“纯推理能力”在衡量AI本质智能中的重要性，并揭示了当前模型在这一核心能力上的局限。

## 8. safety_benchmarks

### 内容概况
此部分内容系统性地介绍了**AI安全性评估**的框架、基准测试与核心挑战。它从“什么是AI安全”这一根本问题出发，依次介绍了基于有害行为分类的**HarmBench**、基于法规政策的**AIR-Bench**、旨在绕过安全限制的**越狱技术**、以及**预部署安全测试**流程。最后，内容深入探讨了安全性的复杂本质，辨析了“能力”与“倾向性”对安全的不同影响，并提出了**“双刃剑”** 这一核心困境。

---

### 要点总结

**1. 核心安全基准测试**
*   **HarmBench**：基于**510种**违反法律或社会规范的有害行为构建，是衡量模型抵制直接有害指令能力的基础基准。
*   **AIR-Bench**：基于**监管框架和公司政策**构建，将风险分类为**314个类别**，包含**5694个提示**，更具系统性和法规符合性。两者均在HELM平台提供公开排行榜。

**2. 安全威胁与防御实践**
*   **越狱**：模型虽被训练拒绝有害指令，但**贪婪坐标梯度（GCG）** 等算法可自动优化提示词以绕过安全防护，且这种攻击能从开源模型**迁移**到闭源模型。
*   **预部署测试**：美国与英国AI安全研究所合作，在模型发布前进行自愿性安全评估，旨在提前发现风险。

**3. 安全性的复杂内涵与关键辨析**
*   **安全性是情境化的**：取决于政治、法律、社会规范等变量，并无统一标准。
*   **能力 vs. 倾向性**：这是理解安全性的关键框架。
    *   **能力**：模型能否完成某项任务（如编写恶意软件）。对于**开源模型**，能力是主要风险，因为安全防护易被微调移除。
    *   **倾向性**：模型是否愿意执行该任务。对于**API模型**，倾向性（即拒绝有害请求）是安全的关键。
*   **“双刃剑”困境**：许多强大能力（如精通网络安全的智能体）本质是**中立的**，既可用来进行渗透测试（正向），也可用于攻击系统（负向）。这使得纯粹的能力评估（如CyBench）是否应被视为安全评估，成为一个开放且关键的争议点。

**总结**：该内容揭示了AI安全性远非简单的“拒绝有害请求”，而是一个涉及基准测试、对抗性攻击、预部署审计，尤其是需要辩证看待“能力”与“应用”的复杂系统工程。

## 9. realism

### 内容概况

此部分内容探讨了语言模型评估中的**现实性**问题。它指出，尽管语言模型已在实践中被大规模使用，但主流评估基准（如MMLU）与真实应用场景存在显著差距。内容通过对比“测验式”和“询问式”两种用户提示类型，强调后者更能反映真实需求并创造价值。随后，重点介绍了两个致力于提升评估现实性的项目——**Clio**（通过模型分析真实用户数据）和**MedHELM**（基于临床医生真实任务构建的医学评估），并最终点明了追求现实性评估时面临的**隐私挑战**。

---

### 要点总结

**1. 核心问题：评估与现实应用的脱节**
*   **现状**：语言模型已在现实世界中产生海量使用（如OpenAI处理了千亿级tokens，Cursor编辑器生成了十亿行代码）。
*   **矛盾**：然而，像MMLU这样的主流基准测试远离真实使用场景；另一方面，直接使用充满“垃圾”的真实用户流量进行评估也非理想方案。

**2. 关键区分：两种用户意图**
*   **测验**：用户知道答案，意在测试系统（如标准化考试）。这是传统基准的模式。
*   **询问**：用户不知道答案，意在利用系统获取信息。**这种模式更为现实，并能真正为用户创造价值**，是评估应关注的重点。

**3. 前沿实践：提升现实性的项目案例**
*   **Clio**：由Anthropic发起。其核心方法是**使用语言模型本身来分析匿名的真实用户查询数据**，以发现和理解人们实际使用模型的模式、意图和需求，从而让评估更贴近现实。
*   **MedHELM**：一个医学评估基准。它摒弃了基于标准化考试的旧模式，转而从**29位临床医生的实际工作中提取了121项临床任务**（混合使用私有和公开数据集），极大地增强了医学评估的现实性和实用性。

**4. 重要挑战：现实性与隐私的冲突**
*   内容最后指出了一个关键困境：追求评估的现实性（需要接触真实用户数据）有时会与**用户隐私保护**的要求产生矛盾。这是该领域发展必须权衡和解决的难题。

**总结**：该部分内容推动评估思维从“学术测验”转向“真实应用”，并通过具体案例展示了如何让评估更贴近用户的实际使用场景和需求，同时提醒了隐私这一重要约束条件。

## 10. validity

### 内容概况

此部分内容旨在回答一个根本性问题：**“我们如何知道我们的评估是有效的？”** 它指出了在当前大模型时代，传统的机器学习评估原则面临的挑战，并重点探讨了两大威胁评估有效性的核心问题：**训练-测试集重叠** 和 **基准数据集的质量**。针对每个问题，内容都介绍了相应的检测方法、改进方案或倡导的行业规范。

---

### 要点总结

**1. 核心挑战：训练-测试集重叠**

*   **传统原则**：机器学习的基本要求是训练集和测试集必须严格分离。
*   **现代挑战**：大模型在互联网海量数据上训练，其训练数据可能早已包含了用于评估的公开基准测试集，导致评估分数虚高，无法反映模型的真实泛化能力。
*   **解决方案与倡导**：
    *   **路线一：技术检测**。通过统计方法（如利用数据点的“可交换性”）来推断模型是否在训练中见过测试数据。
    *   **路线二：建立规范**。倡导模型提供方应主动报告其训练数据与主流基准测试集的重叠情况，并鼓励在报告结果时提供置信区间，以提高透明度。

**2. 核心挑战：基准数据集的质量**

*   **问题**：许多广泛使用的基准数据集本身可能存在质量问题（如标注错误、问题模糊、难度不均），这会影响评估的准确性和可靠性。
*   **改进实践**：
    *   **修复与验证**：例如，对软件工程基准SWE-Bench进行修复和验证，推出了更高质量的**SWE-Bench Verified** 版本。
    *   **创建高质量版本**：提出为现有基准创建经过严格人工审核的“白金”版本，以确保评估所用数据的高质量和准确性。

**总结而言，这部分内容深刻地指出，一个看似优秀的评估分数可能因“数据污染”或“基准本身有缺陷”而失效。它呼吁业界关注评估的“有效性”本身，并通过技术手段和建立行业规范来共同维护评估结果的真实性和可信度。**

## 11. what_are_we_evaluating

### 内容概况

本部分探讨了机器学习领域评估范式的根本性转变。核心论点是：**评估的“游戏规则”已经发生了改变**，从过去严格评估**方法/算法**，转变为如今广泛评估**模型/系统**本身。内容通过对比“过去”与“现在”，并辅以两个现代例外（`nanogpt speedrun` 和 `DataComp-LM`）作为例证，最终总结了两种不同评估目标（鼓励创新 vs. 服务用户）的价值。

---

### 要点总结

1.  **核心议题：评估范式的转变**
    *   图片开篇即提出两个根本性问题：“我们到底在评估什么？”和“游戏的规则是什么？”，点明主题。

2.  **过去 vs. 现在**
    *   **过去（预基础模型时代）**：评估的核心对象是**方法** 或算法。规则是采用**标准化的训练-测试数据集划分**，旨在公平比较不同算法的创新性。
    *   **现在（基础模型时代）**：评估的核心对象是**模型/系统**。规则是“**一切皆可**”，即不限制训练数据、计算规模等因素，只看最终产出的模型或系统的性能。

3.  **现代的特例**
    *   图片指出当前也存在一些回归“方法评估”思想的特例，旨在鼓励低成本的算法创新：
        *   **nanogpt speedrun**：在固定数据集上，竞赛目标是以最短的计算时间达到特定的验证损失。
        *   **DataComp-LM**：给定一个原始数据集，参赛者使用标准的训练流程，目标是获得最佳准确率。

4.  **不同评估目标的价值**
    *   **评估方法**：有助于**鼓励研究人员的算法创新**。
    *   **评估模型/系统**：对**下游用户更具实用价值**，帮助他们选择最适合的模型。

5.  **最终结论**
    *   无论选择哪种评估范式，最关键的是必须**明确定义“游戏的规则”**，以确保评估的公平性和意义。

**总结**：该图片精辟地指出了机器学习领域评估文化的演变，从注重算法创新的“方法论”评估，转向了注重实用结果的“模型系统”评估，并强调了明确规则在任何评估中的基石作用。
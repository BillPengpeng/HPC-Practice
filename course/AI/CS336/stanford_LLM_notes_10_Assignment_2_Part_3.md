本文主要整理Assignment 2 (systems): Systems and Parallelism的主要内容。

## 1.3.2 FlashAttention-2 Forward Pass

### **内容概况**

该章节主要介绍了如何将标准的PyTorch注意力实现替换为基于**FlashAttention-2**的高效Triton实现。

文档首先指出FlashAttention-2通过特定的技巧实现了分块计算前向传播，从而优化了内存访问模式，并避免了在全局内存中显式构建完整的注意力矩阵。接着，文档强烈建议读者在深入本节之前，先阅读原始FlashAttention论文以理解其核心技术原理，并推荐相关技术资料来加深对GPU执行机制的理解。

---

### **要点总结**

1.  **核心目标**：
    *   将**PyTorch标准注意力实现**替换为基于**FlashAttention-2的Triton实现**。
    *   利用FlashAttention-2的先进特性显著提升注意力计算的**内存访问效率和计算性能**。

2.  **FlashAttention-2的技术核心**：
    *   **分块计算（Tiling）**：将前向传播过程分解成多个小块（tiles）进行计算。
    *   **高效内存访问**：通过分块技术优化内存访问模式，减少对全局内存的高延迟访问。
    *   **避免显式存储**：成功**避免在全局内存中构建完整的注意力矩阵**（大小为序列长度×序列长度），从而极大降低内存占用，尤其有利于处理长序列。

3.  **建议的先修知识**：
    *   **原始FlashAttention论文**：理解其实现高效注意力的核心技术——**在线softmax计算**。
    *   **在线softmax技术**：一种跨分块进行softmax计算的技巧，是FlashAttention系列算法的理论基础之一。
    *   **GPU执行模型**：了解GPU如何实际执行PyTorch代码，有助于理解性能优化的深层原因。

4.  **关键价值**：
    *   掌握FlashAttention-2的前向传播实现是为后续**理解并实现其更复杂的反向传播**打下坚实基础。
    *   该技术是现代大语言模型（LLMs）能够**高效处理长上下文**的关键所在。

**总结来说**，该部分内容旨在引导读者从理论过渡到实践，通过实现FlashAttention-2的前向传播，深入理解其如何利用分块计算和在线softmax等技术，革命性地解决标准注意力机制在内存和计算上的瓶颈。

## Understanding ineﬀiciencies in vanilla attention

### 内容概况

这张图片节选自一份技术文档，标题为 **“理解标准注意力机制的低效性”** 。它系统地分析了标准Transformer注意力机制在前向传播和反向传播过程中的计算步骤，并重点揭示了其核心性能瓶颈：**巨大的内存输入/输出（IO）开销**。文档指出，由于需要频繁在GPU的慢速显存（HBM）和高速缓存（SRAM）之间搬运尺寸与序列长度平方成正比的中间矩阵（特别是注意力分数矩阵 `P`），导致了显著的内存占用和速度下降。最后，它引出了**FlashAttention算法**的终极目标——通过**分块计算、重计算和算子融合**这三种技术来避免这一瓶颈。

---

### 要点总结

1.  **核心问题**：标准注意力机制在处理长序列时，其**内存IO成本是主要性能瓶颈**，而非计算速度本身。瓶颈在于对高带宽显存（HBM）的频繁访问。

2.  **问题根源**：注意力分数矩阵 `P` 的形状为 `[batch_size, n_heads, seq_len, seq_len]`。当 `seq_len` 很大时（如8192），`P` 的尺寸会变得极其巨大，消耗大量显存。

3.  **瓶颈具体体现**：
    *   在前向传播中，需要将巨大的 `P` 矩阵写回HBM。
    *   在反向传播中，需要多次从HBM中读取 `P` 矩阵（例如计算 `dV` 和 `dS` 时）。
    *   这种在HBM和SRAM之间的数据搬运速度远远慢于实际计算速度。

4.  **解决方案方向**：FlashAttention的核心思想是**避免将完整的注意力矩阵 `P` 写入HBM**。通过技巧使计算在高速SRAM内完成，从而极大降低IO开销和峰值内存占用。

---

### 公式详解

#### **前向传播**

1.  **公式 (4): `S = QKᵀ / √d`**
    *   **解释**：计算查询（Q）和键（K）的相似度得分矩阵 `S`。除以 `√d`（`d` 是特征维度）是为了防止点积结果过大，导致Softmax梯度消失。
    *   **内存问题**：`S` 已经是 `[seq_len, seq_len]` 的矩阵，但更大的问题在后面。

2.  **公式 (5): `P_ij = softmax_j(S)_ij`**
    *   **解释**：对 `S` 矩阵的每一行（对每个查询向量）应用Softmax函数，将其转化为表示权重的概率分布矩阵 `P`。`P_ij` 表示第 `i` 个查询与第 `j` 个键的关联程度。
    *   **内存问题的核心**：**`P` 就是这个需要被避免的巨型矩阵**。它是标准实现中必须产生并存储的中间变量。

3.  **公式 (6): `O = PV`**
    *   **解释**：使用注意力权重矩阵 `P` 对值向量（V）进行加权求和，得到最终的输出 `O`。

#### **反向传播**（基于链式法则计算梯度）

4.  **公式 (7): `dV = Pᵀ dO`** （被红框突出）
    *   **解释**：计算损失函数对值向量 `V` 的梯度 `dV`。它依赖于注意力矩阵 `P` 的转置和输出 `O` 的梯度 `dO`。
    *   **瓶颈**：这是**第一次需要从HBM中读取庞大的 `P` 矩阵**。

5.  **公式 (8): `dP = dO Vᵀ`**
    *   **解释**：计算损失函数对注意力权重 `P` 的梯度 `dP`。

6.  **公式 (9): `dS_i = (diag(P_i) - P_i P_iᵀ) dP_i`** （被红框突出）
    *   **解释**：这是Softmax函数的反向传播公式。它计算损失对得分矩阵 `S` 的梯度 `dS`。对于第 `i` 行，其梯度计算涉及一个雅可比矩阵 `(diag(P_i) - P_i P_iᵀ)` 与 `dP_i` 的乘积。
    *   **瓶颈**：这是**第二次需要从HBM中读取庞大的 `P` 矩阵**（或其行向量 `P_i`）。

7.  **公式 (10) 和 (11): `dQ = dS K / √d` 和 `dK = dSᵀ Q / √d`**
    *   **解释**：最终计算得到查询 `Q` 和键 `K` 的梯度。

### 总结

图片通过严密的数学公式推导证明，在标准注意力机制的反向传播中，**仅计算 `dV` 和 `dS` 这两个步骤，就至少需要两次从慢速HBM中读取完整的巨型矩阵 `P`**。FlashAttention的革新之处就在于，它通过精巧的算法设计，在不显式构造和存储 `P` 的情况下，直接计算出正确的结果和梯度，从而解决了这一根本性的效率瓶颈。

## Tiling & Recomputation & Operator fusion


### 内容概况

这两张图片系统地阐述了**FlashAttention算法**为优化标准注意力机制而采用的三种核心技术：**分块计算、重计算和算子融合**。图片从技术原理、实现方法和优化效果三个层面，详细说明了这些技术如何协同工作，通过减少对高带宽内存的频繁访问，从根本上解决标准注意力机制在处理长序列时的内存与计算瓶颈。

---

### 要点总结

#### **核心技术一：分块计算**

1.  **核心思想**：
    *   避免将完整的注意力矩阵（尺寸为序列长度的平方）在高速显存和慢速高带宽内存之间来回读写。
    *   将输入数据（Q, K, V）分割成多个小块。

2.  **实现方法**：
    *   重新组织注意力计算流程，使其能够**对数据块进行多次遍历**。
    *   **增量式地执行Softmax归约计算**，即使无法一次性获得全部输入，也能通过迭代得到正确结果。

3.  **关键价值**：
    *   这是实现高效内存访问的**基础策略**，使计算过程能在有限的GPU高速缓存（SRAM）中完成。

#### **核心技术二：重计算**

1.  **核心思想**：
    *   不在HBM中存储巨大的中间注意力矩阵（形状为 `[batch_size, n_heads, seq_len, seq_len]`），以**极大降低峰值内存占用**。

2.  **实现方法**：
    *   在HBM中只保存少量的**激活检查点**。
    *   在反向传播过程中，**动态地重新计算**前向传播的部分步骤，以获取计算梯度所需的其他中间激活值。

3.  **关键优化（LogSumExp）**：
    *   FlashAttention-2会存储一个名为 **`L`** 的中间值，其计算公式为：
        $$ L_{i}=\log\left(\sum_{j}\exp\left(\mathbf{S}_{ij}\right)\right) $$
    *   **`L` 的作用**：在线计算并存储每个查询向量对应的Softmax分母的对数，用于在反向传播中**极大地简化梯度计算**。

4.  **联合效果**：
    *   分块与重计算技术相结合，使得算法的**内存IO和峰值使用量不再与序列长度的平方** 成正比，从而能够支持处理更长的序列。

#### **核心技术三：算子融合**

1.  **核心思想**：
    *   通过**单一内核执行多个连续操作**，避免因启动多个独立内核而产生的重复内存IO开销。

2.  **实现方法**：
    *   为前向传递编写一个**单一的Triton内核**，该内核在SRAM内完成注意力机制涉及的所有操作（如矩阵乘、Softmax、再次矩阵乘），并严格控制与HBM的数据交换。

3.  **协同效应**：
    *   算子融合的实现**部分依赖于重计算技术**，因为正是由于无需将每个中间激活值都存回HBM，才使得将所有操作合并到一个内核中成为可能。

### 总结

这三项技术共同构成了FlashAttention高效性的基石：**分块计算** 是基础，使计算能在SRAM中进行；**重计算** 是策略，用计算时间换取内存空间；**算子融合** 是实现手段，将整个流程“打包”以最小化IO开销。最终，它们合力将注意力机制的内存复杂度从 O(seq_len²) 降低到了 O(seq_len)，这是其能高效处理长序列的根本原因。

## Backward pass with recomputation

### 内容概况

这张图片深入阐述了 **FlashAttention 算法的反向传播过程**，其核心创新在于**通过重计算技术，在不存储巨大中间矩阵 `P`（注意力分数矩阵）的情况下，高效且正确地计算出所有梯度**。图片首先介绍了为实现此目标所需预计算的辅助向量 `D`，并验证了其等价性。随后，通过一系列公式（13）到（19）清晰地展示了完整的反向传播计算步骤。最终点明，由于关键的中间变量 `P` 可以在需要时通过保存的 `Q`, `K`, `L` 实时重新计算，因此成功避免了在高速显存中存储这个尺寸与序列长度平方成正比的大矩阵，从而实现了巨大的内存节省。

---

### 要点总结

1.  **核心目标**：实现**无需存储完整注意力矩阵 `P` 的高效反向传播**。`P` 的尺寸是 `[seq_len, seq_len]`，是标准注意力机制的主要内存瓶颈。

2.  **关键技术**：**重计算**。在前向传播中只保存必要的少量信息（`Q`, `K`, `L`），在反向传播需要时动态重新计算中间结果（`P`）。

3.  **关键预计算值 `D`**：
    *   **定义**：`D = rowsum(O ⊙ dO)`，即输出 `O` 和其梯度 `dO` 的逐元素乘积按行求和得到的向量。
    *   **物理意义**：它编码了最终输出对每个查询位置的总体敏感度。
    *   **关键等价关系**：文中通过数学推导证明了 `rowsum(O ⊙ dO) = rowsum(P ⊙ dP)`。这意味着我们可以绕过 `P` 来计算 `D`，而 `D` 将在后续梯度计算中起到关键作用。

4.  **计算流程**：反向传播被分解为一个清晰的、可逐步执行的序列。公式（13）和（14）执行重计算，公式（15）至（19）利用重算出的 `P` 和预计算的 `D` 来高效计算所有参数的梯度（`dV`, `dQ`, `dK`）。

5.  **最大优势**：将注意力机制的内存复杂度从 **O(seq_len²)** 降低到了 **O(seq_len)**，这是FlashAttention能处理极长序列的根本原因。

---

### 公式详解

1.  **公式 (13): `S = QKᵀ / √d`**
    *   **解释**：这是**重计算的第一步**。利用在前向传播中已保存的 `Q` 和 `K`，重新计算原始的注意力分数矩阵 `S`。这一步是必要的，因为标准的 `P` 没有被保存。

2.  **公式 (14): `P_ij = exp(S_ij - L_i)`
    *   **解释**：这是**重计算的第二步**，也是核心。利用刚算出的 `S` 和在前向传播中保存的 **`L` 向量**（`L_i = LogSumExp(S_i)`），重新计算出标准的注意力权重矩阵 `P`。注意，这里的Softmax计算被分解为基于 `L_i` 的归一化操作。**正是这两步重计算，避免了对庞大矩阵 `P` 的存储。**

3.  **公式 (15): `dV = Pᵀ dO`**
    *   **解释**：计算损失函数对值向量 `V` 的梯度。此公式与标准注意力反向传播公式(7)完全相同。区别在于，这里的 `P` 是刚刚实时重算出来的，而非从HBM中读取。

4.  **公式 (16): `dP = dO Vᵀ`**
    *   **解释**：计算损失对注意力权重 `P` 的“中间”梯度。这个 `dP` 是一个中间变量，用于后续计算。

5.  **公式 (17): `dS_ij = P_ij ⊙ (dP_ij - D_i)`** - **（最关键的优化公式）**
    *   **解释**：这是FlashAttention反向传播的**精髓所在**。它计算损失对原始分数 `S` 的梯度。
        *   **标准方法**（公式9）：需要计算 `dS_i = (diag(P_i) - P_i P_iᵀ) dP_i`，这涉及整个向量 `P_i` 的外积，计算复杂且需要完整的 `P_i`。
        *   **FlashAttention方法**：通过引入预计算的标量 `D_i`，将计算简化为一个**逐元素操作**。`(dP_ij - D_i)` 可以看作是“中心化”的梯度，再与 `P_ij` 逐元素相乘即可。这种写法非常适合在分块计算中高效执行，避免了复杂的矩阵运算。
    *   **推导依据**：此公式是标准Softmax梯度公式经过数学变换后的等价形式，它利用了前面证明的 `D_i = rowsum(P_i ⊙ dP_i)` 这一性质。

6.  **公式 (18) 和 (19): `dQ = dS K / √d` 和 `dK = dSᵀ Q / √d`**
    *   **解释**：在得到 `dS` 后，计算查询 `Q` 和键 `K` 的梯度。这两个步骤与标准反向传播基本一致。

### 总结

这张图片揭示了FlashAttention算法的巧妙之处：它通过**重计算（公式13、14）** 和**引入预计算向量 `D` 来优化核心梯度计算（公式17）**，将反向传播过程中最耗费内存的步骤（存储和读取 `P`）彻底消除。这一系列操作在数学上是等价的，但在实现上极大地降低了对显存的需求，是注意力机制优化的一项里程碑式的技术。

## Details of the flash attention forward pass

### 一、内容概况

这两张图片构成了 **FlashAttention-2 前向传播算法的完整技术说明**。第一张图片从**概念层面**解释了为何需要以及如何通过**在线Softmax** 技术来实现分块计算，从而避免在高速显存中读写巨大的注意力矩阵。第二张图片则以**伪代码算法**的形式，精确地给出了具体的实现步骤。两者结合，清晰地展示了FlashAttention-2如何通过巧妙的迭代计算来分解标准的注意力计算过程。

---

### 二、要点总结

1.  **核心目标**：通过**分块计算**，避免将尺寸为 `[序列长度, 序列长度]` 的注意力矩阵 `P` 写入高带宽内存，从而极大降低内存占用和IO开销。

2.  **核心挑战**：标准的Softmax计算需要看到一整行的数据（即一个查询与所有键的分数）才能得到归一化分母，这与“分块计算”的理念相悖。

3.  **解决方案：在线Softmax**：采用迭代的方式，逐步处理键值块。算法维护两个关键的**行方向运行变量**：
    *   **运行最大值**：追踪当前已处理数据块中的最大值，用于数值稳定计算。
    *   **运行分母**：逐步累加未归一化注意力权重的和，用于最终的正确归一化。

4.  **算法流程**：采用**双重循环**结构。外层循环遍历**查询块**，内层循环遍历**键/值块**。对于每个查询块和键值块的组合，计算一个小的注意力分数块，并即时更新运行变量和输出块的中间结果。

5.  **输出结果**：算法返回两个结果：
    *   **`O`**：标准的注意力输出。
    *   **`L`**：Log-Sum-Exp值。这是为**反向传播**准备的关键中间结果，使得在反向传播时无需存储庞大的中间矩阵 `P`，只需保存 `L` 即可通过重计算得到 `P`。

---

### 三、公式与算法步骤详解

```mermaid
flowchart TD
A[外层循环<br>遍历查询块 Q_i] --> B[初始化运行变量<br>m_old, l_old, O_old]
B --> C[内层循环<br>遍历键/值块 K_j, V_j]
C --> D[计算分数块 S_ij = Q_i @ K_j^T / sqrt(d)]
D --> E[更新运行最大值<br>m_new = max(m_old, rowmax(S_ij))]
E --> F[计算未归一化权重<br>P_tilde_ij = exp(S_ij - m_new)]
F --> G[更新运行分母<br>l_new = exp(m_old - m_new)*l_old<br>+ rowsum(P_tilde_ij)]
G --> H[更新输出块<br>O_new = exp(m_old - m_new)*O_old<br>+ P_tilde_ij @ V_j]
H --> I{是否处理完<br>所有键/值块?}
I --否--> C
I --是--> J[最终归一化<br>O_i = O_new / l_new<br>L_i = m_new + log(l_new)]
J --> K[写入最终结果]
K --> L{是否处理完<br>所有查询块?}
L --否--> A
L --是--> M[算法结束]
```

**初始化（内层循环开始前）**
*   `O_i^{(0)} = 0`, `l_i^{(0)} = 0`, `m_i^{(0)} = -∞`
    *   **解释**：对于每个查询块 `Q_i`，在开始处理键块之前，初始化其对应的输出块 `O`、运行分母 `l` 和运行最大值 `m`。

**步骤1：计算注意力分数块**
*   `S_i^{(j)} = Q_i (K^{(j)})^⊤ / √d`
    *   **解释**：计算当前查询块 `Q_i` 和当前键块 `K^{(j)}` 之间的点积分数块，并除以缩放因子 `√d`。结果 `S_i^{(j)}` 是一个大小为 `[B_q, B_k]` 的小矩阵。

**步骤2：更新运行最大值**
*   `m_i^{(j)} = max( m_i^{(j-1)}, rowmax(S_i^{(j)}) )`
    *   **解释**：计算当前分数块 `S_i^{(j)}` 中每行的最大值，然后与上一轮迭代的运行最大值 `m_i^{(j-1)}` 进行比较，取两者中更大的值作为新的运行最大值。`rowmax` 是沿键块维度进行的操作，结果是一个长度为 `B_q` 的向量。

**步骤3：计算未归一化的注意力权重**
*   `P̃_i^{(j)} = exp(S_i^{(j)} - m_i^{(j)})`
    *   **解释**：根据最新的运行最大值 `m_i^{(j)}`，计算当前块的、经过数值稳定处理的、但还未进行全局归一化的注意力权重。减法的目的是防止指数计算时溢出。

**步骤4：更新运行分母（核心步骤）**
*   `l_i^{(j)} = exp(m_i^{(j-1)} - m_i^{(j)}) * l_i^{(j-1)} + rowsum(P̃_i^{(j)})`
    *   **解释**：这是**在线Softmax的精髓**。它分两步更新代表Softmax分母的运行值：
        1.  **校正旧值**：由于最大值更新了（从 `m_old` 变为 `m_new`），之前累加的分母 `l_old` 需要用因子 `exp(m_old - m_new)` 进行缩放，以与新最大值下的数值尺度保持一致。
        2.  **累加新值**：将当前块的未归一化权重之和 `rowsum(P̃_i^{(j)})` 加到缩放后的旧分母上。

**步骤5：更新输出块**
*   `O_i^{(j)} = diag( exp(m_i^{(j-1)} - m_i^{(j)}) ) * O_i^{(j-1)} + P̃_i^{(j)} V^{(j)}`
    *   **解释**：同样分两步更新输出块：
        1.  **校正旧输出**：用与分母相同的缩放因子 `exp(m_old - m_new)` 对旧的输出块 `O_old` 进行校正。
        2.  **累加新贡献**：将当前块的注意力权重 `P̃_i^{(j)}` 与值块 `V^{(j)}` 相乘的结果累加进去。

**最终归一化（内层循环结束后）**
*   `O_i = diag( l_i^{(T_k)} )^{-1} O_i^{(T_k)}`
    *   **解释**：在处理完所有键块后，运行分母 `l_i` 已经包含了完整的归一化系数。此时，用其对最终累加的输出块 `O_i` 进行真正的归一化，得到正确的注意力输出。
*   `L_i = m_i^{(T_k)} + log( l_i^{(T_k)} )`
    *   **解释**：计算并保存 **Log-Sum-Exp** 值 `L_i`。`m` 是最大值，`log(l)` 是缩放后分母的对数，两者相加即为标准的Log-Sum-Exp。这个值 `L` 将在反向传播中起到至关重要的作用，用于在不保存矩阵 `P` 的情况下正确计算梯度。

FlashAttention-2的前向传播算法通过一套精巧的迭代更新机制，成功地将原本需要全局信息的Softmax计算分解为可逐步进行的局部计算。这套机制的核心在于**运行最大值**和**运行分母**的维护，它们确保了在分块处理过程中，数值计算的稳定性和最终结果的数学等价性。这正是其能够实现极致内存效率的算法基础。

### Problem (flash_forward): 15 points

- (a) Write a pure PyTorch (no Triton) autograd.Function that implements the FlashAttention-2
forward pass. This will be a lot slower than the regular PyTorch implementation, but will help
you debug your Triton kernel.
=> 完成

- (b) Write a Triton kernel for the forward pass of FlashAttention-2 following Algorithm 1. Then,
write another subclass of torch.autograd.Function that calls this (fused) kernel in the
forward pass, instead of computing the result in PyTorch.
=> 完成

- (c) Add a flag as the last argument to your autograd.Function implementation for causal
masking. This should be a boolean flag that when set to True enables an index comparison for
causal masking. Your Triton kernel should have a corresponding additional parameter
is_causal: tl.constexpr (this is a required type annotation). In Triton, construct
appropriate index vectors for queries and keys, and compare them to form a square mask of size
Bq × Bk . For elements that are masked out, add the constant value of -1e6 to the corresponding 
elements of the attention score matrix Si. Make sure to save the mask flag for
backward using ctx.is_causal = is_causal.
=> 完成

## Implementing the backward pass with recomputation

### 内容概况

这张图片内容聚焦于**实现反向传播的一种高效方法：基于重计算的反向传播**。它指出，与标准的反向传播流程（公式7-11）不同，通过利用**重计算技术**，可以避免在反向传播过程中执行复杂的Softmax操作（公式13-19所描述的过程）。这种方法的巨大优势在于，它使得反向传播可以通过一个“普通”的内核来计算，无需使用复杂的在线计算技巧。正因如此，开发者可以直接使用 **`torch.compile` 来编译一个常规的 PyTorch 函数**（而无需依赖 Triton 这样的底层语言）即可实现反向传播，极大地简化了实现难度。

### 要点总结

1.  **核心方法**：**使用重计算技术实现反向传播**。这是在内存和计算之间的一种权衡，用重新计算前向传播的中间结果来换取显存占用的大幅降低。

2.  **关键优势**：**避免在反向传播中进行复杂的Softmax梯度计算**。标准方法（公式7-11）需要存储并回传整个注意力矩阵，而重计算方法（公式13-19）在反向过程中仅需存储少量信息（如LogSumExp值`L`），需要时再动态重算Softmax的中间结果。

3.  **技术实现**：该方法的核心是，前向传播已经通过重计算技术将内存占用优化到了极致（不存放大矩阵`P`）。因此，反向传播可以基于这些已保存的少量信息（如`Q`, `K`, `L`）来重新构建计算图，而无需特殊优化。

4.  **对开发者的直接影响**：
    *   **简化开发**：无需使用Triton编写复杂的反向传播内核。
    *   **利用现有工具**：可以直接编写标准的PyTorch函数来定义反向传播逻辑，然后使用 **`torch.compile`** 对其进行编译优化，就能获得高性能的、可融合算子与内存访问的代码。

5.  **意义**：这体现了PyTorch 2.0及以后版本的一个强大设计思想：**开发者只需关注实现正确的计算逻辑，而将计算性能的优化很大程度上交给`torch.compile`和PyTorch的编译器后端**。这降低了高性能模型实现的入门门槛。

### 核心逻辑解读

图片中的文字揭示了一个非常重要的技术路径：

**标准反向传播 (复杂)**
`存储大矩阵P → 在反向传播中使用P计算梯度 → 高内存占用`

**基于重计算的反向传播 (简化)**
`前向传播不存P，只存小量元数据（如L）→ 反向传播时利用元数据重算P → 低内存占用 → 反向传播逻辑变得“普通”`

正是因为反向传播的逻辑被简化为了“常规”的PyTorch张量操作，所以才能用`torch.compile`这种通用优化工具来轻松获得高性能，而不必求助于手写Triton内核这种更底层、更复杂的技术。

### Problem (flash_backward): 5 points

- Implement the backward pass for your FlashAttention-2 autograd.Function using PyTorch (not
Triton) and torch.compile. Your implementation should take the Q, K, V, O, dO, and L tensors as
output, and return dQ, dK and dV. Remember to compute and use the D vector. You may follow
along the computations of Equations 13 to 19.
=> 完成

### Problem (flash_benchmarking): 5 points

- (a) Write a benchmarking script using triton.testing.do_bench that compares the performance
of your (partially) Triton implementation of FlashAttention-2 forward and backward passes with
a regular PyTorch implementation (i.e., not using FlashAttention).

Specifically, you will report a table that includes latencies for forward, backward, and the end-
to-end forward-backward pass, for both your Triton and PyTorch implementations. Randomly
generate any necessary inputs before you start benchmarking, and run the benchmark on a single
H100. Always use batch size 1 and causal masking. Sweep over the cartesian product of sequence
lengths of various powers of 2 from 128 up to 65536, embedding dimension sizes of various powers
of 2 from 16 up to size 128, and precisions of torch.bfloat16 and torch.float32. You will
likely need to adjust tile sizes depending on the input sizes.

**24GB RTX 4090 scaled_dot_product_attention_func*

**F32**

|   seq_len |   d_model |   forward_time |   forward_backward_time |
|----------:|----------:|---------------:|------------------------:|
|       128 |        16 |       0.156833 |                0.982586 |
|       256 |        16 |       0.190449 |                1.71733  |
|       512 |        16 |       0.18295  |                1.51932  |
|      1024 |        16 |       0.234628 |                1.4311   |
|      2048 |        16 |       0.207333 |                1.62459  |
|      4096 |        16 |       1.11262  |                3.42685  |
|      8192 |        16 |       5.16978  |               15.1548   |
|     16384 |        16 |      20.5922   |               60.4928   |
|     32768 |        16 |      81.9344   |              nan        |
|     65536 |        16 |     nan        |              nan        |

**F32 + torch.compile**

|   seq_len |   d_model |   forward_time |   forward_backward_time |
|----------:|----------:|---------------:|------------------------:|
|       128 |        16 |       0.35568  |                 1.86766 |
|       256 |        16 |       0.529107 |                 2.5616  |
|       512 |        16 |       0.638519 |                 1.71134 |
|      1024 |        16 |       0.543171 |                 1.98594 |
|      2048 |        16 |       0.531558 |                 1.70666 |
|      4096 |        16 |       0.778186 |                 2.21891 |
|      8192 |        16 |       3.92594  |                 9.54409 |
|     16384 |        16 |      16.05     |                38.5516  |
|     32768 |        16 |      63.9304   |               nan       |
|     65536 |        16 |     nan        |               nan       |

**F16**

|   seq_len |   d_model |   forward_time |   forward_backward_time |
|----------:|----------:|---------------:|------------------------:|
|       128 |        16 |      0.0357603 |                 1.20877 |
|       256 |        16 |      0.188798  |                 1.85101 |
|       512 |        16 |      0.215201  |                 1.93274 |
|      1024 |        16 |      0.25797   |                 1.86934 |
|      2048 |        16 |      0.234481  |                 1.84913 |
|      4096 |        16 |      0.83045   |                 2.54912 |
|      8192 |        16 |      4.39276   |                12.7245  |
|     16384 |        16 |     17.496     |                50.9153  |
|     32768 |        16 |     69.5951    |               nan       |
|     65536 |        16 |    nan         |               nan       |

**24GB RTX 4090 triton_flashattention**

**TILE=16 + F32**

|   seq_len |   d_model |   forward_time |   forward_backward_time |
|----------:|----------:|---------------:|------------------------:|
|       128 |        16 |       0.127657 |                1.30448  |
|       256 |        16 |       0.170987 |                1.18802  |
|       512 |        16 |       0.126645 |                0.936629 |
|      1024 |        16 |       0.127323 |                0.888313 |
|      2048 |        16 |       0.155205 |                0.953416 |
|      4096 |        16 |       0.353304 |                1.49713  |
|      8192 |        16 |       1.1241   |                5.90521  |
|     16384 |        16 |       4.14683  |               23.0927   |
|     32768 |        16 |      16.1772   |              nan        |
|     65536 |        16 |     nan        |              nan        |

**TILE=16 + torch.compile**

|   seq_len |   d_model |   forward_time |   forward_backward_time |
|----------:|----------:|---------------:|------------------------:|
|       128 |        16 |      0.0149067 |                 1.09646 |
|       256 |        16 |      0.0203794 |                 1.86122 |
|       512 |        16 |      0.0905886 |                 1.98751 |
|      1024 |        16 |      0.102203  |                 1.99966 |
|      2048 |        16 |      0.136533  |                 1.99043 |
|      4096 |        16 |      0.262158  |                 1.99673 |
|      8192 |        16 |      0.656166  |                 5.43923 |
|     16384 |        16 |      2.35436   |                21.2956  |
|     32768 |        16 |      9.23363   |               nan       |
|     65536 |        16 |    nan         |               nan       |

**TILE=16 + F16**

|   seq_len |   d_model |   forward_time |   forward_backward_time |
|----------:|----------:|---------------:|------------------------:|
|       128 |        16 |       0.121896 |                0.854575 |
|       256 |        16 |       0.12803  |                1.15421  |
|       512 |        16 |       0.121085 |                1.58625  |
|      1024 |        16 |       0.172963 |                1.58348  |
|      2048 |        16 |       0.20387  |                1.99387  |
|      4096 |        16 |       0.391027 |                1.78897  |
|      8192 |        16 |       1.15391  |                5.89331  |
|     16384 |        16 |       4.14878  |               22.9578   |
|     32768 |        16 |      16.1937   |              nan        |
|     65536 |        16 |     nan        |              nan        |

## 1.3.3 FlashAttention-2 Leaderboard

```python
# 24GB RTX 4090 最大sequence_length=8192
test_timing_flash_forward_backward: 70.00166320800781
```
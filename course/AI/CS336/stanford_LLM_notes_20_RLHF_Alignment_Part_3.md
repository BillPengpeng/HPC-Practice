本文主要整理CS336 Lecture 16 RL章节的主要内容。

## 3.0 Case studies: Deepseek R1

### 1. 一个标志性的事件里程碑

DeepSeek-R1的发布不仅仅是一项技术更新，而是一个**文化事件**。它成功地吸引了学术界和业界之外的广泛公众注意力，成为了一个“现象级”的话题。

### 2. 核心亮点：为什么它如此引人注目？

-   **性能超越OpenAI O1**：这是最直接的爆点。OpenAI的o1模型曾被认为是闭源、领先的推理模型标杆。DeepSeek-R1作为开源模型，在性能上实现了**超越**，打破了原有格局，证明了开源社区同样能产出顶尖水平的AI模型。
-   **公开且简单的强化学习配方**：这是最具革命性的一点。以往，顶尖模型的训练细节（尤其是RL）往往是高度保密的“黑魔法”。R1不仅公开了配方，而且强调其**“相当简单”**。这极大地降低了顶级技术复现和应用的门槛，赋能了整个社区。
-   **终结了关于MCTS/PRM必要性的猜测**：在R1之前，一种主流观点认为，实现强大的推理能力必须依赖类似AlphaGo的“蒙特卡洛树搜索”（MCTS）或“过程奖励模型”（PRM）等复杂技术。R1的成功证明了，**无需这些复杂组件**，用更简洁的方法也能达到顶尖水平，这纠正了社区的技术路线猜想。
-   **SFT的见解**：它分享了在**监督微调** 阶段的关键洞察（包括R1-zero和distil-r1），这对于如何高效地准备数据和训练基础模型具有重要价值。

### 3. 时间线与影响力爆发

图中的折线图生动地展示了这一“社会现象”的发酵过程：
-   **横轴时间**：从2024年5月到2025年4月。
-   **关键节点**：论文在**2025年1月22日**正式提交，但图表显示，在**2024年12月29日**（很可能是在预印本发布或消息泄露后），关注度就开始**急剧飙升**。
-   **现象解读**：这说明论文尚未正式发布，其核心思想和初步结果就已经在社区内引发了爆炸性的讨论和传播，印证了其“现象级”的影响力。

### 总结：如何整体理解？

**DeepSeek-R1是一篇里程碑式的论文，它通过公开一种相对简单且开源的强化学习方法，训练出了性能超越OpenAI o1的模型。它的成功不仅证明了无需复杂技术（如MCTS）也能实现顶尖推理能力，更重要的是，它通过“开源”和“简单化”打破了技术垄断，从而在2024年底至2025年初引发了全球AI社区的广泛关注和热议，成为一个标志性的社会现象。**

## 3.1 Rewards

### 1. 准确性奖励 - 确保“结果正确”

-   **是什么**：`Accuracy rewards (is it correct?)`
-   **怎么理解**：这是最根本、最直接的奖励。它只回答一个问题：**模型的最终答案是否正确？**
-   **作用**：
    -   **提供终极目标**：这是模型需要优化的核心指标。它明确告诉模型，一切努力的最终目的是得出正确答案。
    -   **简单、客观**：判断对错通常是一个明确的二进制信号（0或1，正确或错误），避免了复杂、有噪声的评分，使学习信号更清晰。

**可以类比为**：在学校考试中，你最终写在答题卡上的答案是否和标准答案一致。答对了就能得到这道题的全部分数（或基础分）。

### 2. 格式奖励 - 引导“过程规范”

-   **是什么**：`Format rewards (use thinking tags)`
-   **怎么理解**：这要求模型在生成最终答案前，必须使用特定的标签（如 `<thinking>` 和 `</thinking>`）来包裹其推理步骤。
-   **作用**：
    -   **强制展示链式思考**：通过奖励格式，模型被强制要求**显式地**展示其内部推理过程，而不是直接“跳”到答案。
    -   **提升可解释性与可靠性**：这让研究人员和用户能够“看到”模型的思考路径，便于调试、验证其逻辑是否合理。一个能写出合理推理步骤的模型，其答案通常也更可靠。
    -   **可能间接提升性能**：有研究表明，强迫模型进行一步步的推理（思维链），本身就能激发其更好的推理能力，从而间接提高准确性。

**可以类比为**：考试中要求“写出计算过程”。即使你最终答案错了，但如果过程有合理的部分，可能还能得到步骤分。而这里的设计是，**只有写出了符合格式要求的思考过程，你才有资格去争取答案的正确性奖励**。它确保了模型不是靠“蒙”来答题。

## 3.2 Pushing performance further – R1

![Pushing performance further – R1](https://pic1.zhimg.com/v2-f83f9df06fddfbe6c8518380479cd3b0_1440w.jpg)

## 3.3 SFT initialization..

### 内容概况

这张幻灯片阐述了**DeepSeek-R1**为避免强化学习训练初期的不稳定性，所采用的一项关键预处理策略：**利用高质量的“长思维链”数据对基础模型进行监督微调，以此为后续的RL训练提供一个高质量且稳定的初始策略。**

### 要点总结

1.  **核心目标：解决RL训练的“冷启动”问题**
    *   **问题**：直接从基础模型开始进行强化学习训练，初期阶段非常不稳定，模型行为难以预测。
    *   **解决方案**：不直接从基础模型开始RL训练，而是先进行一轮有针对性的监督微调，为RL训练提供一个“热启动”的起点。

2.  **SFT数据的关键特征：长思维链**
    *   与常规SFT不同，此处特意收集和构建的是**长思维链**数据。目的是让模型在RL训练开始前，就先具备进行**深入、多步推理**的能力和习惯。

3.  **数据收集的多种途径（技术要点）**
    *   **提示工程**：通过Few-shot提示或直接指令，要求强大的模型（如R1-Zero自身）生成包含反思和验证的详细答案。
    *   **利用自身输出**：收集DeepSeek-R1-Zero模型生成的、格式可读的输出。
    *   **人工精炼**：最后通过人工标注员对上述结果进行后处理和精炼，确保数据质量。

4.  **声称的主要收益：可解释性**
    *   通过这种方式初始化的模型，倾向于生成更详细、更结构化的推理过程，这大大增强了模型行为的**可解释性**，便于人类理解和调试。

5.  **指出的潜在问题：数据来源模糊**
    *   幻灯片作者特意添加了一条注释，指出一个关键细节尚不清晰：这些用于关键初始化的长思维链数据的**最终具体来源**。这可能是评估该方法可复现性的一个重要考量。

### 核心结论

这项SFT初始化策略是DeepSeek-R1相比R1-Zero的一项关键改进。它通过“授之以渔”的方式，让模型在进入复杂的RL训练前，先学会“如何思考”，从而显著提升了训练过程的稳定性和最终模型输出结果的质量与可解释性。

## 3.4 RL step

### 内容概况

该片段主要说明，在标准的强化学习训练基础上，团队为解决一个特定问题——**思维链中的语言混合现象**，引入了一项名为**“语言一致性奖励”** 的额外优化目标。这一改进旨在提升模型输出对人类而言的易读性，尽管这需要以微小的性能下降为代价。

---

### 要点总结

1.  **发现问题：RL自然导致语言混合**
    -   **核心观察**：在训练过程中，当提示涉及多种语言时，模型的思维链经常出现**语言混合**的问题。例如，在回答中文问题时，思维链中可能夹杂英文单词或句式。
    -   **原因推断**：RL优化过程会自然利用任何可能提升奖励的信号，这可能导致了在思维链中“借用”其他语言的表达。

2.  **解决方案：引入语言一致性奖励**
    -   **方法**：在RL训练阶段增加一个**语言一致性奖励**。
    -   **计算方式**：该奖励的数值非常简单，直接计算为**思维链中目标语言单词所占的比例**。例如，对于一个中文问题，思维链中中文字符的比例越高，此项奖励就越高。

3.  **效果与权衡：对齐人类偏好**
    -   **性能影响**：消融实验表明，强制语言一致性会导致模型在标准基准测试上的性能有**轻微下降**。
    -   **核心价值**：尽管有性能损失，但这项改进**符合人类偏好**。纯净、一致的语言输出的思维链对人类用户来说**更易读、更友好**，这被团队认为是更重要的价值。

4.  **最终奖励设计：简单加权求和**
    -   **整合方式**：最终的奖励函数是推理任务的准确性与语言一致性奖励的**直接相加**。
    -   **训练目标**：使用这个综合奖励函数对模型进行训练，直至其在推理任务上收敛。

### 核心结论

这一改进体现了DeepSeek-R1训练中一个重要的设计理念：**在追求绝对性能指标的同时，高度重视模型输出的实用性和用户体验。** 团队有意识地选择牺牲一小部分性能，以换取更符合人类习惯、更易于理解的推理过程，这反映了其工程实践中的务实与以用户为中心的考量。

## 3.5 SFT/RLHF

### 内容概况
这张图详细说明了DeepSeek-R1模型在完成核心的推理强化学习（RLVR/Reasoning RL）训练后，所采用的**后训练优化流程（SFT/RLHF）**。该流程分为SFT（监督微调）和RLHF（人类反馈强化学习）两个关键阶段，旨在进一步提升模型在各类任务上的综合能力。

### 要点总结

1.  **流程定位**
    *   此SFT/RLHF流程是模型训练流水线中的**后训练阶段**，在核心的推理RL训练完成之后进行，属于最终的“精加工”步骤。

2.  **SFT阶段（监督微调）**
    *   **训练轮数**：共进行 **2个周期** 的训练。
    *   **数据构成**：包含两部分数据：
        *   **推理数据**：针对 **“不可验证任务”**，例如“证明X命题”。这些数据（共60万条）由更强的 **V3模型作为评判者** 来生成或评估。
        *   **非推理数据**：直接取自 **V3模型的SFT数据集**（共20万条），用于保持模型的通用对话能力。

3.  **RLHF阶段（人类反馈强化学习）**
    *   **推理任务**：**复用** 为R1-zero模型开发的**推理RLHF流程**。
    *   **不可验证任务**：采用 **V3模型的RLHF流程**。
    *   **核心算法**：值得注意的是，即使在RLHF阶段，其核心优化算法**仍然使用GRPO**。这表明GRPO被证明是一个足够强大且通用的算法，可同时用于核心推理训练和对齐微调。

### 核心结论
该流程图揭示了DeepSeek-R1训练策略的两个关键点：**一、采用分阶段、分任务类型的精细化后训练流程**；**二、始终围绕其核心算法GRPO构建统一的训练框架**，确保了方法的一致性和有效性。

## 3.6 Other, relevant observations

### 内容概况
这张幻灯片标题为“其他相关观察”，分享了DeepSeek-R1研发过程中对两项重要技术（PRMs和MCTS）的实践评估。这些内容源自论文中“不成功的尝试”章节，体现了团队严谨的研究态度——不仅汇报成功经验，也坦诚分享在实践中遇到挑战的技术路径。

### 要点总结

#### 1. 过程奖励模型（PRMs）的评估
**核心结论**：虽有一定潜力，但局限性明显，最终未大规模采用

**主要局限性**：
- **定义困难**：在通用推理任务中难以明确定义细粒度步骤
- **评估挑战**：自动化评估效果不佳，人工标注难以扩展
- **系统复杂**：基于模型的PRM易导致奖励破解，重新训练奖励模型需要大量额外资源，使整个训练流程复杂化

**有限价值**：PRM在重排序模型生成的Top-N响应或辅助引导搜索方面表现尚可，但其优势被大规模强化学习过程中引入的额外计算开销所抵消

#### 2. 蒙特卡洛树搜索（MCTS）的探索
**核心结论**：在文本生成领域面临根本性挑战，难以有效扩展

**方法理念**：受AlphaGo/AlphaZero启发，通过将答案分解为小部分来系统探索解空间，增强测试时计算可扩展性

**遇到的根本性挑战**：
- **搜索空间问题**：与象棋等游戏不同，token生成的搜索空间定义不明确
- **扩展困难**：在扩大训练规模时遇到严重挑战

### 总体启示
幻灯片揭示了AI研究中的重要现实：某些在理论上很有前景的技术（如PRMs、MCTS），在实际应用中可能因可扩展性、复杂性或领域适应性等问题而受限。这种对“不成功尝试”的坦诚分享，为后续研究提供了宝贵的经验教训。

## 4.0 Case studies: Kimi K1.5

### 核心信息：一个“英雄所见略同”的验证

这张封面图最核心的信息是底部列出的两个研究原因：
1.  **与R1同时发布**
2.  **同样使用RL击败了o1**

这听起来像是一个技术趣闻，但实际上，这在AI研究领域具有**极其重要的意义**。它代表了一次**不约而同的验证**。

-   **R1的发布**：DeepSeek团队证明了，通过其开源的、相对简单的GRPO方法，可以训练出超越OpenAI o1的模型。
-   **K1.5的发布**：月之暗面（Kimi）团队**几乎在同一时间，独立地** 得出了相同的结论——使用强化学习可以训练出顶尖的推理模型，并且性能超越了o1。

### 如何理解这种“巧合”的巨大价值？

这远非巧合，而是一个强有力的**科学信号**。可以类比为：

> 在世界上两个互不联系的实验室，各自独立地发明出了效果相似的疫苗。这极大地增强了我们对疫苗有效性的信心。

具体来说，K1.5与R1的同时出现，具有以下三重价值：

1.  **验证了技术路线的正确性**：
    -   它强有力地证明，**基于强化学习来 Scaling LLMs 的推理能力是一条可行、可靠且强大的技术路径**。
    -   这意味着，超越闭源顶尖模型（如o1）并非某个团队的“独门绝技”或偶然结果，而是一个可复现、可达到的里程碑。

2.  **打破了技术垄断的叙事**：
    -   在OpenAI的o1模型展示出强大能力后，一种观点认为其依赖极其复杂、保密的配方（如MCTS），是其他团队难以企及的。
    -   R1和K1.5的“双响炮”彻底打破了这种叙事，证明开源社区和中国的AI团队完全有能力通过不同的具体方法达到并超越顶尖水平。

3.  **提供了宝贵的对比研究样本**：
    -   虽然目标一致（用RL打造强大模型），但DeepSeek和月之暗面两个团队的具体技术方案（例如，RL算法的细节、奖励函数的设计、训练流程等）必然存在差异。
    -   因此，**将Kimi K1.5的技术报告与DeepSeek R1的报告进行对比阅读**，会成为未来研究者的宝贵资料。我们可以分析：
        -   哪些设计是共通的？（这可能是成功的关键要素）
        -   哪些设计是不同的？（这展示了解决问题的不同思路）
        -   各自的优缺点是什么？

### 总结

总而言之，理解这张图的关键在于，不要将Kimi K1.5视为一个孤立的产品发布，而应将其与DeepSeek R1联系起来，看作一次**重要的科学验证事件**。

**它的核心价值在于：通过两个顶级团队的独立成功，共同证实了“强化学习是扩展大语言模型推理能力的强大引擎”这一技术方向的有效性和普适性，为整个行业的发展注入了强大的信心，并提供了极其宝贵的、可互相对照的研究蓝本。**

## 4.1 Long COT reasoning strategy

### 内容概况

这张幻灯片系统地展示了**Kimi K1.5模型所采用的长链条思维推理策略**及其卓越的性能表现。幻灯片通过清晰的横向对比图表，直观地证明了该策略在数学、代码和视觉理解三大关键领域均能达到或超越现有顶尖模型（如OpenAI o1系列）的性能水平。下方简要列出了实现这一策略的三个关键技术步骤。

### 要点总结

1.  **核心目标与成果**
    *   **目标**：开发并验证一种有效的**长链条思维推理策略**。
    *   **核心成果**：应用此策略的**Kimi k1.5 long-CoT**模型在多项高难度基准测试中表现优异，尤其在数学（AIME 2024, MATH 500）和代码（Codeforces, LiveCodeBench）任务上显著领先于其他先进模型，在视觉任务（MathVista, MMMU）上也具备顶尖竞争力。

2.  **关键技术路径**
    幻灯片指出其成功依赖于三个关键步骤构成的流程：
    *   **数据集构建**：首要步骤是进行**难度过滤**，旨在确保训练数据包含足够复杂、需要深度推理的问题，为模型学习长链条思考提供高质量的“练兵场”。Dataset construction (difficulty filtering)
    *   **监督微调**：专门针对**长思维链进行监督微调**，旨在让模型首先学会生成详细、连贯的推理步骤，为后续的强化学习优化奠定坚实的行为基础。SFT (for long COT)
    *   **强化学习**：采用**自研的策略梯度损失函数进行强化学习**。这是模型的最终优化阶段，通过RL进一步打磨和提升由SFT阶段获得的长链条推理能力。RL (with their own policy gradient loss)

3.  **结论**
    Kimi K1.5的成功表明，**“高质量数据筛选 + 针对性的SFT + 定制化的RL优化”** 这一技术路线，是塑造大模型复杂推理能力的有效范式。其长思维链策略被证实能显著提升模型在需要深度思考的挑战性任务上的性能。

## 4.2 Data curation + SFT

### 内容概况
这张幻灯片详细阐述了Kimi K1.5模型训练流程中的**数据整理**和**监督微调**这两个关键前期阶段。其核心在于通过一套精心设计的数据筛选与准备方法，为后续的强化学习训练奠定高质量的数据基础。

### 要点总结

#### 1. 数据整理 - 核心在于构建高质量、高难度的提示集
数据整理阶段的目标明确：确保训练数据**多样化、需要深度推理、且易于评估**。具体策略包括：

*   **标准化与平衡性**：在数学类场景下进行标准化的数据整理，并**平衡不同主题和学科领域**的覆盖度，以确保模型获得广泛的知识基础。
*   **过滤简单问题**：**排除选择题和判断题**，这类题型容易因猜测导致评估不准（假阳性），从而干扰训练信号。
*   **聚焦模型难点**：一个关键策略是**只选择那些模型在“best-of-8”评估中失败的示例**。这意味着数据集中包含的都是当前模型难以解决、真正具有挑战性的问题。
*   **创新的难度评估**：采用一种**基于模型自评估的难度度量方法**：
    *   让一个SFT模型对每个提示生成10个回答（使用较高的采样温度以增加多样性）。
    *   计算其通过率，通过率越低，则判定该提示的难度越高。
    *   这种方法使难度评估与模型自身能力对齐，能有效过滤掉简单样本，为RL训练提供合适的挑战。

#### 2. 监督微调 - 简要提及，方法未详述
幻灯片对SFT阶段的描述非常简略，仅提及与**提示工程**相关，并暗示可能涉及**知识蒸馏**技术。这表明SFT阶段的主要作用可能是将更强大模型（或复杂流程）的推理能力“蒸馏”到待训练的模型中，为其RL阶段提供一个良好的初始点。

### 核心结论
这套数据处理流程的核心思想是**质量优于数量，难度至关重要**。通过聚焦于模型当前难以解决的、需要复杂推理的问题，并为这些问题赋予一个可靠的难度评分，Kimi团队为后续的强化学习训练准备了极具挑战性且目标明确的“习题集”，这是其成功实现长链条思维推理的关键前提。

## 4.3 Kimi RL

### 内容概况
这张幻灯片详细阐述了Kimi K1.5模型所使用的核心强化学习算法（Kimi RL）的理论推导。该算法旨在优化一个包含参考奖励和KL散度约束的目标函数，其推导过程受到直接偏好优化（DPO）方法的启发，最终得到一个带基线奖励和显式正则化的策略梯度公式。

---

### 要点总结

1.  **问题定义**：优化目标是在最大化由参考答案 $y^*$ 决定的奖励 $r(x, y, y^*)$ 的同时，约束新策略 $\pi_\theta$ 与旧策略 $\pi_{\theta_i}$ 之间的KL散度，以防止策略更新过快偏离太远。这是一个带约束的优化问题。

2.  **核心灵感**：算法的推导受到DPO的启发。DPO通过巧妙的数学变换，将对奖励模型的依赖转换为直接优化策略模型。Kimi RL采用了类似的思路。

3.  **关键推导步骤**：
    *   **建立联系**：首先通过方程将最优策略 $\pi^*$、奖励 $r$ 和当前策略 $\pi_{\theta_i}$ 联系起来。
    *   **非参数假设**：这是一个重要技巧。它假设我们并不需要学习一个显式的奖励函数 $r$，而是可以直接从策略的关系中求解。
    *   **构建代理损失**：基于上述关系，构建一个最小二乘（平方损失）形式的代理损失函数 $L(\theta)$，其目标是让策略模型自己去拟合最优策略所应满足的等式条件。

4.  **最终算法**：通过优化该代理损失函数，得到了最终的策略梯度更新公式。这个公式包含两个核心部分：
    *   **带基线的策略梯度**：使用基线（通常为平均奖励 $\bar{r}$）来降低方差。
    *   **显式的KL正则化项**：直接对策略更新的幅度进行惩罚，确保稳定性。

---

### 打印公式

#### 公式 1: 优化目标
$$
\max _{\theta} \mathbb{E}_{\left(x, y^{*}\right) \sim \mathcal{D}}\left[\mathbb{E}_{(y, z) \sim \pi_{\theta}}\left[r\left(x, y, y^{*}\right)\right]-\tau \mathrm{KL}\left(\pi_{\theta}(x) \| \pi_{\theta_{i}}(x)\right)\right]
$$
**解释**：核心优化问题。目标是最大化期望奖励，同时用KL散度约束新策略 $\pi_\theta$ 不要偏离旧策略 $\pi_{\theta_i}$ 太远，$\tau$ 是控制约束强度的超参数。

#### 公式 2: 受DPO启发的关键等式
$$
r\left(x, y, y^{*}\right)-\tau \log Z=\tau \log \frac{\pi^{*}(y, z \mid x)}{\pi_{\theta_{i}}(y, z \mid x)}
$$
**解释**：这是受DPO启发的核心推导。它将奖励函数 $r$ 与最优策略 $\pi^*$ 和当前策略 $\pi_{\theta_i}$ 的概率比联系起来。$Z$ 是一个归一化常数。

#### 公式 3: 代理损失函数
$$
L(\theta)=\mathbb{E}_{\left(x, y^{*}\right) \sim \mathcal{D}}\left[\mathbb{E}_{(y, z) \sim \pi_{\theta_{i}}}\left[\left(r\left(x, y, y^{*}\right)-\tau \log Z-\tau \log \frac{\pi_{\theta}(y, z \mid x)}{\pi_{\theta_{i}}(y, z \mid x)}\right)^{2}\right]\right]
$$
**解释**：基于公式2构建的平方损失（代理损失）。我们的目标是优化策略参数 $\theta$，使得等式两边的差异最小化。注意期望仍基于旧策略 $\pi_{\theta_i}$ 采样，这是重要性采样的思想。

#### 公式 4: 带基线和正则化的策略梯度
$$
\frac{1}{k} \sum_{j=1}^{k}\left(\nabla_{\theta} \log \pi_{\theta}\left(y_{j}, z_{j} \mid x\right)\left(r\left(x, y_{j}, y^{*}\right)-\bar{r}\right)-\frac{\tau}{2} \nabla_{\theta}\left(\log \frac{\pi_{\theta}\left(y_{j}, z_{j} \mid x\right)}{\pi_{\theta_{i}}\left(y_{j}, z_{j} \mid x\right)}\right)^{2}\right)
$$
**解释**：这是通过优化公式3中的代理损失函数最终得到的**可执行的梯度更新公式**。它由两部分组成：
1.  **带基线的策略梯度**：$\nabla_{\theta} \log \pi_{\theta}(...)(r(…) - \bar{r})$，其中 $\bar{r}$ 是基线（如平均奖励），用于减少方差。
2.  **显式KL正则项**：$-\frac{\tau}{2} \nabla_{\theta}\left(\log \frac{\pi_{\theta}(…)}{\pi_{\theta_{i}}(…)}\right)^{2}$，该项直接惩罚新策略与旧策略之间对数概率比的剧烈变化，起到稳定训练的作用。

## 4.4 Length control in Kimi

### 内容概况
这张幻灯片主题为“Kimi中的长度控制”，主要介绍了Kimi模型为了解决思维链长度控制问题而设计的一种创新的长度奖励机制。该机制通过在训练批次内动态计算长度奖励，针对正确答案和错误答案分别采取不同的策略，旨在压缩思维链长度而不损害模型性能。

### 要点总结

1. **问题背景**
- Kimi目标函数本身不存在GRPO的长度偏差问题
- 但团队希望进一步压缩思维链的长度，提升效率

2. **核心解决方案：动态长度奖励机制**
- 基于每个训练批次内的统计信息进行长度归一化
- 对正确答案和错误答案采用差异化的奖励策略

3. **奖励设计特点**
- **λ值范围**：[0.5, -0.5]，随长度增加而递减
- **正确答案**：获得λ全额奖励，鼓励简洁表达
- **错误答案**：仅当λ为负时获得惩罚，避免过度优化错误路径
- **启用时机**：训练后期启用，避免影响模型性能收敛

4. **算法优势**
- 自适应批次内的长度分布
- 区分答案正误的差异化处理
- 训练稳定的渐进式引入

### 打印公式

#### λ参数计算公式
$$ \lambda=0.5-\frac{\text{len}(i)-\min\_len}{\max\_len-\min\_len} $$

**公式变量说明：**
- $\text{len}(i)$：第i个回答的token长度
- $\min\_len$：当前批次中最短回答的长度
- $\max\_len$：当前批次中最长回答的长度
- $r(x,y_i,y^*)$：答案正确性评估函数（1=正确，0=错误）

## 4.5 Additional details

### 内容概况
本图从“课程设计”和“奖励机制”两个关键技术层面，详细阐述了DeepSeek-R1训练流程中的精细化改进。它揭示了团队如何通过**课程学习** 策略优化训练数据的调度，以及如何构建更精准的**奖励模型** 来提升强化学习训练的效果与效率。

### 要点总结

#### 1. 课程设计：从易到难的渐进式学习策略
- **核心方法**：为训练数据集中的问题标注**难度等级**，并按照**从易到难**的顺序组织训练课程。
- **动态抽样**：根据模型的**未成功率（1 - success_rate）** 来按比例抽样问题。这使得模型能更频繁地接触和练习其尚未掌握的问题，避免在已解决的问题上无效重复，从而显著提升训练数据的利用效率。

#### 2. 奖励机制：面向不同任务的高精度奖励信号设计
团队针对代码和数学这两类核心推理任务，设计了不同的奖励获取策略，以确保反馈信号的高质量和可靠性：
- **代码任务**：对于有标准答案的问题，通过**生成新的测试用例** 来进行自动化评估。这种方法能有效检验代码解决方案的泛化能力和鲁棒性。
- **数学任务**：由于答案的等价性判断更为复杂（如 `0.5` 与 `1/2`），团队专门收集了**80万个样本**，训练了一个**思维链奖励模型**。该模型通过理解整个推理过程来判断答案的正确性，而不仅仅是匹配最终答案字符串。

## 4.6 RL Infra

![RL Infra](https://pic2.zhimg.com/v2-99c9bf430117c08d4f60ee63ca90f84d_1440w.jpg)

### 内容概况

这张题为“RL Infra”的幻灯片深入探讨了强化学习系统在工程实现层面面临的核心效率挑战及其解决方案。它从系统架构和算法优化两个角度，剖析了为何强化学习难以高效部署，并展示了一套完整的训练基础设施设计。

### 要点总结

#### 强化学习效率三大核心挑战

1.  **推理速度瓶颈**
    - 在线策略学习依赖环境交互，需要频繁执行推理生成轨迹数据
    - 推理过程相比训练更为缓慢，成为系统性能瓶颈

2.  **框架切换开销**
    - 训练与推理通常需要不同的技术框架和硬件环境
    - 频繁的框架切换带来显著的性能开销和系统复杂性

3.  **批次处理不均衡**
    - 长思维链导致生成的轨迹长度差异巨大
    - 批次数据大小极不均衡，严重影响计算效率

#### 系统架构解决方案

**分布式训练架构**
- **Rollout Workers**：专用于轨迹生成的推理节点
- **Trainer Workers**：核心训练单元，包含策略模型和参考模型
- **奖励模型集群**：针对不同领域专门优化的奖励模型
- **主控调度器**：统一管理权重分发、数据流和任务调度
- **经验回放缓冲区**：存储和管理生成的轨迹数据

**数据流优化**
- 权重流：从训练器到 rollout 工作器的模型参数同步
- 数据流：从 rollout 到训练器的轨迹数据回传
- 评估请求：主控节点对训练状态的监控和干预

#### 关键优化技术：部分Rollout

**动态轨迹管理**
- **正常停止**：任务自然完成时的完整轨迹保存
- **长度截断**：超过预设长度的轨迹智能截断
- **提前停止**：检测到无效生成时的及时终止

**效率提升机制**
- 避免无效计算，提前终止无望的轨迹生成
- 通过分段处理解决长思维链的批次不均衡问题
- 实现计算资源的按需分配和动态调度

## 4.7 Kimi’s RL setup in detail

### 内容概况
这张图展示了 Kimi 团队设计的**混合部署框架**，该框架通过创新的**分时复用机制**解决了强化学习训练中**训练与推理资源竞争**的核心矛盾。整个系统采用训练（Megatron）与推理（vLLM）分阶段独立运行、通过检查点引擎精确调度GPU内存资源的架构，实现了单一GPU设备上训练与推理任务的高效协同。

### 要点总结

#### 1. 核心创新：分时复用与精确内存调度
- **核心矛盾**：传统RL训练中，数据生成（推理）与模型更新（训练）需要同时占用GPU资源，导致冲突。
- **解决方案**：将训练（Megatron）和推理（vLLM）过程**分离为顺序执行的独立阶段**，避免资源竞争。
- **关键组件**：**检查点引擎** 作为核心调度器，负责在阶段间进行精确的GPU内存卸载与加载，实现无缝切换。

#### 2. 三阶段工作流程
1.  **训练阶段**：
    - Megatron在容器内完成模型训练。
    - 训练结束后，主动卸载GPU内存，准备移交权重。

2.  **推理阶段**：
    - vLLM容器启动，初始使用虚拟权重。
    - 通过 **Mooncake 组件**接收来自Megatron的最新权重，执行生成任务。
    - 生成结束后，由检查点引擎终止vLLM进程，释放内存。

3.  **后续训练阶段**：
    - Megatron重新加载GPU内存，开始新一轮训练，形成闭环。

#### 3. 架构优势与价值
- **资源效率**：极大提升单一GPU的利用率，降低对昂贵计算资源的需求。
- **组件解耦**：训练与推理模块独立，允许分别使用最优工具。
- **工程可行性**：为大规模RL训练提供了稳定、可扩展的工程基础。

### 总结
该框架是RL基础设施设计的优秀范例，其通过精巧的资源调度而非依赖堆砌硬件，高效解决了RL训练的核心工程挑战，体现了深厚的系统优化能力。

## 5.0 Final case study – Qwen 3

### 内容概况

本图系统性地展示了Qwen3系列模型的完整训练流程框架，采用**双路径架构**分别针对旗舰模型和轻量级模型进行差异化训练设计。整个流程呈现出清晰的阶段化特征，体现了从基础预训练模型到最终产品化模型的完整优化路径。

### 要点总结

#### 1. 旗舰模型训练路径（四阶段精细化训练）
- **阶段1：Long-CoT冷启动** - 通过长链思维数据对基础模型进行初始化训练，为后续强化学习奠定基础
- **阶段2：推理强化学习** - 专注提升模型的核心推理能力，此为关键能力建设阶段
- **阶段3：思维模式融合** - 创新性阶段，将不同思维模式进行整合优化（Qwen3独特设计）
- **阶段4：通用强化学习** - 在推理能力基础上进行通用能力强化，产出最终旗舰模型

#### 2. 轻量级模型路径（高效蒸馏压缩）
- **强到弱蒸馏策略**：从强大教师模型向小型学生模型进行知识蒸馏
- **全规格覆盖**：支持从30B到0.6B的多种规模模型产出，满足不同部署需求

#### 3. 流程设计亮点
- **严格的阶段次序**：明确要求RLHF必须在推理RL之后进行，确保能力构建的合理性
- **路径协同**：轻量级模型蒸馏基于旗舰模型的训练成果，形成技术闭环
- **模块化设计**：每个阶段目标清晰，便于迭代优化和问题定位

## 5.1 SFT + Reasoning RL

### 内容概况
这张幻灯片展示了Qwen模型在**SFT（监督微调）与推理强化学习（Reasoning RL）** 阶段采用的关键技术方案。整体策略强调通过**精细化数据筛选**和**高效强化学习**来提升模型推理能力，核心特点是**“小样本、高质量”** 的训练思路。

---

### 要点总结

#### 1. **数据质量优先的筛选策略**
- **难度过滤**：采用类似Kimi的best-of-n方法
  - 剔除模型**无需思维链（CoT）即可答对**的简单样本，Remove things that the model gets right w/o CoT
  - 移除与**验证集过于相似**的内容，防止数据泄漏，Remove things too similar to validation data
- **人工干预**：手动筛选思维链质量，Manual filtering for the quality of CoTs (guessing vs getting it right)
  - 区分**猜测性回答**与**真正理解后的正确回答**
  - 确保训练数据中思维链的**逻辑严谨性**

#### 2. **高效的强化学习配置**
- **算法选择**：采用GRPO算法进行强化学习优化
- **小样本训练**：仅使用**3995个高质量示例**进行训练
  - 体现“质量重于数量”的训练理念
  - 通过数据精选实现高效学习

#### 3. **技术延续性**
- 幻灯片明确指出该方法与Kimi等先进实践一脉相承
- 表明该技术路线已成为业界提升模型推理能力的**有效范式**

### 关键价值
这种“精细筛选+小样本强化学习”的方案，在保证模型推理能力提升的同时，显著提升了训练效率，为资源受限场景下的模型优化提供了实用路径。

## 5.2 Qwen 3 specific new stuff

### 内容概况
本幻灯片重点介绍了Qwen 3模型的一项特色新技术——**思维模式融合**，其核心目标是实现对模型思维链长度的有效控制。该技术通过两种协同机制实现：一是采用标签化方法混合不同思维模式的数据进行训练；二是引入基于特殊字符串的提前停止机制，实现对模型思考过程的预算控制。

---

### 要点总结

#### 1. 思维模式融合的数据基础
- **双模式数据混合**：通过添加 `/think` 和 `/no_think` 指令标签，在训练数据中明确区分“思考模式”与“非思考模式”
- **统一训练框架**：使模型能够根据用户指令动态切换推理策略，平衡答案质量与响应速度

#### 2. 创新的思考预算控制机制
- **提前停止技术**：当模型思考长度达到用户预设阈值时，主动中断思考过程
- **智能切换响应**：通过插入特定指令字符串（*“因时间有限，我将直接基于当前思考给出解决方案”*），引导模型从思考状态无缝过渡到答案生成阶段
- **自然涌现能力**：值得注意的是，这种预算控制能力并非通过专门训练获得，而是思维模式融合技术应用后自然产生的衍生能力

#### 3. 技术价值与创新点
- **精细化控制**：实现了对模型推理过程的深度干预，有效平衡思考深度与计算效率
- **实用性强**：为用户提供了调节“思考-响应”权衡的直接手段，满足不同场景下的效率需求
- **架构简洁**：通过相对简单的技术方案实现了复杂的控制逻辑，体现了工程设计的优雅性

本文主要整理CS336 PyTorch, resource accounting章节的主要内容。

## 7 - tensor_operations_flops

### 张量操作计算量内容概述

本课程深入探讨了深度学习中的计算量（FLOPs）概念及其实际应用，核心内容包括：

#### **1. 核心概念区分**
- **FLOPs**：浮点运算次数（计算量单位）
  - 例：GPT-3训练消耗3.14e23 FLOPs
- **FLOP/s**：每秒浮点运算次数（硬件算力单位）
  - A100峰值算力：312 TFLOPS (312e12 FLOP/s)
  - H100峰值算力：990 TFLOPS (无稀疏优化时)

#### **2. 矩阵乘法计算量分析**
- **计算原理**：
  ```
  FLOPs = 2 × B × D × K
  其中：
    B: 批大小（输入向量数）
    D: 输入维度
    K: 输出维度
  ```
- **示例计算**：
  ```python
  B=16384, D=32768, K=8192 → FLOPs=8.8e15
  ```

#### **3. 各类操作计算量对比**
| **操作类型** | 计算量级 | 示例 | 与大矩阵乘比较 |
|--------------|----------|------|----------------|
| 元素级操作 | O(mn) | `x + y` | 可忽略 |
| 矩阵加法 | O(mn) | `A + B` | 可忽略 |
| 矩阵乘法 | O(mnp) | `X @ W` | 主导地位 |
| 张量卷积 | O复杂 | `Conv2d` | 实际近似矩阵乘 |

#### **4. 模型FLOPs利用率(MFU)**
```math
MFU = \frac{\text{实际FLOP/s}}{\text{硬件标称FLOP/s}}
```
- **计算案例**：
  - FP32实测：实际FLOP/s = 19.5 TFLOP/s，标称=312 → MFU≈0.06
  - BF16实测：实际FLOP/s=82 TFLOP/s，标称=990 → MFU≈0.08
- **业界标准**：MFU≥0.5即优秀（通常大模型可达0.45-0.55）

---

### 核心要点总结

1. **计算规模认知**：
   | **模型** | 训练FLOPs | 等效H100单卡运行时间 |
   |----------|-----------|---------------------|
   | GPT-3 | 3.14e23 | 10年 |
   | GPT-4(推测) | 2e25 | 640年 |
   | 政府监管阈值 | 1e26 | 3200年 |

2. **算力瓶颈解析**：
   ```mermaid
   graph TD
       A[硬件峰值算力] -->|Tensor Core加速| B(BF16>FP32)
       C[实际算力] -->|受限于| D[内存带宽]
       C -->|受限于| E[并行调度效率]
       C -->|受限于| F[通信开销]
       B ---> G[最终MFU]
   ```

3. **工程指导原则**：
   - **精度选择**：BF16可提16倍理论算力（但MFU提升有限）
   - **热点定位**：99%计算量集中在矩阵乘法
   - **优化方向**：
     - 提升大矩阵乘占比
     - 保证内存连续性
     - 减少通信开销

4. **扩展定律**：
   ```
   总训练FLOPs ≈ 6 × 参数量 × 训练token数
   → 70B模型训15T token需6.3e24 FLOPs
   → 1024卡H100需144天（MFU=0.5时）
   ```

> 🔍 **关键洞见**：  
> 现代大模型训练的实质是**超大规模矩阵乘法优化**：
> 1. 算法层面：降低6倍常数项（如Sparse Attention）  
> 2. 硬件层面：提升MFU（逼近Tensor Core峰值）  
> 3. 系统层面：分布式通信优化（减少非矩阵乘时间占比）  
> 三者结合决定训练效率边界。


## 8 - gradients_flops

### 梯度计算FLOPs内容概述

本课程深入分析了梯度计算过程中的浮点运算量（FLOPs），揭示了神经网络训练计算成本的构成。以下是关键内容解析：

#### **1. 模型结构示例**
```python
输入 x (B, D) → 线性层 w1 (D, D) → 隐层 h1 (B, D) → 线性层 w2 (D, K) → 输出 h2 (B, K) → 损失函数 (均方差)
```
- 参数总量：`P = D² + D*K`
- 批量大小：B (数据点数量)

#### **2. 梯度计算数学原理**
##### 前向传播FLOPs
| 计算步骤 | 矩阵公式 | FLOPs计算 | 结果 |
|----------|---------|-----------|------|
| h1 | `h1 = x @ w1` | `2 × B × D × D` | `2B·D²` |
| h2 | `h2 = h1 @ w2` | `2 × B × D × K` | `2B·D·K` |
| **前向总FLOPs** | | **`2B(D² + D·K)`** | **`2B·P`** |

##### 反向传播FLOPs
| 梯度项 | 数学推导 | 矩阵公式 | FLOPs计算 | 结果 |
|--------|----------|----------|-----------|------|
| ∂L/∂w2 | 链式法则 | `w2_grad = h1.T @ h2_grad` | `2 × B × D × K` | `2B·D·K` |
| ∂L/∂h1 | 反向传播 | `h1_grad = h2_grad @ w2.T` | `2 × B × D × K` | `2B·D·K` |
| ∂L/∂w1 | 链式法则 | `w1_grad = x.T @ h1_grad` | `2 × B × D × D` | `2B·D²` |
| **反向总FLOPs** | | **`2B(D² + 2D·K)`** | **`4B·P`** |

#### **3. 通用公式推导**
对任意神经网络层，均满足：
```
前向FLOPs = 2 × B × 输入维度 × 输出维度
反向FLOPs = 4 × B × 输入维度 × 输出维度 = 2 × 前向FLOPs
```
- **总参数量 P** = ∑(层输入维度 × 层输出维度)
- **训练总FLOPs** = 前向 + 反向 = `2B·P + 4B·P = 6B·P`

#### **4. 关键可视化**
!https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VC9y_dHhCKFPXj90Qshj3w.gif
- 红色箭头：前向传播计算路径
- 蓝色箭头：反向传播梯度流
- 梯度计算实质：沿计算图反方向执行张量乘法

---

### 核心要点总结

#### **1. 计算量分布规律**
| 阶段 | FLOPs占比 | 与参数关系 | 工程意义 |
|------|-----------|------------|----------|
| 前向传播 | 33% | `2 × B × P` | 瓶颈在矩阵乘 |
| 反向传播 | 67% | `4 × B × P` | 优化重点领域 |
| **训练总计** | **100%** | **`6 × B × P`** | **计算成本预估基础** |

#### **2. 经典场景验证**
假设Transformer模型：
- 参数量 P = 70B (7e10)
- 训练token数 = B × seq_len = 15T (1.5e13)
```
总训练FLOPs = 6 × 7e10 × 1.5e13 = 6.3e24
匹配真实GPT-3训练量级
```

#### **3. 工程指导价值**
1. **批量大小优化**：
   - FLOPs ∝ B，但大batch降低通信开销
   - 最佳B平衡计算效率与收敛速度
2. **混合精度训练**：
   - BF16梯度计算省50%显存
   - 但FLOPs数量级不变（数值格式转换增加开销）
3. **梯度累积技术**：
   ```python
   # 小批量等效大batch策略
   for micro_batch in batches:
       loss = model(micro_batch)
       loss.backward()  # 梯度累积
   optimizer.step()  # 实际更新
   ```
   - 保持`总FLOPs = 6×总样本数×P`不变
   - 但减少GPU通信次数

> 🔍 **关键洞见**：  
> 梯度计算FLOPs的`4B·P`规律揭示了：  
> 1. **反向传播是训练的主成本源**（2倍于前向）  
> 2. **优化重点应聚焦**：  
>    - 梯度通信效率（NCCL优化）  
>    - 自动微分机制优化（如PyTorch的`torch.compile`）  
>    - 梯度稀疏化技术  
> 3. **算力预估黄金公式**：  
>    `训练总耗时 = (6 × 参数量 × 总token数) / (GPU数 × GPU峰值算力 × MFU)`

## 9 - gradients_flops

### 参数初始化内容概述

本课程详细讲解了神经网络参数的初始化原理与最佳实践，核心内容包括：

#### **1. 参数本质与创建**
- **`nn.Parameter` 对象**：
  - 封装了张量（`torch.Tensor`）
  - 特殊属性：自动注册到模型参数列表
  ```python
  w = nn.Parameter(torch.randn(16384, 32))  # 16384输入维 → 32输出维
  assert isinstance(w, torch.Tensor)  # 本质仍是张量
  ```

#### **2. 默认初始化的灾难**
```python
# 未缩放的正态初始化
w = nn.Parameter(torch.randn(16384, 32))
x = nn.Parameter(torch.randn(16384))
output = x @ w
```
- **输出尺度问题**：输出值随输入维度平方根增长
  ```
  output ≈ √(input_dim) = √16384 = 128
  ```
- **梯度爆炸风险**：大值导致反向传播梯度指数级增大

#### **3. 科学初始化原则**
```python
# 方差缩放初始化
w = nn.Parameter(torch.randn(16384, 32) / np.sqrt(16384))
output = x @ w
```
- **数学原理**：
  ```
  Var(output) = input_dim × Var(weight)
  设定 Var(weight) = 1/input_dim → Var(output) = 1
  ```
- **效果**：输出保持在 O(1) 量级（示例中 ≈0.1~1）

#### **4. Xavier初始化的关系**
!https://ai.stackexchange.com/content/images/2021/10/Screenshot-2021-10-07-at-16.29.19.png
- **标准形式**：适用于线性激活函数
  ```
  Xavier: Var(weight) = 2 / (input_dim + output_dim)
  ```
- **简化实践**：当 `input_dim ≫ output_dim` 时等价于 `1/input_dim`

#### **5. 截断正态分布增强**
```python
# 避免初始化异常值
w = nn.Parameter(
    nn.init.trunc_normal_(
        torch.empty(16384, 32), 
        std=1/np.sqrt(16384),   # 标准差≈0.0078
        a=-3, b=3               # 截断边界[-0.0234, 0.0234]
    )
)
```
- **安全范围**：99.7%权重落在[-3σ, 3σ]

---

### 核心要点总结

#### **初始化方法对比**
| **方法** | 计算公式 | 适用场景 | 输出方差 |
|----------|----------|----------|----------|
| 未缩放正态 | N(0,1) | 无 | O(input_dim) |
| 方差缩放 | N(0,1/√in) | 全连接层 | O(1) |
| Xavier标准 | N(0,2/(in+out)) | 对称激活函数 | O(1) |
| 截断正态 | 截断N(0,1/√in) | 高稳定性需求 | O(1) |

#### **工程实践指导**
1. **基础法则**：
   ```python
   # 全连接层通用初始化
   nn.init.normal_(w, std=1/np.sqrt(w.shape[0]))
   ```

2. **激活函数适配**：
   | **激活类型** | 推荐初始化 | 原理 |
   |--------------|------------|------|
   | Linear/Tanh | Xavier | 保持输入输出方差一致 |
   | ReLU系列 | Kaiming | 补偿ReLU的方差减半效应 |

3. **调试技巧**：
   ```python
   # 检查初始化后第一层输出
   with torch.no_grad():
       outputs = model(torch.randn(16, in_dim))
       print(f"输出均值:{outputs.mean():.3f}, 方差:{outputs.var():.3f}")
   ```
   - 理想值：均值≈0，方差≈1

> 💡 **深度认知**：  
> 初始化本质是解决**信号传播的方差守恒**问题：
> 1. 前向传播：各层输出方差应保持一致  
> 2. 反向传播：各层梯度方差应保持一致  
> 现代初始化方法如Xavier/Kaiming正是通过数学推导（方差分析）实现了这一目标，成为深度神经网络训练的基石。

## 10 - optimizer

### 优化器与内存计算内容概述

本课程深入探讨了优化器的工作原理及训练过程中的资源需求分析，核心内容包括：

#### **1. 优化器演进路线**
```mermaid
graph LR
    A[SGD] -->|+动量| B[Momentum]
    B -->|+梯度平方累积| C[AdaGrad]
    C -->|+指数平均| D[RMSProp]
    D -->|+动量修正| E[Adam]
```
- **AdaGrad核心公式**：
  ```
  cache += grad²
  param -= lr * grad / (√cache + ε)
  ```
- **关键创新**：自适应学习率（高频参数小步更新，低频参数大步更新）

#### **2. 训练流程代码解析**
```python
# 模型定义
model = Cruncher(dim=D, num_layers=2)  # 深度线性模型

# 优化器初始化
optimizer = AdaGrad(model.parameters(), lr=0.01)

# 训练循环
x = torch.randn(B, D)  # 输入
y = torch.tensor([4., 5.])  # 目标
pred_y = model(x)  # 前向传播
loss = F.mse_loss(pred_y, y)  # 损失计算
loss.backward()  # 反向传播
optimizer.step()  # 参数更新
optimizer.zero_grad(set_to_none=True)  # 梯度清零（显存释放）
```

#### **3. 内存占用分析**
| **组件** | 计算式 | 示例值(D=4,B=2) | 内存占比 |
|----------|--------|-----------------|----------|
| 参数(Parameters) | `D²×L + D` | 4²×2 + 4 = 36 | 35% |
| 激活值(Activations) | `B×D×L` | 2×4×2 = 16 | 16% |
| 梯度(Gradients) | 同参数量 | 36 | 35% |
| 优化器状态(Optimizer States) | 同参数量(AdaGrad) | 36 | 14% |
| **总内存(FP32)** | `4×(P+A+G+O)` | 4×(36+16+36+36)=512字节 | 100% |

#### **4. 计算量分析**
- **单步训练FLOPs**：
  ```
  FLOPs = 6 × B × P = 6 × 2 × 36 = 432
  ```
  其中：
  - 前向传播：2×B×P
  - 反向传播：4×B×P

#### **5. Transformer扩展**
- **内存特性**：
  - 激活值占比显著增加（因注意力机制）
  - 优化器状态成为最大开销（Adam需额外动量/方差）
- **计算特性**：
  - 矩阵乘法主导（>95% FLOPs）
  - 序列长度平方级复杂度（注意力机制）

---

### 核心要点总结

#### **优化器选择指南**
| **优化器** | 内存开销 | 收敛特性 | 适用场景 |
|------------|----------|----------|----------|
| SGD | 1×P | 慢但稳定 | 小规模模型 |
| AdaGrad | 1×P | 自适应学习率 | 稀疏数据 |
| Adam | 2×P | 快速收敛 | 主流选择 |
| LAMB | 2×P | 大batch优化 | 分布式训练 |

#### **内存优化四象限**
```mermaid
pie
    title 内存占用分布
    “参数” ： 35
    “梯度” ： 35
    “优化器状态” ： 14
    “激活值” ： 16
```

#### **工程实践洞见**
1. **混合精度训练**：
   - BF16参数：节省50%参数/梯度内存
   - FP32优化器状态：仍需完整精度
2. **优化器状态压缩**：
   - 8-bit Adam：量化优化器状态
   - Zero Redundancy：分布式分片优化器状态
3. **激活值优化**：
   - 梯度检查点：时间换空间
   - 序列分块：降低注意力内存

#### **Transformer黄金公式**
```
总参数量 P ≈ 12 × d_model² × n_layer
训练FLOPs/step ≈ 6 × batch_size × seq_len × P
```
- **示例**：GPT-3 (d_model=12288, n_layer=96)
  ```
  P ≈ 12 × (12288)² × 96 ≈ 175B
  单步FLOPs ≈ 6 × 4M × 2048 × 175e9 ≈ 8.8e21
  ```

> 💡 **关键认知**：  
> 现代大模型训练的瓶颈已从算力转向**内存墙**：  
> 1. H100显存带宽仅3.35TB/s，远低于算力需求  
> 2. 70B模型仅参数就需1.1TB（FP16）  
> 3. 优化重点应是降低数据移动，而非单纯提升算力  
> 这正是ZeRO/FSDP等分布式策略的核心价值所在。
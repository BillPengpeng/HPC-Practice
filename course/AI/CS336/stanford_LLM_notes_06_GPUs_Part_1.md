本文主要整理CS336 GPUs章节的主要内容。

## 1 - ​​计算能力（compute）与大型语言模型（LLM）

### **内容概括**
三张图片共同阐述了**计算能力（compute）与大型语言模型（LLM）性能发展之间的关系**，并强调了硬件进步（尤其是GPU并行计算）对AI scaling的关键推动作用：

1. **第一张图**基于神经缩放定律（Neural Scaling Laws）指出：计算量的增加（以PetaFLOP/s-days为单位）可**预测性地降低语言模型的验证损失**（即提升性能），其关系可近似表示为 \( L = 2.57 \cdot C^{-0.048} \)。当前进展主要由硬件加速、利用率和并行化提升驱动。

![Dennard scaing](https://i-blog.csdnimg.cn/direct/12a00eaea4e54890a79c5f112986b46f.png)

2. **第二张图**回顾了1970–2020年间处理器性能的演变，指出传统依赖** Dennard scaing（晶体管尺寸缩小带来性能提升）** 的方式已接近极限（“tapped out”），从而引出如何满足LLM对计算资源巨大需求的问题。

![并行扩展](https://i-blog.csdnimg.cn/direct/2a85874f4b0548e680a49cd318b3dd90.png)

3. **第三张图**聚焦GPU的并行扩展（parallel scaling），指出过去10年间通过**数值精度优化（如FP16/Int8）、指令集复杂化、制程进步（5nm）、稀疏计算等技术**，单芯片推理性能增长超1000倍，并强调“没有GPU的扩展，就没有LLM的扩展”。

---

### **要点总结**
1. **计算量是语言模型性能的关键驱动力**，且其收益是可预测的（缩放定律）。
2. **传统处理器性能扩展方式（如Dennard scaling）已失效**，需寻找新的计算扩展路径。
3. **GPU并行计算是当前满足LLM算力需求的核心方案**，过去10年性能提升超过1000倍。
4. **硬件、软件与算法协同优化**（如数值表示、稀疏化、模型效率改进）共同推动了计算效能的飞跃。
5. **未来LLM的发展高度依赖计算硬件的持续创新**（如更快的芯片、更好的并行架构）。

## 2 - How is a GPU different from a CPU?

![different](https://i-blog.csdnimg.cn/direct/48f11f6ec74d4d42a13cc67436688494.png)

### 内容概括

CPU为**低延迟**（快速完成单个任务）而优化，而GPU为**高吞吐量**（同时处理大量任务）而优化。通过左右对比的架构图和工作方式示意图，清晰地展示了两者在控制单元、缓存和计算单元数量与结构上的巨大差异。

---

### 要点总结

1.  **核心设计目标不同**
    *   **CPU：优化延迟 (Latency)**。致力于让**单个或少数几个线程**（计算任务）尽可能快地执行完毕。好比是一个**知识渊博的专家**，能非常快地解决一个复杂问题。
    *   **GPU：优化吞吐量 (Throughput)**。致力于在**单位时间内处理海量的数据**（完成大量线程的计算）。好比是**成千上万的学生**一起工作，每人解决一个简单问题，最终在相同时间内完成的总工作量巨大。

2.  **硬件架构差异**
    *   **CPU结构**（图示左侧）：
        *   拥有**强大且复杂的控制单元 (Control)**，擅长逻辑控制和分支预测。
        *   配备**大容量缓存 (Cache)**，用于快速存取数据，减少等待时间。
        *   **计算单元 (ALU)** 数量较少，但每个都非常强大，专为处理复杂运算而设计。
    *   **GPU结构**（图示右侧）：
        *   **控制单元相对简单**，对分支预测等复杂逻辑的支持较弱。
        *   **缓存容量较小**。
        *   **核心是成千上万个简单、微型的计算单元 (ALU)**，以大规模并行阵列的方式排列，专为同时执行大量简单、重复的计算任务而设计。

3.  **工作方式对比**
    *   图示右下角通过流程图对比了两者处理任务的方式：
        *   **CPU**的工作流程更偏向**串行**处理，快速、逐个地解决任务。
        *   **GPU**的工作流程是**大规模并行**处理，将一个大任务分解成无数个微小且相同的子任务，然后由海量的计算单元同时处理，最后将结果合并。

4.  **总结与应用**
    *   **CPU**是计算机的“大脑”，负责处理通用任务、运行操作系统和应用程序的逻辑控制。
    *   **GPU**是“大规模并行计算引擎”，特别适合处理图形渲染、科学计算以及当今最热门的人工智能（AI）和机器学习（ML）模型训练等需要海量计算并行化的任务。

## 3 - GPU工作原理

### 内容概括

这四张幻灯片系统地阐述了**GPU（图形处理器）的核心架构与工作原理**，从**执行单元结构、内存层次、执行模型到内存模型**，完整地揭示了GPU为何能实现大规模并行计算和高吞吐量。内容源于NVIDIA的技术资料（如GA100架构），旨在说明GPU区别于CPU的设计哲学及其在加速计算（如AI、科学计算）中的优势。

---

### 要点总结

![execution units](https://i-blog.csdnimg.cn/direct/d7a4e4d1c29b4cdbbd748d081e9b972f.png)

#### 1. **执行单元结构 (Anatomy of a GPU - execution units)**
*   **核心组件**：GPU由大量**流多处理器 (SM, Streaming Multiprocessor)** 组成（例如GA100有128个）。
*   **并行基础**：每个SM内部又包含许多**流处理器 (SP, Streaming Processor)**，这些SP是执行计算的基本单元，能并行处理大量**线程 (Threads)**。
*   **设计哲学**：**“多而专”**。通过集成海量的SM和SP，GPU专注于同时执行成千上万个相对简单的任务（线程），从而实现极高的吞吐量。

![memory](https://i-blog.csdnimg.cn/direct/34ba88aafe864de7ba2d1ecee4db3e5c.png)

#### 2. **内存层次与访问 (Anatomy of a GPU - memory)**
*   **核心原则**：**“离计算单元越近的内存，速度越快”**。这是一个经典的层次化内存架构。
*   **速度与成本权衡**：
    *   **最快**：**寄存器**和**共享内存**（位于SM内部，SRAM技术）。访问延迟极低（几十个时钟周期）。
    *   **次快**：**L1/L2缓存**（位于GPU芯片上）。
    *   **最慢**：**全局内存**（位于GPU芯片外的DRAM内存芯片）。访问延迟高（数百个时钟周期），但容量大。
*   **关键洞察**：SRAM（缓存/共享内存）比DRAM（全局内存）**快约8倍，但也贵约100倍**。编程时必须尽量减少对慢速全局内存的访问。

![Execution model](https://i-blog.csdnimg.cn/direct/5f98d3d2af6542dd81c684dfcad902ee.png)

#### 3. **执行模型 (Execution model of a GPU)**
*   **三层并行结构**：
    1.  **线程 (Thread)**：最小的执行单元。所有线程以**单指令多线程 (SIMT)** 方式工作，即所有线程执行相同的指令，但处理不同的数据。
    2.  **线程块 (Block)**：一组线程的集合。一个**块被分配到一个SM上执行**，并可使用该SM的共享内存进行线程间通信。
    3.  **线程束 (Warp)**：**32个连续编号的线程**组成一个Warp，这是GPU调度和执行的基本单位。每个SM有多个**Warp调度器**，通过在多个Warp间快速切换来隐藏内存访问延迟，保持计算单元饱和。
*   **编程模型**：CUDA程序由多个块（Grid）组成，每个块包含大量线程。

![Memory model](https://i-blog.csdnimg.cn/direct/fd2f8e3dc44144ab92db88431ebd160b.png)

#### 4. **内存模型 (Memory model of a GPU)**
*   **访问权限**：
    *   **设备代码 (GPU上运行)**：可读写自身的**寄存器**、**本地内存**；可与同块内所有线程共享**共享内存**；可读写全局的**全局内存**；只读**常量内存**。
    *   **主机代码 (CPU上运行)**：主要负责与GPU之间传输**全局内存**和**常量内存**的数据。
*   **关键规则**：
    *   线程可以高效地访问**自己块内的共享内存**进行快速协作。
    *   **不同块之间的线程无法直接通信**，必须通过读写速度较慢的**全局内存**来交换数据。

### **核心结论**
这四张图共同描绘了GPU作为**大规模并行计算引擎**的完整蓝图：其威力源于**海量简单的计算单元(SM/SP)** 与**精心设计的分层内存体系**的结合，并通过**线程块(Block)和线程束(Warp)** 模型进行高效调度和管理。理解这些概念是进行GPU（CUDA）高性能编程的基础。

## 4 - Side thread – What about TPUs

### 内容概括

幻灯片通过一个**TPU TensorCore的抽象布局图**，详细分解了其内部功能单元（标量、矢量、矩阵单元）的协同工作方式，并点明了其“轻量控制、重型计算”的设计哲学。最后，它简要对比了TPU与GPU在硬件结构和执行模型上的关键差异。

![TPU TensorCore](https://i-blog.csdnimg.cn/direct/5865d42742004fc68b5a45cace86bcf3.png)

---

### 要点总结

#### 1. **高层共识：加速器的相似性**
*   **核心观点**：GPU、TPU以及其他AI加速器在**高层架构目标上是相似的**，都致力于通过并行计算来高效处理大规模数据。

#### 2. **TPU核心架构解析**
幻灯片通过一张抽象架构图，揭示了TPU（以TensorCore为例）的内部构造：
*   **标量单元 (Scalar Unit)**：
    *   功能类似一个**轻量级的CPU**。
    *   负责**指挥和调度**，向矢量单元(VPU)和矩阵单元(MXU)分派指令。
*   **矢量单元 (VPU - Vector Unit)**：
    *   负责执行**逐元素操作**（例如应用激活函数如ReLU）。
    *   关键任务：**将数据从内存加载到矩阵单元(MXU)**。
*   **矩阵乘单元 (MXU - Matrix Multiply Unit)**：
    *   **是整个芯片算力(FLOP/s)的核心驱动单元**，专门执行矩阵乘法运算。
    *   这是TPU为AI计算（尤其是神经网络训练/推理）提供极致性能的关键。
*   **内存体系**：
    *   **高带宽内存 (HBM)**：片外大容量内存，用于存储模型权重、激活值、优化器状态和批量数据。其**带宽决定了数据进出计算单元的速度**，是性能关键瓶颈之一。
    *   **矢量内存 (Vmem)**：片上的高速缓存，用于临时存储数据，供VPU和MXU快速访问。

#### 3. **与GPU的关键差异对比**
*   **硬件规模与性能**：
    *   **GPU**：拥有**更多**的流多处理器（**SMs**），适合处理各种并行任务。
    *   **TPU**：拥有**较少**但**更庞大、更专用**的张量核心（**TCs**），在**纯矩阵乘法性能上可与GPU媲美甚至更优**。
*   **执行模型**：
    *   **GPU**：使用**Warp（线程束）** 作为基本调度单位，是SIMT（单指令多线程）执行模型的核心。
    *   **TPU**：**没有Warp的概念**，执行模型更简单，主要以**Block（块）** 为单位。这在矩阵乘和非矩阵乘运算之间做了权衡，使其对矩阵运算极度优化。
*   **设计哲学**：
    *   **TPU的核心结构**可总结为：**轻量级控制单元 + 快速（大型）矩阵乘法单元 + 高速内存**。其设计极度专注于加速AI工作负载中的矩阵运算。
    *   **网络互联**：另一个重要差异在于多个加速器芯片之间**如何组网**（这在并行计算中至关重要）。

### **核心结论**
TPU是Google为机器学习量身定做的**领域专用架构（DSA）**。它与GPU最大的区别在于其**极致的专业化**：通过简化控制逻辑（无Warp）、打造巨型专用矩阵乘法单元（MXU）并配以高速内存体系，在其目标领域（矩阵计算）实现了极高的效率和性能。GPU则更偏向于一种**通用并行计算 accelerator**，灵活性更高。

## 5 - ​​GPU计算模型的演进、优势及其核心挑战​​

### 内容概括

这四张幻灯片共同描绘了**GPU计算模型的演进、优势及其核心挑战**。内容从GPU的编程模型优势出发，回顾了早期利用图形硬件进行通用矩阵计算的探索，展示了专用矩阵计算硬件（Tensor Cores）带来的巨大性能飞跃，并最终揭示了当前GPU发展面临的根本性瓶颈：**计算能力（FLOPS）的增长速度已远超内存带宽的增长速度**，形成了“内存墙”问题。

---

### 要点总结

![Strengths of the GPU model](https://i-blog.csdnimg.cn/direct/98abbb099354497fb970efea2810341b.png)

#### 1. **GPU模型的优势 (Strengths of the GPU model)**
*   **强大的可扩展性**：可以通过增加**流多处理器（SM）** 的数量来轻松扩展处理艰巨并行工作负载的能力。
*   **相对易于编程**：**单指令多线程（SIMT）** 模型允许程序员用相对简单的串行思维编写代码，而由硬件负责将指令广播给成千上万的线程并行执行。
*   **轻量级线程**：GPU线程的创建、切换和调度开销极低，使得海量线程的并行成为可能，从而高效隐藏内存访问延迟。

#### 2. **GPU作为矩阵加速器的起源 (GPUs as fast matrix multipliers)**
*   **早期探索**：在GPU可编程着色器早期（约2001年），研究人员就通过“黑客”技巧（如利用RGBA通道打包数据、优化纹理格式、全屏运行等）将图形硬件用于通用矩阵乘法（GEMM）计算。
*   **历史意义**：这证明了GPU在并行计算上的巨大潜力，为后来GPGPU（通用GPU计算）及CUDA等技术的发展奠定了基础。

#### 3. **专用矩阵计算硬件的革命 (Matmul vs. non-matmul FLOPS)**
*   **性能分水岭**：自**Volta架构（V100）** 引入**张量核心（Tensor Cores）** 起，矩阵乘法的性能开始与非矩阵运算（如常规FP32计算）的性能急剧拉开差距。
*   **巨大优势**：在现代GPU（如A100, H100）上，矩阵乘法运算速度可达到其他浮点运算的**10倍以上**。这使得GPU成为人工智能（尤其是深度学习）训练的绝对核心。

#### 4. **面临的核心挑战：内存墙 (Compute scaling vs. memory scaling)**
*   **根本矛盾**：过去20年，GPU的峰值计算能力（FLOPS）增长了约**60,000倍**，而内存带宽（DRAM）仅增长了约**100倍**，互连带宽增长了约30倍。
*   **“内存墙”问题**：计算单元的增速远远超过了数据供给能力的增速。这意味着**如何高效地将数据输送到计算单元**，避免其“饥饿”和空闲，已成为提升整体性能的最大瓶颈。这也是当前芯片设计（如HBM高速内存、NVLink高速互连）和软件优化（如优化数据局部性、使用共享内存）的重点。

## 6 - GPU性能优化的复杂性

### 内容概括

这两张幻灯片构成了一个完整的逻辑链条：**从揭示GPU性能优化的复杂性出发，引入屋顶模型（Roofline Model）作为核心分析工具，旨在系统性地解决“内存墙”问题**。第一张图通过方阵乘法（Matmul）的性能波动现象，直观展示了即使是最基础的计算内核，在GPU上也难以持续达到峰值算力，其性能受到多种因素影响。第二张图则给出了解决这一问题的理论框架和系统性方法，即利用屋顶模型来诊断性能瓶颈（是算力受限还是带宽受限），并指明了不同层级内存的优化潜力，最终回答了“如何避免内存受限”这一核心问题。

---

### 要点总结

![FLOPs achieved for square matmuls](https://pic2.zhimg.com/v2-5f1ca6049388db03047e47a4aa7088d5_1440w.jpg)

#### **图1：方阵乘法在GPU上的性能复杂性 (FLOPs achieved for square matmuls)**
1.  **核心现象**：即使在GPU上执行**最简单的标准方阵乘法**，其实际实现的算力（TF/s）也并非一条平滑的曲线，而是随着矩阵尺寸（N）变化出现**剧烈的波动和峰值**。
2.  **所揭示的问题**：这表明GPU的峰值算力只是一个理论值，实际性能受到**内存访问模式、数据复用、硬件调度**等多种因素的复杂影响。
3.  **手写标注的关键优化方向**：
    *   **计算强度 (Compute Intensity)**：每个字节从内存中读取后，用于进行多少次浮点运算。这是决定性能上限的关键指标。
    *   **分块 (Tiling!)**：核心优化技术。通过将大矩阵分解成小块，使其能放入GPU的高速缓存（如共享内存）中，极大提高数据复用率，减少访问慢速显存的次数。
    *   **Wave Quantization**：可能与GPU的调度单位（Warp）或计算单元的执行波动有关，指出了算法行为需要与硬件特性匹配。

![Roofline Model](https://pic3.zhimg.com/v2-0ee5ff59b5619e9f7854ba9788e4ed40_1440w.jpg)

#### **图2：屋顶模型与性能优化框架 (The roofline model)**
1.  **核心工具 - 屋顶模型**：
    *   **纵轴（性能）**：可达的算力（GFLOP/s），是对数刻度。
    *   **横轴（计算强度）**：算法固有的属性（Ops/Byte），数值越大，代表算法越“计算密集”。
    *   **“屋顶”**：图中的多条折线代表了不同内存层级（寄存器、共享内存、主内存）的**性能上限**。斜率部分代表**带宽受限**，水平部分代表**算力受限**。

2.  **关键洞察**：
    *   **性能瓶颈诊断**：一个算法的实际性能取决于其计算强度落在屋顶模型的哪个区域。
        *   **强度低（左侧）**：性能位于倾斜的“屋顶”下，是**内存带宽受限**。优化重点是减少内存访问、提高数据复用。
        *   **强度高（右侧）**：性能位于水平的“屋顶”下，是**计算算力受限**。优化重点是提高计算并行度。
    *   **内存层级的重要性**：使用越快的内存（如寄存器、共享内存），对应的“屋顶”就越高，在相同的计算强度下能获得更高的性能。这**量化了使用共享内存进行分块优化的巨大收益**。

3.  **实例分析**：
    *   **密集矩阵乘法 (Dense Matmul)**：通常具有较高的计算强度，其性能点更可能接近GPU的算力屋顶。
    *   **稀疏矩阵乘法 (Sparse Matmul)**：由于大量零元素存在，计算强度很低，其性能点牢牢处于内存带宽的屋顶之下，是典型的内存受限操作。

4.  **本节核心问题与答案**：
    *   **问题**：如何避免内存受限？
    *   **答案**：
        *   **1. 提高算法的计算强度**：通过优化算法和实现（如分块），使每次从内存读取的数据能被多次使用。
        *   **2. 将数据移至更快的内存**：充分利用GPU的存储层次结构，将数据尽可能保留在寄存器或共享内存中，从而突破主内存的带宽限制。

## 7 - Control divergence (not a memory issue)

![Control divergence](https://pica.zhimg.com/v2-9e3a12f0501f13a897125213b4cd64da_1440w.jpg)

### 内容概括

该问题并非由内存带宽限制引起，而是根植于GPU所采用的**单指令多线程（SIMT）执行模型**本身。幻灯片通过架构示意图和代码案例，直观地解释了当同一个线程束（Warp）中的不同线程需要执行不同的指令路径（如if-else条件分支）时，GPU如何串行化所有可能的路径，从而导致显著的性能开销。

---

### 要点总结

1.  **核心问题：控制分歧 (Control Divergence)**
    *   这是GPU编程中由**条件语句**（如if/else）引发的一种性能瓶颈。
    *   其本质是**执行效率问题**，与内存访问无关。

2.  **根本原因：GPU的SIMT执行模型**
    *   **SIMT（单指令多线程）**：GPU的基本执行单位是**线程束（Warp）**，通常包含32个线程。在一个时钟周期内，一个Warp中的所有线程必须**同步地执行相同的指令**。
    *   **架构支撑**：图示展示了SIMT模型的硬件结构，一个**指令解码器（Instruction Decode）** 同时为整个Warp中的多个**执行上下文（Exec Context）** 提供同一条指令。

3.  **分歧的产生与开销**
    *   当代码中出现条件分支（如 `if (threadId < 16)`）时，同一个Warp中的不同线程会进入不同的分支（一些执行if块，一些执行else块）。
    *   **GPU无法让部分线程执行if指令，同时让另一部分执行else指令**。取而代之的是，GPU会**串行化执行所有分支路径**：
        *   首先，让所有需要进入第一个分支（如if块）的线程执行，而让属于其他分支的线程**无效化（或等待）**。
        *   然后，再让所有需要进入第二个分支（如else块）的线程执行，其他线程无效化。
    *   这个过程导致**原本可以并行完成的工作变成了串行执行**，造成了巨大的性能开销。

4.  **核心结论与影响**
    *   **“条件语句没问题，但会因执行模型导致显著开销”**：GPU支持条件语句，但其执行方式决定了这会带来性能损失。
    *   控制分歧会**严重降低GPU的并行效率**，是优化GPU代码时需要重点关注和避免的问题。

## 8 - Low Precision Computation

### 内容概括

这三张幻灯片系统性地阐述了**低精度计算（Low Precision Computation）** 在现代GPU（特别是AI计算）中的核心作用、优势及其具体应用方式。内容从宏观收益出发，通过具体算例解释其提升计算效率的原理，并最终深入到硬件层面（Tensor Cores）的混合精度设计，揭示了低精度计算是驱动GPU性能实现指数级增长的关键技术之一。

---

### 要点总结

#### **图1：低精度计算的宏观收益 (Trick 1: Low precision computation)**
1.  **核心论点**：采用低精度数值格式（如FP16, INT8）是提升GPU计算性能的关键技巧。
2.  **四大收益来源**：
    *   **数值表示 (Number Representation)**：使用FP16、INT8、TF32、BF16等低比特位数格式，直接减少了数据存储和传输的体积。
    *   **复杂指令 (Complex Instructions)**：专为低精度计算设计的硬件指令（如DP4A用于INT4、HMMA/IMMA用于矩阵乘），能在一个时钟周期内完成更多操作，带来**~12.5倍**的性能提升。
    *   **制程工艺 (Process)**：从28nm到5nm的先进制程迭代，提升了晶体管密度和能效，贡献了**~2.5倍**的性能提升。
    *   **稀疏性 (Sparsity)**：利用结构化稀疏等技术，跳过零值计算，贡献了**~2倍**的性能提升。
3.  **最终效果**：上述技术（尤其是低精度）共同驱动了过去10年GPU单芯片推理性能**超过1000倍**的增长。
4.  **根本原理**：**“位数越少，需移动的位数就越少”**。减少数据比特数直接降低了内存带宽的压力和功耗，是提升性能的核心。

![Low precision improves arithmetic intensity](https://i-blog.csdnimg.cn/direct/0c8e72587d344d038c8d4587d1d2893a.png)

#### **图2：低精度提升计算强度 (Low precision improves arithmetic intensity)**
1.  **核心概念**：通过一个具体例子（ReLU激活函数）说明低精度如何提升**计算强度（Arithmetic Intensity）**。
2.  **计算强度定义**：每从内存中读取1字节数据，所能完成的浮点运算次数（FLOPs/Byte）。该值越高，性能越接近算力峰值（越不易受内存带宽限制）。
3.  **实例对比**：
    *   **FP32 case**：读写数据总量为8字节，完成1次FLOP，计算强度 = **1 FLOP / 8 Bytes**。
    *   **FP16 case**：读写数据总量为4字节，完成1次FLOP，计算强度 = **1 FLOP / 4 Bytes**。
4.  **结论**：在执行相同计算的前提下，使用FP16相比FP32，**计算强度翻倍**，使得该操作更不容易受内存带宽限制，从而能更高效地利用GPU的计算能力。

![Low precision drives faster matrix multiplies](https://i-blog.csdnimg.cn/direct/85903bdfa2d44d74bf4a33d8066cfc3a.png)

#### **图3：低精度驱动矩阵乘法加速与混合精度实践 (Low precision drives faster matrix multiplies)**
1.  **核心硬件**：**张量核心（Tensor Cores）** 是专门为加速低精度矩阵乘法而设计的硬件单元。
2.  **工作原理（混合精度）**：
    *   **输入**：使用**16位**数据（如FP16/BF16）进行矩阵乘运算，大幅提升吞吐量。
    *   **计算**：在芯片内部进行**全精度（Full precision）** 的乘法和累加，以避免精度损失。
    *   **输出**：将累加结果以**FP32**格式输出，保证最终结果的数值精度和稳定性。
3.  **不同操作的精度选择策略**：
    *   **使用16位存储/计算**：适用于矩阵乘法和大多数逐元素操作（ReLU, tanh等），以提升速度和节省带宽。
    *   **需要更高精度（FP32）**：适用于累加（Reduction）、求和（Sum）、Softmax、归一化等操作，防止舍入误差累积。
    *   **需要更大数值范围（FP32/BF16）**：适用于指数（exp）、对数（log）、幂运算（pow）等输出范围远大于输入的函数，以及损失函数计算。BF16因其与FP32相似的数值范围，在此类操作中比FP16更有优势。

### **整体结论**
低精度计算是一项系统工程，它通过**算法（选择数值格式）、编译器（生成特定指令）和硬件（Tensor Cores）** 的协同设计，实现了在保证计算准确性的前提下，最大化计算吞吐量和能效，是现代AI计算的基石。

## 9 - Operator fusion

### 内容概括

这四张幻灯片系统性地介绍了GPU性能优化中的一项关键技术——**算子融合（Operator Fusion）**。其核心思想是通过将多个连续的计算操作（算子）合并成一个统一的核函数（Kernel），来**减少对慢速全局内存的访问次数**，从而克服“内存墙”瓶颈，显著提升计算效率。内容从形象的比喻（工厂与仓库）入手，解释了融合的必要性，通过正反案例对比（朴素实现 vs. 融合实现）展示了其巨大优势，并最终指出现代编译器（如PyTorch的torch.compile）已能自动完成此类优化。

---

### 要点总结

#### **1. 核心概念与动机 (Trick 2: Operator fusion)**
*   **核心比喻**：将GPU比作**工厂（Compute）**，将内存比作**仓库（Memory）**。数据如同原材料，需要从仓库运到工厂加工。
*   **根本问题（内存墙）**：**“Compute scales up, memory doesn't”**（计算能力可扩展，内存带宽不可扩展）。计算单元的性能增长远快于内存带宽，因此减少数据搬运是优化的关键。
*   **解决方案**：**算子融合**。即尽可能在“工厂”内完成所有中间步骤的加工，避免将中间产品频繁运回“仓库”，从而极大减少与内存的交互次数。

#### **2. 技术原理与优势 (Operator fusion to minimize memory access)**
*   **朴素（非融合）实现的弊端**：
    *   每个独立的操作都启动一个单独的CUDA核函数。
    *   每个核函数都需要从内存读取输入，并将结果写回内存。
    *   对于连续操作，这意味着大量的中间数据在内存和计算单元之间“来回传输”，非常低效（“somewhat silly”）。
*   **融合实现的优势**：
    *   将多个连续操作合并编译成**一个核函数**。
    *   中间结果保存在GPU的高速寄存器或共享内存中，直接在芯片内部传递，**无需写回全局内存**。
    *   最终只将整体结果写回内存一次，极大减少了内存带宽压力。

#### **3. 具体案例与性能问题 (Example - sines and cosines)**
*   **计算任务**：计算 $\sin^{2}x + \cos^{2}x$。
*   **朴素实现的代价**：
    *   分解为5个步骤：计算sin(x), cos(x), sin²(x), cos²(x), 以及最终求和。
    *   在未优化的情况下，这会**启动5个独立的CUDA内核**，导致5次内存读写循环，性能极低。

#### **4. 解决方案与自动化工具 (Fusion example)**
*   **融合效果**：
    *   所有5个逐点运算（pointwise operations）被融合为**单个CUDA内核调用**。
    *   流程图中左侧多个分散的算子，在右侧被合并为一个简洁的模块。
*   **自动化实现**：
    *   此类“简单”的融合（通常是线性或逐点操作序列）可以**由编译器自动完成**。
    *   **torch.compile（例如其中的TorchInductor编译器）** 就具备这种自动化算子融合的能力，开发者无需手动编写复杂融合代码即可获得性能提升。

### **整体结论**
算子融合是一种通过**减少内存传输**来突破带宽瓶颈、提升GPU计算效率的核心技术。其本质是“用计算换带宽”，通过增加芯片内的计算量来避免昂贵的内存访问。对于常见的计算模式，这一优化过程已可被现代深度学习编译器自动化，成为提升AI模型训练和推理性能的标配手段。

## 10 - Recomputation

![Storing (and retrieving) activations](https://i-blog.csdnimg.cn/direct/c411c46207b044fb9ca806069646e58c.png)
![Throw away the activations](https://i-blog.csdnimg.cn/direct/1db8c9a5db514b26a5502e04095daf62.png)

### 内容概括

这三张幻灯片共同阐述了深度学习模型训练中，为优化内存使用而采用的一项关键技巧——**重计算（Recomputation）**，也称为**梯度检查点（Gradient Checkpointing）**。其核心思想是在反向传播过程中，**选择性丢弃前向传播计算出的中间结果（激活值）**，并在需要时重新计算它们，从而在“计算时间”和“内存占用”之间进行权衡，以解决训练大型模型时的内存瓶颈问题。

---

### 要点总结

#### **1. 背景与问题：反向传播的内存成本 (Storing activations can be expensive!)**
*   **标准反向传播流程**：为了计算梯度，前向传播过程中每一层的**激活值（中间计算结果）** 都需要被存储下来，以供反向传播时使用。
*   **内存开销巨大**：对于深度网络，这些激活值会消耗大量的GPU内存，成为训练超大模型的主要限制因素。图中以3个Sigmoid层叠加为例：
    *   **旧方法（存储所有激活值）**：前向传播需**1次读、3次写**；反向传播需**3次读、1次写**。总共**8次内存访问**。
    *   **性能问题**：这种模式**算术强度极低**，大部分时间都在进行内存读写操作，而非计算，严重限制了性能。

#### **2. 解决方案：用计算换内存 (Throw away the activations, re-compute them!)**
*   **核心策略**：在前向传播时，**只保留关键节点的激活值**，而丢弃其他大部分中间结果。在反向传播需要这些数据时，**通过再次执行前向计算来重新生成**它们。
*   **实例分析（3个Sigmoid层）**：
    *   **新方法（重计算）**：前向传播只写入最终结果，**仅需1次读、1次写**。反向传播时，通过重新计算得到中间激活值，需**2次读、1次写**。
    *   **巨大收益**：总内存访问次数从8次**降至4次**，减少了50%（图中表述为5/8）。虽然增加了一些计算量，但用丰富的计算资源换取了宝贵的内存带宽，整体性能得到提升。

#### **3. 理论支撑与算法 context (Trick 3: recomputation)**
*   **算法基础**：重计算策略建立在**反向传播算法**之上。该算法通过计算图（Computational Graph）来定义前向值（$f_i$）和反向梯度（$g_i$）。
*   **设计空间**：重计算不是盲目丢弃所有数据，而是一种**精细的权衡策略**。需要算法设计者或编译器（如PyTorch的`torch.compile`）智能地决定哪些节点的值值得存储，哪些值得重新计算，以实现总成本（计算时间+内存占用）的最优化。

### **整体结论**
**重计算（Recomputation）** 是一种经典的“**以时间换空间**”的优化策略。在GPU内存带宽成为瓶颈的场景下，通过牺牲部分计算时间（重复计算）来显著降低内存占用和访问次数，从而使得训练更深、更大的神经网络成为可能。这是现代深度学习框架和编译器中一项至关重要且广泛应用的技术。
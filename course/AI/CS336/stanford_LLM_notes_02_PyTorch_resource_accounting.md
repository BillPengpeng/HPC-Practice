本文主要整理CS336 PyTorch, resource accounting章节的主要内容。

## 1 - Overview

### **课程目标**  
1. **掌握训练模型的核心原语（primitives）**：  
   - 从底层构建模块（张量操作）到高层结构（模型架构、优化器、训练循环）的系统性讲解。  
   - **自底向上（bottom-up）路径**：  
     ```mermaid
     graph LR
         A[张量 Tensors] --> B[模型 Models]
         B --> C[优化器 Optimizers]
         C --> D[训练循环 Training Loop]
     ```

2. **聚焦效率（efficiency）**：  
   - 重点分析如何**优化资源使用**（避免浪费算力与显存）。

---

### **两类关键资源**  
| **资源类型** | **衡量指标** | **优化意义**                  | **实际瓶颈案例**                  |
|--------------|--------------|-----------------------------|----------------------------------|
| **内存（Memory）** | GB（显存容量）  | 决定模型规模与批量大小          | 70B模型+AdamW需≥1.12TB显存（需分布式切分） |
| **计算量（Compute）** | FLOPs（浮点运算量） | 决定训练速度与硬件成本          | 6.3e24 FLOPs训练需1024块H100运行144天 |

---

### **效率优化的核心方向**  
1. **内存优化**：  
   - **技术手段**：  
     - 混合精度训练（`bf16`+`fp32`副本）  
     - 梯度检查点（Gradient Checkpointing）  
     - 优化器状态切分（如ZeRO-Offload）  
   - **目标**：在有限显存下训练更大模型或更大批次。  

2. **计算量优化**：  
   - **技术手段**：  
     - 算子融合（Kernel Fusion）减少数据搬运  
     - 分布式并行策略（数据/模型/流水并行）  
   - **目标**：提升MFU（Model FLOPs Utilization），逼近硬件理论算力。  

---

### **课程价值**  
> “理解资源约束是高效训练的基础”——课程将揭示：  
> - **为什么** 70B模型需要千卡集群？  
> - **如何** 在8卡H100上突破40B参数限制？  
> - **何时** 该优化内存 vs 计算量？  

通过剖析张量→模型→优化器→训练循环的全栈细节，学员将获得**工程落地的系统性能力**。

## 2 - motivating questions

### **问题1：训练70B模型需要多长时间？**
Question: How long would it take to train a 70B parameter model on 15T tokens on 1024 H100s?

#### **计算结果**
- **总计算量**：6.3e24 FLOPs（浮点运算次数）  
  $$
  \text{总计算量} = 6 \times (\text{模型参数量}) \times (\text{训练词元量}) = 6 \times 70 \times 10^9 \times 15 \times 10^{12} 
  $$
- **单卡算力**：H100 实测算力 ≈ 990 TFLOPS（理论值1979 TFLOPS，保守取半）  
- **集群日吞吐**：  
  $$
  \text{日吞吐量} = 1024 \text{卡} \times 990 \times 10^{12} \text{ FLOPs/秒} \times 0.5 \text{ (MFU)} \times 86400 \text{秒} ≈ 4.4 \times 10^{19} \text{ FLOPs/天}
  $$
- **训练时间**：  
  $$
  \text{天数} = \frac{\text{总计算量}}{\text{日吞吐量}} = \frac{6.3 \times 10^{24}}{4.4 \times 10^{19}} ≈ 144 \text{ 天}
  $$

#### **核心原因**
1. **公式 `6 × params × tokens` 的来源**：  
   - Transformer 模型训练时，每个词元（token）在每层需进行两次矩阵运算（前向传播 + 反向传播），且每次计算包含约 `3` 倍参数量级操作（乘加运算算作两次FLOPs）。  
   - 经验公式为：`6 × 参数量 × 词元量`（参考 https://arxiv.org/abs/2001.08361）。
2. **MFU (Model FLOPs Utilization) 取 0.5**：  
   - 实际训练中因通信延迟、数据加载、显存瓶颈等，GPU 无法100%利用理论算力。  
   - 工业界经验值：大规模分布式训练时 MFU 通常为 **30%-60%**（取 0.5 是保守估计）。

#### **单位**

- 在AI模型领域（如“70B模型”），​​B​​ 是 ​​Billion（十亿）​​ 的缩写，即10^9（国际单位制中的“吉”）。
- TFLOPS，Tera Floating-Point Operations Per Second（每秒万亿次浮点运算）。

---

### **问题2：8卡H100能训练的最大模型（AdamW优化器）？**
Question: What's the largest model that can you can train on 8 H100s using AdamW (naively)?"

#### **计算结果**
- **显存占用模型**：  
  $$
  \text{单参数内存} = \underbrace{4}_{\text{参数}} + \underbrace{4}_{\text{梯度}} + \underbrace{(4+4)}_{\text{优化器状态}} = 16 \text{ 字节/参数}
  $$
- **8卡总显存**：  
  $$
  \text{总显存} = 8 \times 80 \text{ GB} = 640 \text{ GB}
  $$
- **最大参数量**：  
  $$
  \text{参数上限} = \frac{640 \times 10^9 \text{ 字节}}{16 \text{ 字节/参数}} = 40 \times 10^9 \text{ (40B)}
  $$

#### **核心原因**
1. **AdamW 的显存开销构成**（关键限制因素）：
   | 组件         | 数据类型 | 每参数字节数 | 作用               |
   |--------------|----------|--------------|--------------------|
   | 参数 (params)    | FP32     | 4 字节        | 存储模型权重       |
   | 梯度 (grads)     | FP32     | 4 字节        | 反向传播计算结果   |
   | 动量 (momentum)  | FP32     | 4 字节        | 优化器状态         |
   | 方差 (variance) | FP32     | 4 字节        | 优化器状态         |
   - **优化器状态占大头**（8字节/参数），直接导致显存需求翻倍。

2. **混合精度训练不省显存**：  
   - 若用 BF16 存储参数/梯度（2字节/参数），仍需保留 FP32 副本（4字节/参数），总显存仍为 **16 字节/参数**（无改善）。

3. **忽略激活值的影响**：  
   - 实际训练中，**激活值（activations）** 的显存占用可能比参数更大（尤其在大批次/长序列时），此处仅为理论上限。

---

### **为什么这些计算重要？**
1. **规模化决策**：  
   - 问题1揭示大模型训练的**硬件成本与时间**（70B模型需千卡集群训练近半年）。  
   - 问题2指导**小团队选型**（8卡H100仅支持40B模型，实际因激活值限制可能更低）。

2. **优化方向**：  
   - **优化器改进**：如使用 **Lion**（显存占用减半）或 **Adafactor**（省去方差状态）。  
   - **显存压缩技术**：  
     - **ZeRO 切分优化器状态**（如 DeepSpeed）可突破单卡限制（https://arxiv.org/abs/1910.02054）。  
     - **激活检查点（Activation Checkpointing）** 降低激活值显存开销。

> 💡 **关键结论**：大模型训练的核心瓶颈是**显存带宽**（数据搬运）和**通信效率**（分布式同步），而非单纯算力。算法优化（如 LoRA 微调）和系统工程（混合并行）比堆硬件更重要。

## 3 - motivating questions
æœ¬æ–‡ä¸»è¦æ•´ç†Assignment 5 (alignment)çš„ä¸»è¦å†…å®¹ã€‚

## 7 Group Relative Policy Optimization

## 7.1 GRPO Algorithm

### ğŸ“Š å†…å®¹æ¦‚å†µ

æœ¬æ®µè¯¦ç»†ä»‹ç»äº†**GRPOï¼ˆGroup Relative Policy Optimizationï¼Œç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼‰ç®—æ³•**ï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºè¯­è¨€æ¨¡å‹è®¾è®¡çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ã€‚å†…å®¹æ¶µç›–äº†**ä¼˜åŠ¿ä¼°è®¡ã€é«˜å±‚ç®—æ³•æµç¨‹å’ŒGRPOç›®æ ‡**ä¸‰ä¸ªæ ¸å¿ƒéƒ¨åˆ†ï¼Œé‡ç‚¹è§£é‡Šäº†GRPOå¦‚ä½•é€šè¿‡**ç»„å½’ä¸€åŒ–å¥–åŠ±**æ¥é¿å…è®­ç»ƒå•ç‹¬çš„ä»·å€¼å‡½æ•°ï¼Œå¹¶ç»“åˆäº†ç¦»çº¿ç­–ç•¥æ¢¯åº¦å’ŒPPOè£å‰ªæœºåˆ¶æ¥å®ç°ç¨³å®šé«˜æ•ˆçš„è®­ç»ƒã€‚

### ğŸ¯ è¦ç‚¹æ€»ç»“

#### 1. **ä¼˜åŠ¿ä¼°è®¡çš„åˆ›æ–°**
- **æ ¸å¿ƒæ€æƒ³**ï¼šä¸ºæ¯ä¸ªé—®é¢˜ä»ç­–ç•¥$Ï€_Î¸$é‡‡æ ·å¤šä¸ªè¾“å‡ºï¼ˆGä¸ªï¼‰ï¼Œåˆ©ç”¨è¿™äº›è¾“å‡ºè®¡ç®—åŸºçº¿
- **é¿å…ä»·å€¼å‡½æ•°**ï¼šä¸éœ€è¦è®­ç»ƒç¥ç»ç½‘ç»œä»·å€¼å‡½æ•°$V_Ï†(s)$ï¼Œæ—¢ç®€åŒ–äº†è®­ç»ƒåˆé¿å…äº†ç³»ç»Ÿå¤æ‚æ€§
- **ç»„å½’ä¸€åŒ–å¥–åŠ±**ï¼šé€šè¿‡åŒä¸€ç»„è¾“å‡ºçš„å¥–åŠ±è®¡ç®—æ ‡å‡†åŒ–ä¼˜åŠ¿

#### 2. **é«˜å±‚ç®—æ³•æµç¨‹**
- å‚è€ƒShaoç­‰äºº2024å¹´çš„å·¥ä½œ
- æ•´ä½“è®­ç»ƒå¾ªç¯åŒ…æ‹¬ï¼šé‡‡æ ·å¤šä¸ªè¾“å‡ºã€è®¡ç®—ç»„å½’ä¸€åŒ–ä¼˜åŠ¿ã€åº”ç”¨GRPOç›®æ ‡æ›´æ–°ç­–ç•¥
- å…è®¸åœ¨å•æ‰¹æ•°æ®ä¸Šè¿›è¡Œå¤šæ¬¡æ¢¯åº¦æ›´æ–°ï¼Œæé«˜æ•°æ®æ•ˆç‡

#### 3. **GRPOç›®æ ‡çš„ä¸‰å¤§æ€æƒ³**
1. **ç¦»çº¿ç­–ç•¥æ¢¯åº¦**ï¼šä½¿ç”¨é‡è¦æ€§é‡‡æ ·ï¼Œå…è®¸ç”¨æ—§ç­–ç•¥æ•°æ®æ›´æ–°å½“å‰ç­–ç•¥
2. **ç»„å½’ä¸€åŒ–ä¼˜åŠ¿**ï¼šé€šè¿‡ç»„å†…è¾“å‡ºçš„å¥–åŠ±å‡å€¼å’Œæ ‡å‡†å·®è®¡ç®—æ ‡å‡†åŒ–ä¼˜åŠ¿
3. **è£å‰ªæœºåˆ¶**ï¼šå€Ÿé‰´PPOçš„è£å‰ªæ€æƒ³ï¼Œé˜²æ­¢ç­–ç•¥æ›´æ–°è¿‡å¤§ï¼Œä¿æŒè®­ç»ƒç¨³å®šæ€§

### ğŸ“ˆ æ‰“å°å…¬å¼

#### å…¬å¼(28)ï¼šç»„å½’ä¸€åŒ–ä¼˜åŠ¿è®¡ç®—
$$
A^{(i)} = \frac{r^{(i)} - \text{mean}(r^{(1)}, r^{(2)}, \ldots, r^{(G)})}{\text{std}(r^{(1)}, r^{(2)}, \ldots, r^{(G)}) + \text{advantage\_eps}}
$$

**ç¬¦å·è¯´æ˜**ï¼š
- $râ½â±â¾$ = $R(q, oâ½â±â¾)$ï¼šç¬¬iä¸ªè¾“å‡ºçš„å¥–åŠ±
- $mean(râ½Â¹â¾, ..., râ½á´³â¾)$ï¼šç»„å†…æ‰€æœ‰è¾“å‡ºå¥–åŠ±çš„å‡å€¼
- $std(râ½Â¹â¾, ..., râ½á´³â¾)$ï¼šç»„å†…æ‰€æœ‰è¾“å‡ºå¥–åŠ±çš„æ ‡å‡†å·®
- $advantage_eps$ï¼šé˜²æ­¢é™¤é›¶çš„å°å¸¸æ•°

**æ³¨æ„**ï¼šè¿™ä¸ªä¼˜åŠ¿$Aâ½â±â¾$åœ¨å“åº”çš„æ¯ä¸ªtokenä¸Šéƒ½ç›¸åŒï¼Œå› æ­¤åœ¨åç»­è®¨è®ºä¸­çœç•¥æ—¶é—´ä¸‹æ ‡tã€‚

### ğŸ”¬ æŠ€æœ¯ç»†èŠ‚æ·±å…¥

#### 1. **ä¼˜åŠ¿ä¼°è®¡çš„å·¥ä½œåŸç†**
```python
def compute_group_normalized_advantage(rewards, advantage_eps=1e-8):
    """
    è®¡ç®—ç»„å½’ä¸€åŒ–ä¼˜åŠ¿
    """
    # è½¬æ¢ä¸ºå¼ é‡
    rewards = torch.tensor(rewards)
    
    # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
    mean_reward = torch.mean(rewards)
    std_reward = torch.std(rewards)
    
    # ç»„å½’ä¸€åŒ–
    advantages = (rewards - mean_reward) / (std_reward + advantage_eps)
    
    return advantages

# ç¤ºä¾‹
rewards = [0.9, 0.8, 0.7, 0.6, 0.5]  # 5ä¸ªè¾“å‡ºçš„å¥–åŠ±
advantages = compute_group_normalized_advantage(rewards)
print(f"ä¼˜åŠ¿å€¼: {advantages}")
```

#### 2. **ä¸PPOçš„å…³ç³»**
GRPOç»“åˆäº†PPOçš„ä¸¤ä¸ªå…³é”®æ€æƒ³ï¼š
```python
ppo_concepts_in_grpo = {
    "è£å‰ªæœºåˆ¶": "é˜²æ­¢æ–°æ—§ç­–ç•¥å·®å¼‚è¿‡å¤§ï¼Œä¿æŒè®­ç»ƒç¨³å®š",
    "é‡è¦æ€§é‡‡æ ·": "å…è®¸ç¦»çº¿ç­–ç•¥å­¦ä¹ ï¼Œæé«˜æ•°æ®æ•ˆç‡", 
    "å¤šæ­¥ä¼˜åŒ–": "åœ¨å•æ‰¹æ•°æ®ä¸Šè¿›è¡Œå¤šæ¬¡æ¢¯åº¦æ›´æ–°"
}
```

#### 3. **ç¦»çº¿ç­–ç•¥æ¢¯åº¦å…¬å¼ï¼ˆæåŠçš„Eq. 27ï¼‰**
è™½ç„¶æ²¡æœ‰å®Œå…¨æ˜¾ç¤ºï¼Œä½†Eq. 27å¾ˆå¯èƒ½æŒ‡çš„æ˜¯**ç¦»çº¿ç­–ç•¥æ¢¯åº¦çš„ä¸€èˆ¬å½¢å¼**ï¼š
$$
\nabla_\theta J(\theta) = \mathbb{E}_{(s,a)\sim \pi_{\theta_{\text{old}}}} \left[ \frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)} \nabla_\theta \log \pi_\theta(a|s) A(s,a) \right]
$$

### ğŸª GRPOç®—æ³•æµç¨‹

```python
def grpo_algorithm():
    """
    GRPOç®—æ³•é«˜å±‚æµç¨‹
    """
    steps = [
        "1. å¯¹æ¯ä¸ªæç¤ºqï¼Œä»å½“å‰ç­–ç•¥Ï€_Î¸é‡‡æ ·Gä¸ªè¾“å‡º{oâ½â±â¾}",
        "2. è®¡ç®—æ¯ä¸ªè¾“å‡ºçš„å¥–åŠ±râ½â±â¾ = R(q, oâ½â±â¾)",
        "3. ä½¿ç”¨å…¬å¼(28)è®¡ç®—ç»„å½’ä¸€åŒ–ä¼˜åŠ¿Aâ½â±â¾",
        "4. æ„å»ºæŸå¤±å‡½æ•°ï¼Œç»“åˆç¦»çº¿ç­–ç•¥æ¢¯åº¦å’Œè£å‰ªæœºåˆ¶",
        "5. åœ¨å•æ‰¹æ•°æ®ä¸Šè¿›è¡Œå¤šæ¬¡æ¢¯åº¦æ›´æ–°",
        "6. é‡å¤ä¸Šè¿°è¿‡ç¨‹ç›´åˆ°æ”¶æ•›"
    ]
    
    return steps
```

### ğŸ“Š GRPOçš„ä¼˜åŠ¿åˆ†æ

```python
grpo_advantages = {
    "ç®€åŒ–æ¶æ„": "æ— éœ€å•ç‹¬çš„ä»·å€¼å‡½æ•°ç½‘ç»œï¼Œå‡å°‘å‚æ•°é‡å’Œè®­ç»ƒå¤æ‚åº¦",
    "ç¨³å®šè®­ç»ƒ": "ç»„å½’ä¸€åŒ–æä¾›è‡ªé€‚åº”çš„åŸºçº¿ï¼Œè£å‰ªé˜²æ­¢ç­–ç•¥çªå˜",
    "æ•°æ®é«˜æ•ˆ": "ç¦»çº¿ç­–ç•¥å…è®¸æ•°æ®é‡ç”¨ï¼Œç»„å†…æ¯”è¾ƒæé«˜æ ·æœ¬æ•ˆç‡",
    "é€‚åº”æ€§å¼º": "é€‚ç”¨äºå„ç§å¥–åŠ±å‡½æ•°ï¼Œç‰¹åˆ«é€‚åˆè¯­è¨€æ¨¡å‹ä»»åŠ¡"
}
```

### ğŸ”§ å®ç°æ³¨æ„äº‹é¡¹

#### 1. **è¶…å‚æ•°é€‰æ‹©**
```python
grpo_hyperparameters = {
    "ç»„å¤§å°G": "é€šå¸¸8-32ï¼Œéœ€å¹³è¡¡è®¡ç®—æˆæœ¬å’Œç»Ÿè®¡å¯é æ€§",
    "è£å‰ªç³»æ•°Îµ": "é€šå¸¸0.1-0.3ï¼Œæ§åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦", 
    "ä¼˜åŠ¿å¸¸æ•°eps": "é€šå¸¸1e-8ï¼Œé˜²æ­¢é™¤é›¶é”™è¯¯",
    "æ‰¹æ¬¡å¤§å°": "æ ¹æ®GPUå†…å­˜å’Œä»»åŠ¡å¤æ‚åº¦è°ƒæ•´"
}
```

#### 2. **å®é™…å®ç°ç¤ºä¾‹**
```python
import torch
import torch.nn.functional as F

def grpo_loss(new_log_probs, old_log_probs, advantages, clip_epsilon=0.2):
    """
    è®¡ç®—GRPOæŸå¤±ï¼ˆç»“åˆé‡è¦æ€§é‡‡æ ·å’Œè£å‰ªï¼‰
    """
    # é‡è¦æ€§æƒé‡
    ratio = torch.exp(new_log_probs - old_log_probs)
    
    # è£å‰ªçš„é‡è¦æ€§æƒé‡
    clipped_ratio = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon)
    
    # è£å‰ªçš„æŸå¤±
    loss = -torch.min(ratio * advantages, clipped_ratio * advantages)
    
    return loss.mean()

# ä½¿ç”¨ç¤ºä¾‹
def compute_grpo_update(batch_data, policy_network, reward_function):
    """
    è®¡ç®—GRPOæ›´æ–°
    """
    prompts, old_outputs, old_log_probs = batch_data
    
    # é‡æ–°é‡‡æ ·æ–°è¾“å‡º
    with torch.no_grad():
        new_outputs = policy_network.sample(prompts, num_samples=8)  # G=8
    
    # è®¡ç®—å¥–åŠ±
    rewards = []
    for prompt, outputs in zip(prompts, new_outputs):
        group_rewards = [reward_function(prompt, output) for output in outputs]
        rewards.append(group_rewards)
    
    # è®¡ç®—ç»„å½’ä¸€åŒ–ä¼˜åŠ¿
    advantages = []
    for group_rewards in rewards:
        group_advantages = compute_group_normalized_advantage(group_rewards)
        advantages.append(group_advantages)
    
    # è®¡ç®—æ–°ç­–ç•¥çš„å¯¹æ•°æ¦‚ç‡
    new_log_probs = policy_network.get_log_probs(prompts, new_outputs)
    
    # è®¡ç®—GRPOæŸå¤±
    loss = grpo_loss(
        new_log_probs.flatten(),
        old_log_probs.flatten(),
        torch.tensor(advantages).flatten()
    )
    
    return loss
```

## 7.1 GRPOä¼˜åŒ–ç›®æ ‡

### ğŸ“Š å†…å®¹æ¦‚å†µ

æœ¬å›¾è¯¦ç»†é˜è¿°äº†**GRPO-Clipç®—æ³•çš„ç›®æ ‡å‡½æ•°**ï¼Œè¿™æ˜¯GRPOç®—æ³•ä¸­å¼•å…¥**è£å‰ªæœºåˆ¶**çš„æ ¸å¿ƒéƒ¨åˆ†ã€‚å†…å®¹ä»å®Œæ•´çš„GRPO-Clipç›®æ ‡å‡½æ•°ï¼ˆå…¬å¼29ï¼‰å¼€å§‹ï¼Œé€æ­¥æ‹†è§£åˆ†ææ¯ä¸ªtokençº§åˆ«çš„ç›®æ ‡ï¼Œå®šä¹‰äº†æ§åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦çš„è£å‰ªå‡½æ•°g(Îµ, Aâ½â±â¾)ï¼ˆå…¬å¼30ï¼‰ï¼Œå¹¶åˆ†æƒ…å†µè®¨è®ºäº†ä¼˜åŠ¿Aâ½â±â¾ä¸ºæ­£æˆ–è´Ÿæ—¶ç›®æ ‡å‡½æ•°çš„è¡Œä¸ºæœºåˆ¶åŠå…¶å¯¹ç­–ç•¥æ›´æ–°çš„çº¦æŸä½œç”¨ï¼Œæœ€ç»ˆæ­ç¤ºäº†è£å‰ªæœºåˆ¶å¦‚ä½•ç¡®ä¿æ–°ç­–ç•¥ä¸è¿‡åº¦åç¦»æ—§ç­–ç•¥ï¼Œä»è€Œä¿æŒè®­ç»ƒç¨³å®šæ€§ã€‚

### ğŸ¯ è¦ç‚¹æ€»ç»“

1. **GRPO-Clipç›®æ ‡å‡½æ•°ç»“æ„**ï¼š
   - æ˜¯GRPOç®—æ³•çš„è£å‰ªç‰ˆæœ¬ï¼Œå€Ÿé‰´äº†PPOçš„è£å‰ªæ€æƒ³
   - åœ¨**æ¯ä¸ªç”Ÿæˆçš„tokençº§åˆ«**å®šä¹‰ç›®æ ‡å‡½æ•°
   - åŒ…å«**é‡è¦æ€§é‡‡æ ·æ¯”**å’Œ**ç»„å½’ä¸€åŒ–ä¼˜åŠ¿**çš„ä¹˜ç§¯

2. **è£å‰ªå‡½æ•°g(Îµ, Aâ½â±â¾)çš„è®¾è®¡**ï¼š
   - è¶…å‚æ•°Îµ>0æ§åˆ¶ç­–ç•¥å¯å˜åŒ–çš„æœ€å¤§å¹…åº¦
   - ä¼˜åŠ¿Aâ½â±â¾ä¸ºæ­£æ—¶ï¼Œè£å‰ªä¸Šç•Œä¸º(1+Îµ)
   - ä¼˜åŠ¿Aâ½â±â¾ä¸ºè´Ÿæ—¶ï¼Œè£å‰ªä¸‹ç•Œä¸º(1-Îµ)
   - è¿™ç§éå¯¹ç§°è£å‰ªé€‚åº”äº†ä¼˜åŠ¿çš„ç¬¦å·

3. **ç›®æ ‡å‡½æ•°çš„è¡Œä¸ºåˆ†æ**ï¼š
   - **ä¼˜åŠ¿ä¸ºæ­£æ—¶**ï¼šé¼“åŠ±å¢åŠ å¯¹åº”tokençš„æ¦‚ç‡ï¼Œä½†è¢«(1+Îµ)ä¸Šé™é™åˆ¶
   - **ä¼˜åŠ¿ä¸ºè´Ÿæ—¶**ï¼šé¼“åŠ±å‡å°‘å¯¹åº”tokençš„æ¦‚ç‡ï¼Œä½†è¢«(1-Îµ)ä¸‹é™é™åˆ¶
   - æœ€ç»ˆæ•ˆæœï¼šé˜²æ­¢å•æ¬¡æ›´æ–°ä¸­ç­–ç•¥å˜åŒ–è¿‡å¤§ï¼Œä¿æŒè®­ç»ƒç¨³å®šæ€§

4. **è£å‰ªæœºåˆ¶çš„ç›´è§‚ç†è§£**ï¼š
   - å½“æ–°ç­–ç•¥å¯¹æŸtokençš„æ¦‚ç‡è¶…è¿‡æ—§ç­–ç•¥çš„(1+Îµ)å€æ—¶ï¼Œç›®æ ‡å‡½æ•°ä¸å†å¢åŠ 
   - å½“æ–°ç­–ç•¥å¯¹æŸtokençš„æ¦‚ç‡ä½äºæ—§ç­–ç•¥çš„(1-Îµ)å€æ—¶ï¼Œç›®æ ‡å‡½æ•°ä¸å†å‡å°‘
   - è¿™å½¢æˆäº†ç­–ç•¥æ›´æ–°çš„"ä¿¡èµ–åŸŸ"ï¼Œé¿å…ç­–ç•¥å´©æºƒ

### ğŸ“ˆ æ‰“å°å…¬å¼

#### å…¬å¼29ï¼šå®Œæ•´çš„GRPO-Clipç›®æ ‡å‡½æ•°
$$
J_{\text{GRPO-Clip}}(\theta) = \mathbb{E}_{q\sim\mathcal{D},\,\{o^{(i)}\}_{i=1}^{G}\sim\pi_{\theta}(\cdot\mid q)}
\left[\frac{1}{G}\sum_{i=1}^{G}\frac{1}{\left|o^{(i)}\right|}\sum_{t=1}^{\left|o^{(i)}\right|}\min\left(
\frac{\pi_{\theta}(o^{(i)}_{t}\mid q,o^{(i)}_{<t})}{\pi_{\theta_{\text{old}}}(o^{(i)}_{t}\mid q,o^{(i)}_{<t})}A^{(i)},
\operatorname{clip}\left(\frac{\pi_{\theta}(o^{(i)}_{t}\mid q,o^{(i)}_{<t})}{\pi_{\theta_{\text{old}}}(o^{(i)}_{t}\mid q,o^{(i)}_{<t})},1-\epsilon,1+\epsilon\right)A^{(i)}
\right)\right]
$$

**å…¬å¼è§£æ**ï¼š
- å¤–å±‚æœŸæœ›ï¼šå¯¹é—®é¢˜qå’ŒGä¸ªè¾“å‡ºoâ½â±â¾é‡‡æ ·
- å†…å±‚å¹³å‡ï¼šå¯¹Gä¸ªè¾“å‡ºå¹³å‡ï¼Œå¯¹æ¯ä¸ªè¾“å‡ºçš„æ‰€æœ‰tokenå¹³å‡
- æ ¸å¿ƒæ˜¯**minæ“ä½œ**ï¼šæ¯”è¾ƒåŸå§‹é‡è¦æ€§é‡‡æ ·æ¯”å’Œè£å‰ªåé‡è¦æ€§é‡‡æ ·æ¯”
- clipå‡½æ•°ï¼šå°†é‡è¦æ€§é‡‡æ ·æ¯”é™åˆ¶åœ¨[1-Îµ, 1+Îµ]èŒƒå›´å†…

#### å…¬å¼30ï¼šè£å‰ªå‡½æ•°g(Îµ, Aâ½â±â¾)çš„å®šä¹‰
$$
g(\epsilon,A^{(i)})=\begin{cases}
(1+\epsilon)A^{(i)} & \text{if } A^{(i)}\geq 0 \\
(1-\epsilon)A^{(i)} & \text{if } A^{(i)}<0
\end{cases}
$$

#### æ¨å¯¼åçš„æ¯ä¸ªtokenç›®æ ‡å‡½æ•°
$$
\text{per-token objective} = \min\left(
\frac{\pi_{\theta}(o^{(i)}_{t}\mid q,o^{(i)}_{<t})}{\pi_{\theta_{\text{old}}}(o^{(i)}_{t}\mid q,o^{(i)}_{<t})}A^{(i)},
g(\epsilon,A^{(i)})
\right)
$$

#### ä¼˜åŠ¿ä¸ºæ­£æ—¶çš„ç®€åŒ–å½¢å¼
$$
\text{per-token objective} = \min\left(
\frac{\pi_{\theta}(o^{(i)}_{t}\mid q,o^{(i)}_{<t})}{\pi_{\theta_{\text{old}}}(o^{(i)}_{t}\mid q,o^{(i)}_{<t})},
1+\epsilon
\right)A^{(i)}
\quad \text{å½“ } A^{(i)}>0
$$

#### ä¼˜åŠ¿ä¸ºè´Ÿæ—¶çš„ç®€åŒ–å½¢å¼
$$
\text{per-token objective} = \min\left(
\frac{\pi_{\theta}(o^{(i)}_{t}\mid q,o^{(i)}_{<t})}{\pi_{\theta_{\text{old}}}(o^{(i)}_{t}\mid q,o^{(i)}_{<t})},
1-\epsilon
\right)A^{(i)}
\quad \text{å½“ } A^{(i)}<0
$$

### ğŸ”¬ æŠ€æœ¯ç»†èŠ‚æ·±å…¥

#### 1. **è£å‰ªæœºåˆ¶çš„æ•°å­¦è¡Œä¸º**
```python
def clipping_behavior(ratio, advantage, epsilon=0.2):
    """è£å‰ªæœºåˆ¶çš„è¡Œä¸ºåˆ†æ"""
    if advantage >= 0:
        # ä¼˜åŠ¿ä¸ºæ­£ï¼šé¼“åŠ±å¢åŠ æ¦‚ç‡ï¼Œä½†æœ‰ä¸Šé™
        clipped_ratio = min(ratio, 1 + epsilon)
        objective = clipped_ratio * advantage
    else:
        # ä¼˜åŠ¿ä¸ºè´Ÿï¼šé¼“åŠ±å‡å°‘æ¦‚ç‡ï¼Œä½†æœ‰ä¸‹é™
        clipped_ratio = min(ratio, 1 - epsilon)  # æ³¨æ„ï¼šratio â‰¥ 0
        objective = clipped_ratio * advantage  # advantageä¸ºè´Ÿï¼Œæ‰€ä»¥objectiveä¸ºè´Ÿ
    
    return objective

# ç¤ºä¾‹ï¼šè§‚å¯Ÿä¸åŒratioä¸‹çš„ç›®æ ‡å€¼
epsilon = 0.2
advantage = 1.0
for ratio in [0.5, 1.0, 1.5, 2.0]:
    obj = clipping_behavior(ratio, advantage, epsilon)
    print(f"ratio={ratio}: objective={obj}")
```

#### 2. **ç›®æ ‡å‡½æ•°çš„å®ç°ç¤ºä¾‹**
```python
import torch
import torch.nn.functional as F

def grpo_clip_loss(new_log_probs, old_log_probs, advantages, epsilon=0.2):
    """
    è®¡ç®—GRPO-ClipæŸå¤±
    """
    # è®¡ç®—é‡è¦æ€§é‡‡æ ·æ¯”
    ratio = torch.exp(new_log_probs - old_log_probs)
    
    # æ ¹æ®ä¼˜åŠ¿ç¬¦å·è®¡ç®—è£å‰ªè¾¹ç•Œ
    upper_bound = torch.where(advantages >= 0, 1 + epsilon, float('inf'))
    lower_bound = torch.where(advantages < 0, 1 - epsilon, float('-inf'))
    
    # è£å‰ªçš„é‡è¦æ€§é‡‡æ ·æ¯”
    clipped_ratio = torch.clamp(ratio, lower_bound, upper_bound)
    
    # è®¡ç®—åŸå§‹ç›®æ ‡å’Œè£å‰ªç›®æ ‡
    surrogate1 = ratio * advantages
    surrogate2 = clipped_ratio * advantages
    
    # å–æœ€å°å€¼ï¼ˆå› ä¸ºæ˜¯æœ€å°åŒ–æŸå¤±ï¼Œä½†è¿™é‡Œæ˜¯æœ€å¤§åŒ–ç›®æ ‡ï¼Œæ‰€ä»¥åŠ è´Ÿå·ï¼‰
    loss = -torch.min(surrogate1, surrogate2)
    
    return loss.mean()

# ä½¿ç”¨ç¤ºä¾‹
def compute_grpo_clip_update(batch_data, policy_network, epsilon=0.2):
    """
    è®¡ç®—GRPO-Clipæ›´æ–°
    """
    prompts, old_outputs, old_log_probs, advantages = batch_data
    
    # è®¡ç®—æ–°ç­–ç•¥çš„å¯¹æ•°æ¦‚ç‡
    new_log_probs = policy_network.get_log_probs(prompts, old_outputs)
    
    # è®¡ç®—GRPO-ClipæŸå¤±
    loss = grpo_clip_loss(
        new_log_probs.flatten(),
        old_log_probs.flatten(),
        advantages.flatten(),
        epsilon
    )
    
    return loss
```

### ğŸª GRPO-Clipçš„ä¼˜åŠ¿

#### ä¸ä¼ ç»ŸPPOçš„å¯¹æ¯”
```python
comparison_with_ppo = {
    "ç›¸ä¼¼ç‚¹": [
        "éƒ½ä½¿ç”¨è£å‰ªæœºåˆ¶é™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦",
        "éƒ½åŸºäºé‡è¦æ€§é‡‡æ ·çš„ç¦»çº¿ç­–ç•¥å­¦ä¹ ",
        "ç›®æ ‡å‡½æ•°ç»“æ„ç±»ä¼¼ï¼ˆminæ“ä½œï¼‰"
    ],
    "GRPO-Clipçš„ç‰¹è‰²": [
        "ä½¿ç”¨ç»„å½’ä¸€åŒ–ä¼˜åŠ¿è€Œéä»·å€¼å‡½æ•°ä¼°è®¡çš„ä¼˜åŠ¿",
        "æ¯ä¸ªè¾“å‡ºçš„æ‰€æœ‰tokenå…±äº«åŒä¸€ä¸ªä¼˜åŠ¿å€¼",
        "ç‰¹åˆ«ä¸ºè¯­è¨€æ¨¡å‹å¤šè¾“å‡ºé‡‡æ ·åœºæ™¯è®¾è®¡"
    ]
}
```

#### åœ¨è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­çš„ä»·å€¼
```python
value_in_lm_training = {
    "ç¨³å®šæ€§": "é˜²æ­¢ç­–ç•¥åœ¨å•æ¬¡æ›´æ–°ä¸­å‰§çƒˆå˜åŒ–ï¼Œé¿å…æ–‡æœ¬è´¨é‡å´©æºƒ",
    "æ•ˆç‡": "å…è®¸åœ¨å•æ‰¹æ•°æ®ä¸Šå¤šæ¬¡æ›´æ–°ï¼Œæé«˜æ•°æ®åˆ©ç”¨ç‡",
    "é€‚åº”æ€§": "é€‚ç”¨äºå„ç§æ–‡æœ¬ç”Ÿæˆä»»åŠ¡å’Œå¥–åŠ±å‡½æ•°è®¾è®¡",
    "ç®€å•æ€§": "æ— éœ€è®­ç»ƒå•ç‹¬çš„ä»·å€¼å‡½æ•°ç½‘ç»œ"
}
```

### ğŸ“Š è¶…å‚æ•°Îµçš„é€‰æ‹©ç­–ç•¥

```python
epsilon_selection_strategies = {
    "å°å€¼(0.1-0.2)": "ä¿å®ˆæ›´æ–°ï¼Œè®­ç»ƒç¨³å®šä½†æ”¶æ•›æ…¢ï¼Œé€‚åˆå¤æ‚ä»»åŠ¡",
    "ä¸­ç­‰å€¼(0.2-0.3)": "å¹³è¡¡ç¨³å®šæ€§å’Œæ”¶æ•›é€Ÿåº¦ï¼Œé€šç”¨é€‰æ‹©",
    "å¤§å€¼(>0.3)": "æ¿€è¿›æ›´æ–°ï¼Œæ”¶æ•›å¿«ä½†ä¸ç¨³å®šï¼Œé€‚åˆç®€å•ä»»åŠ¡",
    "è‡ªé€‚åº”è°ƒæ•´": "è®­ç»ƒåˆæœŸç”¨è¾ƒå¤§Îµæ¢ç´¢ï¼ŒåæœŸå‡å°Îµç²¾ç»†è°ƒä¼˜"
}
```

## 7.2 Implementation

### ğŸ“Š å†…å®¹æ¦‚å†µ

æœ¬é¡µæ˜¯**GRPOï¼ˆGroup Reward Proximal Policy Optimizationï¼‰ç®—æ³•å®ç°**çš„ç¬¬ä¸€éƒ¨åˆ†ï¼Œé‡ç‚¹è®²è§£äº†**ä¼˜åŠ¿è®¡ç®—ï¼ˆç»„å½’ä¸€åŒ–å¥–åŠ±ï¼‰**çš„å…·ä½“å®ç°ã€‚å†…å®¹ä»GRPOè®­ç»ƒå¾ªç¯çš„é«˜å±‚ç†è§£è¿‡æ¸¡åˆ°å…·ä½“å®ç°ï¼Œç‰¹åˆ«è®¨è®ºäº†ä¸¤ç§è®¡ç®—ç»„å½’ä¸€åŒ–å¥–åŠ±çš„æ–¹æ³•ï¼Œå¹¶å¼•ç”¨äº†æœ€æ–°çš„ç ”ç©¶æˆæœã€‚

### ğŸ¯ è¦ç‚¹æ€»ç»“

#### 1. **å®ç°èƒŒæ™¯ä¸è¿ç»­æ€§**
- åœ¨ç†è§£äº†GRPOè®­ç»ƒå¾ªç¯å’Œç›®æ ‡çš„**é«˜å±‚æ¦‚å¿µ**åï¼Œå¼€å§‹å®ç°å…·ä½“ç»„ä»¶
- è®¸å¤šç»„ä»¶åœ¨**SFTï¼ˆç›‘ç£å¾®è°ƒï¼‰å’ŒEIï¼ˆä¸“å®¶è¿­ä»£ï¼‰éƒ¨åˆ†**å·²ç»å®ç°ï¼Œå¯ä»¥å¤ç”¨
- ä½“ç°äº†ç®—æ³•å®ç°çš„**æ¨¡å—åŒ–è®¾è®¡å’Œä»£ç å¤ç”¨**æ€æƒ³

#### 2. **ä¼˜åŠ¿è®¡ç®—çš„æ ¸å¿ƒä»»åŠ¡**
- å®ç°è®¡ç®—**å›æ”¾æ‰¹æ¬¡ä¸­æ¯ä¸ªç¤ºä¾‹çš„ä¼˜åŠ¿**ï¼ˆå³ç»„å½’ä¸€åŒ–å¥–åŠ±ï¼‰çš„é€»è¾‘
- è¿™æ˜¯GRPOè®­ç»ƒå¾ªç¯çš„**ç¬¬ä¸€ä¸ªå…³é”®æ­¥éª¤**

#### 3. **ä¸¤ç§ç»„å½’ä¸€åŒ–æ–¹æ³•å¯¹æ¯”**

##### æ–¹æ³•Aï¼šåŸå§‹æ–¹æ³•ï¼ˆEq. 28ï¼‰
```math
A^{(i)} = \frac{r^{(i)} - \text{mean}(r^{(1)}, r^{(2)}, \ldots, r^{(G)})}{\text{std}(r^{(1)}, r^{(2)}, \ldots, r^{(G)}) + \text{advantage\_eps}}
```
- é€šè¿‡**æ ‡å‡†å·®å½’ä¸€åŒ–**å¤„ç†
- é—®é¢˜ï¼šå¯èƒ½ä¼š**å¥–åŠ±æ‰¹æ¬¡å†…ç­”æ¡ˆæ­£ç¡®æ€§å˜åŒ–è¾ƒå°çš„é—®é¢˜**ï¼Œè¿™å¯èƒ½ä¸æ˜¯ç†æƒ³çš„

##### æ–¹æ³•Bï¼šç®€åŒ–æ–¹æ³•ï¼ˆLiu et al., 2025æå‡ºï¼ŒEq. 31ï¼‰
```math
A^{(i)} = r^{(i)} - \text{mean}(r^{(1)}, r^{(2)}, \ldots, r^{(G)})
```
- **ç§»é™¤äº†å½’ä¸€åŒ–æ­¥éª¤**ï¼Œåªå‡å»å‡å€¼
- é¿å…äº†å¯¹ä½å˜åŒ–é—®é¢˜çš„åå¥½
- è®¡ç®—æ›´ç®€å•ï¼Œå‡å°‘äº†**é™¤ä»¥æ¥è¿‘é›¶çš„æ ‡å‡†å·®**å¯èƒ½å¸¦æ¥çš„æ•°å€¼ä¸ç¨³å®šé—®é¢˜

#### 4. **æ–‡çŒ®å¼•ç”¨ä¸å­¦æœ¯åŸºç¡€**
- å¼•ç”¨äº†**Liuç­‰äºº2025å¹´çš„å·¥ä½œ**ï¼ˆDr. GRPOï¼‰ï¼Œè¿™æ˜¯è¯¥é¢†åŸŸçš„æœ€æ–°ç ”ç©¶
- ç»¿è‰²æ–¹æ¡†é«˜äº®æ˜¾ç¤ºäº†é‡è¦çš„**å‚è€ƒæ–‡çŒ®ä½œè€…**
- çº¢è‰²æ–¹æ¡†é«˜äº®äº†**å…¬å¼ç¼–å·(31)**ï¼Œå¼ºè°ƒäº†è¿™æ˜¯æœ¬èŠ‚çš„æ ¸å¿ƒå…¬å¼

### ğŸ”¬ æŠ€æœ¯ç»†èŠ‚æ·±å…¥

#### åŸå§‹æ–¹æ³•çš„é—®é¢˜åˆ†æ
```python
def analyze_std_normalization_issue():
    """åˆ†ææ ‡å‡†å·®å½’ä¸€åŒ–å¯èƒ½å­˜åœ¨çš„é—®é¢˜"""
    
    issues = [
        "æ•°å€¼ä¸ç¨³å®šï¼šå½“ç»„å†…å¥–åŠ±å·®å¼‚å¾ˆå°æ—¶ï¼Œstdæ¥è¿‘é›¶ï¼Œå¯èƒ½å¯¼è‡´é™¤ä»¥é›¶æˆ–æå¤§å€¼",
        "åå¥½åå·®ï¼šä¼šå¥–åŠ±é‚£äº›ç»„å†…æ‰€æœ‰ç­”æ¡ˆéƒ½è¡¨ç°ä¸€è‡´çš„é—®é¢˜ï¼ˆæ— è®ºå¥½åï¼‰",
        "å°ºåº¦æ•æ„Ÿï¼šå¯¹å¥–åŠ±çš„ç»å¯¹å°ºåº¦æ•æ„Ÿï¼Œå¯èƒ½éœ€è¦é¢å¤–çš„å¥–åŠ±ç¼©æ”¾"
    ]
    
    return issues
```

#### ç®€åŒ–æ–¹æ³•çš„ä¼˜åŠ¿
```python
def simplified_method_advantages():
    """ç®€åŒ–æ–¹æ³•çš„ä¼˜åŠ¿åˆ†æ"""
    
    advantages = [
        "æ•°å€¼ç¨³å®šï¼šé¿å…äº†é™¤ä»¥æ¥è¿‘é›¶çš„æ ‡å‡†å·®",
        "è®¡ç®—é«˜æ•ˆï¼šå‡å°‘äº†ä¸€æ¬¡æ ‡å‡†å·®è®¡ç®—",
        "ç›´è§‚è§£é‡Šï¼šä¼˜åŠ¿å°±æ˜¯ç›¸å¯¹äºå¹³å‡è¡¨ç°çš„åç¦»",
        "æ— ååå¥½ï¼šä¸å¯¹ç»„å†…å˜åŒ–å¤§å°æ–½åŠ åå¥½"
    ]
    
    return advantages
```

### ğŸ”§ å®ç°å»ºè®®

#### 1. **ä¸¤ç§æ–¹æ³•çš„å®ç°ç¤ºä¾‹**
```python
import torch
import numpy as np

def compute_advantages_rewards(rewards, method="simplified", eps=1e-8):
    """
    è®¡ç®—ç»„å½’ä¸€åŒ–ä¼˜åŠ¿
    
    Args:
        rewards: å½¢çŠ¶ä¸º(G,)çš„å¼ é‡ï¼Œè¡¨ç¤ºç»„å†…Gä¸ªæ ·æœ¬çš„å¥–åŠ±
        method: "original" æˆ– "simplified"
        eps: é˜²æ­¢é™¤é›¶çš„å°å¸¸æ•°
    
    Returns:
        ä¼˜åŠ¿å€¼ï¼Œå½¢çŠ¶åŒrewards
    """
    mean_reward = torch.mean(rewards)
    
    if method == "original":
        # åŸå§‹æ–¹æ³•ï¼šé™¤ä»¥æ ‡å‡†å·®
        std_reward = torch.std(rewards)
        advantages = (rewards - mean_reward) / (std_reward + eps)
    elif method == "simplified":
        # ç®€åŒ–æ–¹æ³•ï¼šåªå‡å»å‡å€¼
        advantages = rewards - mean_reward
    else:
        raise ValueError(f"æœªçŸ¥çš„æ–¹æ³•: {method}")
    
    return advantages

# ä½¿ç”¨ç¤ºä¾‹
rewards = torch.tensor([0.9, 0.8, 0.85, 0.87, 0.82])
advantages_original = compute_advantages_rewards(rewards, method="original")
advantages_simplified = compute_advantages_rewards(rewards, method="simplified")

print(f"åŸå§‹æ–¹æ³•ä¼˜åŠ¿: {advantages_original}")
print(f"ç®€åŒ–æ–¹æ³•ä¼˜åŠ¿: {advantages_simplified}")
```

#### 2. **åœ¨è®­ç»ƒå¾ªç¯ä¸­çš„é›†æˆ**
```python
def grpo_training_step(self, batch, use_simplified=True):
    """GRPOè®­ç»ƒæ­¥éª¤ï¼ŒåŒ…å«ä¼˜åŠ¿è®¡ç®—"""
    # é‡‡æ ·å¤šä¸ªè¾“å‡º
    outputs = self.sample_multiple_outputs(batch, num_samples=self.G)
    
    # è®¡ç®—å¥–åŠ±
    rewards = self.compute_rewards(batch, outputs)
    
    # è®¡ç®—ä¼˜åŠ¿
    batch_size = len(batch['prompt'])
    advantages = []
    
    for i in range(batch_size):
        group_rewards = rewards[i]  # å½¢çŠ¶: (G,)
        
        if use_simplified:
            # ä½¿ç”¨ç®€åŒ–æ–¹æ³•
            mean_reward = torch.mean(group_rewards)
            group_advantages = group_rewards - mean_reward
        else:
            # ä½¿ç”¨åŸå§‹æ–¹æ³•
            mean_reward = torch.mean(group_rewards)
            std_reward = torch.std(group_rewards)
            group_advantages = (group_rewards - mean_reward) / (std_reward + 1e-8)
        
        advantages.append(group_advantages)
    
    advantages = torch.stack(advantages)
    
    # åç»­ä½¿ç”¨è¿™äº›ä¼˜åŠ¿è®¡ç®—GRPO-ClipæŸå¤±
    loss = self.compute_grpo_clip_loss(outputs, advantages)
    
    return loss
```

### Problem (compute_group_normalized_rewards): Group normalization (2 points)

Deliverable: Implement a method compute_group_normalized_rewards that calculates raw
rewards for each rollout response, normalizes them within their groups, and returns both the
normalized and raw rewards along with any metadata you think is useful.
- å®Œæˆ

### Problem (compute_naive_policy_gradient_loss): Naive policy gradient (1 point)

Deliverable: Implement a method compute_naive_policy_gradient_loss that computes the
per-token policy-gradient loss using raw rewards or pre-computed advantages.
- å®Œæˆ

### Problem (compute_grpo_clip_loss): GRPO-Clip loss (2 points)
Deliverable: Implement a method compute_grpo_clip_loss that computes the per-token
GRPO-Clip loss.
- å®Œæˆ

### Problem (compute_policy_gradient_loss): Policy-gradient wrapper (1 point)
Deliverable: Implement compute_policy_gradient_loss, a convenience wrapper that dispatches
to the correct loss routine (no_baseline, reinforce_with_baseline, or grpo_clip) and returns
both the per-token loss and any auxiliary statistics.
- å®Œæˆ

### Problem (masked_mean): Masked mean (1 point)
Deliverable: Implement a method masked_mean that averages tensor elements while respecting a
boolean mask.
- å®Œæˆ

### Problem (grpo_microbatch_train_step): Microbatch train step (3 points)
Deliverable: Implement a single micro-batch update for GRPO, including policy-gradient loss,
averaging with a mask, and gradient scaling.
- å®Œæˆ

### Problem (grpo_train_loop): GRPO train loop (5 points)
Deliverable: Implement a complete train loop for GRPO. Begin training a policy on MATH and
confirm that you see validation rewards improving, along with sensible rollouts over time. Provide a
plot with the validation rewards with respect to steps, and a few example rollouts over time.

- é‡‡ç”¨gsm8kä½“éªŒæµç¨‹
```python
Eval n_grpo_idx: 149 correct_num: 1069 error_num: 250
```
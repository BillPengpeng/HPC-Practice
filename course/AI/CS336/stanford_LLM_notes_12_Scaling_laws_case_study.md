本文主要整理CS336 Lecture 11 Scaling laws case study章节的主要内容。

## 1.0 Motivation

1.  **对现有扩展定律的验证与质疑**
    -   要点：探讨著名的“Chinchilla定律”（即模型参数与训练数据应保持平衡）在实际应用中的真实效果与普适性。这反映了业界对现有理论是否放之四海而皆准的审慎态度。

2.  **对计算效率的极致追求**
    -   要点：关注如何在模型训练和拟合过程中**显著节省计算成本**。这是一个非常实际且重要的问题，直接关系到研究的可行性和经济成本。

3.  **对模型架构的前瞻性选择**
    -   要点：思考是否存在某些特定的**模型架构或参数化方法**，其本身天然具备更好的“扩展友好性”，从而在规模增大时能获得更优的性能或效率。这涉及到对未来技术路线的选择。

总而言之，这张图片清晰地指出了在追求更大模型规模的同时，研究者们需要重点关注的几个关键挑战：**理论的有效性、计算的成本、以及架构的可持续性**。

## 2.0 CerebrasGPT μP (the maximum update parametrization)

![CerebrasGPT](https://pic1.zhimg.com/v2-6ef7210836550ab0b600a1f054f574c8_1440w.jpg)
![Hyperparam scaling strategy](https://pic3.zhimg.com/v2-cef2327bc30e12e32dcad7f67e3d3a24_1440w.jpg)

### 内容概况与核心结论

这两张新图片的核心内容是**Cerebras GPT团队关于使用μP参数化方法来改进模型扩展性的实证研究**。

*   **核心结论**：使用**μP参数化** 方法，可以显著提升模型扩展过程的**稳定性和可预测性**。这使得研究者能够通过在小模型上调试出最优超参数，然后直接将其应用到参数量大得多的模型上，而无需重新调试，从而极大地节省了计算成本和时间。
*   **逻辑关系**：这直接回答了第一张图中提出的三个疑问，特别是针对“**是否存在对扩展更友好的模型架构或参数化方法？**”这一问题，给出了肯定的答案（即μP），并展示了其在“**节省计算成本**”和“**验证/改进扩展定律**”方面的实际效果。

---

### 要点总结

1.  **提出了解决方案：μP参数化**
    *   μP是一种特殊的模型参数初始化与缩放方法，旨在解决传统方法在模型规模增大时超参数需要反复调整的问题。

2.  **验证了方法的有效性**
    *   通过对比实验（Cerebras-GPT传统方法 vs. Cerebras-GPT μP）证明，使用μP的模型其性能损失（Test Loss）与扩展定律的预测曲线贴合得更紧密，表现出高度的可预测性。

3.  **μTransfer**
    *   **仅在最小的40M参数模型上精心调试超参数，然后将这套参数直接用于训练高达2.7B参数的模型**。

4.  **指出了技术实现细节**
    *   采用μP需要对基线模型进行一些改动，例如：增加逐元素的激活张量缩放、调整相关层的初始化器、为特定层增加分层学习率缩放等。这些是实现稳定扩展的必要调整。

## 2.1 mμP parametrization

![μP](https://pica.zhimg.com/v2-bda42d5c5c2e8baaa4f96f2a7d3afc60_1440w.jpg)

## 3.0 MiniCPM Techique 1: muP to stabilize scalin

![muP to stabilize scalin](https://picx.zhimg.com/v2-77020e86d7dca36999de0971500923f9_1440w.jpg)

## 3.1 What remains – model size vs data tradeoffs

![model size vs data tradeoffs](https://pica.zhimg.com/v2-e28838bc116efcbc827a30e0da089a4c_1440w.jpg)

成本从n到n²：如果探索n种不同的数据量大小，就需要进行n次独立且完整的“从头训练”。这使得总成本从探索单一规模的O(n) 激增到探索最优权衡的O(n²) 量级。

## 3.2 (partial) solution in miniCPM – WSD learning rate

![WSD learning rate](https://picx.zhimg.com/v2-58f60671502a68ca9ef46ac76baf5c2d_1440w.jpg)

### 内容概况

这张图片介绍了一项名为**WSD学习率调度器** 的创新技术，该技术是miniCPM团队为**高效、低成本地探索深度学习模型的缩放定律** 而提出的部分解决方案。其核心思想是用一个由**预热、稳定和衰减** 三个阶段组成的调度方案，取代业界标准的余弦学习率衰减。这种方法的核心优势在于，它能将探索数据轴缩放定律的计算成本从惊人的**O(n²)** 显著降低到可管理的**O(n)**，从而极大地提升了研究效率。

---

### 要点总结

1.  **解决核心痛点**：WSD旨在解决精确测量缩放定律时面临的**巨大计算成本**问题。传统方法需要为每个待测试的数据量都“从头开始”训练模型，成本随测试点数量呈平方级增长。WSD通过其独特的设计避免了这种重复训练。

2.  **三阶段式设计**：WSD将训练过程清晰划分为：
    *   **预热**：逐步将学习率升至峰值，保证训练初始稳定性。
    *   **稳定**：在峰值学习率下进行一段时间的稳定训练，这是WSD的关键。此阶段末的模型检查点被认为是高质量的基准点。
    *   **衰减**：从稳定阶段的检查点开始，进行快速的余弦衰减，使模型收敛到最终状态。

3.  **实现线性成本探索**：WSD的革命性在于，**只需训练一次模型至稳定阶段结束，然后可以从该检查点出发，快速衰减出对应于不同训练数据量（如10N, 20N, ..., 60N tokens）的最终模型**。这避免了为每个数据量都进行完整的漫长训练。

## 3.3 Side note – other ways of estimating chinchilla curves

### 内容概况

这张幻灯片是介绍了研究者Gadre等人提出的**一种基于曲线拟合来估算Chinchilla最优缩放定律的替代方法**。该方法的核心洞察是：模型训练中因**数据量不足（即“过度训练”，overtraining）而导致的性能“惩罚”是相对稳定的**。基于这一洞察，他们通过数学公式对大量不完整的训练运行结果进行拟合，从而更高效地预测出完整训练后的最终性能，避免了为每个配置都进行成本极高的完整训练。

---

### 要点总结

1.  **提出替代方法**：幻灯片主旨是指出，除了进行大量完整的“从头训练”实验来精确测量缩放定律外，还存在其他（可能计算成本更低的）估算方法，这里介绍的是Gadre等人的曲线拟合法。

2.  **核心理论基石**：该方法的有效性依赖于一个关键观察——**“过度训练惩罚”的稳定性**。这意味着，如果一个模型在固定数据量上训练得过久（超过了其“计算最优”点），其性能损失相对于理想情况的偏离模式是可预测的。

3.  **旨在降低计算成本**：这种方法的目的是通过分析部分训练结果来**外推或预测**完整训练的结果，从而大幅减少探索最优模型规模（N）与数据量（D）组合所需的总计算量。

4.  **建立数学模型**：方法的核心是将模型的最终损失（L）表达为模型参数量（N）和数据量（D）的函数，并通过参数拟合来确定其中的关键常数。

5.  **连接不同公式**：幻灯片底部的数学推导展示了他们提出的新公式与文献中常见的缩放定律公式（如Hoffmann等人提出的公式）之间的等价转换关系，增强了其理论依据。

---

### 解释公式

**公式1：标准的缩放定律模型**
$$ L(N,D) = E + AN^{-\alpha} + BD^{-\beta} $$

*   **L(N, D)**： 表示一个参数量为 **N**、使用数据量为 **D** 的模型所能达到的**最终损失**。损失越低，模型性能越好。
*   **E**： 代表**不可约损失**，可以理解为在无限模型和无限数据下的理想损失，即任务本身固有的难度下限。
*   **AN^{-α}**： 这项是**模型规模不足带来的惩罚**。随着模型参数N增大，这项惩罚会减小。
*   **BD^{-β}**： 这项是**数据量不足带来的惩罚**。随着训练数据D增大，这项惩罚会减小。
*   **A, B, α, β**： 这些都是需要通过实验数据拟合的**常数**。

**公式2：Gadre等人重新参数化的形式**
$$ L(C, M) = E + \left( aM^{\alpha_{C}} + bM^{-\alpha_{C}} \right) C^{-\alpha_{C}} $$

*   **C**： **总计算浮点运算次数**，通常近似正比于 **N × D**。
*   **M**： 是 **模型参数量N** 和 **数据量D** 的比值（**M = N/D**），它描述了固定的总计算预算C在模型规模和数据量之间的“分配策略”。
*   **L(C, M)**： 表示在总计算预算为C、分配策略为M时的最终损失。
*   **公式含义**： 括号内的项 $(aM^{α_C} + bM^{-α_C})$ 捕捉了**分配不当的惩罚**。
    *   当M过小（即数据量D相对过大），$aM^{α_C}$ 项（模型太小惩罚）主导。
    *   当M过大（即数据量D相对过小），$bM^{-α_C}$ 项（数据不足惩罚）主导。
    *   **Chinchilla最优点就是找到使整个括号内项最小的那个M值**。

幻灯片最后一行说明了如何通过变量代换（设 $α_C = α/2$ 等）将公式(4)与上面的标准公式(3)联系起来，证明两者是同一规律的不同数学表达形式。

## 3.4 Tiny models with lots of data

![Tiny models with lots of data](https://pic4.zhimg.com/v2-3394a9221d2b723a26747f905194d733_1440w.jpg)

### 内容概况

这张幻灯片的核心内容是**展示并论证一项关于语言模型缩放定律的重要研究发现：训练“计算最优”模型所需的数据量远高于之前的普遍认知**。幻灯片通过直接对比本研究与经典的Chinchlla定律，指出其发现的最优数据-模型比例高达192:1，而非传统的20:1，并认为像LLaMA这样的现代架构本应使用更高的数据比例。最后，幻灯片以LLaMA 3为例，指出业界趋势已印证这一发现，预示着我们可以突破“20倍模型大小”的经验法则。

---

### 要点总结

1.  **核心发现：更高的最优数据比例**
    *   本研究得出的核心结论是，在“计算最优”的训练范式下，**训练数据量应该是模型参数量的192倍**。这一发现挑战了由Hoffmann等人（2022）在Chinchilla论文中提出的、曾被广泛引用的**20倍经验法则**。

2.  **趋势一致，数值迥异**
    *   幻灯片明确指出，虽然本研究发现的“数据-模型比例随计算预算增加而变化”的**趋势**与Chinchlla定律一致，但二者在具体的**最优比例数值上存在巨大差距**。这表明缩放定律的具体常数可能对模型架构、训练方法或评估方式高度敏感。

3.  **对LLaMA架构的特别说明**
    *   幻灯片特别提到，他们认为**LLaMA架构本应享有更高的数据比例**。这暗示了不同的模型架构（如LLaMA的特定结构）其“数据胃口”可能不同，最优的数据配比需要针对性地重新评估。

4.  **引用内部证据进行佐证**
    *   为增强说服力，作者指出这一发现与本研究报告内第4.3节和图6的观察结果相一致，引导读者查阅更详细的实验数据和论证过程。

5.  **指出业界趋势与未来方向**
    *   幻灯片最后以LLaMA 3为例，指出**最新的业界模型（如LLaMA 3）实际上已经采用了显著更高的数据-模型比例**。这既验证了本研究的发现，也指向了一个重要的未来方向：通过更精细化的优化，我们完全可以突破旧有经验法则的限制，通过为模型提供远超其参数规模的海量数据，来获得更优的性能。

## 4.0 DeepSeek Scaling strategy – batch + LR

![Scaling strategy – batch + LR](https://pic3.zhimg.com/v2-63ebafe30c07f113166b0d03986713ae_1440w.jpg)

首先在较小计算规模（1e17 FLOPs）上进行广泛的网格搜索（grid search），以确定最佳学习率（Learning Rate）和批处理大小（Batch Size）的大致范围。然后，他们在多个不同的计算量级下重复这个过程，并收集那些能达到“接近最优”性能（例如，损失在最小值的 0.25% 以内）的超参数组合 。

## 4.1 Scaling analysis of learning rates

![Scaling analysis of learning rates](https://pic2.zhimg.com/v2-a25451a04f1fc682c4e7687b8b60e953_1440w.jpg)

## 5.0 What is muP, anyway?

![What is muP, anyway?](https://pic4.zhimg.com/v2-bf7fcd12a70d66d54e5c454b49e83c31_1440w.jpg)

1.  **定义核心目标：稳定的训练动态**
    *   muP的根本目标是确保神经网络在**不同宽度（规模）下，其训练动态保持稳定和可预测**。这里的“动态”主要指网络中激活值的尺度变化。

2.  **基于两项关键断言（A1 & A2）**
    *   **A1（初始化稳定性）**：在网络初始化时，每一层的激活值（activation）的幅度应该是 **Θ(1)**。这意味着激活值的大小不应随着网络宽度（$n_l$）的增加而爆炸或消失。
    *   **A2（更新稳定性）**：在进行**一次梯度更新**后，激活值的变化量（the change in activation）也应该是 **Θ(1)**。这确保了训练步骤对网络的影响是可控且一致的，与模型规模无关。

## 5.1 muP mini recap..

![muP mini recap..](https://pic2.zhimg.com/v2-70130efe57a03b12c2b2a7b97aa5b0b1_1440w.jpg)

## 6.0 Recent scaling law recipes

### 内容概况

这张幻灯片系统地汇总和对比了**2023年至2024年末期间，多个知名大模型团队在探索和应用模型缩放定律方面所采用的核心方案**。

---

### 要点总结

1.  **CerebrasGPT方案（理论化与直接应用）**
    *   **核心工具**：采用**μP参数化**，使超参数（如学习率）在不同模型规模下保持稳定，简化缩放过程。
    *   **缩放依据**：直接应用**Chinchilla缩放公式**来计算给定计算预算下的最优模型大小与数据量配比。
    *   **特点**：方案完整、理论性强，依赖于Chinchilla定律的普适性假设。

2.  **DeepSeek方案（实证分析与实用主义）**
    *   **基本假设**：假设大多数Transformer架构的超参数本身对规模变化不敏感。
    *   **关键步骤**：
        *   **缩放分析**：对批次大小和学习率进行实验分析，以确定最优的缩放路径。
        *   **等计算量分析**：通过IsoFLOP分析来确定最佳的模型尺寸。
    *   **效率技巧**：使用**分段线性学习率调度**来低成本地生成进行Chinchilla分析所需的数据点。
    *   **特点**：侧重于通过可控实验来寻找最优解，非常实用。

3.  **miniCPM方案（混合方法）**
    *   **参数化**：与CerebrasGPT类似，使用**μP**来稳定Transformer超参数和学习率。
    *   **分析方法**：采用**分段线性调度**来获取数据，但应用于**Chinchilla方法3**，即基于曲线拟合来预测最优缩放点，而非直接使用公式。
    *   **特点**：结合了μP的理论优势和曲线拟合的灵活性。

4.  **近期方案（LLaMA 3 / Hunyuan, Minimax）- 细节较少**
    *   **LLaMA 3 / Hunyuan**：仅提及使用了**等计算量分析**，没有透露其他缩放细节。这表明该方法是业界基准，但可能辅以未公开的专有技术。
    *   **Minimax**：提到了“**架构选择/决策缩放**”，这暗示其缩放方案可能紧密集成或依赖于特定的模型架构创新，而非通用的训练超参数缩放方法。
    *   **共同特点**：反映了2024年末的趋势，即最前沿的方案细节披露更少，可能更具定制化和创新性。

### 总结

总而言之，这张幻灯片揭示了业界在模型缩放定律应用上的**多种技术路线并存**的局面：既有基于严谨数学框架的**理论化方案**，也有依赖于实验数据的**实证性方案**，还有将两者结合的**混合方案**。而最新的趋势表明，顶级的缩放“配方”可能正变得更加专有化和与特定架构深度集成。

## 7.0 What is muP robust to?

### 内容概况

本部分系统性地评估了**muP参数化方法** 在面对现代大语言模型中常见的各种组件和技术时的**鲁棒性**。

---

### 要点总结

#### **muP表现鲁棒的方面**

1.  **非线性激活函数**：如 **SwiGLU** 和 **Squared ReLU**。这些复杂的激活函数虽然不在muP原始理论推导范围内，但实验显示它们与标准激活函数具有**相同的最优基础学习率**，甚至能带来轻微性能提升。
2.  **批次大小**：无论是比标准**更大（4x）** 还是**更小（4x）** 的批次大小，muP的超参数迁移性依然有效。这表明muP的稳定性对批次大小变化不敏感。
3.  **特殊初始化方法**：如 **SP Unembedding初始化**（一种不同的输出层初始化策略）和 **Zero Query初始化**（将查询矩阵初始化为零，使注意力均匀），均未破坏muP的超参数迁移性。

#### **muP表现不鲁棒的方面**

1.  **RMSNorm的可学习增益**：这是muP的一个**显著失败案例**。当RMSNorm层使用可学习的缩放参数（增益）时，最优学习率无法在不同宽度的模型间可靠迁移。
2.  **奇异优化器**：如 **Lion优化器**。这种基于梯度符号而非幅度进行更新的优化器，与muP的兼容性不佳，尤其是在大模型上会导致训练失败（损失值剧增）。
3.  **强权重衰减**：使用**较强的权重衰减**（如值为0.1）也会破坏muP的超参数迁移性。报告指出这可能是muP“唯一显著的失败”。

---

### 重点解释：muP“不鲁棒”的原因

#### 1. **RMSNorm可学习增益：破坏参数化尺度**
*   **原因**：muP为网络中的每个参数矩阵都设定了与其维度相关的特定初始化尺度。**RMSNorm层的可学习增益（gain）本身也是一个需要训练的参数**。问题在于，这个增益参数的“最佳尺度”通常被设计为与模型宽度无关的 **Θ(1)**。这意味着它的更新幅度不会像其他参数那样随模型宽度缩放。
*   **冲突**：在muP框架下，其他参数的学习率都根据宽度进行了精确缩放，但这个增益参数的学习率缩放规则与整个系统不匹配。这种**尺度上的不匹配**导致当模型宽度变化时，增益参数的训练动态发生紊乱，从而破坏了整体训练的稳定性，使得为小模型找到的最优学习率在大模型上不再有效。

#### 2. **Lion等奇异优化器：更新规则与理论前提不符**
*   **原因**：muP的理论推导通常基于像SGD或Adam这样的标准优化器。这些优化器的更新量与梯度的大小有关。而**Lion优化器只使用梯度的符号（sign）**，完全忽略了梯度的大小。
*   **冲突**：muP通过控制参数初始化尺度和学习率，本质上是在管理**参数更新量（update magnitude）** 的尺度。当优化器只使用符号时，参数更新的幅度被固定（由学习率直接决定），而与梯度幅度（muP试图控制的对象）脱钩。这使得muP赖以维持训练动态稳定的机制失效，因此无法保证超参数在不同规模模型间的迁移性。

#### 3. **强权重衰减：引入额外的尺度干扰**
*   **原因**：权重衰减作为一种正则化手段，会在参数更新时引入一个与参数当前值成正比的收缩项。
*   **冲突**：在muP框架下，参数的初始化和更新幅度是精心校准过的。**强的权重衰减项** 会成为一个“不守规矩”的强力干扰项，其强度可能与其他参数更新量不匹配，从而扰乱muP建立起来的平衡。特别是在模型缩放时，这种干扰的负面影响会被放大，导致为小模型调好的学习率（和权重衰减强度）在大模型上导致训练不稳定或性能下降。


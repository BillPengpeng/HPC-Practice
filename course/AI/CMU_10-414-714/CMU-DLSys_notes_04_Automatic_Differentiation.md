本文主要整理10-414/714 lecture4 - Automatic Differentiation的要点。

## 1.0 How does differentiation fit into machine learning

### **内容概括**
该幻灯片以 **“微分如何融入机器学习”** 为核心主题，系统性地拆解了机器学习算法的三大核心要素：
1. **假设类（Hypothesis Class）**  
   - 定义模型如何从输入 $x$ 映射到输出 $h_\theta(x)$，配有小狗图片示意图（图中未展示具体内容）。
2. **损失函数（Loss Function）**  
   - 使用**交叉熵损失**公式量化预测 $h_\theta(x)$ 与真实标签 $y$ 的误差：  
     $$
     \ell(h_\theta(x), y) = -h_y(x) + \log \sum_{j=1}^{k} \exp(h_j(x))
     $$
3. **优化方法（Optimization Method）**  
   - 采用**梯度下降法**更新模型参数 $\theta$，公式中**梯度项 $\nabla_\theta \ell$ 用红圈标出**：  
     $$
     \theta := \theta - \frac{\alpha}{B} \sum_{i=1}^{B} \nabla_\theta \ell(h_\theta(x^{(i)}), y^{(i)})
     $$

幻灯片底部总结指出：**计算损失函数对参数的梯度是机器学习中最常见的操作**，凸显微分（梯度计算）的核心地位。

---

### **要点总结**
1. **机器学习三要素**  
   - **假设类**：定义模型结构（输入→输出映射）。  
   - **损失函数**：量化预测误差（例：交叉熵损失）。  
   - **优化方法**：通过梯度下降调整参数（依赖梯度计算）。

2. **微分（梯度）的核心作用**  
   - 优化过程中需**反复计算损失函数的梯度** $\nabla_\theta \ell$，以确定参数更新方向。  
   - 梯度是连接损失函数与参数优化的桥梁，使模型能够迭代改进。

3. **关键公式**  
   - **损失函数**：交叉熵损失（多分类任务常用）。  
   - **梯度下降更新**：$\theta$ 沿梯度负方向调整，步长由学习率 $\alpha$ 控制。

4. **设计亮点**  
   - 红色标注梯度项，强调微分在优化中的关键性。  
   - 三要素逻辑清晰：**模型定义 → 误差衡量 → 梯度驱动优化**。

---

### **核心结论**
微分（梯度计算）是机器学习优化的引擎，通过**高效求解损失函数的梯度**，驱动模型参数迭代更新，最终实现预测精度提升。

## 1.1 微分计算

### **内容概括**
#### **1. 数值微分（Numerical Differentiation）**
- **核心方法**：通过微小扰动近似计算梯度
  - **前向差分**：基于定义直接计算偏导数  
    $$ \frac{\partial f(\theta)}{\partial \theta_{i}}=\lim _{\epsilon \rightarrow 0} \frac{f\left(\theta+\epsilon e_{i}\right)-f(\theta)}{\epsilon} $$
  - **中心差分**：更精确的近似（误差阶 $o(\epsilon^2)$）  
    $$ \frac{\partial f(\theta)}{\partial \theta_{i}}=\frac{f\left(\theta+\epsilon e_{i}\right)-f\left(\theta-\epsilon e_{i}\right)}{2 \epsilon} $$
- **缺点**：存在数值误差，计算效率低（需多次函数调用）

#### **2. 梯度检验（Gradient Checking）**
- **应用场景**：验证自动微分算法正确性  
  $$ \delta^{T}\nabla_{\theta}f(\theta)=\frac{f(\theta+\epsilon\delta)-f(\theta-\epsilon\delta)}{2\epsilon}+o(\epsilon^{2}) $$
- **操作步骤**：  
  1. 从单位球随机采样方向向量 $\delta$  
  2. 对比数值梯度与自动微分结果是否满足等式关系

#### **3. 符号微分（Symbolic Differentiation）**
- **原理**：基于求导规则（和、积、链式法则）解析计算梯度  
  $$ \begin{align*} 
  &\text{和法则：}\frac{\partial(f+g)}{\partial\theta}=\frac{\partial f}{\partial\theta}+\frac{\partial g}{\partial\theta} \\ 
  &\text{积法则：}\frac{\partial(f\cdot g)}{\partial\theta}=g\frac{\partial f}{\partial\theta}+f\frac{\partial g}{\partial\theta} \\ 
  &\text{链式法则：}\frac{\partial f(g(\theta))}{\partial\theta}=\frac{\partial f}{\partial g}\cdot\frac{\partial g}{\partial\theta} 
  \end{align*} $$
- **效率问题**：  
  - 示例：$ f(\theta)=\prod_{i=1}^{n}\theta_i $ 的偏导数计算  
    $$ \frac{\partial f}{\partial\theta_k}=\prod_{j\neq k}\theta_j $$
  - 朴素实现需 $n(n-2)$ 次乘法（存在大量冗余计算）

---

### **要点总结**
| **方法**          | **核心思想**                          | **优势**                     | **局限性**                     |
|--------------------|---------------------------------------|------------------------------|--------------------------------|
| **数值微分**       | 通过微小扰动逼近梯度                  | 实现简单，不依赖表达式形式    | 数值误差大，计算效率低（$O(n)$次函数调用） |
| **梯度检验**       | 用数值方法验证自动微分结果            | 可靠性高，适用于算法调试      | 仅用于验证，非实际优化工具      |
| **符号微分**       | 基于数学规则解析求导                  | 结果精确，无截断误差          | 表达式膨胀导致计算冗余（示例中复杂度 $O(n^2)$） |

#### **关键结论**
1. **数值微分**是梯度计算的**基础近似方法**，但效率和精度受限，适合小型问题或验证场景。
2. **梯度检验**是算法实现的**质检工具**，通过随机方向验证自动微分结果的正确性。
3. **符号微分**可精确求导，但需警惕**表达式膨胀**问题——直接应用求导规则可能导致计算量指数级增长（如示例中乘法操作从 $n$ 次增至 $n(n-2)$ 次）。

> 💡 **深层思考**：三种方法共同指向自动微分（AD）的必要性——它结合数值法的计算图与符号法的规则，避免冗余计算，实现高效精确的梯度求解，成为现代深度学习框架的基石。

## 1.2 Limitation of forward mode AD

### **内容概括**
#### **1. 计算图构建（第1张图）**
- **函数定义**：  
  $ y = f(x_1, x_2) = \ln(x_1) + x_1x_2 - \sin x_2 $  
- **计算图结构**：  
  - 输入节点：$ x_1, x_2 $  
  - 中间节点（$ v_1 $ 至 $ v_7 $)：  
    $ v_1 = x_1 $, $ v_2 = x_2 $ → $ v_3 = \ln v_1 $ → $ v_4 = v_1 \times v_2 $ → $ v_6 = v_3 + v_4 $  
    $ v_5 = \sin v_2 $ → $ v_7 = v_6 - v_5 = y $  
- **前向计算轨迹**（$ x_1=2, x_2=5 $)：  
  $ v_3 = \ln 2 \approx 0.693 $, $ v_4 = 10 $, $ v_5 = \sin 5 \approx -0.959 $, $ y \approx 11.652 $

#### **2. 前向模式AD原理（第2张图）**
- **核心目标**：计算 $ \frac{\partial y}{\partial x_1} $  
- **定义导数传播**：$ \dot{v}_i = \frac{\partial v_i}{\partial x_1} $  
- **按拓扑顺序迭代计算**：  
  - $ \dot{v}_1 = 1 $（因 $ v_1 = x_1 $）  
  - $ \dot{v}_2 = 0 $（$ x_2 $ 与 $ x_1 $ 无关）  
  - $ \dot{v}_3 = \dot{v}_1 / v_1 \approx 0.5 $  
  - $ \dot{v}_4 = \dot{v}_1 v_2 + v_1 \dot{v}_2 = 5 $  
  - $ \dot{v}_5 = \dot{v}_2 \cos v_2 = 0 $  
  - $ \dot{v}_6 = \dot{v}_3 + \dot{v}_4 = 5.5 $  
  - $ \dot{v}_7 = \dot{v}_6 - \dot{v}_5 = 5.5 $  
- **结果**：$ \frac{\partial y}{\partial x_1} = \dot{v}_7 = 5.5 $

#### **3. 前向模式AD的局限性（第3张图）**
- **问题场景**：函数 $ f: \mathbb{R}^n \rightarrow \mathbb{R}^k $（常见于 $ k=1 $, $ n $ 较大）  
- **核心缺陷**：  
  计算所有输入的梯度需 $ n $ 次独立前向AD遍历（例如求 $ \nabla_\theta f $ 需 $ n $ 次计算）  
- **结论**：高维输入时效率低下，需转向**反向模式AD**（如反向传播）

---

### **要点总结**
| **主题**                | **关键内容**                                                                 | **技术细节**                                                                 |
|-------------------------|-----------------------------------------------------------------------------|-----------------------------------------------------------------------------|
| **计算图构建**          | - 将函数分解为节点与边的计算结构<br>- 记录中间变量计算路径                  | 输入 $ (x_1, x_2) $ → 中间运算（对数、乘、正弦等）→ 输出 $ y $           |
| **前向模式AD**          | - **沿计算图拓扑顺序逐层求导**<br>- 利用链式法则局部传递导数                | $ \dot{v}_i = \frac{\partial v_i}{\partial x_1} $ 的迭代计算（示例结果：5.5） |
| **前向模式AD局限性**    | - **计算梯度需 $ n $ 次遍历**（$ n $ 为输入维度）<br>- 高维输入时效率极低 | 典型场景：损失函数 $ \ell: \mathbb{R}^d \rightarrow \mathbb{R} $（$ d $ 为参数数量，通常极大） |

#### **核心结论**
1. **前向模式AD适用于低维输入**：当输入维度 $ n $ 较小时，可高效计算单个输入的偏导数（如示例中 $ \partial y / \partial x_1 $）。  
2. **高维场景需反向模式AD**：机器学习中参数维度 $ n $ 常达百万级，前向模式需 $ n $ 次计算，而**反向模式AD只需一次反向遍历即可获得全部梯度**，效率显著提升。  
3. **计算图是自动微分的基础**：无论前向或反向模式，均依赖计算图分解复杂函数，实现导数的自动化、模块化计算。

> 💡 **深层理解**：前向模式AD本质是**对输入变量的方向导数计算**（沿 $ x_1 $ 方向），而反向模式AD则直接计算**输出对输入的梯度**。这一差异决定了二者在机器学习的互补性：前向模式适合Jacobian矩阵的行计算，反向模式适合梯度（列向量）计算。

## 2.0 Reverse AD algorithm

### **内容概括**
#### **1. 多路径梯度推导（第1张图）**
- **核心问题**：  
  节点 $ v_1 $ 通过**两条独立路径**影响输出 $ y $（$ v_1 \to v_2 \to v_4 $ 和 $ v_1 \to v_3 \to v_4 $）。
- **梯度分解公式**：  
  $$
  \overline{v_1} = \frac{\partial y}{\partial v_1} = \underbrace{\overline{v_2} \frac{\partial v_2}{\partial v_1}}_{\text{路径 } v_2} + \underbrace{\overline{v_3} \frac{\partial v_3}{\partial v_1}}_{\text{路径 } v_3}
  $$
- **偏共轭（Partial Adjoint）定义**：  
  对任意节点对 $ (i,j) $，定义 $ \overline{v_{i \to j}} = \overline{v_j} \frac{\partial v_j}{\partial v_i} $（表示节点 $ i $ 通过直接边 $ i \to j $ 对输出的贡献）。
- **节点总梯度计算**：  
  $$
  \overline{v_i} = \sum_{j \in \text{next}(i)} \overline{v_{i \to j}}
  $$
  > **关键思想**：将复杂路径的梯度拆解为**各条边的偏共轭之和**，实现模块化计算。

#### **2. 反向AD算法实现（第2张图）**
- **算法目标**：计算输出对输入的梯度（$\nabla_\theta f$）。
- **核心数据结构**：  
  `node_to_grad` 字典，存储每个节点的**偏共轭列表**（初始时输出节点伴随值为 `[1]`）。
- **计算步骤**：  
  1. **逆拓扑序遍历**所有节点（从输出向输入反向传播）。  
  2. **聚合当前节点的偏共轭**：  
     $$
     \overline{v_i} = \sum \text{(node\_to\_grad[i])}
     $$  
  3. **向输入节点传播**：  
     对每个输入节点 $ k $，计算偏共轭 $ \overline{v_{k \to i}} = \overline{v_i} \frac{\partial v_i}{\partial v_k} $，并追加到 `node_to_grad[k]`。  
  4. **返回输入节点梯度**：最终 `node_to_grad[input]` 即为所求。

---

### **要点总结**
| **主题**               | **核心贡献**                                                                 | **技术细节**                                                                 |
|------------------------|-----------------------------------------------------------------------------|-----------------------------------------------------------------------------|
| **多路径梯度理论**     | 提出**偏共轭分解法**，解决变量复用场景的梯度计算问题                          | - 总梯度 = 所有下游路径偏共轭之和<br>- 公式：$ \overline{v_i} = \sum_{j \in \text{next}(i)} \overline{v_j} \frac{\partial v_j}{\partial v_i} $ |
| **反向AD算法**         | 实现高效梯度计算的**通用框架**                                              | - **逆拓扑序遍历**确保梯度无遗漏传播<br>- **偏共轭列表**存储多路径贡献<br>- **动态追加**实现梯度聚合 |

#### **关键结论**
1. **多路径梯度分解**是反向AD的理论基础：  
   - 通过偏共轭 $ \overline{v_{i \to j}} $ **隔离不同路径的贡献**，避免重复计算。  
   - 节点总梯度只需对直接下游节点的偏共轭求和（**局部性原理**）。  
2. **反向AD算法的高效性**：  
   - **一次反向遍历**即可计算所有参数的梯度（对比前向AD需 $ n $ 次遍历）。  
   - 通过 `node_to_grad` 字典动态管理多路径梯度，**天然支持计算图的分支结构**。  
3. **工程实现核心**：  
   - **逆拓扑序**保证梯度从输出层向输入层无循环传播。  
   - 偏共轭列表的**追加-求和**机制是处理变量复用的关键（如 $ v_1 $ 被 $ v_2, v_3 $ 复用）。

> 💡 **深层关联**：  
> 多路径推导公式中的 $ \overline{v_1} = \sum \overline{v_{1 \to j}} $ 直接对应反向AD算法中 `node_to_grad[v1]` 的求和操作——理论推导与算法实现在此完美统一。

## 2.1 Reverse mode AD on Tensors

### **内容概括**
#### **1. 问题场景**
- **目标**：计算标量输出 $ y $ 对输入张量 $ X $（矩阵）和 $ W $（矩阵）的梯度。
- **计算路径**：  
  $ X, W \xrightarrow{\text{matmul}} Z = XW \xrightarrow{f} v = f(Z) \to y $（$ v $ 为中间标量，$ y $ 为最终输出标量）。

#### **2. 核心概念：张量伴随（Adjoint）**
- **定义**：  
  对任意张量节点（如 $ Z $)，其伴随矩阵 $ \bar{Z} $ 的元素为输出 $ y $ 对该节点元素的偏导数：  
  $$
  \bar{Z}_{ij} = \frac{\partial y}{\partial Z_{ij}}
  $$
- **物理意义**：  
  $ \bar{Z} $ 量化了 $ Z $ 的每个元素对最终输出 $ y $ 的影响程度。

#### **3. 反向梯度计算**
- **标量形式推导**（元素级计算）：  
  $$
  \overline{X_{i,k}} = \sum_j \frac{\partial Z_{i,j}}{\partial X_{i,k}} \bar{Z}_{i,j} = \sum_j W_{k,j} \bar{Z}_{i,j}
  $$
  - **关键步骤**：通过链式法则，将 $ X $ 的梯度拆解为 $ Z $ 的伴随 $ \bar{Z} $ 与权重 $ W $ 的组合。
- **矩阵形式简化**（高效实现）：  
  $$
  \bar{X} = \bar{Z} W^T
  $$
  - **本质**：将标量形式的求和转化为矩阵乘法，避免逐元素计算。

---

### **要点总结**
| **主题**                | **核心内容**                                                                 | **意义**                                                                 |
|-------------------------|-----------------------------------------------------------------------------|--------------------------------------------------------------------------|
| **张量伴随定义**        | $ \bar{Z}_{ij} = \frac{\partial y}{\partial Z_{ij}} $                     | 建立张量节点与输出标量的梯度关联                                        |
| **反向传播标量形式**    | $ \overline{X_{i,k}} = \sum_j W_{k,j} \bar{Z}_{i,j} $                     | 揭示梯度计算本质：**权重 $ W $ 与伴随 $ \bar{Z} $ 的线性组合**       |
| **反向传播矩阵形式**    | $ \bar{X} = \bar{Z} W^T $                                                | **工程优化关键**：通过矩阵乘法并行化计算，复杂度从 $ O(n^3) $ 降至 $ O(n^2) $ |
| **计算图结构**          | 输入 $ (X, W) \to $ 矩阵乘法 $ \to Z \to $ 函数 $ f \to $ 输出 $ y $ | 可视化正向计算路径，反向传播沿相同路径逆向求导                          |

#### **关键结论**
1. **张量伴随是梯度传播的载体**：  
   - 反向AD通过伴随矩阵 $ \bar{Z} $ **传递梯度信息**，替代了标量链式法则的逐元素计算。
2. **矩阵形式的高效性**：  
   - 公式 $ \bar{X} = \bar{Z} W^T $ 是**深度学习框架的基石**（如PyTorch/TensorFlow的自动微分）。  
   - 将梯度计算转化为矩阵运算，充分利用GPU并行能力。
3. **与标量反向AD的统一性**：  
   - 矩阵形式本质是**多路径梯度聚合**（标量形式中的求和 $ \sum_j $）的向量化表达，符合此前多路径偏共轭理论（$ \overline{v_i} = \sum_j \overline{v_{i \to j}} $）。

> 💡 **深层关联**：  
> 图中公式 $ \bar{X} = \bar{Z} W^T $ 可视为多路径理论的张量版本——$ W^T $ 的每一列对应一条从 $ Z $ 到 $ X $ 的路径梯度贡献，矩阵乘法实现了所有路径的梯度聚合。
本文主要整理《DeepSeek-V3 Technical Report》的主要内容。

## 13.0 - Post-Training Supervised Fine-Tuning

### **内容概括**

这两张图片系统地阐述了DeepSeek-V3模型在完成预训练后，进入的**监督微调**阶段。这一阶段的核心目标是利用高质量的指令数据，将基础的预训练模型（Base Model）塑造成一个擅长理解和遵循用户指令的对话模型。

其工作分为两大主线：
1.  **高质量数据构建**：这是SFT成功的基石。团队为不同领域的任务定制了数据生成方法。对于**逻辑推理、数学、代码**等复杂任务，借助已具备强大“长思考链”推理能力的内部DeepSeek-R1模型来生成数据。同时，通过一套包含 **“SFT→强化学习（RL）→高质量拒绝采样”** 的专家模型训练流程，平衡了R1数据的高准确性和理想格式的清晰简洁性。对于**非推理任务**（如创意写作、角色扮演），则主要依赖DeepSeek-V2.5生成并由人工审核。
2.  **微调训练执行**：使用精心构建的、覆盖多领域的**150万条指令数据**，对DeepSeek-V3-Base模型进行了**两个周期的微调**。训练采用了余弦衰减学习率调度，并实施了**样本掩码策略**，以确保在序列打包时不同训练样本间的信息隔离。

### **要点总结**

| 模块 | 核心要点 |
| :--- | :--- |
| **1. 整体数据规模** | 构建了包含 **1.5M（150万）个实例** 的指令微调数据集，涵盖多个领域。 |
| **2. 推理数据生成** | • **来源**：利用内部**DeepSeek-R1模型**生成。<br>• **挑战**：R1数据准确率高，但存在**过度思考、格式不佳、内容冗长**的问题。<br>• **解决方案**：通过训练**领域专家模型**（SFT+RL）作为数据生成器，并结合**拒绝采样**，最终得到**既准确又清晰简洁**的训练数据。 |
| **3. 非推理数据处理** | 对于**创意写作、角色扮演、简单问答**等数据，使用 **DeepSeek-V2.5** 生成响应，并依赖**人工标注员**进行验证和修正。 |
| **4. 微调训练设置** | • **周期**：使用SFT数据集进行**2个周期**的微调。<br>• **学习率**：采用**余弦衰减**，从 **5e-6** 逐渐降至 **1e-6**。<br>• **序列打包**：将多个样本打包成单个训练序列以提高效率，但通过**样本掩码策略**确保样本间不可见，防止信息泄露。 |

## 13.1 - Reinforcement Learning Reward Model

### **内容概括**

此部分阐述了DeepSeek-V3在强化学习阶段所使用的**混合奖励模型策略**。为了确保反馈信号的**可靠性**并覆盖**多样化的任务类型**，系统并未依赖单一的奖励模型，而是并行采用了两种不同原理的奖励模型：

1.  **基于规则的奖励模型**：针对**答案具有确定性、可验证性**的任务（如数学计算、编程题）。该模型通过预设的规则（如格式匹配、代码编译与测试）自动判断答案正确与否，提供**客观、可重复、抗操纵**的奖励信号。
2.  **基于模型的奖励模型**：针对**开放性、创意性**或**缺乏唯一标准答案**的任务（如创意写作、开放式问答）。该模型本身是一个经过训练的神经网络，能够根据问题和回答的综合输入来评估回答质量。为了提升其可靠性和可解释性，训练数据中包含了导致最终评判的**思维链**。

### **要点总结**

| 奖励模型类型 | 核心机制 | 适用任务类型 | 优势与特点 |
| :--- | :--- | :--- | :--- |
| **基于规则的RM** | **自动化规则验证**。例如：<br>• **数学题**：要求模型将最终答案置于特定格式（如`\boxed{}`）中，通过规则提取并比对。<br>• **编程题**：使用编译器运行测试用例，根据通过率给出反馈。 | **答案确定、可验证的任务**：<br>• 数学计算<br>• 编程（LeetCode类）<br>• 其他有明确真值的问题 | • **可靠性高**：结果客观，不受主观判断影响。<br>• **抗操纵性强**：模型难以通过“花言巧语”骗取高分，必须给出正确答案。<br>• **无需训练**：基于预定逻辑，开发后直接应用。 |
| **基于模型的RM** | **神经网络评估**。模型以**（问题，回答）** 作为输入，输出一个代表回答质量的奖励分数。<br>• **训练基础**：从DeepSeek-V3的SFT检查点开始训练。<br>• **数据增强**：使用包含**思维链**的偏好数据进行训练，即数据不仅提供最终偏好判断，还包含了得出该判断的**推理过程**。 | **开放性或无确定答案的任务**：<br>• 创意写作<br>• 角色扮演<br>• 开放式问答<br>• 文本摘要等 | • **灵活性高**：可以处理复杂、主观的评价任务。<br>• **缓解奖励黑客**：包含思维链的训练数据使奖励模型的决策过程更透明、更符合人类价值观，降低了模型通过“投机”方式骗取高奖励的风险。<br>• **可泛化**：能够对未见过的回答类型进行评估。 |

## 13.2 - Group Relative Policy Optimization

### **内容概括**

该图片节选自论文强化学习部分，详细介绍了DeepSeek-V3所采用的 **“组相对策略优化”** 算法。该算法继承自DeepSeek-V2，其核心创新在于**摒弃了传统强化学习中需要额外训练一个与策略模型同等规模的“评论家”模型的做法**。

GRPO的核心原理是：对于一个给定的问题，从当前的策略模型中采样生成**一组** 回答，然后利用这组回答**内部**的奖励分数来互相评估，计算出每个回答的“相对优势”。优化目标是在鼓励高优势回答的同时，通过裁剪机制保证训练稳定性，并引入KL散度惩罚来防止模型过度偏离参考模型（通常为SFT模型）的行为分布。此外，在RL训练过程中融入来自编码、数学、写作等多领域的提示，被证明能更好地对齐人类偏好，并在监督微调数据有限的场景下显著提升模型在基准测试上的性能。

### **要点总结**

1.  **核心目标**：在强化学习阶段优化模型，使其输出更符合人类偏好。
2.  **关键创新**：**无需训练独立的“评论家”模型**，极大降低了计算成本和训练复杂性。
3.  **核心机制**：**组内相对评估**。通过比较同一问题下**一组输出答案的奖励分数**，来估计每个答案的相对质量（优势值）。
4.  **训练过程**：
    *   **采样**：对一个问题，用当前策略模型生成 $G$ 个答案。
    *   **评分**：用奖励模型为每个答案打分。
    *   **计算优势**：在组内标准化这些分数，得到每个答案的优势值 $A_i$。
    *   **优化**：通过最大化目标函数 $J_GRPO$ 来更新策略模型，该函数鼓励模型增加高优势答案的产生概率，并约束其不过度偏离原始行为。
5.  **领域覆盖**：使用**多领域提示**进行RL训练，以全面提升模型在各种任务上的能力。

### **公式解释**

公式定义了GRPO的完整优化目标。

**1. 核心优势函数（公式28）**
$$ A_{i}=\frac{r_{i}-\text{mean}(\{r_{1},r_{2},\cdots,r_{G}\})}{\text{std}(\{r_{1},r_{2},\cdots,r_{G}\})} $$
*   **$r_i$**：第 $i$ 个答案的原始奖励分数。
*   **$mean(...)$**：计算该组所有 $G$ 个答案奖励分数的**平均值**。
*   **$std(...)$**：计算这组分数的**标准差**。
*   **$A_i$**：第 $i$ 个答案的**优势值**。它表示该答案的奖励**高于或低于组内平均水平的程度**，并经过了标准化（以标准差为单位）。$A_i$ > 0 表示该答案优于组内平均水平；$A_i$ < 0 则表示劣于平均水平。**这一计算完全在组内进行，无需外部基准。**

**2. KL散度项（公式27）**
$$ \mathbb{D}_{KL}\left(\pi_{\theta}||\pi_{ref}\right)=\frac{\pi_{ref}(o_{i}|q)}{\pi_{\theta}(o_{i}|q)}-\log\frac{\pi_{ref}(o_{i}|q)}{\pi_{\theta}(o_{i}|q)}-1 $$
*   **$π_θ$**：正在被优化的**新策略模型**。
*   **$π_ref$**：**参考模型**，通常是监督微调后的模型，代表“安全”、“良好”的初始行为。
*   **$D_KL$**：衡量新策略模型 $π_θ$ 与参考模型 $π_ref$ 之间概率分布的差异。**最小化此项是为了防止强化学习过程“走火入魔”**，确保模型在提升奖励的同时，不会过度偏离其原有的、符合人类预期的表达方式。$β$ 是控制此惩罚项权重的超参数。

**3. 总体目标函数（公式26）**
$$ \mathcal{J}_{\text{GRPO}}(\theta)=\mathbb{E}\left[\frac{1}{G}\sum_{i=1}^{G}\left(\min\left(\frac{\pi_{\theta}(o_{i}|q)}{\pi_{\theta_{\text{old}}}(o_{i}|q)}A_{i},\text{clip}\left(\frac{\pi_{\theta}(o_{i}|q)}{\pi_{\theta_{\text{old}}}(o_{i}|q)},1-\varepsilon,1+\varepsilon\right)A_{i}\right)-\beta\mathbb{D}_{KL}\left(\pi_{\theta}||\pi_{ref}\right)\right)\right] $$
这是需要**最大化**的目标。它包含两个主要部分：
*   **策略比裁剪项**：$min( (π_θ/π_old) * A_i, clip(π_θ/π_old, 1-ε, 1+ε) * A_i )$
    *   **$π_θ/π_old$**：新策略与旧策略产生某个答案的概率比。比值>1表示新策略更倾向于该答案。
    *   如果优势 $A_i$ 为正，我们希望增加该答案的概率（即让比值增大）。
    *   **$clip$函数**：将策略比限制在 $[1-ε, 1+ε]$ 之间。这**防止了单次更新中策略发生剧变**，是保障训练稳定的关键技术。$ε$ 是超参数。
    *   **$min$操作**：在原始比率和裁剪后的比率之间取最小值，构成一个**悲观（保守）估计**，进一步稳定训练。
*   **KL惩罚项**：$-β * D_KL(π_θ||π_ref)$
    *   如前所述，此项用于约束模型不要偏离参考模型太远。

**总结**：GRPO通过巧妙的**组内相对评估**替代了传统的Critic模型，并结合**策略裁剪**和**KL约束**，形成了一个高效、稳定且节省资源的强化学习框架。这是DeepSeek-V3能够通过RL阶段进一步优化其对话能力的关键算法保障。

本文主要整理《Qwen3-VL Technical Report》的主要内容。

## 1 - Abstract

- **定位**：Qwen 系列目前最强的视觉语言模型，支持文本、图像、视频的多模态输入与输出。
- **核心能力**：
  - 原生支持 **256K 长上下文**（文本与视频）。
  - 在多模态基准测试（如 MMMU、MathVista）中达到领先水平。
  - 在纯文本理解、多视角文本理解、跨模态推理（单图/多图/视频）方面表现突出。
- **关键技术升级**：
  - 增强的 **多视角-旋转位置编码（MRoPE-Interleave）**。
  - **深度堆叠集成**与多层级视觉特征融合，加强视觉-语言对齐。
  - **基于文本的时间对齐技术**（用于视频理解），提升时间定位精度。
- **训练优化**：
  - 采用平方根权重偏置平衡文本与多模态学习目标。
  - 支持 256K 上下文预训练，并提供“非思考”与“思考”两种后训练变体以适应不同场景。
- **应用前景**：旨在成为图像推理、智能体决策、多模态代码智能等实际工作流的基础引擎。

## 2 - Introduction

### **1. 研发背景与目标**
*   **背景**：视觉语言模型快速发展，应用场景（如长上下文理解、STEM推理、智能体工作流）日益扩展。
*   **核心要求**：在扩展多模态能力的同时，**必须保持或超越纯文本模型的语言精通度**。
*   **目标**：推出Qwen3-VL，在通用和高级应用上取得进步，成为一个强大的多模态智能平台。

### **2. 模型基础与系列**
*   **基础**：基于Qwen3系列大语言模型构建。
*   **模型规模**：包含四个密集模型（2B/4B/8B/32B）和两个混合专家模型（30B-A3B/235B-A22B）。
*   **关键特性**：支持长达**256K令牌的上下文窗口**，以实现长上下文理解。
*   **模型变体**：发布了 **“非思考”** 和 **“思考”** 两种变体，后者在复杂推理任务上表现更优。

### **3. 三大架构创新**
1.  **增强的位置编码（交错式M-RoPE）**：改进了Qwen2.5-VL中的MRoPE方案，通过交错式设计均匀分布时空维度，解决了频谱不平衡问题，从而实现了对长视频更忠实的位置表征。
2.  **用于跨层融合的DeepStack机制**：将视觉编码器不同层的特征，通过轻量级残差连接路由到LLM的对应层，增强了多层级视觉-语言对齐，且不增加额外上下文长度。
3.  **显式视频时间戳**：用显式的时间戳令牌来标记帧组，取代了之前基于位置编码的绝对时间对齐方案，提供了更简单、更直接的时间表征。

### **4. 训练数据与优化**
*   **总体策略**：全面改革训练数据的**质量、多样性和结构**。
*   **关键升级**：
    *   增强的标注监督。
    *   扩展的全域识别和OCR覆盖。
    *   标准化的3D/空间推理定位。
    *   新增代码、长文档、时序定位视频语料。
    *   引入思维链推理及高质量GUI-智能体交互数据，以**桥接感知、推理与行动**。

### **5. 训练流程（两阶段）**
*   **预训练**：分为四步进行——
    1.  热身对齐阶段：仅更新视觉-语言投影层。
    2.  逐步扩大上下文窗口（8K, 32K, 256K）的全参数训练。
*   **后训练**：分为三步——
    1.  在长思维链数据上进行监督微调。
    2.  从更强的教师模型进行知识蒸馏。
    3.  强化学习。

## 3.0 - Model Architecture

![qwen3](https://pic1.zhimg.com/v2-6f07e67e4f73341ab2494e2eb67a2ef4_1440w.jpg)

### **1. 整体架构设计（三模块）**
Qwen3-VL沿用了成熟的设计范式，由三个核心组件串联而成：
1.  **视觉编码器 (Vision Encoder)**：负责将输入的图像或视频帧转换为视觉特征。
2.  **视觉-语言融合器 (Vision-Language Merger)**：一个基于MLP（多层感知机）的模块，用于将视觉特征“翻译”或“映射”为大型语言模型能够理解的视觉标记。
3.  **大型语言模型 (LLM Decoder)**：基于Qwen3的文本解码器，接收视觉标记和文本标记，执行最终的理解和生成任务。

### **2. 模型规模与变体**
*   提供了灵活的模型系列，包括：
    *   **密集型 (Dense)** 变体：参数量为2B、4B、8B、32B。
    *   **混合专家 (MoE)** 变体：包括30B-A3B和**旗舰模型**235B-A22B。
*   **旗舰模型特点**：Qwen3-VL-235B-A22B拥有2350亿总参数，但通过MoE技术，每个Token仅激活220亿参数，在保持强大能力的同时提升了计算效率。

### **3. 处理流程（由图1详解）**
架构图清晰地展示了从输入到输出的端到端流程：
*   **输入**：系统可以同时处理**多张图片**和**多个视频**。
*   **视觉编码**：视觉编码器处理**动态分辨率**的输入，为每张图片或视频帧生成**可变长度**的视觉标记序列（图中举例：Picture 1生成11427个标记，Picture 2生成8个，Picture 3生成1125个）。
*   **标记融合**：这些视觉标记与用户的文本指令标记一起，被输入给**Qwen3语言模型解码器**（可以是密集或MoE版本）。
*   **输出**：解码器最终生成回答文本。

### **4. 核心设计思想**
这一架构体现了 **“分离与协同”** 的思想：专有的模块（视觉编码器、融合器）处理模态对齐，而强大的通用底座（Qwen3 LLM）则专注于核心的语言理解和推理。这种设计有利于继承和发挥基座模型在文本方面的强大能力，并专注于提升多模态融合效果。

## 3.1 - Interleaved MRoPE

![Interleaved MRoPE 对比 MRoPE 区别](https://zhuanlan.zhihu.com/p/1983865217701152574)

1.  **技术背景与问题**：
    *   Qwen2-VL 引入了 **MRoPE** 来为多模态输入（图像、视频）建模位置信息。
    *   其原始设计将嵌入维度分割为**时间（t）、水平（h）、垂直（w）** 三个子空间，并为每个子空间分配了不同的旋转频率。
    *   这导致了**频率频谱不平衡**，已被证明会损害模型在**长视频理解**任务上的性能。

2.  **解决方案（核心创新）**：
    *   Qwen3-VL 重新设计了频率分配方案，采纳了 **交错（Interleaving）** 策略。
    *   具体做法是：将 **t、h、w** 这三个分量在嵌入维度中**交错排列**，而非简单地分区。
    *   这使得每一个时空轴（无论是时间还是空间维度）都能在**低频和高频波段**中得到均匀的表征。

3.  **改进效果**：
    *   由此产生的**平衡频谱**缓解了原有的频谱偏差。
    *   最终**显著改善**了模型对于视频的**长程位置建模能力**，这是支撑复杂视频理解的关键基础。

**总而言之**，这项改进是针对多模态模型处理长序列输入（尤其是视频）时的一个关键技术优化，旨在使模型能更公平、有效地感知和理解输入内容在时间和空间上的长远依赖关系。

## 3.2 - DeepStack

### **内容概括**

**从视觉编码器（ViT）的中间层提取多层级视觉特征，并将其注入大型语言模型（LLM）的多个对应层中**。这一设计是对原始DeepStack方法的扩展，旨在更有效地融合不同抽象层次的视觉信息，以增强模型的视觉-语言对齐和理解能力。

---

### **要点总结**

#### **1. 技术来源与核心思想**
*   **灵感来源**：借鉴了**DeepStack**（Meng等人，2024年）的核心思想。
*   **核心思想**：将视觉信息（视觉标记）注入到LLM的**多个层**中，而非仅在初始输入层融合。这允许视觉信息在LLM的深层推理过程中持续发挥作用。

#### **2. 对原始方法的改进与扩展**
*   **原始DeepStack**：通过堆叠来自**多尺度视觉输入**（例如，不同分辨率的图像）的标记来实现。
*   **Qwen3-VL的改进**：将其扩展为从**视觉Transformer（ViT）的中间层**提取视觉标记。
*   **改进的优势**：
    *   **保留丰富的视觉信息**：ViT的不同中间层自然捕获了从**低级**（边缘、纹理）到**高级**（物体部件、整体语义）的视觉表征。提取这些特征能保留更完整的视觉信息谱。
    *   **更精细的融合**：将不同抽象级别的视觉特征与LLM不同深度的语义处理层进行对齐和融合。

#### **3. 具体实现方式**
*   **特征选取**：如图1所示，从视觉编码器（ViT）中选取**三个不同层级**的特征。
*   **处理流程**：
    1.  **投影转换**：使用专用的**视觉-语言融合器模块**，分别将这三个层级的特征投影转化为视觉标记序列。
    2.  **注入LLM**：将这些生成的视觉标记，**直接添加（Add）到LLM前三个对应层的隐藏状态中**，从而实现早期的、多层次的特征融合。

---

### **技术价值**
这项设计与之前讨论的**交错式MRoPE**（改进位置编码）相辅相成，共同构成了Qwen3-VL强大的多模态理解基础：
*   **交错式MRoPE** 负责在时间和空间维度上提供**均衡、精确的位置感知**（解决“在哪里”的问题）。
*   **DeepStack多层级融合** 负责将**丰富且多尺度的视觉内容信息**有效地输入给语言模型（解决“是什么/怎么样”的问题）。

## 3.3 - Video Timestamp

### **内容概括**

本部分阐述了Qwen3-VL在**视频时序信息编码**上的一项关键改进。它指出前代模型（Qwen2.5-VL）所采用的、基于绝对时间的位置编码方案存在两大缺陷，并提出了一个更优雅的解决方案：**使用基于文本标记的显式时间戳**。这种方法不仅更有效地建模长视频的时序关系，也降低了数据构建的复杂度。

---

### **要点总结**

#### **1. 前代方案的问题识别**
Qwen2.5-VL通过一种**时间同步的MRoPE变体**来赋予模型时间感知能力，但该方案存在两个核心局限：
*   **长时程建模问题**：将时序位置ID直接绑定到绝对时间（如帧序号），导致**长视频的时序ID变得极大且稀疏**，损害了模型对长时序上下文的理解能力。
*   **训练数据成本高**：该方案的有效学习依赖于对各种帧率（fps）进行**广泛且均匀的采样**，这显著增加了构建高质量训练数据集的成本和复杂性。

#### **2. 新的解决方案：文本标记时间编码**
为解决上述问题，Qwen3-VL采纳了一项创新策略：
*   **核心方法**：不再依赖难以泛化的绝对位置ID，而是为每个视频时序片段**前置一个格式化的文本字符串作为时间戳**（例如：`<3.0 seconds>`）。
*   **范式转变**：将**隐含的、基于数学运算的时序编码**，转变为**显式的、基于自然语言的时序描述**。这使模型能够像理解其他文本指令一样，直接“阅读”和理解时间信息。

#### **3. 实现细节与优势**
*   **训练数据多样性**：在训练时，会生成**两种格式**的时间戳（如“3.0秒”和“00:00:03”），以确保模型能解读多样化的时间表示法。
*   **主要优势**：
    1.  **解决长视频问题**：文本描述的时间（如“第300秒”）与绝对位置ID（如“第7200帧”）不同，其数值不会过度膨胀，更利于模型建立长程的时间关联。
    2.  **降低数据成本**：模型通过文本学习时间概念，降低了对训练数据在帧率覆盖上的严苛要求，简化了数据构建流程。
    3.  **提升任务适用性**：这种显式的时间表达天然适用于**视频定位**（找出特定时刻）和**密集描述**（为每段时间生成字幕）等需要精确时间感知的下游任务。

#### **4. 技术关联与价值**
此项改进与架构部分提到的**交错式MRoPE**形成了层次化互补：
*   **交错式MRoPE** 在底层**位置编码层面**优化了频谱，为模型提供了均衡、连续的时空位置感知基础。
*   **文本时间戳** 在高层**语义注入层面**提供了精确、可解释的时间描述，直接服务于复杂的时序推理任务。

## 4.0 - Training Recipe

### **内容概括**

该章节系统阐述了Qwen3-VL模型**分阶段、渐进式的训练策略**。整个训练流程被精心设计为四个连续的阶段（S0至S3），目标从基础的**视觉-语言模态对齐**开始，逐步过渡到**大规模多模态知识学习**，最后将模型的上下文理解能力扩展到**超长范围**（256K）。每个阶段都有明确的训练目标、数据配比、序列长度和参数更新策略，旨在高效、稳固地构建模型的多模态理解和长上下文处理能力。

---

### **要点总结**

#### **总体训练框架**
*   **基础架构**：基于预训练的SigLIP-2视觉编码器、MLP视觉-语言融合器和Qwen3 LLM主干。
*   **核心策略**：采用**四阶段渐进训练法**，从易到难，从短到长。

#### **分阶段详解**
**第一阶段（S0）：视觉-语言对齐**
*   **目标**：高效弥合视觉与语言模态之间的鸿沟，为后续训练打下坚实基础。
*   **关键策略**：**仅训练MLP融合器**，冻结视觉编码器和LLM主干。
*   **数据**：约670亿token的精选数据，包括高质量图文对、视觉知识和OCR数据。
*   **序列长度**：8,192。

**第二阶段（S1）：多模态预训练**
*   **目标**：进行全参数端到端训练，让模型学习广泛的多模态知识。
*   **关键策略**：**解冻并联合训练所有组件**（视觉编码器、融合器、LLM）。
*   **数据**：约1万亿token的大规模多样化数据。混合了**纯文本数据**（保持LLM语言能力）和**多模态数据**（增加图文交织文档、视觉问答、STEM数据及少量视频数据）。
*   **序列长度**：8,192。

**第三阶段（S2）：长上下文预训练**
*   **目标**：显著扩展模型的上下文处理能力。
*   **关键策略**：序列长度**提升至32,768**，继续全参数训练。
*   **数据**：约1万亿token。调整数据配比，**增加纯文本数据比例**以增强长文本理解，并在多模态数据中**大幅增加视频和面向智能体的指令跟随数据**。
*   **意义**：使模型能够处理和推理更长的视频及复杂的多步骤任务。

**第四阶段（S3）：超长上下文适应**
*   **目标**：将模型的上下文窗口推至极限（256K），巩固其处理超长序列的能力。
*   **关键策略**：序列长度**急剧增至262,144**。
*   **数据**：更聚焦的1000亿token数据集，强调**长视频和长文档理解任务**。
*   **应用价值**：为综合文档分析、长视频摘要等需要处理超长输入的应用奠定基础。

### **核心设计思想**
1.  **对齐优先**：在投入大量算力进行全参数训练前，先用小成本确保视觉和语言信号能正确“对话”。
2.  **能力渐进**：遵循“对齐 → 知识学习 → 长上下文扩展 → 极限适应”的科学路径，稳步提升模型复杂度。
3.  **数据驱动**：每个阶段都根据目标精心策划数据混合比例，平衡多模态与纯文本、通用与特定任务数据。
4.  **保留语言能力**：在整个多模态训练过程中，始终混合纯文本数据，防止LLM的原始强大语言能力退化。

## 4.1 - Image Caption and Interleaved Text-Image Data

### **1. 总体目标与策略**
*   **目标**：为通用目的视觉-语言理解构建**强大的基础模型**。
*   **核心数据**：聚焦于**图像-说明对**和**交错图文序列**这两种关键数据形态。
*   **策略核心**：强调**高质量、多样性、语义丰富性**，并依赖**专用模型和严格的过滤流程**来实现。

### **2. 图像-说明对数据 (Image Caption Data)**
*   **数据收集**：从网络来源大规模收集**以中英为主的多语言、当代的图像-文本对**。
*   **核心优化流程**：
    *   **重写说明**：使用一个专为“重写说明”任务微调的 **Qwen2.5-VL-32B模型**，基于图像的原始文本，生成**更全面、流畅、细粒度**的新说明。这能丰富对视觉元素（如物体属性、空间布局）和上下文语义的描述。
    *   **去重**：仅对**重写后的文本**进行语义相似度去重，在移除冗余样本的同时，**保持视觉多样性**。
    *   **增强覆盖**：对视觉嵌入进行**聚类分析**，识别数据分布中的稀疏区域，并进行**针对性增强**，以覆盖 underrepresented 的概念。
*   **最终产出**：一个在**规模、多样性和描述粒度**上取得平衡的高保真说明数据集。

### **3. 交错图文序列数据 (Interleaved Text-Image Data)**
*   **数据来源**：收集来自近期中英文网站的多样化现实世界多模态文档。
*   **关键处理步骤**：
    1.  **领域分类与过滤**：使用轻量级、基于Qwen微调的评分器进行细粒度领域识别，系统性地排除有害或低价值类别（如广告、促销内容）。
    2.  **书籍级数据的精准解析**：使用微调的 **Qwen2.5-VL-7B模型** 进行高精度多模态解析，精确提取文本并与其中的图表、照片对齐。
    3.  **超长上下文构建**：为了支持超长上下文建模，专门构建一个子集，将连续页面合并成**最长256K令牌**的序列，保持自然的页面顺序和多模态连贯性。
*   **严格的质量控制**：
    *   (i) 移除纯文本或图文对齐度低的片段。
    *   (ii) 对超长书籍序列，要求**最小页面数**和**最低图文比例**，确保在整个上下文中存在有意义的视觉-文本交互。
*   **最终产出**：一个**干净、多样、具有布局感知能力**的交错图文语料库，专为**基于理解的任务**和**长程多模态推理**优化。

## 4.2 - Knowledge

### **1. 核心目标与重要性**
*   **目标**：使Qwen3-VL全面掌握现实世界与虚构概念，以支撑强大的视觉理解、基于事实的推理和实体感知的生成能力。
*   **重要性**：世界知识是多模态大语言模型（MLLMs）在多样化下游任务中表现出色的**基石**。

### **2. 数据集构建范围与挑战**
*   **覆盖范围**：构建了一个大规模预训练数据集，围绕**超过十几种语义类别**的明确定义实体展开，包括动物、植物、地标、食品以及车辆、电子产品、服装等日常物品。
*   **核心挑战**：现实世界实体遵循**长尾分布**——少数知名概念出现频繁且标注质量高，而绝大多数实体则非常罕见，导致数据不平衡。

### **3. 关键技术策略**
*   **基于重要性的采样策略**：
    *   对**高显著性（知名）实体**进行更密集的采样，确保模型获得充足、高质量的学习信号。
    *   **低显著性（罕见）实体**则以较小比例被包含，以维持广泛的覆盖范围，避免训练过程被噪声淹没。
    *   **效果**：在数据质量、实用性和多样性之间取得了有效平衡。
*   **多阶段数据精炼与增强流程**：
    1.  **标准过滤**：去除噪声和图文不匹配的样本。
    2.  **描述增强**：用**大语言模型（LLM）生成的、更丰富的描述**替换原始或稀疏的说明（如通用的替代文本）。
    3.  **增强描述内容**：新描述不仅识别主体实体，还详细描述其**视觉属性、周围环境、空间布局以及与其他物体或人的互动**，从而提供更完整、更 grounded 的文本表征。

## 4.3 - OCR, Document Parsing and Long Document Understanding

### **1. 核心目标**
构建一个能够理解现实世界中复杂文档（如扫描件、PDF、多页报告）的模型，使其不仅能“看见”文字，还能理解**文档结构、布局逻辑**，并能**综合多页信息进行复杂推理**。

### **2. OCR性能增强**
*   **数据构建**：采用**从粗到精的自动化流程**，收集3000万内部样本。无需人工标注，通过整合**OCR专用模型的伪标签**与**Qwen2.5-VL的优化**来精炼标注。
*   **多语言扩展**：在Qwen2.5-VL已支持的10种语言基础上，**新增29种语言**，合成了约3000万高质量多语言OCR样本，并整理了超过100万张真实场景多语言图片。

### **3. 文档解析**
*   **数据来源**：从Common Crawl收集300万份PDF（10种类型各30万份）及400万内部文档。
*   **解析流程**：
    1.  **内部布局模型**：预测文本/非文本区域、阅读顺序和边界框。
    2.  **Qwen2.5-VL-72B**：对特定区域进行识别。
    3.  **数据重组**：将输出重组成**包含位置感知、布局对齐的解析数据**。
*   **统一标注框架**：
    *   **QwenVL-HTML**：包含细粒度的元素级边界框，信息最全。
    *   **QwenVL-Markdown**：仅定位图片和表格（表格用LaTeX编码），格式更简洁。
*   **数据构建**：大规模合成带有精确标注的HTML语料库，并系统转换为Markdown。同时，在大量真实文档上生成伪标签并过滤，最终结合**合成数据与高质量伪标签数据**进行训练，兼顾可扩展性与鲁棒性。

### **4. 长文档理解**
旨在提升模型对**数十页多页PDF**的理解与推理能力。
*   **构建长文档解析序列**：通过合并单页文档样本来合成。**将多页图片置于序列开头，后接对应的OCR/HTML解析文本**，模拟真实阅读流。
*   **构建长文档视觉问答数据**：
    1.  对高质量多页PDF进行采样。
    2.  生成多样化的VQA样例，要求模型进行**跨页、跨模态推理**（涉及图表、表格、图文、正文）。
    3.  精心平衡问题类型，确保支撑证据来自多种模态和布局组件，从而促进在**长上下文**中进行有根据的、多跳的稳健推理。

## 4.4 - Grounding and Counting

### **1. 核心能力目标**
*   **视觉定位**：是模型理解图像内容并与之交互的**基础能力**。Qwen3-VL系统性地增强了此能力，并支持**边界框**和**点**两种定位模态，以适应不同精度要求和任务场景。
*   **视觉计数**：扩展了模型的**定量推理**能力，使其不仅能找到目标，还能统计数量。

### **2. 数据构建三大流程**
为了训练这些能力，团队设计了三条并行的数据构建管线：

*   **A. 基于框的定位数据构建**：
    *   **基础数据**：整合主流开源数据集（COCO, Objects365, OpenImages, RefCOCO系列）。
    *   **自动合成扩充**：为解决数据多样性不足，开发了三阶段自动流程：
        1.  **候选提取**：用 **Qwen2.5-VL** 从无标签图像中描述潜在物体。
        2.  **标注与定位**：结合 **开放词汇检测器（Grounding DINO）** 和 **Qwen2.5-VL** 对候选进行定位和标注。
        3.  **质量过滤**：系统性地过滤低置信度或不准确的标注。
    *   **产出**：一个大规模、高多样性的边界框定位数据集。

*   **B. 基于点的定位数据构建**：
    *   **数据集成**：为确保鲁棒性，融合了三个来源：
        1.  公开的点标注与计数数据（如PixMo）。
        2.  从公开目标检测/实例分割基准数据中转化得到的数据。
        3.  专为目标细粒度图像细节而设计的**合成管线**生成的高精度点标注。

*   **C. 计数数据构建**：
    *   **基础**：建立在上述定位数据之上。
    *   **任务形式**：构建了包含三种任务的综合计数数据集：
        1.  **直接计数**：直接回答数量。
        2.  **基于框的计数**：先标框再计数。
        3.  **基于点的计数**：先点点再计数。

### **3. 关键工程技术改进**
*   **归一化坐标系统**：与Qwen2.5-VL不同，本版本采用了 **`[0, 1000]` 范围的归一化坐标系**。
*   **改进带来的优势**：
    1.  **提升鲁棒性**：对不同输入图像的**分辨率**和**宽高比**变化更加稳健。
    2.  **简化后处理**：方便了预测坐标的后处理。
    3.  **增强实用性**：直接提高了预测坐标在下游任务（如机器人操作、图像编辑）中的可用性。

### **核心思想**
这部分内容揭示了Qwen3-VL在追求“强基础能力”上的工程实践：**不满足于仅整合现有数据集，而是通过设计自动化的合成与验证管线（利用已有模型如Qwen2.5-VL和Grounding DINO）来主动创造高质量、多样化的训练数据**。同时，通过一个**深思熟虑的坐标系统设计**，将模型能力扎实地锚定在实际应用的可靠性上。这体现了其从数据源头保障模型性能与实用性的方法论。

## 4.5 - Spatial Understanding and 3D Recognition

### **1. 总体目标**
旨在赋予模型深度的**空间上下文理解能力**，使其能够：
*   **解读空间关系**。
*   **推断物体功能**。
*   **执行动作规划和具身推理**。
*   **从单目图像估计物体的3D空间位置**。

### **2. 空间理解**
*   **能力范围**：超越基础的对象定位，训练模型对**2D场景**进行推理，包括：
    *   **空间关系**（如“笔记本电脑左边的杯子”）。
    *   **物体功能**（如“可抓握的”、“可按压的”、“可坐的”）。
    *   **条件性动作查询与规划**（如“要拿到显示器后面的书，我应该先移动什么？”）。
*   **数据构建**：
    *   **数据来源**：精心策划的真实场景与合成生成的布局。
    *   **查询生成**：通过**模板化**和**基于LLM**的方法自动生成自然语言查询，确保多样性与复杂性。
    *   **关键设计**：所有空间参照都**表达为相对于其他物体或场景框架的形式**，而非绝对坐标，以此鼓励模型进行更鲁棒的关系推理。
*   **价值**：使模型不仅能回答“在哪里”，还能回答“如何”以及“可以做什么”，为与视觉环境的**智能体交互**打下基础。

### **3. 3D定位**
*   **能力目标**：增强模型从图像理解物理世界的能力，支持**单目3D视觉定位**。
*   **数据格式**：将数据构建为**视觉问答格式**，每个样本包含：
    1.  一张单视角相机图像。
    2.  一个自然语言指代表达式。
    3.  对应的**9自由度3D边界框标注**（以结构化JSON格式指定物体的空间位置和语义标签）。
*   **数据处理**：
    *   **统一坐标系**：由于数据来自多传感器和来源，存在相机参数差异和噪声，因此将所有数据统一到一个**虚拟相机坐标系**中（遵循Omni3D方法）。
    *   **质量过滤**：过滤掉被严重遮挡和不准确的标签。
    *   **文本增强**：合成了大量描述性说明，生成超越物体类别的、包含**详细属性、布局安排、空间位置、视觉功能和与周围物体交互**的丰富文本查询，从而产生更细粒度、更 grounded 的指代表达式。

## 4.6 - Code

### 1.  **双管齐下的能力构建**：
    通过纳入**纯文本编码**和**多模态编码**两大类数据，系统性地提升模型的全栈编程能力。

### 2.  **纯文本代码能力奠基**：
    *   **数据来源**：复用Qwen3和Qwen3-Coder系列的大规模纯代码语料库。
    *   **覆盖范围**：涵盖广泛的编程语言、软件开发、算法问题求解、数学推理和智能体导向任务。
    *   **核心目标**：建立模型对**代码语法、算法逻辑和通用程序生成**的基础性理解。

### 3.  **多模态编码能力拓展**：
    *   **核心目标**：解决需要同时理解**视觉输入**并生成**功能性代码**的任务。
    *   **数据来源**：整合开源数据集与内部合成管线。
    *   **关键任务覆盖**：
        *   **UI转代码**：将UI截图转换为响应式HTML/CSS。
        *   **图像转SVG**：从图像生成可编辑的SVG代码。
        *   **视觉编程解题**：解决基于视觉描述的编程挑战。
        *   **多模态编程问答**：回答带有图像的编程问题（如StackOverflow帖子）。
        *   **视觉图表转录**：将流程图、图表、LaTeX方程等视觉表示转录为相应的代码或标记。

### 4.  **最终价值**：
    这一独特的数据组合使Qwen3-VL能够**在视觉内容与可执行程序之间建立直接联系**，极大地扩展了其在自动化开发、设计转代码、教育辅助等领域的应用潜力。

## 4.7 - Video

### **1. 核心目标**
显著提升Qwen3-VL的视频理解能力，使其能够：
*   对帧间**时序动态**进行鲁棒建模。
*   对**空间关系**进行细粒度感知。
*   对**超长视频序列**进行连贯总结。

### **2. 关键技术一：时间感知的视频理解**
通过创新性的数据标注方法，赋予模型深度的时序与空间理解能力。
*   **A. 密集字幕合成**：
    *   **策略**：采用 **“从短到长”的合成策略**，为长视频生成整体性、时间戳交错且时序连贯的故事级描述。
    *   **细化**：利用内部字幕生成模型，进一步产生**细粒度标注**，同时捕捉**事件级的时序摘要**和**片段级的视觉细节**。
*   **B. 时空视频定位**：
    *   **方法**：策划与合成了大规模的视频数据，并在**物体、动作、人物**级别进行标注。
    *   **目的**：强化模型对视频中特定元素在时间和空间上进行定位的能力，从而提升其**细粒度视频理解**水平。

### **3. 关键技术二：视频数据平衡与采样**
通过数据集的构建与采样策略优化，保障训练的有效性与全面性。
*   **A. 源平衡**：
    *   **做法**：汇集大规模、多样化的视频源（如教学视频、电影、第一人称视角录像等）。
    *   **平衡机制**：依据视频**标题、时长、类别标签**等元数据进行系统性的策划，以确保数据集的平衡与多样性。
*   **B. 长度自适应采样**：
    *   **问题**：固定的采样策略（如帧率过低或帧数过少）会导致信息丢失。
    *   **解决方案**：在预训练阶段，根据不同的序列长度限制，**动态调整采样参数**（如每秒帧数、最大帧数）。
    *   **价值**：在保留视觉细节和优化训练效率之间取得平衡，避免因采样不当造成的信息损失。

## 4.8 - Science, Technology, Engineering, and Mathematics (STEM)

### **1. 核心策略：分而治之**
*   **理念**：将复杂的STEM多模态推理任务分解为两个可独立优化的子能力。
*   **第一步**：独立发展 **“细粒度视觉感知”** （精准理解图表、几何图形中的元素与关系）。
*   **第二步**：独立发展 **“鲁棒的语言推理”** （掌握数学推导、逻辑分析、分步求解等能力）。
*   **最终集成**：将两种能力协同融合，实现有效的多模态推理。

### **2. 视觉感知数据构建**
*   **目标**：专门提升模型对STEM图表（如几何图、函数图）的**细粒度理解能力**。
*   **方法**：开发专用的**合成数据生成管道**，通过**程序化（基于代码）渲染**构建几何图表。
*   **生成数据**：
    *   **100万点定位样本**：用于精确定位交点、角点、重心等关键几何点。
    *   **200万感知导向的视觉问答对**：针对图表的细粒度视觉理解进行提问和回答。
*   **高质量文本描述**：采用**两阶段标注框架**：
    1.  **初始生成**：由专门模型生成图表描述。
    2.  **严格验证**：使用模型集成进行验证，确保准确性和描述粒度。
*   **最终产出**：一个包含**600万个**涵盖多学科、标注丰富的图表说明的数据集。

### **3. 多模态推理数据构建**
*   **数据来源**：超过**6000万**个K-12和本科水平的练习题。
*   **核心处理流程**：
    *   **质量过滤**：剔除图像损坏、内容不相关、答案不完整或错误的低质量题目。
    *   **格式重构**：
        *   在中英文之间互译题目。
        *   标准化答案格式（如分步求解列表、数学表达式、符号标记），确保一致性和统一呈现。
*   **长思维链数据合成**：
    *   合成超过**1200万**个与图像配对的多模态推理样本。
    *   使用**强大的推理模型生成原始的推理轨迹**，确保推理过程的连续性和丰富性。
    *   **严格验证**：结合**基于规则的检查**和**基于模型的验证**，过滤掉包含模糊答案或语码转换的样本，保证数据可靠性。
    *   **拒绝采样**：仅保留具有挑战性的问题，以增强模型的推理质量。

### **4. 语言推理数据整合**
*   **理念**：认识到多模态推理能力在很大程度上源于语言推理能力。
*   **行动**：除了多模态数据，还**融入了来自Qwen3（纯文本模型）的推理数据**，以此作为多模态推理的坚实基础和能力来源。

## 4.9 - Agent

### **1. 三大核心能力构建**

*   **A. 图形用户界面交互**
    *   **目标**：赋予模型与图形界面自主交互的能力。
    *   **方法**：
        1.  **数据收集**：策划与合成了大规模、跨平台（桌面、移动、网页）的数据。
        2.  **界面感知**：利用元数据、解析工具和人工标注，构建界面元素描述、密集字幕、密集定位等任务，以增强模型对多样界面的鲁棒理解。
        3.  **智能体能力构建**：通过**自进化的轨迹生成框架**组装多步骤任务执行轨迹，并辅以针对性人工审核；同时精心设计和增强思维链推理，以强化实际执行中的**规划、决策和自校正**能力。

*   **B. 通用函数调用**
    *   **目标**：构建模型在**多模态上下文**中进行通用函数调用的能力。
    *   **方法**：
        1.  **合成管线**：构建了一个多模态函数调用轨迹合成流程。流程包括：基于图像生成用户查询和函数定义→采样带推理的函数调用→合成函数响应。
        2.  **迭代执行**：重复“调用-响应”步骤，直至判定用户的查询被解决，期间可过滤格式错误的轨迹。
        3.  **关键优势**：此流程使得模型能够**从海量图像中直接构建大规模的多模态函数调用轨迹，而无需实际实现可执行函数**，极大地提升了数据构建的效率与规模。

*   **C. 搜索能力**
    *   **目标**：将**执行搜索**视为促进知识整合的关键能力，以应对现实场景中的**长尾实体**问题。
    *   **方法**：
        1.  **收集搜索轨迹**：收集包含在线图片搜索和文本搜索工具的多模态事实查找轨迹。
        2.  **鼓励主动搜索**：在训练中鼓励模型为不熟悉的实体主动执行搜索。
        3.  **最终效果**：使模型学会从网络上搜集信息，从而生成**更准确的回答**，增强了其知识获取与事实核查能力。

### **2. 核心思想**

该部分揭示了Qwen3-VL从“被动响应”的视觉语言模型向“主动执行”的多模态智能体演进的关键路径。其核心在于**通过系统性的数据工程，模拟并生成人机交互、工具使用和信息检索的完整决策与执行链条**，让模型在预训练阶段就学习到如何分解任务、使用工具、验证结果，从而为实际部署为智能体打下坚实基础。

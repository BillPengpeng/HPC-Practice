本文主要整理《DeepSeek-V3 Technical Report》的主要内容。

## 5.0 - Compute Clusters

![Overlapping strategy](https://ucc.alicdn.com/pic/developer-ecology/6ibaby6qg4ku4_d45aad21dda54081a15b831916464c60.png?x-oss-process=image%2Fresize%2Cw_1400%2Cm_lfit%2Fformat%2Cwebp)

### **内容概括**

首先，它介绍了训练所使用的硬件基础设施：一个包含 **2048块 NVIDIA H800 GPU** 的大型集群。集群采用分层互联设计，节点内8块GPU通过**NVLink和NVSwitch**实现高速互联，节点间则通过**InfiniBand**网络进行通信，为大规模分布式训练提供了必要的带宽。

其次，图片的核心（图4）展示了一种精妙的**前向与后向计算块的重叠策略**。通过将计算过程精细切分，并让不同阶段的计算（前向、反向）与必需的通信（流水线并行通信）在时间上交错进行，系统能够几乎完全隐藏通信开销，从而将GPU的计算能力利用率推向极致，这是实现其惊人训练效率（低成本、高速度）的关键系统级优化。

---

### **要点总结**

1.  **硬件集群配置**：
    *   **规模**：2048块 NVIDIA H800 GPU。
    *   **节点内互联**：NVLink + NVSwitch，提供超高的GPU间带宽，适用于模型并行、数据并行等紧密协作。
    *   **节点间互联**：InfiniBand (IB)，提供低延迟、高带宽的网络，适用于跨节点的流水线并行、专家并行等通信模式。

2.  **核心优化策略**：**前向与后向计算块的重叠**。
    *   **目的**：最大化GPU利用率，隐藏分布式训练中不可避免的通信延迟。
    *   **关键成就**：如图例和文字所述，该策略实现了 **“Both all-to-all and PP communication can be fully hidden”**（全连接通信和流水线并行通信均可被完全隐藏）。

3. 前向传递：通常按顺序执行ATTN（计算流）、DISPATCH（通信流）、MLP（计算流）、COMBINE（通信流）操作。
    * ATTN（计算流）：指注意力机制计算，使大模型能够捕捉文本等数据中的依赖关系，提升模型的性能和泛化能力。
    * DISPATCH（通信流）：指数据或任务在不同GPU节点之间的传递，提高整个训练系统的并行度和效率。
    * MLP（计算流）：即多层感知机计算，由输入层、输出层和一个或多个隐藏层构成，利用梯度下降法更新权重参数。
    * COMBINE（通信流）：指将不同GPU节点上将计算结果进行合并的通信操作。

4. 反向传递：需要执行COMBINE（通信流）、MLP_B（计算流）、MLP_W（计算流）、DISPATCH（通信流）、ATTN_B（计算流）、ATTN_W（计算流）操作:
    * COMBINE（通信流）：定义与前向传递相同，此处特指反向传递时的数据汇总，以便进行全局参数更新。
    * MLP_B（计算流）：此处的B指的是Bias，指的是反向传播中对于偏执项的计算，以便更新大模型的偏执参数。
    * MLP_W（计算流）：此处的W指的是Weight，指的是反向传播中对于权重参数的计算，用于更新权重参数。
    * DISPATCH（通信流）：在反向计算中，DISPATCH 的作用仍然是负责数据或任务的分发。
    * ATTN_B（计算流）：偏置项也会影响注意力权重的计算和最终的输出，通过计算调整注意力机制中的偏置参数。
    * ATTN_W（计算流）：计算损失函数对注意力权重的梯度，通过更新注意力权重捕捉数据中的依赖关系和重要信息。

## 5.1 - backward for input和backward for weights

| 特性 | **Backward for Inputs (输入的梯度)** | **Backward for Weights (权重的梯度)** |
| :--- | :--- | :--- |
| **计算对象** | 当前神经网络层的**输入数据** `x`。 | 当前神经网络层的**可学习参数** `W` (权重) 和 `b` (偏置)。 |
| **计算目的** | **传播梯度信号**。为了计算更早层（上游）的权重梯度，梯度必须像链条一样从输出层逐层回传。计算输入的梯度，就是为了传递给前一层，作为它计算其权重梯度时所需的“上游梯度”。 | **直接获取优化目标**。这些梯度直接指明了应该如何调整本层的参数，以使模型的最终输出更接近目标。 |
| **数学形式** | 计算损失函数 `L` 相对于该层输入 `x` 的梯度：**`∂L/∂x`**。这利用了**链式法则**，通过本层权重和输出的梯度计算得来。 | 计算损失函数 `L` 相对于该层参数 `W` 和 `b` 的梯度：**`∂L/∂W`**, **`∂L/∂b`**。 |
| **是否被更新** | **否**。输入的梯度 `∂L/∂x` **不会**被用来更新输入数据本身（因为输入数据通常是上一层的输出或给定的训练样本）。它是一个**中间变量**。 | **是**。权重的梯度 `∂L/∂W` 和 `∂L/∂b` 是**优化器（如Adam、SGD）的直接输入**。优化器使用这些梯度来更新权重 `W` 和 `b`，这是模型学习的本质。 |
| **在计算图中的角色** | **梯度流的通道**。它是在计算图上向后传递的“接力棒”。 | **优化的终点站**。梯度流到这里的目的地，被用于改变计算图本身的参数。 |

## 5.2 - DISPATCH和COMBINE

### **具体解释**

在MoE模型中，每个Token会根据路由门控网络的计算结果，被发送到多个专家（前馈神经网络）上进行处理。由于专家通常分布在不同的计算设备（如不同的GPU或节点）上，这个过程必然涉及跨设备的通信。

**1. DISPATCH（分发）**
*   **作用**：将当前设备上的 **输入Token数据（前向传播时）或梯度数据（反向传播时）**，按照路由权重，**发送到其他设备上存放对应专家的位置**。
*   **具体类型**：
    *   **DISPATCH(F)**：在**前向传播**中，分发的是Token的**隐藏状态（输入数据）**。
    *   **DISPATCH(B)**：在**反向传播**中，分发的是从下一层传回来的、关于专家输出的**梯度**。

**2. COMBINE（组合）**
*   **作用**：从其他设备**接收**经过专家处理后的 **结果数据（前向传播时）或梯度数据（反向传播时）**，并按照路由权重进行**加权求和**，得到最终的输出。
*   **具体类型**：
    *   **COMBINE(F)**：在**前向传播**中，组合的是各个专家的**输出结果**，形成该MoE层的最终输出。
    *   **COMBINE(B)**：在**反向传播**中，组合的是各个专家关于其**输入（即之前Dispatch来的数据）的梯度**，以便继续向更早的网络层反向传播。

### **结合示意图的流程**

图中的彩色条展示了这些操作在时间线上如何与计算重叠：
1.  **前向传播过程**：
    *   先进行一部分本地计算（橙色块）。
    *   然后执行 **`DISPATCH(F)`** （浅绿色块），将数据分发出去。
    *   在等待其他专家计算的同时，可以继续执行其他计算或通信。
    *   最后执行 **`COMBINE(F)`** （蓝色块），将各专家的结果组合回来。
2.  **反向传播过程**：
    *   类似地，在执行完一部分反向计算（绿色和蓝色块）后，需要进行 **`DISPATCH(B)`** 和 **`COMBINE(B)`** （黄色块）来传递和组合梯度。

### **为什么将它们与计算重叠是关键？**
*   `DISPATCH` 和 `COMBINE` 是**通信密集型操作**，如果与计算串行进行，GPU就会空闲等待，造成资源浪费。
*   DeepSeek-V3的训练框架通过精细的调度，使得这些通信操作能够与相邻的**前向计算（橙色）**、**反向计算（绿、蓝色）** 以及**流水线并行通信（紫色）** **同时进行**（如图中时间轴上的并行条所示）。
*   这种“重叠”几乎完全**隐藏了通信延迟**，让GPU一直处于忙碌的计算状态，从而实现了极高的训练效率。

## 6.0 - Training Framework

### **内容概括**

训练由深度求索自研的 **HAI-LLM框架** 支持。在整体并行策略上，模型采用了三管齐下的方案：**16路流水线并行** 将模型层在GPU间进行纵向切分；**64路专家并行** 将MoE层的专家分布到8个计算节点上；同时配合 **ZeRO-1数据并行** 在不同副本间分割优化器状态，以支持更大的全局批次大小。

为实现极致的训练效率，团队进行了三项核心工程优化：1）设计了 **DualPipe算法**，有效减少了流水线并行中的空闲气泡，并实现了计算与通信的重叠；2）开发了高效的 **跨节点全互联通信内核**，以充分利用InfiniBand和NVLink的带宽，并节省GPU的流多处理器资源；3）精细地 **优化了训练时的内存占用**，从而得以避免使用通信开销极大的张量并行，简化了并行架构。

### **要点总结**

| 方面 | 具体策略与优化 | 目的与效果 |
| :--- | :--- | :--- |
| **并行策略组合** | **PP（流水线并行）**：16路 <br> **EP（专家并行）**：64路，跨越8个节点 <br> **DP（数据并行）**：ZeRO-1阶段 | **纵向**拆分模型层（PP），**横向**拆分专家（EP），**数据**维度拆分批次（DP），构成三维并行，以将巨型模型加载到大规模集群中。 |
| **核心工程优化** | **1. DualPipe算法** | 减少PP气泡，并**重叠计算与通信**，特别是隐藏了跨节点EP引入的沉重通信开销。 |
| | **2. 高效通信内核** | 为EP所需的“All-to-All”通信定制内核，**榨干硬件带宽**，释放GPU计算单元用于计算。 |
| | **3. 内存占用优化** | 精心管理激活值和中间状态，使得训练无需引入**TP（张量并行）**，避免了其高昂的同步通信成本。 |
| **框架与硬件** | **自研HAI-LLM框架** | 轻量、高效，为上述优化提供底层支持，实现算法、框架与硬件的协同设计。 |

---

### **如何划分2048个GPU进行并行策略**

根据图片中给出的并行配置（16-way PP， 64-way EP across 8 nodes， with ZeRO-1 DP），我们可以推导出一个与硬件拓扑紧密结合的、合理的2048个GPU划分方案。其核心思想是**分层映射**：先将GPU分组以匹配物理节点和通信域，再在其上应用并行策略。

#### **第一步：理解给定的约束条件**
1.  **专家并行跨越8个节点**：这意味着64个专家被平均分配到 **8个物理服务器节点** 上，每个节点承载 **8个专家**。
2.  **流水线并行度为16**：整个模型被切分成 **16个流水线阶段**。
3.  **数据并行**：在PP和EP划分后剩余的维度上使用ZeRO-1进行数据并行。

#### **第二步：进行逻辑推导与划分**

*   **节点内（256个GPU）**：通过 **NVLink/NVSwitch** 全互联，带宽极高，延迟极低。适合部署需要紧密通信的 **流水线并行阶段内部** 以及专家并行中**节点内8个专家**之间的通信。
*   **节点间（8个节点）**：通过 **InfiniBand** 网络互联。这主要用于 **专家并行（EP）** 所需的跨节点All-to-All通信（DISPATCH/COMBINE），这也是为什么需要优化跨节点通信内核的原因。
*   **数据并行组（128组）**：每个组包含来自8个节点、共16个流水线阶段各1个GPU（总计16个GPU？需要修正）。实际上，一个数据并行组是处理同一份数据副本的逻辑GPU集合。在ZeRO-1下，它们之间主要进行梯度同步的All-Reduce通信。这些通信可以很好地通过分层集合算法，先在节点内利用NVLink聚合，再在节点间通过IB网络聚合，从而优化通信效率。

**结论**：2048个GPU的划分是一个 **1（EP组） × 16（PP阶段） × 128（DP副本）** 的逻辑结构。这种划分紧密契合了“**节点内高速通信用于PP和部分EP，节点间IB网络用于大规模EP通信，同时支撑高维度数据并行**”的硬件优势，并且通过放弃TP，避免了最昂贵的逐层同步，最终实现了图片中所述的极致训练效率。

## 6.1 - DualPipe and Computation-Communication Overlap

![DualPipe](https://ucc.alicdn.com/pic/developer-ecology/6ibaby6qg4ku4_a2330bca22ba4b518ed90714fbb2206d.png?x-oss-process=image%2Fresize%2Cw_1400%2Cm_lfit%2Fformat%2Cwebp)

*   **Bubble（气泡）**：指在流水线并行中，由于需要“填充”第一个微批次和“排空”最后一个微批次，导致部分计算设备（GPU）处于空闲等待状态的时间。**这是衡量流水线效率的核心指标，越小越好。**
*   **Parameter（参数）**：指在训练过程中，需要在设备内存中**同时保存的完整模型参数的副本数量**。这直接影响内存开销，**越小越好**。
*   **Activation（激活值）**：指在执行反向传播时，需要为计算梯度而**暂存在内存中的中间结果（激活值）的数量**。通常与流水线阶段数（PP）相关，**越小越好**。

---

### **各方法公式详解**

#### **1. 1F1B（One-Forward-One-Backward）**
这是最经典、最基础的流水线并行调度算法。

*   **Bubble: `(PP - 1)(F + B)`**
    *   **`PP`**: 流水线并行的总阶段数（GPU数量）。
    *   **`F`**: 执行**一个微批次**的**完整前向传播**所需的时间。
    *   **`B`**: 执行**一个微批次**的**完整后向传播**所需的时间。
    *   **计算逻辑**：在流水线启动时，需要 `PP-1` 个时间步来让第一个微批次流经所有阶段（填充）。同样，在结束时，也需要 `PP-1` 个时间步来排空最后一个微批次。每个时间步的“单位”就是一个微批次的前向或后向计算时间 `F+B`。因此总气泡时间为 `(PP-1)(F+B)`。
*   **Parameter: `1×`**
    *   整个集群只需保存一份完整的模型参数。每个GPU只存储自己负责的那部分层的参数。
*   **Activation: `PP`**
    *   在流水线中，最多有 `PP` 个处于不同完成度的微批次在同时处理。为了给后续的反向传播保存中间结果，整个集群最多需要存储 `PP` 个微批次的激活值。

#### **2. ZB1P（可能代表 Zero Bubble-1 Phase 或类似优化）**
这是在1F1B基础上的优化，核心思想是**重叠权重梯度的计算**。

*   **Bubble: `(PP - 1)(F + B - 2W)`**
    *   **`W`**: 执行**一个微批次**的 **`Backward for weights`**（即计算并更新权重梯度）这部分计算所需的时间。通常 `W < B`。
    *   **计算逻辑**：ZB1P通过精细调度，将权重梯度计算（`W`）与其他计算（如前向、或反向中的其他部分）**重叠执行**。这样，每个时间步的有效计算时间就从 `F+B` 缩短到了 `F+B-2W`。气泡公式因此得到优化，气泡时间减少。
*   **Parameter: `1×`** 和 **Activation: `PP`**
    *   在参数和激活值的存储开销上，ZB1P与1F1B相同。

#### **3. DualPipe (Ours)**
这是DeepSeek-V3论文提出的创新算法。其核心是**双向调度**和**在单个计算块内重叠计算与通信**。

*   **Bubble: `((PP/2) - 1)(F&B + B - 3W)`**
    *   **`F&B`**: 这是DualPipe的关键。它代表**一个相互重叠的前向块和一个后向块的组合执行时间**。由于在块内部实现了计算与通信的完全重叠，`F&B` 显著小于 `F + B`。
    *   **`PP/2`**: 因为DualPipe采用**双向流水线**（从首尾两端同时注入微批次），有效流水线长度减半，因此公式中的阶段数因子从 `(PP-1)` 变为 `((PP/2)-1)`。
    *   **`- 3W`**: 这体现了DualPipe更激进的重叠优化。它不仅仅像ZB1P那样重叠 `W`，而是在 `F&B` 和 `B` 的组合计算中，通过调度重叠掉了更多的 `W` 相关计算，总计相当于 `3W`。
    *   **计算逻辑**：结合**减半的流水线因子**和**大幅缩短的单位计算时间**，DualPipe的气泡时间理论上远小于前两种方法。
*   **Parameter: `2×`**
    *   这是为了实现双向调度和计算/通信重叠所付出的代价。DualPipe需要在内存中**同时保存两份模型参数**，一份用于正向流动的微批次，另一份用于反向流动的微批次（或在某些优化上下文中使用）。
*   **Activation: `PP + 1`**
    *   激活值存储比传统的 `PP` 多了一份（`+1`）。这是因为双向调度和更复杂的重叠执行模式，需要在某些时刻多保存一个微批次的中间结果。但论文指出，这份额外的开销相对于 `PP` 来说很小（仅为 `1/PP`），是完全可以接受的。

### **总结对比**

| 方法 | 设计思想 | Bubble (气泡) | 内存开销 |
| :--- | :--- | :--- | :--- |
| **1F1B** | 经典单向流水线 | 大 `(PP-1)(F+B)` | 低 (1×参数，PP激活) |
| **ZB1P** | 重叠权重更新 | 中等 `(PP-1)(F+B-2W)` | 低 (同1F1B) |
| **DualPipe** | **双向流水线 + 块内计算/通信重叠** | **小 `((PP/2)-1)(F&B+B-3W)`** | **中等 (2×参数，PP+1激活)** |

## 6.2 - Efficient Implementation of Cross-Node All-to-All Communication

### **内容概括**

其核心目标是：在超大规模混合专家模型训练中，将跨节点通信带来的性能损耗降至最低。

由于 DualPipe 算法要求极高的计算吞吐，传统的通信库会成为瓶颈。因此，团队**定制开发了高效的跨节点全对全通信内核**。该内核的实现与**MoE门控算法**和**集群硬件拓扑**进行了深度协同设计。

具体而言，团队充分利用了集群中**节点内NVLink（160 GB/s）带宽远高于节点间InfiniBand（50 GB/s）** 的特点，设计了智能的令牌路由与转发策略：将每个令牌的通信限制在最多4个节点内，优先利用高带宽的NVLink进行节点内转发，并使IB与NVLink的通信完全重叠。此外，通过**Warp Specialization**技术将少量流式多处理器专用于通信，并精细调整通信块大小，在榨干硬件带宽的同时，最大限度地减少对计算单元的干扰，最终实现了通信开销近乎被完全隐藏的效果。

**Warp Specialization（Warp专业化）**：其核心思想是让不同的Warp长期“专职”处理特定类型的任务，形成一个静态或半静态的“流水线”或“服务通道”。这样，每个Warp可以针对其专职任务进行高度优化，避免了频繁的任务切换开销，并能实现更精细的负载均衡。

### **要点总结**

| 层面 | 核心策略 | 具体实现与效果 |
| :--- | :--- | :--- |
| **设计理念** | **算法-硬件协同设计** | 通信内核的实现与 **MoE门控算法** 和 **集群网络拓扑** 深度结合，而非使用通用库。 |
| **硬件拓扑利用** | **分层带宽优化** | 识别并利用带宽差异：**节点内NVLink (160 GB/s)** >> **节点间InfiniBand (50 GB/s)**。 |
| **路由策略** | **限制跨节点流量** | 通过门控算法，将每个令牌调度到**最多4个节点**，显著减少低速的IB网络流量。 |
| **传输流程** | **IB与NVLink重叠** | 1. 令牌通过IB网络发送至目标节点上**索引相同的GPU**。<br>2. 到达后，立即通过**NVLink**转发到承载目标专家的**特定GPU**。<br>3. 流程设计确保无阻塞，实现**IB发送与NVLink转发的完全重叠**。 |
| **扩展性** | **通信成本不变下的专家扩展** | 得益于上述优化，虽然实际选择**8个路由专家**，但理论上可将数量**扩展至13个**（4节点 × 3.2专家/节点）而保持通信成本不变。 |
| **GPU资源利用** | **专用、动态的通信通道** | 1. **极少占用**：仅需 **20个SM** 即可饱和IB与NVLink带宽。<br>2. **Warp Specialization**：将20个SM划分为 **10个通信通道**。<br>3. **动态负载均衡**：在分发(`DISPATCH`)和合并(`COMBINE`)过程中，为IB发送、转发、接收等任务分配的Warp数量**根据实际工作量动态调整**。 |
| **对计算的影响** | **最小化干扰** | 1. 通信内核与计算流**重叠执行**。<br>2. 使用**定制PTX指令**和**自动调优的通信块大小**，显著减少对L2缓存的占用和对其余SM上计算内核的干扰。 |

### **核心逻辑链总结**

这项优化揭示了DeepSeek-V3实现高效率的深层逻辑：
1.  **识别瓶颈**：大规模MoE训练的核心瓶颈是跨节点通信。
2.  **协同设计**：从顶层算法（DualPipe、MoE路由）到底层内核（通信、PTX指令）进行一体化设计。
3.  **尊重硬件**：根据NVLink与IB的带宽差异，设计“**先IB跨节点，后NVLink节点内**”的传输路径，并限制跨节点数。
4.  **极致优化**：采用Warp Specialization、动态负载、定制指令等手段，用最少资源（20个SM）实现带宽饱和，并将对计算的干扰降到最低。

## 6.3 - Extremely Memory Saving with Minimal Overhead

### **内容概况**

这些技术的目标是在**最小化额外开销**的前提下，显著降低训练过程中的内存占用，从而支持更大规模的模型或批次，并提升整体系统效率。

具体来说，技术一通过**选择性重计算**，用少量计算时间换取了大量激活值内存的节省；技术二通过**改变存储位置和更新策略**，将辅助参数（EMA）移出宝贵的GPU显存；技术三则通过**创新的模型部署策略**，在系统层面实现了关键参数的物理共享，消除了重复存储。三者共同构成了其高效内存管理的基石。

### **要点总结**

| 序号 | 技术名称 | 核心原理 | 实现效果与优势 |
| :--- | :--- | :--- | :--- |
| **1** | **RMSNorm与MLA上投影的重计算** | 在反向传播过程中，**临时重新计算** RMSNorm层和多头潜在注意力架构中“上投影”操作的输出，而非一直存储在显存中。 | **显著减少激活值内存**：避免了为这些大量中间结果持久化存储，以**微小的计算开销**换取了巨大的显存空间释放。 |
| **2** | **CPU内存中的指数移动平均** | 将用于评估模型性能的**参数指数移动平均值（EMA）存储在CPU内存**中，并在每个训练步后**异步更新**。 | **零GPU显存开销**：完全不影响宝贵的GPU显存容量，且异步更新避免了阻塞训练流程，实现了**无额外时间和内存成本**的EMA维护。 |
| **3** | **多令牌预测的共享嵌入与输出头** | 利用 **DualPipe策略**，将模型的最浅层（嵌入层）和最深层（输出头）部署在**同一个流水线并行等级**上。 | **物理级参数与梯度共享**：使得MTP模块和主模型可以**真正共享同一份嵌入层和输出头的参数及梯度内存**，避免了双份存储，进一步提升了内存效率。 |

### **技术协同价值总结**

这三种技术分别从 **“计算图优化”**、**“存储介质优化”** 和 **“系统部署优化”** 三个维度解决了内存瓶颈：
- **重计算** 优化了训练过程中的**瞬时峰值内存**。
- **CPU存储EMA** 优化了**辅助参数的长期占用**。
- **参数物理共享** 优化了**模型本身的静态内存布局**。


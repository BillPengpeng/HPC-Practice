本文主要整理《DeepSeek-V3 Technical Report》的主要内容。

## 1 - Abstract

### 内容概括
DeepSeek-V3 是一个强大的专家混合（MoE）语言模型，总参数量为 6710B，但每个 Token 实际激活的参数量为 370B，兼顾了强大的能力与高效的推理。为了提升训练和推理效率，它采用了已验证的 **多头潜在注意力（MLA）架构** 和 **DeepSeekMoE 架构**，同时引入了**无需辅助损失的负载均衡策略**和**多 Token 预测训练目标**以增强性能。模型在海量高质量数据上进行预训练，并经过了监督微调与强化学习优化，最终在多项评测中表现优异，达到了与顶尖闭源模型相媲美的水平。其训练过程稳定高效，总训练时间显著低于同类模型，且所有检查点已开源。

### 要点总结
1.  **基本参数**：
    *   模型类型：专家混合语言模型。
    *   总参数量：671B。
    *   激活参数量：37B/Token。

2.  **核心架构**：
    *   采用经过验证的 **多头潜在注意力（MLA）** 和 **DeepSeekMoE** 架构，确保高效推理与训练成本效益。
    *   首创 **无辅助损失的负载均衡策略** 和 **多 Token 预测训练目标**。

3.  **训练过程**：
    *   **预训练**：使用了 14.8 万亿个高质量、多样化的 Token。
    *   **优化阶段**：包含监督微调和强化学习。

4.  **性能表现**：
    *   综合评估表现优于其他开源模型。
    *   性能与领先的闭源模型相当。

5.  **训练效率与稳定性**：
    *   **资源消耗**：全程训练仅需 278.8 万 H800 GPU 小时，资源需求相对较低。
    *   **过程稳定**：整个训练过程未出现不可恢复的损失尖峰，也无需进行任何回滚，稳定性极佳。

## 2.0 - Introduction

### **内容概括**

这篇文档介绍了由深度求索公司开发的大型开源语言模型 **DeepSeek-V3**。该模型是一个总参数量达 **6710亿** 的混合专家模型，其核心创新在于每次推理仅激活 **370亿** 参数，在保持强大能力的同时实现了极高的训练和推理效率。

为达成这一目标，DeepSeek-V3 采用了经过验证的 **多头潜在注意力（MLA）** 架构和 **DeepSeekMoE** 架构，并引入了两项关键策略：**无需辅助损失的负载均衡策略(auxiliary-loss-free strategy for load balancing)** 以提升模型性能，以及**多 Token 预测训练目标(multi-token prediction training objective)** 以增强评估基准上的表现。

在训练方面，团队通过支持 **FP8 混合精度训练**、设计减少通信气泡的 **DualPipe 并行算法**、开发高效通信内核等一系列框架级优化，实现了极高的训练效率。模型在 **14.8万亿** 高质量且多样化的 Token 上进行了稳定的预训练，未出现不可恢复的损失尖峰，随后还进行了两阶段上下文长度扩展（至 128K）以及监督微调和强化学习等训练后对齐。

评估结果表明，DeepSeek-V3 的基座模型已成为当前能力最强的开源基座模型之一，其对话版本在多项标准评测中表现优异，性能可比肩 GPT-4o、Claude-3.5-Sonnet 等领先闭源模型。

文档最后强调了其极致的训练成本效益。通过算法、框架和硬件的协同优化，DeepSeek-V3 的完整训练仅消耗了 **278.8万 H800 GPU 小时**，若按每小时 2 美元计算，总成本仅为 **约 557.6万美元**，远低于同类规模模型。

---

### **要点总结**

1.  **模型定位**：一个旨在推动开源模型能力边界的大型混合专家语言模型。
2.  **核心参数**：总参数 671B，每次前向传播激活 37B 参数。
3.  **关键架构与策略**：
    *   沿用已验证高效的 **MLA**（推理）和 **DeepSeekMoE**（训练）架构。
    *   首创 **无辅助损失的负载均衡策略**，避免为平衡而损害性能。
    *   采用 **多 Token 预测训练目标**，提升模型整体能力。
4.  **高效训练优化**：
    *   支持 **FP8 混合精度训练**，加速并节省显存。
    *   设计 **DualPipe 算法** 优化管道并行，减少流水线气泡（pipeline parallelism），实现计算通信重叠。
    *   开发高效通信内核，充分利用带宽。
    *   精细优化内存占用，使得训练无需昂贵的张量并行。
5.  **训练过程**：
    *   **预训练数据**：14.8T 高质量多样化 Token。
    *   **过程稳定性**：全程无不可恢复损失尖峰，无需回滚。
    *   **上下文扩展**：分两阶段依次扩展至32K、128K。
    *   **训练后对齐**：进行了监督微调和强化学习，并蒸馏了 DeepSeek-R1 系列的推理能力。
6.  **性能表现**：
    *   **基座模型**：成为当前最强的开源基座模型，尤其在代码和数学领域。
    *   **对话模型**：综合评测超越其他开源模型，性能与 GPT-4o、Claude-3.5-Sonnet 等顶尖闭源模型相当。
7.  **训练成本**（核心优势）：
    *   通过算法-框架-硬件协同设计，实现了极致的成本效益。
    *   完整训练总成本仅为 **2.788M H800 GPU 小时**。
    *   按 $2/小时 计，总费用约 **$5.576M**（约 557.6万美元）。

| Training Costs         | Pre-Training | Context Extension | Post-Training | **Total**      |
| :--------------------- | :----------- | :---------------- | :------------ | :------------- |
| in H800 GPU Hours      | 2,664K       | 119K              | 5K            | **2,788K**     |
| in USD (assume $2/hr)  | $5.328M      | $0.238M           | $0.01M        | **$5.576M**    |

**Table 1 | Training costs of DeepSeek-V3, assuming the rental price of H800 is $2 per GPU hour.**

## 2.1 - Contribution

### **内容概括**

首先，在**模型架构**上，团队在高效的DeepSeek-V2架构基础上进行了两项关键创新：一是提出了**无需辅助损失的负载均衡策略**，旨在减少传统上为鼓励负载均衡而导致的模型性能下降；二是深入研究了**多令牌预测训练目标**，不仅验证了其对提升模型性能的有效性，还指出其可用于推测解码以加速推理。

其次，在**预训练阶段**，团队致力于实现极致的训练效率。他们设计了**FP8混合精度训练框架**，并首次在超大规模模型上验证了其可行性与有效性。更重要的是，通过**算法、框架与硬件的协同设计**，团队克服了跨节点MoE（混合专家）训练中的通信瓶颈，实现了近乎完全的计算-通信重叠。这极大地提升了训练效率、降低了成本，使得团队能够在仅消耗**266.4万H800 GPU小时**的经济成本下，基于14.8万亿Token完成预训练，打造出当前最强的开源基座模型。

最后，在**后训练（对齐）阶段**，团队创新性地提出了一种方法，将**DeepSeek-R1系列模型**（一种长思维链模型）的**推理能力**蒸馏到标准大语言模型（特别是DeepSeek-V3）中。该方法成功地将R1模型的**验证与反思模式**融入DeepSeek-V3，显著提升了其推理性能，同时团队还能有效控制DeepSeek-V3的输出风格与长度。

### **要点总结**

**1. 架构创新**
*   **负载均衡**：提出“无辅助损失”的负载均衡策略，避免为平衡专家使用而损害模型性能。
*   **训练目标**：验证了“多令牌预测”目标对模型性能的增益，该目标还可用于推理加速（推测解码）。

**2. 预训练效率突破**
*   **计算精度**：成功设计并验证了适用于超大规模模型的**FP8混合精度训练框架**。
*   **系统优化**：通过算法-框架-硬件协同设计，解决MoE跨节点通信瓶颈，实现**近100%的计算-通信重叠**。
*   **成本效益**：以极低的成本（**266.4万GPU小时**）完成了14.8T Token的预训练，产出顶尖开源基座模型。

**3. 后训练能力蒸馏**
*   **方法创新**：开发了从专门的长思维链模型（DeepSeek-R1）到标准LLM的推理能力蒸馏流程。
*   **核心转移**：将R1模型的**验证与反思模式**成功迁移至DeepSeek-V3。
*   **成果**：显著提升DeepSeek-V3的推理性能，同时保持对其输出风格和长度的可控性。

## 2.2 - Summary of Core Evaluation Results

### **内容概括**

在**知识**方面：
1.  **教育基准测试**：在MMLU、MMLU-Pro、GPQA等测试中，DeepSeek-V3的表现优于所有其他开源模型，其得分与GPT-4o、Claude-Sonnet-3.5等领先闭源模型相当，显著缩小了开源与闭源模型在此领域的差距。
2.  **事实性测试**：在中文事实性知识基准（Chinese SimpleQA）上，DeepSeek-V3超越了包括GPT-4o和Claude-Sonnet-3.5在内的所有模型，展示了其在中文领域的强大优势。在英文事实性知识（SimpleQA）上，它虽是开源模型中最优的，但仍稍逊于上述顶尖闭源模型。

在**代码、数学与推理**方面：
1.  **数学推理**：在数学相关基准测试中，DeepSeek-V3在所有**非长链思维**的开源和闭源模型中达到了最先进的性能。特别是在MATH-500测试上，其表现甚至超过了OpenAI的o1-preview模型，展示了卓越的数学推理能力。
2.  **编程与工程**：在编程竞赛类基准（如LiveCodeBench）中，DeepSeek-V3是表现最好的模型。在工程相关任务上，其表现虽略低于Claude-Sonnet-3.5，但仍显著优于所有其他模型，展现了全面的技术竞争力。

### **要点总结**

**1. 知识能力：表现卓越，中文优势突出**
*   **教育测试领先**：在MMLU、MMLU-Pro、GPQA等综合知识测试中，超越所有开源模型，性能与顶尖闭源模型（GPT-4o, Claude-3.5）相当。
*   **事实性知识强劲**：在中文事实性知识（Chinese SimpleQA）上**全面领先**，包括超越顶尖闭源模型；在英文事实性知识（SimpleQA）上为开源最优。

**2. 代码、数学与推理能力：达到先进水平**
*   **数学推理顶尖**：在所有**非长链思维**模型中达到最先进水平，部分测试（如MATH-500）甚至超越专门的推理模型o1-preview。
*   **编码竞赛领先**：在编程竞赛基准（如LiveCodeBench）中是**表现最佳的模型**。
*   **工程任务强劲**：在工程相关任务中表现仅次于Claude-3.5，但**大幅领先于其他所有模型**。

## 2.3 - the remainder of this paper

### **内容概括**

文章将首先详细介绍DeepSeek-V3的核心**模型架构**，然后阐述支撑其高效训练与推理的**软硬件基础设施与框架**。接着，会详细说明**预训练**的全过程，包括数据处理、参数设置和长上下文扩展等。之后，会讨论预训练之后的**对齐工作**，如监督微调和强化学习。最后，对全文进行总结，并讨论模型的局限性与未来研究方向。

### **要点总结（论文结构）**

| 章节 | 标题/核心内容 | 主要涵盖内容 |
| :--- | :--- | :--- |
| **第2节** | **模型架构** | 详细阐述DeepSeek-V3的核心设计，包括**MLA架构、DeepSeekMoE、创新的负载均衡策略和多Token预测目标**等。 |
| **第3节** | **基础设施** | 介绍计算集群、训练框架、**FP8训练支持**、推理部署策略，并提出对未来硬件设计的建议。 |
| **第4节** | **预训练过程** | 说明训练数据的构建、超参数设置、**长上下文扩展技术**、相关评估与分析讨论。 |
| **第5节** | **预训练后工作** | 详述**监督微调**和**强化学习**等对齐阶段的工作、相应的模型评估结果与分析。 |
| **第6节** | **结论与展望** | 总结全文工作，客观讨论DeepSeek-V3的**现有局限性**，并展望未来的研究方向。 |

**总而言之**，这张图为您提供了一份清晰的“论文阅读地图”。您之前深入讨论的**模型亮点**、**训练效率**和**评估结果**，将分别在**第2、3、4、5节**中找到详细对应的技术阐述和实验数据。

## 3.0 - Multi-Head Latent Attention

![Multi-Head Latent Attention](https://gnnclub-1311496010.cos.ap-beijing.myqcloud.com/wp-content/uploads/2025/02/20250213142656387.png)


### **内容概括**

其核心思想是**对注意力计算中的键、值和查询进行低秩压缩**。通过一个“**下投影-上投影**”的架构，模型首先将原始的、维度较高的键/值/查询向量压缩到一个维度低得多的“潜在向量”中。在推理时，只需要缓存这个压缩后的潜在向量和一个额外的、解耦的键向量，从而**大幅减少 KV 缓存**。同时，该架构也在训练时减少了激活内存的占用。整个计算过程在保持与标准多头注意力相当的性能的同时，实现了显著的内存效率提升。

### **要点总结**

1.  **核心目标**：减少自回归生成（推理）时 KV 缓存的内存占用，并降低训练时的激活内存。
2.  **关键技术**：**低秩联合压缩**。对键、值和查询分别进行下投影（压缩）和上投影（恢复）。
3.  **推理优化**：
    *   只需缓存两个小向量：压缩的键值潜在向量 $c_t^{KV}$ 和携带位置信息的解耦键 $k_t^R$（图中蓝框部分）。
    *   与缓存所有头的完整键和值相比，极大节省了内存。
4.  **位置编码**：将 **RoPE** 应用于一个独立的、解耦的键/查询分量上，以更好地融入位置信息。
5.  **性能保持**：通过低秩投影和精心设计，MLA 在减少缓存的同时，保持了与标准多头注意力相当的性能。

---

### **公式解释**

**第一部分：键与值的压缩与缓存（公式 1-5）**

这是 MLA 减少 **KV 缓存** 的核心。

1.  **公式 (1) - 键值的潜在压缩**：
    $c_t^{KV} = W^{DKV} h_t$
    *   **目的**：将当前 token 的注意力输入 $h_t$ 投影到一个低维的潜在空间。
    *   **含义**：$c_t^{KV}$ 是一个维度为 $d_c$ 的压缩向量，它将作为后续生成所有注意力头的键和值的“基础”。$d_c$ 远小于总头维度 $d_h n_h$。

2.  **公式 (2) - 生成压缩的键**：
    $k_t^C = W^{UK} c_t^{KV}$
    *   **目的**：从压缩的潜在向量 $c_t^{KV}$ 中恢复出所有注意力头的键向量。
    *   **含义**：上投影矩阵 $W^{UK}$ 将 $c_t^{KV}$ 映射回高维空间，得到连接在一起的各头键向量 $k_t^C$。

3.  **公式 (3) - 生成解耦的、带位置信息的键**：
    $k_t^R = \text{RoPE}(W^{KR} h_t)$
    *   **目的**：生成一个独立的、专门用于携带旋转位置信息（RoPE）的键分量。
    *   **含义**：这部分键直接从原始输入 $h_t$ 通过 $W^{KR}$ 生成，并直接应用 RoPE。**这是需要缓存的蓝框向量之一**，它确保了位置信息的精确性。

4.  **公式 (4) - 组合最终键**：
    $k_{t,i} = [k_{t,i}^{C}; k_t^R]$
    *   **目的**：为第 i 个头构造完整的键向量。
    *   **含义**：将第 i 个头的压缩键分量 $k_{t,i}^C$ 与**共享的**、带位置信息的解耦键 $k_t^R$ 拼接起来，形成该头最终用于注意力计算的键。

5.  **公式 (5) - 生成压缩的值**：
    $v_t^C = W^{UV} c_t^{KV}$
    *   **目的**：从同一个压缩潜在向量 $c_t^{KV}$ 中恢复出所有注意力头的值向量。
    *   **含义**：与公式(2)类似，通过另一个上投影矩阵 $W^{UV}$ 生成各头的值向量 $v_t^C$。**注意**：值向量不拼接额外的 RoPE 分量。

**核心缓存收益**：在推理时，对于第 t 个 token，传统 MHA 需要缓存所有头的完整键 $k_t$ 和值 $v_t$，维度为 $2 * n_h * d_h$。而 MLA 仅需缓存：
*   压缩的键值潜在向量 $c_t^{KV}$（维度 $d_c$）
*   解耦的 RoPE 键 $k_t^R$（维度 $d_h^R$）
由于 $d_c + d_h^R \ll 2 * n_h * d_h$，因此 KV 缓存被大幅压缩。

---

**第二部分：查询的压缩（公式 6-9）**

这部分结构与键值压缩对称，主要用于减少训练时的**激活内存**。

6.  **公式 (6) - 查询的潜在压缩**：
    $c_t^Q = W^{DQ} h_t$
    *   **目的**：将输入压缩为查询的潜在向量 $c_t^Q$，维度为 $d_c'$。

7.  **公式 (7) - 生成压缩的查询**：
    $q_t^C = W^{UQ} c_t^Q$
    *   **目的**：从 $c_t^Q$ 恢复出各头的查询分量 $q_t^C$。

8.  **公式 (8) - 生成解耦的、带位置信息的查询**：
    $q_t^R = \text{RoPE}(W^{QR} c_t^Q)$
    *   **目的**：从压缩的查询潜在向量（而非原始输入）生成独立的、带 RoPE 的查询分量。
    *   **含义**：所有头的 RoPE 查询分量都从 $c_t^Q$ 生成，这进一步节省了计算和存储。

9.  **公式 (9) - 组合最终查询**：
    $q_{t,i} = [q_{t,i}^{C}; q_{t,i}^{R}]$
    *   **目的**：为第 i 个头构造完整的查询向量。
    *   **含义**：将第 i 个头的压缩查询与**其专属的** RoPE 查询分量拼接。

---

**第三部分：注意力计算与输出（公式 10-11）**

10. **公式 (10) - 注意力加权求和**：
    $o_{t,i} = \sum_{j=1}^{t} \text{Softmax}_j(\frac{q_{t,i}^T k_{j,i}}{\sqrt{d_h + d_h^R}}) v_{j,i}^C$
    *   **目的**：计算第 i 个注意力头的输出。
    *   **含义**：这是标准的缩放点积注意力计算。使用拼接后的查询 $q_{t,i}$ 和键 $k_{j,i}$ 计算注意力权重，然后对**压缩的值向量** $v_{j,i}^C$ 进行加权求和。分母中的 $\sqrt{d_h + d_h^R}$ 是拼接后向量的总维度。

11. **公式 (11) - 多头输出投影**：
    $u_t = W^O [o_{t,1}; o_{t,2}; ...; o_{t,n_h}]$
    *   **目的**：将所有注意力头的输出合并，并投影到模型维度。
    *   **含义**：这是标准的多头注意力最后一步，输出投影矩阵 $W^O$ 将拼接后的各头输出转换回模型嵌入维度 $d$。

### **MLA 核心优势总结表**

| 方面 | 标准多头注意力 (MHA) | DeepSeek-V3 的多头潜在注意力 (MLA) | MLA 的优势 |
| :--- | :--- | :--- | :--- |
| **推理 KV 缓存** | 缓存所有头的完整键 $K$ 和值 $V$ | 仅缓存压缩的潜在向量 $c^{KV}$ 和解耦键 $k^R$ | **缓存量大幅减少** |
| **位置编码** | RoPE 直接应用于完整的键/查询 | RoPE 应用于独立的、解耦的键/查询分量 | 位置信息更集中，与压缩部分解耦 |
| **训练激活内存** | 需存储完整的中间键、值、查询 | 通过低秩压缩，存储压缩后的潜在向量 | **减少训练激活内存占用** |
| **核心思想** | 直接计算与存储 | **下投影-上投影的低秩压缩** | 在性能和效率间取得更好平衡 |

## 3.1 - DeepSeekMoE

### **内容概括**

其核心思想是将所有的 **FFN 专家（专家即一个小的前馈神经网络）分为两类**：
1.  **共享专家**：对于每一个输入 Token，**所有共享专家都会被激活和计算**。它们捕获所有 Token 共有的、基础的特征。
2.  **路由专家**：对于每一个输入 Token，系统会根据其特性，通过一个“路由门”机制**动态地选择并激活其中最相关的少数几个（K个）**。它们负责处理特定领域的、细粒度的特征。

相比于 GShard 等传统 MoE 架构，DeepSeekMoE 采用了**更精细的专家（数量更多、参数更少）** 并**明确分离了共享与路由专家**，这种设计旨在提升模型的表达能力与训练稳定性。

图片通过数学公式清晰定义了该架构的计算过程：首先计算输入与每个路由专家的**亲和力得分**，然后根据得分**选择 Top-K 个路由专家**，接着对所选专家的亲和力进行**归一化以得到门控值**，最后将所有共享专家与所选路由专家的输出加权求和，得到 FFN 模块的最终输出。

---

### **要点总结**

1.  **架构基础**：FFN 模块采用 **DeepSeekMoE** 架构，是 DeepSeek-V2 架构的延续与改进。
2.  **专家分类**：专家被明确分为两类：
    *   **共享专家**：固定激活，处理通用特征。
    *   **路由专家**：动态激活，处理特定特征。
3.  **核心机制**：对于每个输入 Token，路由机制会：
    *   计算它与所有路由专家的亲和力。
    *   选出亲和力最高的 K 个路由专家。
    *   将选中的亲和力分数归一化，作为这些专家的权重（门控值）。
4.  **关键改进（相较于 DeepSeek-V2）**：
    *   使用 **Sigmoid** 函数而非 Softmax 来计算亲和力，使得专家选择更具独立性。
    *   门控值由**所有被选中专家的亲和力分数归一化**产生，而非全局所有专家的 Softmax。

---

### **公式解释**

公式清晰地描述了 DeepSeekMoE 的前向计算过程：

**公式 (12) - 整体输出计算**
$$ h_{t}^{\prime}=u_{t}+\sum_{i=1}^{N_{s}}FFN_{i}^{(s)}\left(u_{t}\right)+\sum_{i=1}^{N_{r}}g_{i,t}FFN_{i}^{(r)}\left(u_{t}\right) $$
*   **解读**：这是 FFN 模块的最终输出公式。它由三部分组成：
    1.  **残差连接**：直接加上输入 $u_t$。
    2.  **共享专家贡献**：将所有 $N_s$ 个共享专家的输出直接相加（相当于每个共享专家的门控值固定为1）。
    3.  **路由专家贡献**：对 $N_r$ 个路由专家的输出进行加权求和，权重就是动态计算的门控值 $g_{i,t}$。对于未被选中的专家，其 $g_{i,t}=0$。

**公式 (13) - 门控值归一化**
$$ g_{i,t}=\frac{g_{i,t}^{\prime}}{\sum_{j=1}^{N_{r}}g_{j,t}^{\prime}} $$
*   **解读**：此公式对**原始的、经过 TopK 筛选后的权重** $g_{i,t}^{\prime}$ 进行**归一化**，确保所有被激活的路由专家的门控值之和为 1。这保证了路由专家输出的加权和是一个稳定的组合。

**公式 (14) - TopK 专家选择**
$$ g_{i,t}^{\prime}=\begin{cases}s_{i,t},&s_{i,t}\in Topk(\{s_{j,t}|1\leqslant j\leqslant N_{r}\},K_{r}),\\0,&\text{otherwise},\end{cases} $$
*   **解读**：这是路由选择的**稀疏化**步骤。系统从所有路由专家的亲和力得分 $\{s_{j,t}\}$ 中选出最高的 $K_r$ 个。**只有这些被选中的专家**，其原始门控值 $g_{i,t}^{\prime}$ 才等于其亲和力得分 $s_{i,t}$；其余所有专家的 $g_{i,t}^{\prime}$ 被置为 0。这一步是 MoE 实现计算稀疏性、保持高效推理的关键。

**公式 (15) - 亲和力得分计算**
$$ s_{i,t}=\text{Sigmoid}\left(u_{t}^{T}e_{i}\right) $$
*   **解读**：此公式计算输入 Token 的表征 $u_t$ 与第 $i$ 个路由专家的**中心向量** $e_i$ 之间的**亲和力（相关性）**。
    *   $u_{t}^{T}e_{i}$ 是向量点积，值越大表示相关性越高。
    *   **Sigmoid** 函数将点积结果映射到 (0, 1) 区间，作为一个独立的“相关性概率”。使用 Sigmoid 而非 Softmax 意味着每个专家被选中的概率相对独立，而不是在全体专家中强制进行概率分布竞争。

### **核心概念与流程总结表**

| 符号 | 含义 | 在流程中的作用 |
| :--- | :--- | :--- |
| $u_t$ | 第 t 个 Token 的 FFN 输入 | 计算的起点，用于激活专家。 |
| $N_s, N_r$ | 共享专家与路由专家的总数 | 决定了模型的容量和稀疏性。 |
| $K_r$ | 激活的路由专家数量（如 K=6） | 控制计算开销与模型稀疏度的超参数。 |
| $e_i$ | 第 i 个路由专家的中心向量 | 学习得到的参数，代表该专家擅长的特征方向。 |
| $s_{i,t}$ | 亲和力得分 | 衡量输入 $u_t$ 与专家 $i$ 的匹配程度（公式15）。 |
| $g_{i,t}^{\prime}$ | 原始门控值 | 通过 TopK 选择，使模型具有稀疏性（公式14）。 |
| $g_{i,t}$ | 归一化后的门控值 | 用于对所选路由专家的输出进行加权求和（公式13）。 |
| $FFN_{i}^{(s/r)}$ | 第 i 个共享/路由专家网络 | 实际进行特征变换的神经网络模块。 |

**计算流程总结**：`输入u_t` → `计算与所有路由专家的亲和力s_i,t` → `选出Top-K个专家` → `对选中的亲和力得分进行归一化得到门控值g_i,t` → `计算所有共享专家与加权后路由专家的输出之和` → `加上残差得到最终输出h_t’`。

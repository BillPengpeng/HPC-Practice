本文主要整理《DeepSeek-V3 Technical Report》的主要内容。

## 7.0 - FP8 Training

![FP8 Training](https://pic4.zhimg.com/v2-be9827ee9fd5893b598e72fed874d29b_1440w.jpg)

### **内容概括**

**背景与挑战**：尽管低精度训练（如FP8）前景广阔，能显著提升计算速度和降低内存/通信开销，但其在大规模语言模型训练中的应用仍面临挑战。主要瓶颈在于激活值、权重和梯度中存在的**异常值**，这些异常值会限制低精度格式的有效动态范围，影响训练稳定性和模型最终性能。

**解决方案**：为应对这一挑战，团队提出了一套**细粒度的混合精度框架**。其核心创新在于引入**细粒度量化策略**（如按Tile-wise或Block-wise分组），有效扩展了FP8格式的动态范围。同时，通过**高精度累加**过程，很大程度上缓解了反量化带来的计算开销，这是实现精确FP8矩阵乘法的关键。此外，为最大化节省MoE训练中的内存与通信成本，该框架还将激活值以FP8格式缓存和分发，并将低精度优化器状态存储在BF16格式中。

**验证与效果**：该框架在与DeepSeek-V2规模相近的模型上进行了验证，训练了约1万亿Token。结果表明，**与标准的BF16训练基线相比，采用FP8训练的模型其相对损失误差始终低于0.25%**。这一误差水平完全在训练随机波动可接受的范围内，证明了该FP8训练框架在保持模型性能的前提下，有效实现了对计算和内存资源的巨大节省。

### **要点总结**

| 方面 | 核心要点 |
| :--- | :--- |
| **1. 目标与技术** | 提出一个**细粒度的混合精度框架**，使用**FP8数据格式**来训练DeepSeek-V3，旨在提升速度、降低内存与通信开销。 |
| **2. 核心挑战** | 低精度训练受限于激活值、权重和梯度中的**异常值**，它们会挤占有限的动态范围，影响训练有效性。 |
| **3. 关键创新** | **细粒度量化策略**：采用 **Tile-wise（1×Nc）** 或 **Block-wise（Nc×Nc）** 分组量化，而非整个张量，以**有效扩展FP8的动态范围**，更好地容纳异常值。 |
| **4. 工程优化** | • **高精度累加**：在FP8矩阵乘法中使用更高精度（如FP16/FP32）进行中间累加，**减少反量化开销，保证计算精度**。<br>• **内存与通信优化**：**以FP8缓存和分发激活值**，并将**优化器状态以BF16存储**，双重节省内存与带宽。 |
| **5. 实验验证** | 在近似DeepSeek-V2规模的模型上进行了充分验证（训练约1万亿Token）。 |
| **6. 最终效果** | **性能无损**：与BF16基线相比，FP8训练的**相对损失误差始终<0.25%**，差异在训练正常波动范围内，**证明了其可行性与有效性**。 |

**总结**：这项FP8训练技术是DeepSeek-V3实现其革命性**低成本训练**（如前文提到的约557万美元总成本）的又一核心支柱。它通过算法层面的细粒度量化与系统层面的精细调度，成功攻克了低精度训练的关键障碍，使大规模语言模型训练在保证性能的前提下，速度更快、资源消耗更少。

## 7.1 - 解释 Tile-wise（1×Nc）或 Block-wise（Nc×Nc）分组量化

### **内容概括**

该框架的核心是采用了 **Tile-wise** 或 **Block-wise 分组量化**策略，以有效扩展FP8格式有限的动态范围，解决激活值、权重和梯度中“异常值”干扰量化效果的关键难题。此外，通过**高精度累加**过程减轻了反量化的计算开销，并以FP8格式缓存激活值、用BF16存储优化器状态，进一步优化了MoE训练的内存与通信成本。

### **要点总结**

1.  **目标**：利用FP8数据格式训练DeepSeek-V3，以提升计算速度，降低内存和通信开销。
2.  **核心挑战**：模型中的**异常值**会挤占FP8格式本已有限的数值表示范围，导致量化后精度严重损失。
3.  **关键创新**：
    *   **细粒度分组量化**：放弃传统的对整个张量进行量化的方法，改为对张量进行更小粒度的分组（Tile-wise或Block-wise），每个小组独立量化，从而**隔离异常值的负面影响，有效扩展整体动态范围**。
    *   **高精度累加**：在FP8矩阵乘法核心中，使用更高精度（如FP16/FP32）进行中间结果的累加，以保障计算精度，并减轻反复量化/反量化带来的开销。
4.  **系统优化**：
    *   **以FP8缓存和分发激活值**，节省通信带宽和缓存内存。
    *   **以BF16存储优化器状态**，在保证足够精度的同时节省内存。
5.  **验证结果**：在大规模（约1万亿Token）预训练中，FP8训练模型的损失与BF16基线相比，**相对误差始终低于0.25%**，证明该框架在保持模型性能的前提下，实现了显著的效率提升。

### **解释 Tile-wise（1×Nc） 或 Block-wise（Nc×Nc） 分组量化**

这是一种为了应对“异常值”问题而设计的、**比传统方法更精细的量化策略**。

*   **传统量化的问题**：通常，对一个权重或激活值张量（可能是一个巨大的矩阵），我们会计算整个张量的最大值和最小值，然后根据这个**全局范围**将其线性映射到FP8的有限数值区间（如[-448, 448]）。**问题在于**，如果张量中混杂了极少数的极大或极小的“异常值”，它们就会主导这个全局范围，导致其余绝大多数“正常”数值被挤压到一个非常小的区间内，量化后分辨率极低，信息损失严重。

*   **分组量化的解决方案**：不把整个张量视为一个整体，而是将其**切割成多个更小的组（Group或Tile/Block）**，每个小组**独立寻找自己的最大值/最小值进行量化**。

    *   **Tile-wise (1×Nc) 分组**：
        *   “Tile”意为“瓦片”或“块”。这里 `1×Nc` 指的是分组形状。
        *   对于一个二维矩阵（例如，形状为 `[行， 列]`），**Tile-wise (1×Nc)** 通常意味着沿着**行方向**进行分组。它将矩阵的每一行（`1` 行）切分成若干段，每段包含 `Nc` 个连续的列元素，形成一个细长的“瓦片”。**每个这样的小瓦片独立量化**。
        *   **优点**：特别适用于异常值在行内分布不均的情况。即使某一行有一个异常值，它也只会影响包含它的那个小瓦片，该行其他瓦片以及其他行的量化完全不受影响。

    *   **Block-wise (Nc×Nc) 分组**：
        *   将矩阵切割成一个个小的、**方形或矩形的块**，每个块的大小是 `Nc×Nc`（例如，64×64）。
        *   **每个这样的二维小块独立量化**。
        *   **优点**：比Tile-wise更“局部化”，能更好地处理在二维区域内随机出现的异常值，提供更精细的动态范围控制。

## 7.2 - Mixed Precision Framework

![Mixed Precision Framework](https://pic1.zhimg.com/v2-d7fbb976038e0384313e906d8803990c_1440w.jpg)

### **内容概括**

**第一张图片（章节3.3.1）** 阐述了该框架的**顶层设计哲学与策略**。其核心是**选择性混合精度**：将模型中计算最密集、最能受益于低精度的核心运算（主要是所有GEMM操作）在FP8精度下执行，以实现理论上的**双倍计算速度**和**显著的内存节省**（例如，以FP8格式缓存激活值用于反向传播）。同时，为保障训练稳定性，对精度敏感或开销不大的关键组件（如嵌入层、输出头、MoE门控、归一化和注意力算子）则保留在BF16/FP32等高精度格式。此外，关键的模型状态（如主权重、梯度、优化器状态）也以高精度存储。

**第二张图片（Figure 7）** 则揭示了实现高效、稳定FP8训练的**两项底层核心技术**：
1.  **细粒度量化**：通过将张量划分为更小的块（Tile/Block）并独立量化，有效缓解了因异常值导致的量化误差，扩展了FP8的有效动态范围。
2.  **高精度累加**：在FP8矩阵乘法核心中，每处理128个元素（`Nc=128`）后，便将中间结果从专用计算单元提升至CUDA Core中的高精度累加器（如FP32）进行累加，从而大幅提升计算整体精度，减轻反复量化/反量化带来的开销。

### **要点总结**

| 层面 | 核心策略 | 具体实现与目的 |
| :--- | :--- | :--- |
| **顶层框架** | ****选择性混合精度** | • **FP8部分**：所有线性层的**前向、反向、权重梯度**GEMM计算。旨在**加速计算、节省内存**。<br>• **高精度部分**：嵌入层、输出头、MoE门控、归一化、注意力算子。旨在**保障数值稳定性**。<br>• **高精度存储**：主权重、权重梯度、优化器状态。确保优化过程可靠。 |
| **内存与通信优化** | **FP8激活缓存与高精度状态分片** | • 在反向传播中重用**FP8格式的激活值**，大幅减少激活内存。<br>• 高精度组件产生的内存开销，通过**在数据并行维度上进行高效分片**来最小化影响。 |
| **底层核心技术** | **细粒度量化 (Fine-grained Quantization)** | 将输入/权重张量划分为小块（如Tile-wise或Block-wise），**每个小块独立计算缩放因子并进行量化**。这能将“异常值”的破坏性影响限制在局部，保护大部分数据的表示精度，从而**有效缓解异常值导致的量化误差**。 |
| | **高精度累加 (High-precision Accumulation)** | 在执行FP8矩阵乘时，在专用计算单元（如Tensor Core）完成小规模乘加后，定期（如每128个元素）将低精度累加结果**移交至CUDA Core中的高精度寄存器（如FP32）进行累加**。这**避免了在低精度下连续累加导致的精度损失和舍入误差**，是保证FP8计算最终精度的关键。 |

## 7.3 - Fine-Grained Quantization 

### **内容概括**

这段文字阐述了DeepSeek-V3低精度训练框架中的核心技术之一——**细粒度量化**。其核心目标是解决FP8格式因指数位减少而导致的**动态范围有限**问题，从而避免训练过程中的数值上溢/下溢，并减轻模型激活值中**异常值对量化精度的破坏性影响**。

传统的量化方法通常对整个输入张量使用一个**全局缩放因子**（即将其最大值对齐到FP8可表示的最大值），这使得量化过程对异常值极为敏感，因为少数极端值会压缩绝大多数正常值的表示精度。

为解决此问题，DeepSeek-V3提出了**细粒度分组量化**：不再以整个张量为单位，而是在更精细的粒度上分组并独立缩放。具体而言，对**激活值**按`1x128`（即每个Token的每128个通道）的瓦片进行分组量化；对**权重**则按`128x128`的块进行分组量化。这种策略使量化过程能根据每个小组内元素的实际情况自适应缩放，从而更好地容纳异常值，提升整体量化精度。

文中还指出，此方法的关键修改在于沿GEMM操作的内维度引入了**逐组缩放因子**，这虽非标准FP8 GEMM原生支持，但结合其高精度累加策略可实现高效执行。最后，该设计与**微缩放格式**的理念高度一致，并与英伟达下一代Blackwell GPU架构宣布支持的更细粒度量化特性相契合，体现了其前瞻性。

### **要点总结**

| 要点 | 具体说明 |
| :--- | :--- |
| **1. 问题背景** | FP8格式**动态范围有限**，在训练中易发生数值上溢/下溢。传统**全局缩放量化**方法使整个量化过程对**激活异常值**极度敏感，严重降低量化精度。 |
| **2. 解决方案** | 提出**细粒度量化**，在比整个张量更小的**分组级别**上独立进行缩放和量化，以隔离异常值的影响。 |
| **3. 具体策略** | • **激活值**：采用 **`1x128`（Tile-wise）** 分组（即**每个Token、每128个通道**为一组）。<br>• **权重**：采用 **`128x128`（Block-wise）** 分组（即**每128个输入通道、每128个输出通道**为一个块）。 |
| **4. 核心优势** | **自适应局部缩放**：每个小组根据自身数值范围独立确定缩放因子，使量化能**更好地适应小组内的数值分布**，有效缓解异常值造成的精度损失。 |
| **5. 关键技术实现** | 在GEMM运算中引入了**沿内维度的逐组缩放因子**。虽然标准FP8 GEMM不支持此操作，但结合框架的**高精度累加策略**，可以高效实现。 |
| **6. 与硬件趋势的关联** | 此设计与**微缩放格式**理念一致。值得注意的是，英伟达下一代**Blackwell架构GPU的Tensor Core已宣布支持更细粒度的微缩放格式**，表明该设计具有前瞻性，能与未来硬件发展同步。 |

## 7.4 - Increasing Accumulation Precision

### **内容概括**

这段文字详细阐述了DeepSeek-V3低精度训练框架中，用以解决FP8 GEMM（通用矩阵乘法）核心精度瓶颈的关键技术——**提升至CUDA Core的高精度累加**。

**问题**：在NVIDIA H800 GPU上，Tensor Core执行FP8 GEMM时的**累加精度被限制在大约14位**，远低于标准FP32累加（约23位）。当矩阵乘法的内维度 `K` 很大时（这是大规模模型训练的典型场景，批次和模型宽度都很大），此问题会加剧，导致显著的相对误差（例如在 `K=4096` 的测试中误差可达2%）。然而，一些现有FP8框架仍默认使用此低精度累加，严重制约了训练精度。

**解决方案**：采用 **“提升至CUDA Core”** 的策略。具体流程如示意图Figure 7(b)所示：在Tensor Core上执行乘累加时，中间结果先用有限位宽累加；每完成 `Nc` 个元素的累加间隔后，**将这些部分和结果复制到CUDA Core的FP32寄存器中**，进行全精度的FP32累加。这一过程与框架的**细粒度量化**方案（沿内维度 `K` 的每组缩放因子）协同，可在CUDA Core上高效地完成解量化操作。

**工程实现与优化**：此修改会降低单个Warpgroup的WGMMA指令发射率。但在H800架构上，通过设计**让两个Warpgroup并发工作**：当一个Warpgroup执行“提升”操作时，另一个可以执行MMA计算，从而**重叠这两种操作，保持了Tensor Core的高利用率**。实验确定，将累积间隔设置为 `Nc = 128` 个元素（相当于4个WGMMA），是能在显著提高精度的同时，不引入显著开销的最小间隔。

### **要点总结**

| 维度 | 核心要点 |
| :--- | :--- |
| **1. 问题识别** | H800 GPU的Tensor Core进行FP8 GEMM时，**累加精度仅为~14位**，远低于FP32的~23位，在大维度K下会产生不可忽略的误差（如2%），限制训练精度。 |
| **2. 核心方案** | **提升至CUDA Core的高精度累加**：在Tensor Core上完成小规模乘累加后，定期将部分和转移到**CUDA Core的FP32寄存器**进行全精度累积。 |
| **3. 协同设计** | 此过程与**细粒度量化**（每组缩放因子）天然契合，缩放因子可在CUDA Core上作为解量化的一部分高效相乘。 |
| **4. 硬件级优化** | 利用H800架构支持**两个Warpgroup并发**的特性，**重叠**“提升至CUDA Core”与“Tensor Core计算”这两个阶段，从而**隐藏开销，维持Tensor Core高利用率**。 |
| **5. 关键参数** | 确定最佳累积间隔 **`Nc = 128` 个元素**（相当于4个WGMMA），是**显著提升精度与最小化开销之间的最优平衡点**。 |

**总结**：“提升至CUDA Core的高精度累加”是DeepSeek-V3 FP8训练框架中，与“细粒度量化”并列的另一大核心技术支柱。它精准地发现了硬件默认行为的精度缺陷，并通过巧妙的软硬件协同设计，在几乎不损失计算效率的前提下，**将关键计算路径的精度从~14位恢复至全FP32精度**，从根本上保障了低精度训练的数字稳定性和最终模型质量。这体现了其框架对计算精度极致的、不妥协的追求。

## 7.5 - Mantissa over Exponents

### **内容概括**

这段节选的论文文字，清晰地阐述了DeepSeek-V3在**FP8浮点数格式选择上的一项关键创新决策**。

**先前的主流做法**（由NVIDIA等机构的研究为代表）是一种 **“混合FP8格式”**：
*   在**前向传播**中使用精度更高的 **`E4M3`** 格式（4位指数，3位尾数）。
*   在计算**数据梯度**和**权重梯度**时，则切换到动态范围更大的 **`E5M2`** 格式（5位指数，2位尾数），以应对反向传播中可能出现的更大数值范围。

**DeepSeek-V3的改进方案**：**在所有张量（前向和反向）上统一使用 `E4M3` 格式**，以追求更高的整体计算精度。

**其可行性保障**在于论文提出的**细粒度量化策略**（即瓦片/块状缩放）。该策略通过将张量划分为更小的元素组并独立缩放，使得组内元素能有效共享指数位，从而**巧妙地缓解了 `E4M3` 格式动态范围有限的固有缺陷**，使得全程使用高精度格式成为可能。

### **要点总结**

| 对比维度 | **先前工作的混合格式 (Hybrid FP8)** | **DeepSeek-V3 的统一格式 (Unified FP8)** |
| :--- | :--- | :--- |
| **核心策略** | **因“阶段”制宜，动态切换**：<br>• **Fprop (前向)**：`E4M3` (高精度)<br>• **Dgrad/Wgrad (反向)**：`E5M2` (大范围) | **全程统一，追求精度**：<br>• **所有计算阶段**：统一使用 **`E4M3`** 格式 |
| **设计目标** | 在精度和动态范围之间取得**阶段性平衡**，防止训练溢出。 | **最大化全局计算精度**，为模型性能提供更优的数值基础。 |
| **关键支撑技术** | 依赖于硬件对两种格式的原生支持及自动切换。 | **细粒度量化 (瓦片/块状缩放)**：<br>在小元素组内独立缩放，共享指数，**扩展有效动态范围**。 |
| **主要优势** | 稳健，能更好地适应反向传播中广泛的数值分布。 | **理论精度更高**，统一的格式简化了系统设计，并与细粒度量化形成算法-硬件协同优化。 |

## 7.6 - Online Quantization

### **内容概括**

这段文字阐述了DeepSeek-V3的FP8混合精度训练框架中，与“细粒度量化”配套的一项关键执行策略——**在线量化**，并与传统的“延迟量化”方法进行了对比。

传统方案（如NVIDIA 2024b、Peng et al., 2023b等工作中采用的**张量级量化框架**）多使用 **“延迟量化”** 。该方法为了确定当前张量的缩放因子，会维护一个历史记录，追踪该张量在**之前多次迭代（prior iterations）** 中的最大绝对值，并用其来推断当前迭代的缩放因子。

为追求更高的精度和更简化的框架设计，DeepSeek-V3采用了 **“在线量化”** 。具体做法是：对于每一个需要量化的张量（无论是按`1x128`分块的激活值，还是按`128x128`分块的权重），都**实时地（online）计算其当前值的最大绝对值**。基于这个实时计算出的最大值，立即推导出缩放因子，并随后将张量在线地量化到FP8格式。

### **要点总结**

| 对比维度 | **延迟量化 (Delayed Quantization)** | **在线量化 (Online Quantization)** |
| :--- | :--- | :--- |
| **核心逻辑** | **基于历史推断当前**：使用过去若干次迭代中统计的最大绝对值（如采用移动平均）来估算当前迭代的缩放因子。 | **基于实时计算当前**：直接计算当前张量在本次迭代中的最大绝对值，并立即用于量化。 |
| **缩放因子来源** | 历史统计值（跨迭代平均或指数平均）。 | **当前张量的瞬时实际值**。 |
| **精度** | 可能引入误差，因为历史统计值无法完全精确匹配当前数据分布，尤其是当数据分布动态变化时。 | **理论精度更高**，缩放因子完全适配当前数据，能更准确地表示异常值。 |
| **框架复杂性** | 需要为每个张量维护和更新历史统计状态，增加了框架的复杂性和内存开销。 | **框架更简化**，无需维护历史状态，按需实时计算，逻辑更直接。 |
| **与细粒度量化的协同** | 可与细粒度量化结合，但历史统计需在每个细粒度分组上进行，状态管理更复杂。 | **与细粒度量化天然契合**：对每个`1x128`或`128x128`的块，独立、实时地计算其最大值并量化，实现真正的动态逐块适配。 |
| **典型应用场景** | 常见于追求稳定、对微小精度变化不敏感，或需要平滑缩放因子以避免训练波动的场景。 | 适用于对精度要求极高、数据分布可能动态变化，且追求框架简洁高效的场景（如DeepSeek-V3）。 |

## 8.0 - Low-Precision Optimizer States

### **内容概括**

这段文字描述了DeepSeek-V3训练框架中针对**优化器状态的内存占用**所采用的一项关键优化技术。

在深度学习训练中，优化器（如AdamW）需要维护额外的状态变量（如梯度的一阶矩和二阶矩），这些变量通常会消耗与模型参数本身相当甚至更多的显存。为了减少这部分内存开销，DeepSeek-V3采用了**混合精度存储策略**：

*   **降精度部分**：将AdamW优化器中用于追踪梯度统计信息（第一矩`m`和第二矩`v`）的状态变量，**从传统的FP32格式改为占用更少内存的BF16格式**。实践表明，这一改变没有导致可观测的模型性能下降。
*   **保持高精度部分**：为了确保整个训练过程的**数值稳定性**，两项关键数据仍保留在FP32格式中：
    1.  **主权重**：由优化器维护的、用于更新的模型参数主副本。
    2.  **梯度**：用于在数据并行中进行梯度累加（如支持更大的全局批次大小）。

### **要点总结**

| 优化对象 | 采用的精度格式 | 目的与原因 |
| :--- | :--- | :--- |
| **AdamW优化器状态**<br>（第一矩 `m`、第二矩 `v`） | **BF16** (替代 FP32) | **大幅减少内存占用**。经验证，这一改变**未引起可观测的性能损失**，是安全有效的内存优化手段。 |
| **主权重**<br>（Master Weights） | **FP32** | **确保数值更新的稳定性**。保持高精度的主权重副本是混合精度训练中的标准做法，可以避免舍入误差累积导致优化过程发散。 |
| **梯度**<br>（用于批次累积） | **FP32** | **保证梯度累加的精度**。在大规模分布式训练中，梯度需要在多个GPU间进行通信和累加，使用FP32格式可以防止精度损失影响最终的更新方向。 |

## 8.1 - Low-Precision Activation

### **内容概括**

此段文字阐述了在DeepSeek-V3的FP8混合精度训练框架中，为了**在反向传播时进一步减少内存消耗**所采取的一项精细优化。其核心策略是：将前向传播中计算出的中间结果（激活值）**以FP8格式缓存起来**，以备反向传播时重用，从而避免在反向时重新进行高精度计算。

然而，对于特定组件，直接应用此策略可能影响精度或效率。因此，DeepSeek-V3团队针对两类特殊的激活值进行了**定制化设计**：
1.  **注意力层之后Linear算子的输入**：因其也用于注意力算子的反向传播，对精度极为敏感，采用了**定制数据格式**和**特殊的量化瓦片转换**。
2.  **MoE中SwiGLU算子的输入**：为追求极致的内存节省，采用了**“缓存输入-重算输出”** 的策略，并结合细粒度量化进行存储。

### **要点总结**

| 序号 | 优化对象 | 核心挑战 | 解决方案 | 设计目标与效果 |
| :--- | :--- | :--- | :--- | :--- |
| **(1)** | **注意力层后Linear算子的输入激活** | 这些激活值需要同时用于**Linear层**和**注意力机制**的反向传播，对**精度异常敏感**，简单的FP8量化会引入较大误差。 | 1. **定制数据格式**：采用一种非标准的 **`E5M6`** 格式（5位指数，6位尾数），以提供比标准FP8更高的尾数精度。<br>2. **量化瓦片转换**：在反向传播时，将存储时的 `1x128` 瓦片转换为 `128x1` 瓦片，以匹配注意力反向计算的数据访问模式。<br>3. **幂次缩放**：所有缩放因子都圆整为**2的整数次幂**，使得量化/反量化可通过高效的位移操作完成，**避免引入额外的舍入误差**。 | **在保证关键路径计算精度的前提下，实现内存节省**。确保注意力机制这一核心组件在低精度环境下仍能稳定训练。 |
| **(2)** | **MoE中SwiGLU算子的输入激活** | MoE层包含大量专家，其激活值缓存是内存消耗的主要部分之一，需要**更激进但可控的优化**。 | **“缓存输入-重算输出”策略**：<br>• 前向时，仅缓存SwiGLU的**输入**（占用更少内存）。<br>• 反向时，利用缓存的输入**临时重新计算**SwiGLU的输出，以进行梯度计算。<br>• 这些缓存的输入同样使用**细粒度量化方法**存储在FP8中。 | **以微小的额外计算开销（重算）为代价，换取显著的内存峰值降低**。在内存效率和计算开销之间达成精细平衡。 |

### **技术关联与深层价值**

这两项优化并非孤立，而是与DeepSeek-V3的整体设计哲学一脉相承：
*   **延续了“算法-硬件协同设计”**：针对不同算子的特性（如注意力对精度的敏感性、MoE对内存的敏感性），设计不同的数据格式（`E5M6`）和缓存策略，而不是一刀切地使用FP8。
*   **是“细粒度量化”技术的落地应用**：在SwiGLU输入的缓存中，直接应用了细粒度量化，验证了该技术的普适性。
*   **共同构成极致内存管理**：与“低精度优化器状态”、“CPU存储EMA”等技术协同，系统性地压低了训练全流程的峰值内存，使得在有限硬件上训练超大规模模型成为可能。

**总结**：DeepSeek-V3对“低精度激活”的处理，体现了其**在微观层面进行精细化权衡的能力**——知道在哪里必须坚守精度（注意力路径），在哪里可以用计算换内存（MoE路径），并通过定制化的工程实现来落地这种权衡，这正是其实现高效率、低成本训练的核心竞争力所在。

## 8.2 - Low-Precision Communication

### **内容概括**

这段文字阐述了DeepSeek-V3训练框架中，针对混合专家模型分布式训练核心瓶颈——**跨节点通信带宽**——所设计的一项关键优化策略。

由于MoE模型训练中，Token需要在不同计算节点间被分发和组合，其通信量巨大，极易成为性能瓶颈。为缓解此问题，DeepSeek-V3采用了**混合精度通信**方案：

1.  **对通信密集型数据降精度**：将在节点间传输的、体积庞大的**激活值**和**激活梯度**，在通信前量化到**FP8格式**。具体包括：
    *   **前向传播**：在MoE的“上投影”操作**之前**，将激活值量化为FP8，然后进行分发。
    *   **反向传播**：在MoE的“下投影”操作**之前**，将激活梯度量化为FP8，然后进行分发。
2.  **对计算关键路径保精度**：对于需要在接收端对专家输出进行加权求和的**组合操作**（前向的`COMBINE`和反向的`COMBINE`），则保留在**BF16精度**下执行，以保障模型训练关键路径的计算精度。
3.  **关键技术细节**：为保持高精度并简化操作，对这些FP8数据的缩放因子同样约束为**2的整数次幂**，使得量化/反量化可通过高效的位移操作完成。

### **要点总结**

| 优化方面 | 具体策略 | 目的与原因 |
| :--- | :--- | :--- |
| **通信对象降精度** | 将**跨节点传输**的激活值/梯度**量化为FP8**。 | **核心目标：大幅减少通信数据量，缓解带宽瓶颈。** FP8格式相比BF16/FP32可减少50%-75%的通信量，从而显著提升通信效率。 |
| **计算路径保精度** | 在节点内执行**组合操作**时，使用**BF16精度**。 | **保障训练精度**。组合操作涉及对多个专家输出的加权求和，是影响模型更新的关键计算，保留较高精度以防止信息损失。 |
| **实现优化** | 缩放因子为**2的整数次幂**。 | **实现高效的无损转换**。2的幂次缩放使得量化/反量化可通过**位移指令**快速完成，避免了浮点乘除法引入的额外开销或精度损失。 |
| **设计协同** | 与此前描述的**FP8训练框架**、**细粒度量化**及**节点限制路由**深度融合。 | 形成**端到端的低精度优化流水线**：模型内部用FP8计算和缓存，节点间用FP8通信，再在关键位置恢复高精度计算，实现了计算、内存、通信三者的极致协同优化。 |

### **核心价值与关联**

这项“低精度通信”技术是DeepSeek-V3实现其革命性训练效率的**关键拼图之一**：
*   **它直接攻克了MoE分布式训练的绝对瓶颈**，使得沉重的All-to-All通信开销变得可管理。
*   **它与“节点限制路由”策略完美互补**：前者从**算法上**减少了需要通信的节点数量（M个），后者从**数据精度上**减少了每个通信单元的大小（FP8）。两者结合，产生了通信开销的乘数级下降。
*   **它体现了全局系统观**：不是孤立地优化通信或计算，而是根据数据在系统中的流向（计算→通信→计算），智能地分配不同的精度格式，在确保最终模型质量的前提下，榨干了每一比特的带宽和算力。

## 8.3 - BF16 和 FP16 格式区别

BF16和FP16都是16位的浮点数格式，但它们在**动态范围**和**精度**的分配上做了截然不同的权衡。

简单来说：
*   **BF16 的设计目标是：优先保证不溢出，范围要大。** 它继承了FP32的指数范围，但大幅牺牲了精度。
*   **FP16 的设计目标是：在有限的位数内，尽可能平衡范围和精度。** 它的范围较小，但精度更高。

### **核心区别详解**

| 特性 | **BF16 (Brain Floating Point)** | **FP16 (Half Precision)** |
| :--- | :--- | :--- |
| **总位数** | 16 bits | 16 bits |
| **符号位** | 1 bit | 1 bit |
| **指数位** | **8 bits** | **5 bits** |
| **尾数位（精度位）** | **7 bits** | **10 bits** |
| **设计哲学** | **“动态范围优先”** | **“精度范围平衡”** |
| **指数范围**| **与FP32完全一致** (8位指数，偏置127) | 较小 (5位指数，偏置15) |
| **近似范围** | ~1.18e-38 到 ~3.40e+38 **(非常大)** | ~5.96e-8 到 ~65504 **(相对较小)** |
| **精度** | 约 **2位十进制有效数字** | 约 **3-4位十进制有效数字** |
| **与FP32的兼容性** | **极高**。可以直接通过**截断/补零**与FP32相互转换，硬件和软件实现极简。 | 较低。与FP32转换需要复杂的**缩放和舍入**操作。 |

---

### **为什么会有这样的设计？**

这源于它们在AI硬件发展史上解决的不同痛点：

1.  **FP16 的困境**：
    FP16是一种很早就存在的通用计算格式。在AI训练中，人们发现用它代替FP32可以节省内存、加速计算。但问题来了：FP16的**动态范围太小**（最大值约6.5e4）。在训练深度网络时，梯度、激活值很容易超过这个范围，导致**数值上溢（变成INF）或下溢（变成0）**，从而使训练崩溃。虽然可以通过“损失缩放”等技术缓解，但增加了复杂性。

2.  **BF16 的诞生**：
    BF16是由**Google Brain团队**专门为神经网络训练设计的。它的设计思路非常巧妙：
    *   **直接拷贝FP32的8位指数**：这样就拥有了和FP32**一模一样的巨大动态范围**（~1e-38 到 ~3e+38）。在训练中几乎不用担心溢出问题，稳定性极大提升。
    *   **大幅削减尾数位到7位**：精度虽然降低了（只有约2位十进制精度），但神经网络训练被证明对**梯度/激活值的精度不那么敏感**，而对**动态范围异常敏感**。只要更新方向大致正确，模型就能收敛。牺牲一些精度来换取巨大的稳定性和便利性是值得的。

### **在DeepSeek-V3语境下的应用**

回顾您之前看的论文内容，就很好理解为什么在某些地方用BF16：

*   **低精度优化器状态**：论文提到将AdamW优化器的状态（`m`, `v`）存为BF16。这是因为优化器状态主要用来记录梯度的统计信息，**对范围的要求高**（梯度幅值可能变化很大），**对绝对精度要求相对较低**。用BF16既能大幅节省内存，又保证了稳定性。
*   **与FP32的简便转换**：在很多混合精度训练框架中，主权重保持在FP32。BF16与FP32间廉价的转换开销是一个巨大优势。

### **总结**

| 格式 | 优点 | 缺点 | 主要应用场景 |
| :--- | :--- | :--- | :--- |
| **BF16** | **动态范围大**，训练稳定；**与FP32转换成本极低**；硬件支持简单。 | **精度低**，不适合需要高精度计算的场景。 | **神经网络训练**（尤其是激活值、梯度、优化器状态） |
| **FP16** | **精度更高**，在有限范围内能表示更精细的数值。 | **动态范围小**，易导致数值溢出/下溢，训练不稳定。 | **神经网络推理**（范围固定且可预测）、**科学计算**、**图形渲染** |

**简单记忆**：在AI训练领域，**BF16因其卓越的稳定性和便利性，已成为比FP16更主流的选择**。而FP16则在推理和一些对精度有严格要求的计算中保有一席之地。DeepSeek-V3论文中根据不同的子系统需求（计算、存储、通信）混合使用FP8、BF16、FP32，正是这种“因材施教”精度策略的典范。
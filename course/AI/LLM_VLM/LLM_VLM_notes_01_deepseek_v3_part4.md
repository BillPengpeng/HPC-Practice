本文主要整理《DeepSeek-V3 Technical Report》的主要内容。

## 7.0 - FP8 Training

![FP8 Training](https://pic4.zhimg.com/v2-be9827ee9fd5893b598e72fed874d29b_1440w.jpg)

### **内容概括**

**背景与挑战**：尽管低精度训练（如FP8）前景广阔，能显著提升计算速度和降低内存/通信开销，但其在大规模语言模型训练中的应用仍面临挑战。主要瓶颈在于激活值、权重和梯度中存在的**异常值**，这些异常值会限制低精度格式的有效动态范围，影响训练稳定性和模型最终性能。

**解决方案**：为应对这一挑战，团队提出了一套**细粒度的混合精度框架**。其核心创新在于引入**细粒度量化策略**（如按Tile-wise或Block-wise分组），有效扩展了FP8格式的动态范围。同时，通过**高精度累加**过程，很大程度上缓解了反量化带来的计算开销，这是实现精确FP8矩阵乘法的关键。此外，为最大化节省MoE训练中的内存与通信成本，该框架还将激活值以FP8格式缓存和分发，并将低精度优化器状态存储在BF16格式中。

**验证与效果**：该框架在与DeepSeek-V2规模相近的模型上进行了验证，训练了约1万亿Token。结果表明，**与标准的BF16训练基线相比，采用FP8训练的模型其相对损失误差始终低于0.25%**。这一误差水平完全在训练随机波动可接受的范围内，证明了该FP8训练框架在保持模型性能的前提下，有效实现了对计算和内存资源的巨大节省。

### **要点总结**

| 方面 | 核心要点 |
| :--- | :--- |
| **1. 目标与技术** | 提出一个**细粒度的混合精度框架**，使用**FP8数据格式**来训练DeepSeek-V3，旨在提升速度、降低内存与通信开销。 |
| **2. 核心挑战** | 低精度训练受限于激活值、权重和梯度中的**异常值**，它们会挤占有限的动态范围，影响训练有效性。 |
| **3. 关键创新** | **细粒度量化策略**：采用 **Tile-wise（1×Nc）** 或 **Block-wise（Nc×Nc）** 分组量化，而非整个张量，以**有效扩展FP8的动态范围**，更好地容纳异常值。 |
| **4. 工程优化** | • **高精度累加**：在FP8矩阵乘法中使用更高精度（如FP16/FP32）进行中间累加，**减少反量化开销，保证计算精度**。<br>• **内存与通信优化**：**以FP8缓存和分发激活值**，并将**优化器状态以BF16存储**，双重节省内存与带宽。 |
| **5. 实验验证** | 在近似DeepSeek-V2规模的模型上进行了充分验证（训练约1万亿Token）。 |
| **6. 最终效果** | **性能无损**：与BF16基线相比，FP8训练的**相对损失误差始终<0.25%**，差异在训练正常波动范围内，**证明了其可行性与有效性**。 |

**总结**：这项FP8训练技术是DeepSeek-V3实现其革命性**低成本训练**（如前文提到的约557万美元总成本）的又一核心支柱。它通过算法层面的细粒度量化与系统层面的精细调度，成功攻克了低精度训练的关键障碍，使大规模语言模型训练在保证性能的前提下，速度更快、资源消耗更少。

## 7.1 - 解释 Tile-wise（1×Nc）或 Block-wise（Nc×Nc）分组量化

### **内容概括**

该框架的核心是采用了 **Tile-wise** 或 **Block-wise 分组量化**策略，以有效扩展FP8格式有限的动态范围，解决激活值、权重和梯度中“异常值”干扰量化效果的关键难题。此外，通过**高精度累加**过程减轻了反量化的计算开销，并以FP8格式缓存激活值、用BF16存储优化器状态，进一步优化了MoE训练的内存与通信成本。

### **要点总结**

1.  **目标**：利用FP8数据格式训练DeepSeek-V3，以提升计算速度，降低内存和通信开销。
2.  **核心挑战**：模型中的**异常值**会挤占FP8格式本已有限的数值表示范围，导致量化后精度严重损失。
3.  **关键创新**：
    *   **细粒度分组量化**：放弃传统的对整个张量进行量化的方法，改为对张量进行更小粒度的分组（Tile-wise或Block-wise），每个小组独立量化，从而**隔离异常值的负面影响，有效扩展整体动态范围**。
    *   **高精度累加**：在FP8矩阵乘法核心中，使用更高精度（如FP16/FP32）进行中间结果的累加，以保障计算精度，并减轻反复量化/反量化带来的开销。
4.  **系统优化**：
    *   **以FP8缓存和分发激活值**，节省通信带宽和缓存内存。
    *   **以BF16存储优化器状态**，在保证足够精度的同时节省内存。
5.  **验证结果**：在大规模（约1万亿Token）预训练中，FP8训练模型的损失与BF16基线相比，**相对误差始终低于0.25%**，证明该框架在保持模型性能的前提下，实现了显著的效率提升。

### **解释 Tile-wise（1×Nc） 或 Block-wise（Nc×Nc） 分组量化**

这是一种为了应对“异常值”问题而设计的、**比传统方法更精细的量化策略**。

*   **传统量化的问题**：通常，对一个权重或激活值张量（可能是一个巨大的矩阵），我们会计算整个张量的最大值和最小值，然后根据这个**全局范围**将其线性映射到FP8的有限数值区间（如[-448, 448]）。**问题在于**，如果张量中混杂了极少数的极大或极小的“异常值”，它们就会主导这个全局范围，导致其余绝大多数“正常”数值被挤压到一个非常小的区间内，量化后分辨率极低，信息损失严重。

*   **分组量化的解决方案**：不把整个张量视为一个整体，而是将其**切割成多个更小的组（Group或Tile/Block）**，每个小组**独立寻找自己的最大值/最小值进行量化**。

    *   **Tile-wise (1×Nc) 分组**：
        *   “Tile”意为“瓦片”或“块”。这里 `1×Nc` 指的是分组形状。
        *   对于一个二维矩阵（例如，形状为 `[行， 列]`），**Tile-wise (1×Nc)** 通常意味着沿着**行方向**进行分组。它将矩阵的每一行（`1` 行）切分成若干段，每段包含 `Nc` 个连续的列元素，形成一个细长的“瓦片”。**每个这样的小瓦片独立量化**。
        *   **优点**：特别适用于异常值在行内分布不均的情况。即使某一行有一个异常值，它也只会影响包含它的那个小瓦片，该行其他瓦片以及其他行的量化完全不受影响。

    *   **Block-wise (Nc×Nc) 分组**：
        *   将矩阵切割成一个个小的、**方形或矩形的块**，每个块的大小是 `Nc×Nc`（例如，64×64）。
        *   **每个这样的二维小块独立量化**。
        *   **优点**：比Tile-wise更“局部化”，能更好地处理在二维区域内随机出现的异常值，提供更精细的动态范围控制。

## 7.2 - Mixed Precision Framework

![Mixed Precision Framework](https://pic1.zhimg.com/v2-d7fbb976038e0384313e906d8803990c_1440w.jpg)

### **内容概括**

**第一张图片（章节3.3.1）** 阐述了该框架的**顶层设计哲学与策略**。其核心是**选择性混合精度**：将模型中计算最密集、最能受益于低精度的核心运算（主要是所有GEMM操作）在FP8精度下执行，以实现理论上的**双倍计算速度**和**显著的内存节省**（例如，以FP8格式缓存激活值用于反向传播）。同时，为保障训练稳定性，对精度敏感或开销不大的关键组件（如嵌入层、输出头、MoE门控、归一化和注意力算子）则保留在BF16/FP32等高精度格式。此外，关键的模型状态（如主权重、梯度、优化器状态）也以高精度存储。

**第二张图片（Figure 7）** 则揭示了实现高效、稳定FP8训练的**两项底层核心技术**：
1.  **细粒度量化**：通过将张量划分为更小的块（Tile/Block）并独立量化，有效缓解了因异常值导致的量化误差，扩展了FP8的有效动态范围。
2.  **高精度累加**：在FP8矩阵乘法核心中，每处理128个元素（`Nc=128`）后，便将中间结果从专用计算单元提升至CUDA Core中的高精度累加器（如FP32）进行累加，从而大幅提升计算整体精度，减轻反复量化/反量化带来的开销。

### **要点总结**

| 层面 | 核心策略 | 具体实现与目的 |
| :--- | :--- | :--- |
| **顶层框架** | ****选择性混合精度** | • **FP8部分**：所有线性层的**前向、反向、权重梯度**GEMM计算。旨在**加速计算、节省内存**。<br>• **高精度部分**：嵌入层、输出头、MoE门控、归一化、注意力算子。旨在**保障数值稳定性**。<br>• **高精度存储**：主权重、权重梯度、优化器状态。确保优化过程可靠。 |
| **内存与通信优化** | **FP8激活缓存与高精度状态分片** | • 在反向传播中重用**FP8格式的激活值**，大幅减少激活内存。<br>• 高精度组件产生的内存开销，通过**在数据并行维度上进行高效分片**来最小化影响。 |
| **底层核心技术** | **细粒度量化 (Fine-grained Quantization)** | 将输入/权重张量划分为小块（如Tile-wise或Block-wise），**每个小块独立计算缩放因子并进行量化**。这能将“异常值”的破坏性影响限制在局部，保护大部分数据的表示精度，从而**有效缓解异常值导致的量化误差**。 |
| | **高精度累加 (High-precision Accumulation)** | 在执行FP8矩阵乘时，在专用计算单元（如Tensor Core）完成小规模乘加后，定期（如每128个元素）将低精度累加结果**移交至CUDA Core中的高精度寄存器（如FP32）进行累加**。这**避免了在低精度下连续累加导致的精度损失和舍入误差**，是保证FP8计算最终精度的关键。 |

## 7.3 - Fine-Grained Quantization 

### **内容概括**

这段文字阐述了DeepSeek-V3低精度训练框架中的核心技术之一——**细粒度量化**。其核心目标是解决FP8格式因指数位减少而导致的**动态范围有限**问题，从而避免训练过程中的数值上溢/下溢，并减轻模型激活值中**异常值对量化精度的破坏性影响**。

传统的量化方法通常对整个输入张量使用一个**全局缩放因子**（即将其最大值对齐到FP8可表示的最大值），这使得量化过程对异常值极为敏感，因为少数极端值会压缩绝大多数正常值的表示精度。

为解决此问题，DeepSeek-V3提出了**细粒度分组量化**：不再以整个张量为单位，而是在更精细的粒度上分组并独立缩放。具体而言，对**激活值**按`1x128`（即每个Token的每128个通道）的瓦片进行分组量化；对**权重**则按`128x128`的块进行分组量化。这种策略使量化过程能根据每个小组内元素的实际情况自适应缩放，从而更好地容纳异常值，提升整体量化精度。

文中还指出，此方法的关键修改在于沿GEMM操作的内维度引入了**逐组缩放因子**，这虽非标准FP8 GEMM原生支持，但结合其高精度累加策略可实现高效执行。最后，该设计与**微缩放格式**的理念高度一致，并与英伟达下一代Blackwell GPU架构宣布支持的更细粒度量化特性相契合，体现了其前瞻性。

### **要点总结**

| 要点 | 具体说明 |
| :--- | :--- |
| **1. 问题背景** | FP8格式**动态范围有限**，在训练中易发生数值上溢/下溢。传统**全局缩放量化**方法使整个量化过程对**激活异常值**极度敏感，严重降低量化精度。 |
| **2. 解决方案** | 提出**细粒度量化**，在比整个张量更小的**分组级别**上独立进行缩放和量化，以隔离异常值的影响。 |
| **3. 具体策略** | • **激活值**：采用 **`1x128`（Tile-wise）** 分组（即**每个Token、每128个通道**为一组）。<br>• **权重**：采用 **`128x128`（Block-wise）** 分组（即**每128个输入通道、每128个输出通道**为一个块）。 |
| **4. 核心优势** | **自适应局部缩放**：每个小组根据自身数值范围独立确定缩放因子，使量化能**更好地适应小组内的数值分布**，有效缓解异常值造成的精度损失。 |
| **5. 关键技术实现** | 在GEMM运算中引入了**沿内维度的逐组缩放因子**。虽然标准FP8 GEMM不支持此操作，但结合框架的**高精度累加策略**，可以高效实现。 |
| **6. 与硬件趋势的关联** | 此设计与**微缩放格式**理念一致。值得注意的是，英伟达下一代**Blackwell架构GPU的Tensor Core已宣布支持更细粒度的微缩放格式**，表明该设计具有前瞻性，能与未来硬件发展同步。 |

## 7.4 - Increasing Accumulation Precision

### **内容概括**

这段文字详细阐述了DeepSeek-V3低精度训练框架中，用以解决FP8 GEMM（通用矩阵乘法）核心精度瓶颈的关键技术——**提升至CUDA Core的高精度累加**。

**问题**：在NVIDIA H800 GPU上，Tensor Core执行FP8 GEMM时的**累加精度被限制在大约14位**，远低于标准FP32累加（约23位）。当矩阵乘法的内维度 `K` 很大时（这是大规模模型训练的典型场景，批次和模型宽度都很大），此问题会加剧，导致显著的相对误差（例如在 `K=4096` 的测试中误差可达2%）。然而，一些现有FP8框架仍默认使用此低精度累加，严重制约了训练精度。

**解决方案**：采用 **“提升至CUDA Core”** 的策略。具体流程如示意图Figure 7(b)所示：在Tensor Core上执行乘累加时，中间结果先用有限位宽累加；每完成 `Nc` 个元素的累加间隔后，**将这些部分和结果复制到CUDA Core的FP32寄存器中**，进行全精度的FP32累加。这一过程与框架的**细粒度量化**方案（沿内维度 `K` 的每组缩放因子）协同，可在CUDA Core上高效地完成解量化操作。

**工程实现与优化**：此修改会降低单个Warpgroup的WGMMA指令发射率。但在H800架构上，通过设计**让两个Warpgroup并发工作**：当一个Warpgroup执行“提升”操作时，另一个可以执行MMA计算，从而**重叠这两种操作，保持了Tensor Core的高利用率**。实验确定，将累积间隔设置为 `Nc = 128` 个元素（相当于4个WGMMA），是能在显著提高精度的同时，不引入显著开销的最小间隔。

### **要点总结**

| 维度 | 核心要点 |
| :--- | :--- |
| **1. 问题识别** | H800 GPU的Tensor Core进行FP8 GEMM时，**累加精度仅为~14位**，远低于FP32的~23位，在大维度K下会产生不可忽略的误差（如2%），限制训练精度。 |
| **2. 核心方案** | **提升至CUDA Core的高精度累加**：在Tensor Core上完成小规模乘累加后，定期将部分和转移到**CUDA Core的FP32寄存器**进行全精度累积。 |
| **3. 协同设计** | 此过程与**细粒度量化**（每组缩放因子）天然契合，缩放因子可在CUDA Core上作为解量化的一部分高效相乘。 |
| **4. 硬件级优化** | 利用H800架构支持**两个Warpgroup并发**的特性，**重叠**“提升至CUDA Core”与“Tensor Core计算”这两个阶段，从而**隐藏开销，维持Tensor Core高利用率**。 |
| **5. 关键参数** | 确定最佳累积间隔 **`Nc = 128` 个元素**（相当于4个WGMMA），是**显著提升精度与最小化开销之间的最优平衡点**。 |

**总结**：“提升至CUDA Core的高精度累加”是DeepSeek-V3 FP8训练框架中，与“细粒度量化”并列的另一大核心技术支柱。它精准地发现了硬件默认行为的精度缺陷，并通过巧妙的软硬件协同设计，在几乎不损失计算效率的前提下，**将关键计算路径的精度从~14位恢复至全FP32精度**，从根本上保障了低精度训练的数字稳定性和最终模型质量。这体现了其框架对计算精度极致的、不妥协的追求。

## 7.5 - Mantissa over Exponents

### **内容概括**

这段节选的论文文字，清晰地阐述了DeepSeek-V3在**FP8浮点数格式选择上的一项关键创新决策**。

**先前的主流做法**（由NVIDIA等机构的研究为代表）是一种 **“混合FP8格式”**：
*   在**前向传播**中使用精度更高的 **`E4M3`** 格式（4位指数，3位尾数）。
*   在计算**数据梯度**和**权重梯度**时，则切换到动态范围更大的 **`E5M2`** 格式（5位指数，2位尾数），以应对反向传播中可能出现的更大数值范围。

**DeepSeek-V3的改进方案**：**在所有张量（前向和反向）上统一使用 `E4M3` 格式**，以追求更高的整体计算精度。

**其可行性保障**在于论文提出的**细粒度量化策略**（即瓦片/块状缩放）。该策略通过将张量划分为更小的元素组并独立缩放，使得组内元素能有效共享指数位，从而**巧妙地缓解了 `E4M3` 格式动态范围有限的固有缺陷**，使得全程使用高精度格式成为可能。

### **要点总结**

| 对比维度 | **先前工作的混合格式 (Hybrid FP8)** | **DeepSeek-V3 的统一格式 (Unified FP8)** |
| :--- | :--- | :--- |
| **核心策略** | **因“阶段”制宜，动态切换**：<br>• **Fprop (前向)**：`E4M3` (高精度)<br>• **Dgrad/Wgrad (反向)**：`E5M2` (大范围) | **全程统一，追求精度**：<br>• **所有计算阶段**：统一使用 **`E4M3`** 格式 |
| **设计目标** | 在精度和动态范围之间取得**阶段性平衡**，防止训练溢出。 | **最大化全局计算精度**，为模型性能提供更优的数值基础。 |
| **关键支撑技术** | 依赖于硬件对两种格式的原生支持及自动切换。 | **细粒度量化 (瓦片/块状缩放)**：<br>在小元素组内独立缩放，共享指数，**扩展有效动态范围**。 |
| **主要优势** | 稳健，能更好地适应反向传播中广泛的数值分布。 | **理论精度更高**，统一的格式简化了系统设计，并与细粒度量化形成算法-硬件协同优化。 |

## 7.6 - Online Quantization

### **内容概括**

这段文字阐述了DeepSeek-V3的FP8混合精度训练框架中，与“细粒度量化”配套的一项关键执行策略——**在线量化**，并与传统的“延迟量化”方法进行了对比。

传统方案（如NVIDIA 2024b、Peng et al., 2023b等工作中采用的**张量级量化框架**）多使用 **“延迟量化”** 。该方法为了确定当前张量的缩放因子，会维护一个历史记录，追踪该张量在**之前多次迭代（prior iterations）** 中的最大绝对值，并用其来推断当前迭代的缩放因子。

为追求更高的精度和更简化的框架设计，DeepSeek-V3采用了 **“在线量化”** 。具体做法是：对于每一个需要量化的张量（无论是按`1x128`分块的激活值，还是按`128x128`分块的权重），都**实时地（online）计算其当前值的最大绝对值**。基于这个实时计算出的最大值，立即推导出缩放因子，并随后将张量在线地量化到FP8格式。

### **要点总结**

| 对比维度 | **延迟量化 (Delayed Quantization)** | **在线量化 (Online Quantization)** |
| :--- | :--- | :--- |
| **核心逻辑** | **基于历史推断当前**：使用过去若干次迭代中统计的最大绝对值（如采用移动平均）来估算当前迭代的缩放因子。 | **基于实时计算当前**：直接计算当前张量在本次迭代中的最大绝对值，并立即用于量化。 |
| **缩放因子来源** | 历史统计值（跨迭代平均或指数平均）。 | **当前张量的瞬时实际值**。 |
| **精度** | 可能引入误差，因为历史统计值无法完全精确匹配当前数据分布，尤其是当数据分布动态变化时。 | **理论精度更高**，缩放因子完全适配当前数据，能更准确地表示异常值。 |
| **框架复杂性** | 需要为每个张量维护和更新历史统计状态，增加了框架的复杂性和内存开销。 | **框架更简化**，无需维护历史状态，按需实时计算，逻辑更直接。 |
| **与细粒度量化的协同** | 可与细粒度量化结合，但历史统计需在每个细粒度分组上进行，状态管理更复杂。 | **与细粒度量化天然契合**：对每个`1x128`或`128x128`的块，独立、实时地计算其最大值并量化，实现真正的动态逐块适配。 |
| **典型应用场景** | 常见于追求稳定、对微小精度变化不敏感，或需要平滑缩放因子以避免训练波动的场景。 | 适用于对精度要求极高、数据分布可能动态变化，且追求框架简洁高效的场景（如DeepSeek-V3）。 |

## 8.0 - Low-Precision Optimizer States

### **内容概括**

这段文字描述了DeepSeek-V3训练框架中针对**优化器状态的内存占用**所采用的一项关键优化技术。

在深度学习训练中，优化器（如AdamW）需要维护额外的状态变量（如梯度的一阶矩和二阶矩），这些变量通常会消耗与模型参数本身相当甚至更多的显存。为了减少这部分内存开销，DeepSeek-V3采用了**混合精度存储策略**：

*   **降精度部分**：将AdamW优化器中用于追踪梯度统计信息（第一矩`m`和第二矩`v`）的状态变量，**从传统的FP32格式改为占用更少内存的BF16格式**。实践表明，这一改变没有导致可观测的模型性能下降。
*   **保持高精度部分**：为了确保整个训练过程的**数值稳定性**，两项关键数据仍保留在FP32格式中：
    1.  **主权重**：由优化器维护的、用于更新的模型参数主副本。
    2.  **梯度**：用于在数据并行中进行梯度累加（如支持更大的全局批次大小）。

### **要点总结**

| 优化对象 | 采用的精度格式 | 目的与原因 |
| :--- | :--- | :--- |
| **AdamW优化器状态**<br>（第一矩 `m`、第二矩 `v`） | **BF16** (替代 FP32) | **大幅减少内存占用**。经验证，这一改变**未引起可观测的性能损失**，是安全有效的内存优化手段。 |
| **主权重**<br>（Master Weights） | **FP32** | **确保数值更新的稳定性**。保持高精度的主权重副本是混合精度训练中的标准做法，可以避免舍入误差累积导致优化过程发散。 |
| **梯度**<br>（用于批次累积） | **FP32** | **保证梯度累加的精度**。在大规模分布式训练中，梯度需要在多个GPU间进行通信和累加，使用FP32格式可以防止精度损失影响最终的更新方向。 |

## 8.1 - Low-Precision Activation

### **内容概括**

此段文字阐述了在DeepSeek-V3的FP8混合精度训练框架中，为了**在反向传播时进一步减少内存消耗**所采取的一项精细优化。其核心策略是：将前向传播中计算出的中间结果（激活值）**以FP8格式缓存起来**，以备反向传播时重用，从而避免在反向时重新进行高精度计算。

然而，对于特定组件，直接应用此策略可能影响精度或效率。因此，DeepSeek-V3团队针对两类特殊的激活值进行了**定制化设计**：
1.  **注意力层之后Linear算子的输入**：因其也用于注意力算子的反向传播，对精度极为敏感，采用了**定制数据格式**和**特殊的量化瓦片转换**。
2.  **MoE中SwiGLU算子的输入**：为追求极致的内存节省，采用了**“缓存输入-重算输出”** 的策略，并结合细粒度量化进行存储。

### **要点总结**

| 序号 | 优化对象 | 核心挑战 | 解决方案 | 设计目标与效果 |
| :--- | :--- | :--- | :--- | :--- |
| **(1)** | **注意力层后Linear算子的输入激活** | 这些激活值需要同时用于**Linear层**和**注意力机制**的反向传播，对**精度异常敏感**，简单的FP8量化会引入较大误差。 | 1. **定制数据格式**：采用一种非标准的 **`E5M6`** 格式（5位指数，6位尾数），以提供比标准FP8更高的尾数精度。<br>2. **量化瓦片转换**：在反向传播时，将存储时的 `1x128` 瓦片转换为 `128x1` 瓦片，以匹配注意力反向计算的数据访问模式。<br>3. **幂次缩放**：所有缩放因子都圆整为**2的整数次幂**，使得量化/反量化可通过高效的位移操作完成，**避免引入额外的舍入误差**。 | **在保证关键路径计算精度的前提下，实现内存节省**。确保注意力机制这一核心组件在低精度环境下仍能稳定训练。 |
| **(2)** | **MoE中SwiGLU算子的输入激活** | MoE层包含大量专家，其激活值缓存是内存消耗的主要部分之一，需要**更激进但可控的优化**。 | **“缓存输入-重算输出”策略**：<br>• 前向时，仅缓存SwiGLU的**输入**（占用更少内存）。<br>• 反向时，利用缓存的输入**临时重新计算**SwiGLU的输出，以进行梯度计算。<br>• 这些缓存的输入同样使用**细粒度量化方法**存储在FP8中。 | **以微小的额外计算开销（重算）为代价，换取显著的内存峰值降低**。在内存效率和计算开销之间达成精细平衡。 |

### **技术关联与深层价值**

这两项优化并非孤立，而是与DeepSeek-V3的整体设计哲学一脉相承：
*   **延续了“算法-硬件协同设计”**：针对不同算子的特性（如注意力对精度的敏感性、MoE对内存的敏感性），设计不同的数据格式（`E5M6`）和缓存策略，而不是一刀切地使用FP8。
*   **是“细粒度量化”技术的落地应用**：在SwiGLU输入的缓存中，直接应用了细粒度量化，验证了该技术的普适性。
*   **共同构成极致内存管理**：与“低精度优化器状态”、“CPU存储EMA”等技术协同，系统性地压低了训练全流程的峰值内存，使得在有限硬件上训练超大规模模型成为可能。

**总结**：DeepSeek-V3对“低精度激活”的处理，体现了其**在微观层面进行精细化权衡的能力**——知道在哪里必须坚守精度（注意力路径），在哪里可以用计算换内存（MoE路径），并通过定制化的工程实现来落地这种权衡，这正是其实现高效率、低成本训练的核心竞争力所在。

## 8.2 - Low-Precision Communication

### **内容概括**

这段文字阐述了DeepSeek-V3训练框架中，针对混合专家模型分布式训练核心瓶颈——**跨节点通信带宽**——所设计的一项关键优化策略。

由于MoE模型训练中，Token需要在不同计算节点间被分发和组合，其通信量巨大，极易成为性能瓶颈。为缓解此问题，DeepSeek-V3采用了**混合精度通信**方案：

1.  **对通信密集型数据降精度**：将在节点间传输的、体积庞大的**激活值**和**激活梯度**，在通信前量化到**FP8格式**。具体包括：
    *   **前向传播**：在MoE的“上投影”操作**之前**，将激活值量化为FP8，然后进行分发。
    *   **反向传播**：在MoE的“下投影”操作**之前**，将激活梯度量化为FP8，然后进行分发。
2.  **对计算关键路径保精度**：对于需要在接收端对专家输出进行加权求和的**组合操作**（前向的`COMBINE`和反向的`COMBINE`），则保留在**BF16精度**下执行，以保障模型训练关键路径的计算精度。
3.  **关键技术细节**：为保持高精度并简化操作，对这些FP8数据的缩放因子同样约束为**2的整数次幂**，使得量化/反量化可通过高效的位移操作完成。

### **要点总结**

| 优化方面 | 具体策略 | 目的与原因 |
| :--- | :--- | :--- |
| **通信对象降精度** | 将**跨节点传输**的激活值/梯度**量化为FP8**。 | **核心目标：大幅减少通信数据量，缓解带宽瓶颈。** FP8格式相比BF16/FP32可减少50%-75%的通信量，从而显著提升通信效率。 |
| **计算路径保精度** | 在节点内执行**组合操作**时，使用**BF16精度**。 | **保障训练精度**。组合操作涉及对多个专家输出的加权求和，是影响模型更新的关键计算，保留较高精度以防止信息损失。 |
| **实现优化** | 缩放因子为**2的整数次幂**。 | **实现高效的无损转换**。2的幂次缩放使得量化/反量化可通过**位移指令**快速完成，避免了浮点乘除法引入的额外开销或精度损失。 |
| **设计协同** | 与此前描述的**FP8训练框架**、**细粒度量化**及**节点限制路由**深度融合。 | 形成**端到端的低精度优化流水线**：模型内部用FP8计算和缓存，节点间用FP8通信，再在关键位置恢复高精度计算，实现了计算、内存、通信三者的极致协同优化。 |

### **核心价值与关联**

这项“低精度通信”技术是DeepSeek-V3实现其革命性训练效率的**关键拼图之一**：
*   **它直接攻克了MoE分布式训练的绝对瓶颈**，使得沉重的All-to-All通信开销变得可管理。
*   **它与“节点限制路由”策略完美互补**：前者从**算法上**减少了需要通信的节点数量（M个），后者从**数据精度上**减少了每个通信单元的大小（FP8）。两者结合，产生了通信开销的乘数级下降。
*   **它体现了全局系统观**：不是孤立地优化通信或计算，而是根据数据在系统中的流向（计算→通信→计算），智能地分配不同的精度格式，在确保最终模型质量的前提下，榨干了每一比特的带宽和算力。

## 8.3 - BF16 和 FP16 格式区别

BF16和FP16都是16位的浮点数格式，但它们在**动态范围**和**精度**的分配上做了截然不同的权衡。

简单来说：
*   **BF16 的设计目标是：优先保证不溢出，范围要大。** 它继承了FP32的指数范围，但大幅牺牲了精度。
*   **FP16 的设计目标是：在有限的位数内，尽可能平衡范围和精度。** 它的范围较小，但精度更高。

### **核心区别详解**

| 特性 | **BF16 (Brain Floating Point)** | **FP16 (Half Precision)** |
| :--- | :--- | :--- |
| **总位数** | 16 bits | 16 bits |
| **符号位** | 1 bit | 1 bit |
| **指数位** | **8 bits** | **5 bits** |
| **尾数位（精度位）** | **7 bits** | **10 bits** |
| **设计哲学** | **“动态范围优先”** | **“精度范围平衡”** |
| **指数范围**| **与FP32完全一致** (8位指数，偏置127) | 较小 (5位指数，偏置15) |
| **近似范围** | ~1.18e-38 到 ~3.40e+38 **(非常大)** | ~5.96e-8 到 ~65504 **(相对较小)** |
| **精度** | 约 **2位十进制有效数字** | 约 **3-4位十进制有效数字** |
| **与FP32的兼容性** | **极高**。可以直接通过**截断/补零**与FP32相互转换，硬件和软件实现极简。 | 较低。与FP32转换需要复杂的**缩放和舍入**操作。 |

---

### **为什么会有这样的设计？**

这源于它们在AI硬件发展史上解决的不同痛点：

1.  **FP16 的困境**：
    FP16是一种很早就存在的通用计算格式。在AI训练中，人们发现用它代替FP32可以节省内存、加速计算。但问题来了：FP16的**动态范围太小**（最大值约6.5e4）。在训练深度网络时，梯度、激活值很容易超过这个范围，导致**数值上溢（变成INF）或下溢（变成0）**，从而使训练崩溃。虽然可以通过“损失缩放”等技术缓解，但增加了复杂性。

2.  **BF16 的诞生**：
    BF16是由**Google Brain团队**专门为神经网络训练设计的。它的设计思路非常巧妙：
    *   **直接拷贝FP32的8位指数**：这样就拥有了和FP32**一模一样的巨大动态范围**（~1e-38 到 ~3e+38）。在训练中几乎不用担心溢出问题，稳定性极大提升。
    *   **大幅削减尾数位到7位**：精度虽然降低了（只有约2位十进制精度），但神经网络训练被证明对**梯度/激活值的精度不那么敏感**，而对**动态范围异常敏感**。只要更新方向大致正确，模型就能收敛。牺牲一些精度来换取巨大的稳定性和便利性是值得的。

### **在DeepSeek-V3语境下的应用**

回顾您之前看的论文内容，就很好理解为什么在某些地方用BF16：

*   **低精度优化器状态**：论文提到将AdamW优化器的状态（`m`, `v`）存为BF16。这是因为优化器状态主要用来记录梯度的统计信息，**对范围的要求高**（梯度幅值可能变化很大），**对绝对精度要求相对较低**。用BF16既能大幅节省内存，又保证了稳定性。
*   **与FP32的简便转换**：在很多混合精度训练框架中，主权重保持在FP32。BF16与FP32间廉价的转换开销是一个巨大优势。

### **总结**

| 格式 | 优点 | 缺点 | 主要应用场景 |
| :--- | :--- | :--- | :--- |
| **BF16** | **动态范围大**，训练稳定；**与FP32转换成本极低**；硬件支持简单。 | **精度低**，不适合需要高精度计算的场景。 | **神经网络训练**（尤其是激活值、梯度、优化器状态） |
| **FP16** | **精度更高**，在有限范围内能表示更精细的数值。 | **动态范围小**，易导致数值溢出/下溢，训练不稳定。 | **神经网络推理**（范围固定且可预测）、**科学计算**、**图形渲染** |

**简单记忆**：在AI训练领域，**BF16因其卓越的稳定性和便利性，已成为比FP16更主流的选择**。而FP16则在推理和一些对精度有严格要求的计算中保有一席之地。DeepSeek-V3论文中根据不同的子系统需求（计算、存储、通信）混合使用FP8、BF16、FP32，正是这种“因材施教”精度策略的典范。

## 9.0 - Inference and Deployment

### **内容概括**

部署基于与训练相同的 **H800 GPU集群**，充分利用其高速互联特性：节点内GPU通过**NVLink**互联，集群内所有GPU通过**InfiniBand（IB）** 网络全互联。

为了在线上服务中**同时满足低延迟的服务级别目标（SLO）和高吞吐量的要求**，团队采用了业界先进的 **“分离预填充与解码阶段”** 的部署策略。该策略将一次生成请求的计算清晰地划分为两个不同的阶段，并可能进行差异化的资源调度与优化。

### **要点总结**

| 方面 | 具体内容 | 目的与意义 |
| :--- | :--- | :--- |
| **1. 硬件基础设施** | 部署在 **H800集群** 上，网络拓扑与训练时一致：<br>• **节点内**：NVLink高速互联。<br>• **集群内**：InfiniBand全互联。 | 为推理提供高带宽、低延迟的通信保障，尤其有利于MoE模型所需的专家间数据交换。 |
| **2. 核心部署策略** | **分离预填充阶段与解码阶段**。 | **解决核心矛盾**：同时优化**SLO**和高**吞吐量**。<br>• **预填充**：处理用户的整个输入提示（Prompt），计算量大，可并行，对延迟有一定容忍度，适合批量处理以提高吞吐。<br>• **解码**：逐个生成输出令牌（Token），计算量相对小但次数多，对延迟极其敏感（直接影响用户体验）。 |
| **3. 策略优势** | 允许对两个阶段进行**差异化的资源分配和优化**：<br>• 可为解码阶段预留专用、低负载的资源以确保低延迟。<br>• 预填充阶段可进行大规模的动态批处理以提高硬件利用率。 | 在统一的硬件平台上，实现**服务质量（延迟）与资源效率（吞吐）的最佳平衡**，是生产级大模型服务的标准且关键的做法。 |

## 9.1 - Prefilling

### **内容概括**

预填充阶段负责处理用户输入的整个提示（Prompt），其设计目标是**在保证效率的前提下最大化系统吞吐量**。

为实现这一目标，部署采用了多层并行的复合策略：
1.  **最小部署单元**：由**4个节点**（共**128个GPU**）构成。
2.  **注意力部分并行**：采用**4路张量并行** 结合 **8路数据并行**。较小的TP维度（4）有效限制了张量并行带来的通信开销。
3.  **MoE部分并行**：采用**32路专家并行**，确保每个专家都能处理足够大的批次尺寸，从而提升计算效率。其跨节点通信流程（先IB，后NVLink转发）与训练时保持一致。
4.  **特殊优化**：对浅层的稠密MLP层使用**1路张量并行**，以节省不必要的通信。

### **要点总结**

| 优化维度 | 具体策略 | 核心目的与效果 |
| :--- | :--- | :--- |
| **1. 硬件与并行基础** | • **最小单元**：4节点，128 GPU。<br>• **注意力部分**：TP4 + DP8。<br>• **MoE部分**：EP32。<br>• **通信路径**：跨节点用IB，节点内用NVLink。 | • 奠定高吞吐基础。<br>• **平衡计算与通信**：小TP减少开销，大EP提升计算效率。<br>• 复用训练期优化，降低工程复杂性。 |
| **2. 负载均衡策略** | **冗余专家部署**：<br>• 根据在线统计，定期（如10分钟）检测高负载专家。<br>• 复制高负载专家，将其冗余部署在不同GPU上。<br>• 每台GPU除原有8个专家外，额外托管**1个冗余专家**（共9个）。<br>• 在节点内精心重排专家分布，以平衡负载。 | • **解决MoE推理核心难题**：防止热门专家成为瓶颈。<br>• **实现GPU间负载均衡**，最大化集群整体利用率。<br>• **动态自适应**，能响应请求分布的变化。 |
| **3. 微批次与通信隐藏** | **同时处理两个相似计算量的微批次**：<br>让一个微批次的**注意力/MoE计算**与另一个微批次的**All-to-All通信**操作相互重叠。 | • **隐藏通信延迟**，提升硬件利用率。<br>• 进一步提高系统吞吐量。 |
| **4. 前瞻性探索策略** | **动态冗余策略**：<br>• 每台GPU物理部署更多专家（如16个）。<br>• 每次推理时，根据实时计算的**全局最优路由方案**，仅激活其中的一部分（如9个）。 | • 在计算开销可忽略（因预填充计算量大）的前提下，**追求极致的负载均衡与资源灵活性**。<br>• 代表了未来MoE服务部署的潜在进化方向。 |

## 9.2 - Decoding

### **内容概括**

这两张图片系统地阐述了DeepSeek-V3推理服务中**解码阶段**的部署架构、核心挑战与优化技术。解码阶段负责模型的自回归逐令牌生成，其核心设计目标是**在保证极低延迟的前提下，尽可能提升系统吞吐量**。

为实现这一目标，系统采用了大规模的部署单元和精细的资源调度：
*   **最小部署单元**：**40个节点，共320个GPU**，规模远大于预填充阶段。
*   **并行策略**：注意力部分结合**张量并行**、**序列并行**与**数据并行**；MoE部分则采用大规模的**专家并行**，确保每个GPU仅承载一个专家，以最大化单个专家的处理效率。
*   **核心挑战与均衡策略**：面对MoE负载不均问题，系统定期根据线上统计确定**冗余专家集**，由64个专用GPU承载，并正在探索更灵活的**动态冗余策略**。
*   **性能优化**：为隐藏通信开销并提升吞吐，系统探索了**双微批次重叠处理**技术：将一个微批次的**注意力计算**与另一个微批次的 **`DISPATCH`、`MoE`、`COMBINE`** 过程重叠，充分利用了解码阶段注意力耗时较长的特点。

### **要点总结**

| 维度 | 核心策略 | 目的与效果 |
| :--- | :--- | :--- |
| **1. 部署规模与专家路由** | • **最小单元**：40节点，320 GPU。<br>• **路由策略**：每个Token选择**9个专家**，其中包括**1个必定被选的共享专家**（被视作高负载专家）。 | • 大规模部署为高并发、低延迟服务提供资源基础。<br>• 共享专家的固定选择简化了路由决策，确保了基础能力的稳定提供。 |
| **2. 并行架构设计** | • **注意力部分**：`TP4` + `SP` + `DP80`。<br>• **MoE部分**：`EP320`，**每GPU仅1个专家**。<br>• **通信优化**：`DISPATCH/COMBINE` 通过 **IB点对点直连** 和 **IBGDA技术** 实现超低延迟。 | • `TP/SP`处理注意力计算，`DP`处理大量并发请求。<br>• 极致的专家并行度（EP320）匹配了解码阶段专家计算轻量化的特点，减少了单个GPU的资源争抢。<br>• 专用高速通信技术是保障低延迟的关键。 |
| **3. 负载均衡机制** | • **定期静态冗余**：基于在线负载统计，定期（如几分钟）在**64个专用GPU**上确定并部署冗余的热门专家副本。<br>• **探索动态冗余**：研究更灵活的算法，以在每次推理时动态计算全局最优路由方案（即动态选择激活哪些专家）。 | • 静态冗余有效缓解了专家负载不均，是当前生产就绪的方案。<br>• 动态冗余代表了前沿探索，旨在实现极致的负载均衡和资源利用率，但其算法与内核融合的开销需要精心优化。 |
| **4. 吞吐量优化技术** | **双微批次计算-通信重叠**：<br>利用解码阶段**注意力计算占主导**的特点，让一个微批次的“注意力计算”与另一个微批次的“通信+MoE计算+结果组合”过程**重叠执行**。 | • **隐藏All-to-All通信延迟**。<br>• 提升GPU整体利用率，从而在保证低延迟的同时，**显著提高系统吞吐量**。由于每个专家批次小，MoE部分计算轻，仅需分配少量SM即可，不会拖慢关键的注意力计算。 |

## 10.0 - Communication Hardware

### **内容概括**

本节阐述了DeepSeek-V3在**实现计算与通信重叠以隐藏延迟方面**所取得的效果，**分析了当前基于GPU流式多处理器实现此功能的局限性**，并在此基础上提出了对**未来理想通信硬件的展望**。

当前，团队通过精细调度，利用部分昂贵的流式多处理器执行通信任务，成功将计算与通信重叠，显著降低了对纯通信带宽的依赖。然而，这种方法占用了本应用于核心计算的硬件资源（如H800 GPU中宝贵的SM），限制了整体计算吞吐量，并造成了张量核心的闲置。

因此，文章呼吁硬件供应商未来能开发一种专用的**协处理器**，将文中列举的几类繁重通信任务从通用计算单元中彻底卸载。同时，该硬件应能为计算单元提供一个**统一的网络视图**，简化跨异构网络（如IB与NVLink）的编程复杂性。

### **要点总结**

| 方面 | 核心内容 |
| :--- | :--- |
| **1. 当前成就与核心策略** | **实现了计算与通信的重叠**，成功隐藏了通信延迟，降低了对绝对通信带宽的依赖。 |
| **2. 现有方案的代价与局限** | • **资源占用**：依赖**昂贵的流式多处理器**来执行通信任务（如在H800上分配了20/132个SM）。<br>• **核心矛盾**：这**限制了计算吞吐量**，并导致**张量核心利用率不足**，因为本应用于计算的SM被通信任务占用。 |
| **3. 当前SM承担的通信任务** | 主要包括：<br>• **域间数据转发**：在IB网络与NVLink域之间转发并聚合数据。<br>• **缓冲区数据传输**：在RDMA缓冲区与计算缓冲区之间搬运数据。<br>• **归约操作执行**：执行All-to-All通信中“组合”阶段的归约运算。<br>• **细粒度内存管理**：管理跨IB/NVLink域、面向多个专家的分块数据传输中的内存布局。 |
| **4. 对未来硬件的愿景** | • **硬件卸载**：希望未来能有类似**NVIDIA SHARP**的**专用协处理器**（GPU协处理器或网络协处理器），将上述通信任务从SM中完全卸载。<br>• **网络统一**：从计算单元视角，**统一IB（跨节点）和NVLink（节点内）网络**，提供一个单一、简化的接口。<br>• **简化编程**：通过该统一接口，计算单元能轻松地通过提交基于简单原语（如读、写、组播、归约）的请求，来完成跨整个统一网络域的操作。 |

## 10.1 - Higher FP8 GEMM Accumulation Precision in Tensor Cores

### **内容概括**

本节指出了当前（Hopper架构）**NVIDIA Tensor Core在执行FP8 GEMM运算时存在的硬件级精度限制**，并介绍了当前的一种软件缓解方案及其局限性。

文档首先点明问题：在Hopper架构的Tensor Core中，FP8矩阵乘加运算的**累加精度被限制在大约14位**。其具体流程是，在对齐32个尾数乘积后，仅使用每个乘积的最高14位进行加法运算，并将结果以14位精度存入累加寄存器。这种限制在大规模累加（K维度大）时会引入不可忽略的数值误差。

为部分解决此问题，文档提及了一种**通过在CUDA Core中使用FP32精度寄存器来累积128次FP8乘法的结果**的软件实现方法。该方法虽有助于实现成功的FP8训练，但被明确指出这仅仅是针对Hopper架构硬件缺陷的一种妥协方案。文档最后呼吁，未来的芯片需要采用更高精度的累加器来从根本上解决此问题。

### **要点总结**

| 要点 | 具体说明 |
| :--- | :--- |
| **1. 核心问题** | **Hopper架构Tensor Core的FP8 GEMM累加器精度有限**，仅使用约**14位**进行中间结果的加法和存储，导致高精度损失。 |
| **2. 问题根源** | 硬件设计如此：在对齐指数后，**仅截取每个尾数乘积的最高14位用于累加**，超出的低位直接被舍弃。 |
| **3. 当前解决方案** | 一种软件层面的部分缓解方案：在Tensor Core完成小规模乘加后，**将128次乘法的累积结果转移到CUDA Core的FP32寄存器中进行高精度累加**。 |
| **4. 方案定位** | 该方案被定义为针对**硬件缺陷的妥协**，虽有效但非根本解决之道。 |
| **5. 未来期望** | **下一代芯片需在硬件层面提供更高精度的累加器**，以从根本上保障FP8计算的数值精度。 |

### **与您之前阅读内容的关联**

此图片内容恰好为DeepSeek-V3论文中 **“提升至CUDA Core的高精度累加”** 一节提供了**底层的硬件原理说明和背景**。
*   **问题对应**：论文中指出的“约14位累加精度”限制，其根源正是本图片所描述的Hopper Tensor Core的硬件设计。
*   **方案对应**：论文中描述的将部分和转移至CUDA Core用FP32累加的方法，正是本图片提到的软件缓解方案的具体工程实现。
*   **观点一致**：两者都认为当前方案是妥协，并隐含了对未来硬件改进的期待。

## 10.2 - Support for Tile- and Block-Wise Quantization

### **内容概括**

该图片从硬件支持层面，指出了当前GPU在实现如DeepSeek-V3所采用的**瓦片级（Tile-wise）和块级（Block-wise）细粒度量化**时所面临的瓶颈，并提出了对未来芯片的改进建议。

图片指出，**当前的GPU仅原生支持张量级（per-tensor）量化**，即整个张量使用单一的缩放因子。这无法直接支持更先进的细粒度量化。在现有实现中，当累积达到 `Nc` 间隔时，部分结果需要从**张量核心（Tensor Cores）** 复制到 **CUDA核心**，在那里乘以缩放因子并进行高精度（FP32）累加。尽管结合高精度累加策略减轻了反量化的开销，但**张量核心与CUDA核心之间频繁的数据移动**，仍然严重限制了整体的计算效率。

因此，图片建议，未来的芯片应通过在**张量核心内部**支持接收缩放因子并实现**带组缩放的矩阵乘加（MMA）运算**，来原生支持细粒度量化。这样，从部分和累加到反量化的整个计算流程都可以在张量核心内部高效完成，从而避免昂贵的数据移动，大幅提升计算效率。

### **要点总结**

| 方面 | 当前状况与挑战 | 未来愿景与建议 |
| :--- | :--- | :--- |
| **1. 硬件支持现状** | GPU仅原生支持**张量级量化**，缺乏对**瓦片/块级（细粒度）量化**的硬件加速。 | 未来芯片的**张量核心应能直接接收和处理分组缩放因子**，实现对细粒度量化的原生支持。 |
| **2. 现有实现瓶颈** | 为了实现细粒度量化，必须在`Nc`间隔点，将部分结果从**Tensor Cores** 复制到 **CUDA Cores** 进行缩放与高精度累加。 | 应将**整个部分和累加及反量化过程完全集成在张量核心内部**完成，直到产出最终结果。 |
| **3. 核心性能限制** | **频繁的Tensor Core与CUDA Core间的数据移动**成为主要性能瓶颈，限制了计算效率的进一步提升。 | 通过上述集成，**彻底消除这种低效的数据移动**，让张量核心能够持续、高效地工作。 |
| **4. 关键技术特性** | 计算流被人为分割，涉及不同处理单元（专用计算单元 vs 通用计算单元）间的协同。 | 张量核心需要支持 **“带组缩放的矩阵乘加（MMA with group scaling）”** 操作，这是实现集成的关键。 |

### **深层解读：揭示系统性瓶颈**

此图片内容与您先前讨论的 **“细粒度量化”** 和 **“高精度累加”** 技术紧密相连，并揭示了其**在现有硬件上实现的根本性约束**：
*   **细粒度量化**是一项先进的算法，但受限于硬件，其实现被迫采用了“计算-搬运-再计算”的低效模式。
*   **高精度累加**是解决Tensor Core内部精度缺陷的必要方案，但同样加剧了数据搬运的开销。

## 10.3 - Support for Online Quantization

### **内容概括**

**核心问题**在于流程割裂导致的内存带宽浪费：当前的实现需要先从高带宽内存中读取BF16格式的激活值，在计算单元中量化为FP8，再将量化后的FP8值写回HBM，最后为了执行矩阵乘累加又需要重新读取这些FP8数据。这个“读-转-写-再读”的过程造成了**冗余的内存读写操作**，严重制约了性能。

为此，图片从三个层面提出了硬件优化建议：1）通过**指令融合**将量化与数据搬运合并；2）增加**细粒度类型转换指令**以优化特定计算图融合；3）探索**近内存计算**架构，从根本上减少片外数据访问。这些建议旨在将量化操作“隐身”在数据流动的过程中，从而释放内存带宽和计算潜力。

### **要点总结**

| 层面 | 问题 / 现状 | 优化建议与愿景 |
| :--- | :--- | :--- |
| **根本问题** | **冗余内存访问**：支持在线量化需要 **“读取(BF16)→量化(FP8)→写回(HBM)→再次读取(FP8)”** 的流程，导致**内存带宽成为瓶颈**。 | 从芯片架构和指令集层面进行优化，**将量化过程与数据移动过程深度融合甚至消除**。 |
| **建议一：指令与搬运融合** | 量化（类型转换）与数据搬运（通过TMA）是**两个独立的操作**。 | **将FP8类型转换与TMA内存访问融合为单一操作**，使量化能在数据从全局内存传输到共享内存的过程中完成，避免额外读写。 |
| **建议二：细粒度指令支持** | 缺乏高效的细粒度类型转换指令，阻碍了像“层归一化+量化”这类计算的算子融合优化。 | **支持Warp级别的类型转换指令**，以便更好地将归一化等操作与量化转换融合，提升局部计算效率。 |
| **建议三：架构级革新** | 数据必须在GPU计算核心和HBM之间来回移动，量化计算占用核心资源并消耗带宽。 | **采用近内存计算**：将量化计算逻辑放置在HBM内存附近。数据从HBM读入GPU芯片时，**直接以FP8格式转换并传输**，预计可减少约**50%的片外内存访问**。 |

## 10.4 - Support for Transposed GEMM Operations

### **内容概括**

该图片指出，在当前的GPU架构下，将**矩阵转置操作**与核心的**GEMM（通用矩阵乘法）运算**高效地融合在一起非常困难且流程繁琐，这成为量化工作流中的一个性能瓶颈。

文中以DeepSeek-V3的工作流程为例说明了这个问题：在前向传播时，激活值被量化为 `1x128` 的FP8瓦片并存储。然而，在反向传播阶段，为了计算梯度，需要将这些矩阵从内存中读出、进行反量化、执行转置、再重新量化为 `128x1` 的瓦片，最后写回高带宽内存（HBM）。这一系列操作产生了大量冗余的内存访问。

因此，文章建议未来的芯片应在执行矩阵乘加（MMA）操作之前，支持**从共享内存中直接以转置的方式读取矩阵**（对于训练和推理中都需要的精度）。结合之前提出的 **“FP8格式转换与TMA访问相融合”** 的优化，这一增强将能显著简化和加速整个量化工作流程。

### **要点总结**

| 层面 | 当前问题与挑战 | 未来硬件优化建议 |
| :--- | :--- | :--- |
| **核心问题** | **矩阵转置与GEMM融合困难**：当前的硬件和软件流程无法高效地将这两个操作合并，导致需要多次显式的内存读写和格式转换。 | **支持直接的转置读取**：使芯片能够在执行MMA前，直接从共享内存中读取已转置的矩阵数据，省去显式的转置操作和关联的内存搬运。 |
| **具体工作流瓶颈** | 在反向传播中，处理先前缓存的 `1x128` FP8激活瓦片时，流程为：<br>**读取(HBM) → 反量化 → 转置 → 重新量化(为128x1) → 写回(HBM)**。<br>这是一个**内存密集型**的繁琐过程。 | 理想的新工作流：<br>通过融合的 **“TMA访问+格式转换”** 指令，直接从全局内存读取BF16数据并在线量化为FP8瓦片进入共享内存；随后，MMA单元能**直接以转置布局**从共享内存读取数据进行计算。 |
| **优化本质** | 将本应紧密耦合的**数据重排（转置）** 与**核心计算（GEMM）** 在硬件层面解耦，迫使在软件层面进行低效的显式处理。 | 在内存子系统（共享内存）与计算单元（Tensor Core）的接口层面，**增加对数据布局的灵活感知和支持**，实现计算友好的数据供给。 |
| **协同效应** | 单独解决转置或量化的问题，都只能带来局部优化。 | 将 **“转置读取”** 与 **“在线量化融合”** 两项建议结合，能**从数据搬运的源头到计算的核心，系统性地重塑和简化整个数据通路**，释放最大性能潜力。 |
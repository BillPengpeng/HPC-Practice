本文主要整理《Qwen2.5-VL Technical Report》的主要内容。

## 1 - Abstract

1.  **模型定位**：Qwen2.5-VL是Qwen多模态系列的最新旗舰模型，旨在实现对世界的深入理解和交互。
2.  **核心能力**：
    *   **多模态理解**：在视觉识别、物体精确定位、文档/表格解析、图表分析、长视频理解（支持小时级，秒级事件定位）方面有显著提升。
    *   **视觉代理**：可作为交互式智能体，在操作电脑、手机等真实场景中进行推理、使用工具和执行任务。
3.  **技术创新**：
    *   **动态分辨率处理**：可原生处理不同尺寸的图像，无需依赖传统归一化技术，更好地感知空间尺度。
    *   **绝对时间编码**：支持处理长视频，理解时间动态。
    *   **高效架构**：通过从头训练动态分辨率ViT并结合窗口注意力，在保持原生分辨率的同时显著降低计算开销。
4.  **模型尺寸与性能**：
    *   提供 **3B、7B、72B** 三种参数规模的版本，以满足从边缘计算到高性能服务器的不同场景。
    *   其中，**旗舰版72B模型在文档和图表理解方面，性能可比肩GPT-4o、Claude 3.5 Sonnet等顶尖模型**（第2张图的环形对比图直观支撑了此结论）。
    *   较小的7B和3B模型在同类竞品中也表现出色，适合资源受限环境。
5.  **语言能力保持**：在提升视觉能力的同时，保持了Qwen2.5大模型的核心语言能力。

## 2 - Introduction

Qwen2.5-VL的核心要点可归纳为“**一个目标、四项技术、四大能力**”：

### **一个核心目标**
- **攻克精细视觉感知瓶颈**：致力于解决当前LVLMs在细节理解上的不足，为高级多模态推理和现实世界智能体应用打下坚实基础。

### **四项关键技术贡献**
1.  **视觉编码器优化**：在视觉编码器中实现**窗口注意力机制**，提升推理效率。
2.  **动态视频理解**：引入**动态FPS采样**，将动态分辨率处理能力扩展到时间维度，实现对不同帧率视频的全面理解。
3.  **时序位置编码升级**：在时间域升级MRoPE（多维旋转位置编码），对齐绝对时间，促进更复杂的时序序列学习。
4.  **大规模高质量数据**：精心构建预训练与指令微调数据，并将**预训练语料从1.2万亿词元大幅扩展至4.1万亿词元**。

### **四大卓越能力特性**
1.  **强大的文档解析**：将文本识别升级为**全文档解析**，擅长处理多场景、多语言及包含手写、表格、图表、化学式、乐谱等元素的复杂文档。
2.  **精确的对象定位**：大幅提升在检测、指向和计数物体方面的准确性，支持绝对坐标和JSON格式，适用于高级空间推理。
3.  **超长视频与精细时序理解**：支持理解**长达数小时的视频**，并能以**秒级精度**定位和提取事件片段。
4.  **增强的设备代理功能**：借助先进的定位、推理和决策能力，显著增强了模型在**智能手机和电脑**上作为智能体的操作功能。

## 3.0 - Model Architecture

![Qwen2.5-VL](https://pic3.zhimg.com/v2-fb9df7fad7cf616e6dee4c763cb3455c_1440w.jpg)

### 1. 三层级架构设计
模型采用清晰的三段式处理流程：
- **视觉编码器**：负责从原始图像中提取特征。
- **视觉-语言融合器**：负责将视觉特征序列压缩并投影到语言模型空间。
- **大语言模型**：负责接收融合后的特征，并进行最终的理解与推理生成。

### 2. 核心架构创新
- **统一的视觉编码器**：三种尺寸的模型**共享完全相同的视觉编码器架构**（ViT参数一致），确保了底层视觉特征提取能力的一致性。其关键创新包括：
    - 引入**2D旋转位置编码（2D-RoPE）** 和**窗口注意力机制**，以支持原生处理不同分辨率的输入图像，并加速计算。
    - 输入图像的尺寸在训练和推理时会被调整至**28的倍数**，再以**14的步长**进行分块处理。
- **高效的融合机制**：采用简单的**多层感知机**来动态压缩视觉特征序列的长度，解决因高分辨率图像产生的长序列效率问题，再将其投影到与LLM匹配的维度。
- **适应多模态的LLM**：在强大的Qwen2.5语言模型基础上，将传统的1D文本位置编码（RoPE）升级为**对齐绝对时间的多模态旋转位置编码**，使其能更好地理解视觉特征在空间和时间上的关系。

### 3. 三种模型的配置策略
从配置表可看出明确的缩放策略：
- **视觉编码器完全一致**：隐藏层大小、层数、注意力头数等在所有版本中固定，表明视觉骨干网络是统一且轻量化的设计。
- **融合器与LLM逐级放大**：
    - **融合器的输出通道** 和 **LLM的隐藏层大小** 严格对应（2048, 3584, 8192），确保二者无缝对接。
    - **LLM部分差异显著**：72B模型的层数、注意力头数、中间层维度等远大于7B和3B模型，这是其强大推理能力的主要来源。
    - **训练数据统一**：所有模型均在**4.1万亿词元**的语料上训练，保证了基础语言能力的下限。

**总结**：Qwen2.5-VL采用了一个经过精心优化、模块化的架构。它通过一个**统一的视觉编码器**保障了基础的视觉感知能力，再通过一个**可缩放的“融合器-LLM”核心**来提供不同级别的计算与推理能力，从而实现了从3B到72B的全系列高性能覆盖。这种设计在保证架构统一和训练效率的同时，为用户提供了灵活的性能选择。

## 3.1 - Fast and Efficient Vision Encoder

### 内容概括

1.  **第1张图（架构设计）**：详细介绍了为应对多模态大语言模型中视觉编码器的计算挑战而进行的**架构重设计**。核心目标是解决因支持动态分辨率输入而带来的计算负载不均衡问题。为此，引入了**窗口注意力机制**大幅降低计算复杂度，采用**2D旋转位置编码（RoPE）** 以捕捉空间关系，并设计了适配视频处理的**3D分块策略**。此外，网络结构也与大语言模型（LLM）对齐，采用了RMSNorm和SwiGLU。

2.  **第2张图（训练方法）**：补充说明了训练过程中的一个关键策略：**动态分辨率训练**。即在训练时，图像根据其**原始宽高比**进行随机采样，而非固定缩放至同一尺寸。这一方法确保了模型能够有效泛化到不同分辨率和大小的输入数据，提升了模型的适应性和训练稳定性。

### 要点总结

综合来看，Qwen2.5-VL视觉编码器的设计与训练围绕 **“高效处理动态分辨率输入”** 这一核心目标展开，主要要点如下：

#### 1. **核心目标：解决动态分辨率下的计算难题**
*   设计初衷是让视觉编码器能够**原生处理任意分辨率的输入图像**，避免因统一缩放导致的失真或信息损失。
*   主要挑战在于不同分辨率图像的计算负载（FLOPs）差异巨大，需要平衡训练与推理的效率。

#### 2. **关键创新：窗口注意力与混合注意力设计**
*   为将计算复杂度从与图像块数量的平方关系降至线性关系，在大部分网络层中引入了**窗口注意力**。
*   采用一种**混合设计**：仅保留4层使用全局自注意力以获取全局上下文，其余层均使用最大窗口为112x112（对应8x8个图像块）的窗口注意力。对于小于此窗口的区域，则不做填充直接处理，从而真正实现**原生分辨率计算**。

#### 3. **多模态扩展：支持视频与结构对齐**
*   **位置编码**：采用**2D RoPE**，能更好地理解图像在二维空间中的位置关系。
*   **视频处理**：将视频视为3D数据，将连续两帧作为一组进行分块，显著减少了输入语言模型的令牌数量，提升了处理序列视频数据的效率。
*   **结构对齐LLM**：使用**RMSNorm**和**SwiGLU**作为归一化和激活函数，使其网络设计与大语言模型保持一致，提升了视觉与语言组件间的兼容性和整体计算效率。

#### 4. **训练策略：动态采样与端到端训练**
*   **动态分辨率训练**：在训练阶段，图像按其**原始纵横比随机采样**，迫使模型学习适应各种尺寸和比例的输入，这是其强大泛化能力的基础。
*   **训练流程**：视觉编码器采用**从头开始训练**的方式，经历了CLIP预训练、视觉-语言对齐、端到端微调等多个阶段，确保学到的表征既通用又与语言模型深度对齐。

## 3.2 - Native Dynamic Resolution and Frame Rate

### 内容概括
本节阐述了 **Qwen2.5-VL在处理多模态输入时，在空间和时间维度上的两项原生动态处理能力**：
1.  **空间动态分辨率**：模型能够原生处理任意尺寸的图像，直接使用图像的真实像素尺寸来表示空间位置信息，从而学习到尺度信息。
2.  **时间动态帧率与编码**：针对视频输入，模型结合了动态帧率训练和一种新颖的绝对时间编码策略，使其能够理解视频的时间动态，且无需额外计算开销。

### 要点总结

#### 1. 空间维度：原生动态分辨率处理
- **核心做法**：将不同尺寸的图像动态转换为对应长度的标记序列。与传统方法不同，它**直接使用输入图像的实际宽高（像素值）** 来表示边界框、坐标点等空间特征。
- **核心优势**：使模型能够**内在地学习尺度信息**，无需依赖坐标归一化，从而显著提升其处理各种分辨率图像的能力和精度。

#### 2. 时间维度：动态帧率与绝对时间编码
- **动态帧率训练**：模型训练时能适应可变的视频帧率，从而更好地捕捉视频内容的时间动态变化。
- **创新的时间编码策略**：
    - **方法**：引入了一种高效的新策略，将**多模态旋转位置编码（MRoPE）的ID直接与时间戳对齐**。
    - **原理**：模型通过时间维度ID之间的间隔来理解时间的节奏（即事件发生的先后与快慢）。
    - **最大优势**：在实现精确时间理解（时间定位）的同时，**无需引入任何额外的计算开销或辅助网络模块**（如文本时间戳或额外的预测头），效率极高。

## 3.3 - Multimodal Rotary Position Embedding Aligned to Absolute Time

### 内容概括
本节阐述了 **Qwen2.5-VL 对其位置编码系统的一项关键改进**。它在 Qwen2-VL 引入的**多模态旋转位置嵌入**基础上，通过将时间组件与绝对时间对齐，有效解决了模型处理不同帧率视频时的时间理解问题。

### 要点总结
本节核心是**对MRoPE的优化，使其能理解视频的绝对时间节奏**。

#### 1. 基础：Qwen2-VL 的 MRoPE
- **作用**：为视觉语言模型中的序列数据（文本、图像、视频）提供位置信息。
- **三维分解**：将位置嵌入分解为三个独立组件：
    - **时间**：区分不同时刻（如视频帧序列）。
    - **高度** 与 **宽度**：区分二维空间中的位置。
- **应用方式**：
    - **文本**：三维ID相同，功能等价于传统1D RoPE。
    - **图像**：时间ID固定，高度和宽度ID根据图像块的空间位置分配。
    - **视频**：时间ID随帧数递增，高度和宽度ID分配方式同静态图像。

#### 2. 改进：Qwen2.5-VL 的时间对齐
- **原有问题**：Qwen2-VL 中，MRoPE 的**时间ID仅与输入帧的序号绑定**。这导致模型无法感知视频内容变化的真实速度（快慢）和事件的绝对时间点，因为它只“数帧数”，而不理解帧之间的实际时间间隔。
- **核心创新**：Qwen2.5-VL 将 MRoPE 的**时间组件与绝对时间戳对齐**。
- **工作原理**：模型通过**学习时间ID之间的间隔**，来理解视频内容的时间节奏。无论视频以何种帧率采样，相同的事件都能在时间ID上获得一致的对齐。
- **关键优势**：使模型能够**一致地理解不同帧率下的视频时间动态**，为精确的时间定位和长视频理解提供了基础，而无需引入额外计算模块。

## 4.0 - Pre-Training Data

### 内容概括

1.  **数据规模与策略（第1张图）**：预训练数据量从上一代的1.2万亿词元大幅扩充至约**4万亿词元**。数据集通过多种方法构建，包含图像描述、交错图文、OCR、知识问答、定位、文档、视频及智能体交互等丰富类型，并在训练中动态调整比例以优化学习。特别强调了**交错图文数据**对模型的三点关键价值，并指出其存在噪声和质量问题。

2.  **数据质量控制与核心能力数据构建（第2张图）**：
    *   **质量把控**：为解决交错数据质量问题，建立了两步清洗与四维评分流程（文本质量、图文相关性、互补性、信息密度平衡）。
    *   **基础能力**：采用**绝对位置坐标**（基于图像实际像素）进行训练，以精准感知世界尺度。
    *   **基础与指向数据**：通过合成与增强技术构建了包含边界框和指向点的大规模综合数据集，并扩充了超万类物体类别以提升开放词汇检测能力。
    *   **文档解析数据**：为让模型具备全能文档解析能力，合成了包含表格、图表、公式、乐谱、化学式等元素的文档数据，并创新性地采用统一的**HTML格式**来结构化表示所有元素及其布局坐标。

3.  **数据格式细节与扩展（第3张图）**：
    *   具体展示了**QwenVL HTML格式**的示例，说明了如何用标签统一表示各类文档元素。
    *   补充说明了**OCR数据**（含多语言支持）、**图表/表格数据**的合成与处理，以及**视频数据**（动态采样帧率、生成长视频描述与多格式时间戳）和**智能体数据**（截图感知与决策动作统一格式化）的构建方法。

4.  **智能体推理数据生成（第4张图）**：专门介绍了为增强智能体决策鲁棒性、防止过拟合而引入的**推理链数据生成方法**。通过人工与模型标注者，为每个真实操作步骤生成解释意图的推理内容，并用模型过滤器确保质量。

### 要点总结

#### **一个基础：数据量的飞跃与类型的完备**
*   预训练数据量**扩充超3倍**（至4万亿词元），并囊括了支撑其各项能力所需的**全部关键数据类型**，从基础图文对到复杂的文档、视频、智能体交互数据。

#### **两大支柱：质量与统一**
1.  **严格的质量控制**：针对关键的交错图文数据，建立了**精细的四维度评分与清洗流程**，确保用于训练的数据具有高相关性、互补性和平衡性，这是模型实现复杂多模态推理的基础。
2.  **统一的坐标与格式**：
    *   **坐标统一**：训练中坚持使用基于图像真实尺寸的**绝对像素坐标**，使模型能直接、准确地理解物体在真实世界中的尺度和空间关系。
    *   **格式统一**：为文档解析设计了**统一的QwenVL HTML格式**，将文档的布局、文本、图表等所有元素及其空间坐标整合在一个结构化框架中，解决了传统多模型碎片化解析的难题。

#### **三类关键数据创新**
1.  **针对核心感知能力的综合数据**：不仅大量合成基础（定位）数据，还刻意构造了**包含“不存在类别”和“多实例”的图像**，以提升模型在开放词汇和极端场景下的物体检测与定位泛化能力。
2.  **面向时序理解的动态视频数据**：通过**训练时动态采样视频帧率（FPS）**，使模型能鲁棒地理解不同节奏的视频；为长视频（超半小时）专门合成描述，并使用多种时间戳格式，强化其长视频理解与精确定位能力。
3.  **提升智能体决策鲁棒性的推理数据**：为智能体操作数据引入了**“推理链”生成步骤**，要求解释每个操作背后的意图。这一设计能有效**防止模型对操作序列的简单过拟合**，使其在真实场景中具备更强的泛化和决策能力。

**总结**：Qwen2.5-VL的数据构建体系是一个规模宏大、设计精密、目标明确的系统工程。它通过**量质并举的数据扩展、严格的质量过滤、统一的表征规范以及针对每项子任务（定位、文档、视频、智能体）的特化数据设计**，为模型最终实现全面、精准且鲁棒的多模态理解与交互能力提供了坚实的数据燃料。

## 4.1 - Training Recipe

### 内容概括
训练采用**三阶段流程**，从视觉与语言的对齐开始，逐步扩展到复杂多模态任务和长序列处理。同时，为应对不同输入（图像尺寸、文本长度）带来的计算负载不均衡挑战，采用了**动态批处理和序列长度规范化**技术来优化训练效率。

### 要点总结
Qwen2.5-VL 的训练配方核心在于 **“渐进式能力构建”** 与 **“动态计算优化”**。

#### 1. 渐进式三阶段训练
训练过程被精心划分为三个明确的阶段，逐步解锁并增强模型能力：
- **第一阶段：视觉-语言对齐**
    - **目标**：为多模态理解奠定基础。
    - **操作**：**仅训练视觉编码器**，保持LLM参数冻结。
    - **数据**：使用图像描述、视觉知识、OCR数据，重点提升视觉特征与语言模型的初步对齐能力。
- **第二阶段：全参数复杂推理**
    - **目标**：增强处理复杂视觉信息和多模态推理的能力。
    - **操作**：**解冻并训练全部模型参数**。
    - **数据**：引入更复杂的数据集，如交错图文、多任务学习、视觉问答、多模态数学、智能体任务、视频理解及纯文本数据，以建立更深层次的跨模态连接。
- **第三阶段：长序列与高级任务**
    - **目标**：提升处理长上下文和依赖关系的能力，以应对更高级的任务。
    - **操作**：在第二阶段基础上，**增加输入序列长度**。
    - **数据**：重点引入**更长序列的视频数据和智能体数据**，以增强模型的长程推理和复杂任务处理精度。

#### 2. 动态计算负载均衡
针对多模态输入尺寸不一导致的GPU计算负载不均问题，设计了一套高效的动态打包策略：
- **核心挑战**：图像分辨率和文本长度的差异，使得不同数据样本在LLM部分的计算量（FLOPs）差异巨大。
- **解决方案**：基于LLM的输入序列长度**动态打包数据样本**，确保每个训练批次在不同GPU上产生的计算负载基本一致。
- **具体实施**：
    - 第一、二阶段：统一将序列长度规范至 **8,192**。
    - 第三阶段：为适应长视频和复杂任务，将序列长度提升至 **32,768**。

## 5.0 - Post-training

### 内容概括
本节旨在通过两个连续的阶段，分别解决模型能力的“专业化”和“行为优化”问题：
1.  **监督微调**：旨在弥合预训练表示与下游任务需求之间的差距，通过特定格式的指令数据进行优化。
2.  **直接偏好优化**：在此阶段之上，基于人类反馈进一步优化模型的行为，使其输出更符合人类的偏好。

### 要点总结
后训练阶段的核心是 **“能力对齐”与“偏好对齐”的递进式优化**，其要点如下：

#### 1. 总体策略：双阶段范式
- **监督微调**：首先进行**参数高效的领域适应**，目标是**表征的落地**，即让模型学会正确理解和执行多模态指令。
- **直接偏好优化**：随后进行**人类偏好蒸馏**，目标是**行为的精炼**，即让模型的回答风格和质量更符合人类的评判标准。

#### 2. 监督微调阶段的关键创新
此阶段采用了 **ChatML格式** 来组织数据，与预训练数据模式区分，实现了三项关键的技术性适应：
- **显式的对话角色标记**：为多模态的轮流对话（如用户上传图片、助手回复）提供清晰的结构。
- **视觉嵌入的结构化注入**：将视觉特征与文本指令一同以结构化的方式输入模型。
- **跨模态位置关系的保持**：通过格式感知的数据打包，保持视觉与语言特征之间的空间和时序对应关系。
- **作用**：通过接触精心构建的多模态指令-响应对，SFT阶段实现了高效的知识迁移，同时保持了预训练特征的完整性。

#### 3. 直接偏好优化阶段的目标
- **方法**：基于 **DPO（Direct Preference Optimization）** 方法。
- **目标**：利用人类对模型不同输出的偏好数据（如选择更佳的回答），直接优化模型策略，使其输出分布更符合人类的价值判断，从而获得更安全、更优质、更符合期望的模型行为。

**总结**：Qwen2.5-VL的后训练是一个精心设计的两步走过程。**SFT阶段**通过特定的数据格式和结构化注入，高效地教会模型“如何正确使用”其预训练获得的多模态能力来执行任务。**随后的DPO阶段**则像一位“教练”，根据人类的反馈进一步打磨和优化模型的“回答风格”与“判断标准”，使其最终行为更优、更可控。

## 5.1 - Instruction Data

### 内容概括
本节介绍了用于模型**监督微调（SFT）** 的指令数据集。该数据集经过精心策划，旨在全面提升模型遵循多模态指令的能力。其核心特点在于**规模、平衡性与多样性**：总量约200万条，**纯文本与多模态数据各占一半**，并结构化地涵盖了从简单到复杂的对话交互以及广泛的下游任务场景。

### 要点总结

#### 1. **数据构成与规模**
- **总体规模**：数据集包含约 **200万** 条指令数据。
- **模态平衡**：数据**均匀分配**为**纯文本数据（50%）** 和**多模态数据（50%）**。后者包括图像-文本和视频-文本组合。
- **资源消耗**：尽管数量对等，但多模态数据因包含视觉和时序信息，在训练中会消耗**显著更多的计算资源**。

#### 2. **语言与对话设计**
- **语言分布**：以**中文和英文**为主，同时包含其他语言数据以支持多语言能力。
- **对话复杂度**：结构化地包含了**单轮**和**多轮**对话交互。
- **场景模拟**：通过设计从**单张图像**到**多张图像序列**的输入场景，模拟真实的对话动态。

#### 3. **广泛的任务覆盖与专用数据**
- **通用应用子集**：包含针对**通用视觉问答、图像描述、数学解题、代码任务和安全查询**的专用数据。
- **专业领域数据集**：还专门构建了用于提升特定领域能力的数据库，包括：
    - **文档与光学字符识别**
    - **物体定位**
    - **视频分析**
    - **智能体交互**

#### 4. **数据来源与目标**
- **来源多样**：主要来自开源库，并补充了精选的购买数据和在线查询数据，以确保广泛的覆盖面和代表性。
- **核心目标**：通过这种结构化、多样化的数据构成，确保SFT阶段能够有效地将预训练获得的模型表征，与复杂多样的下游多模态任务需求对齐，从而培养出强大且具有上下文感知能力的模型性能。

**总结**：该指令数据集是一个为**高效对齐**而设计的综合性训练资源。它通过**严格的模态平衡、模拟真实对话的结构、以及对通用任务与专业领域的全面覆盖**，为模型将从预训练中学到的“知识”转化为可靠“技能”提供了关键的训练基础。

## 5.2 - Data Filtering Pipeline

### 内容概括
本节指出，开源与合成数据通常存在噪声、冗余和低质量问题，会损害模型性能。为此，构建了一个**两阶段数据过滤流程**，旨在系统性地提升监督微调数据集的质量：
1.  **第一阶段：领域特定分类**：使用专门的分类模型对海量问答对进行层次化分类，将其组织到不同的任务领域和子类别中，以便后续进行有针对性的过滤。
2.  **第二阶段：领域定制过滤**：结合基于规则和基于模型的过滤方法，针对不同领域（如文档处理、OCR、视觉定位）的特点，实施精细化的清洗与筛选，以确保最终数据的质量和相关性。

### 要点总结
该数据过滤流程的核心目标是**通过系统化、多层次的筛选，确保用于指令微调的数据具备高质量与高相关性**，其要点如下：

#### 1. **核心目标：解决数据质量的根本挑战**
- **问题**：开源与合成数据集质量参差不齐，包含噪声、冗余和低质量样本，会干扰模型对齐，损害其处理复杂任务的能力。
- **对策**：实施严格的清洗与过滤流程，确保数据质量是获得鲁棒、可靠模型性能的关键前提。

#### 2. **第一阶段：分层分类以实现精细化处理**
- **方法**：使用基于 **Qwen2-VL-72B** 的专用分类模型 **Qwen2-VL-InstaG**，对问答对进行层次化分类。
- **结构**：将数据组织到**8个主领域**（如编程、规划等）及其下属的**30个细分子类别**中。
- **目的**：建立“领域感知”的过滤策略，使得后续清洗能根据不同任务类别的具体特点进行优化，提升过滤的针对性和效率。

#### 3. **第二阶段：结合规则与模型的复合过滤**
- **规则过滤**：
    - **去除重复**：识别并去除重复模式，防止模型学习过程被扭曲。
    - **修正格式**：排除不完整、截断或格式错误的回答。
    - **安全与伦理**：丢弃无关或可能产生有害输出的查询和回答。
- **模型过滤**：
    - **评估工具**：利用在 **Qwen2.5-VL系列** 上训练的奖励模型进行多维度评估。
    - **评估维度**：
        - **对问题**：评估**复杂性**与**相关性**，保留具有适当挑战性且上下文相关的样本。
        - **对回答**：评估**正确性、完整性、清晰度、相关性及帮助性**。在视觉定位任务中，特别关注对视觉信息的准确理解和利用。

**总结**：Qwen2.5-VL的数据过滤流程是一个从**粗粒度分类到细粒度评估**的严谨工程。它首先通过**领域分类**为数据打上精细化标签，然后针对不同领域特点，综合运用**规则剔除**（处理显性问题）和**模型评分**（处理隐性问题）两种手段，从多个维度确保最终进入监督微调阶段的都是高质量、高相关性的数据，为模型与人类指令的高效对齐奠定了坚实基础。

## 5.3 - Rejection Sampling for Enhanced Reasoning

### 内容概括
本节阐述了在监督微调阶段，为提升模型在**复杂推理任务**上的能力，所采用的一项关键数据精炼策略——**拒绝采样**。该方法作为结构化数据过滤流程的补充，旨在从已有数据集中筛选出高质量、准确的推理样本，特别是那些包含有效思维链的样本，以教会模型进行清晰、连贯且基于多模态信息的推理。

### 要点总结
拒绝采样策略的核心目标是**构建一个高质量的“推理教学”数据集**，其要点可归纳为以下四个步骤：

#### 1. **目标与起点：针对复杂推理任务**
- **目标**：显著提升模型在需要多步推理的任务上的能力，如**数学解题、代码生成和特定领域视觉问答**。
- **理论依据**：研究证实，融入**思维链**推理能大幅提升模型的推理性能。
- **起点**：从一个包含**真实标注**的、精心策划的数据集开始，该数据集专门收录了需要复杂推理的任务。

#### 2. **核心流程：基于答案匹配的初筛**
- **方法**：使用一个**中间版本的Qwen2.5-VL模型**对数据集中的样本生成回答。
- **筛选标准**：仅保留那些**模型输出与标准答案完全匹配**的样本。
- **结果**：由此确保初步筛选出的数据集由高精度、高质量的示例构成。

#### 3. **质量控制：应用额外约束与验证推理链**
- **约束过滤**：为进一步提升质量，应用额外规则过滤掉不符合要求的输出，例如排除存在**代码切换、过长或重复模式**的回答，以确保思维链的清晰与连贯。
- **多模态验证**：针对视觉语言模型特有的挑战（即推理步骤可能忽略或误解视觉信息），开发了**基于规则和模型的过滤策略**，以验证**中间推理步骤**是否有效整合了视觉与文本信息。这是确保推理质量的关键，但实现完美的模态对齐仍是持续挑战。

#### 4. **最终成效：迭代优化与奠基未来**
- **模型受益**：通过迭代式地精炼数据集并移除低质量或错误样本，模型得以从**高保真度的示例中学习**，这些示例强调了准确、连贯的推理过程。
- **长远价值**：此方法不仅增强了模型处理复杂任务的能力，也为**视觉语言建模的未来改进奠定了基础**。

**总结**：拒绝采样是一个针对性的数据提纯过程。它通过**“生成-匹配-过滤-验证”** 的流程，从一个广泛的推理数据集中，蒸馏出那些模型能**正确解答且推理过程清晰**的样本，从而构建出一个高效的“教学数据集”，专门用于训练和提升模型的核心推理能力。

## 5.4 - Training Recipe

### 内容概括
本节详细说明了 **Qwen2.5-VL 在后训练（Post-training）阶段所采用的具体、高效的两阶段训练方案**。该方案在监督微调与直接偏好优化过程中，均保持视觉编码器参数冻结，并利用不同类型的数据分阶段优化模型，旨在高效地提升模型的任务性能并与人类偏好对齐。

### 要点总结
该训练方案的核心特点是 **“分阶段数据聚焦”** 与 **“高效参数优化”** ，其要点如下：

#### 1. **两阶段训练流程**
- **第一阶段：监督微调**：使用多样化的多模态指令数据对模型进行微调，以激发和塑造其执行各种任务的能力。
- **第二阶段：直接偏好优化**：专注于使用人类偏好数据进一步优化模型，使其输出更符合人类的价值观和选择。

#### 2. **关键约束与设计**
- **视觉编码器冻结**：在整个后训练阶段，**视觉Transformer的参数始终保持冻结**。这意味着优化只针对语言模型和视觉-语言融合器部分，大幅减少了可训练参数量，提高了训练效率和稳定性。
- **DPO阶段的高效性**：在DPO阶段，为确保优化效率并防止过拟合，**每个偏好数据样本仅被处理一次**。

#### 3. **各阶段的数据策略**
- **SFT阶段的数据**：非常广泛，旨在全面激活能力。包括：
    - **通用数据**：如图像-文本对、视频数据、纯文本数据。
    - **高质量专项数据**：来自拒绝采样流程的数据。
    - **专业领域数据**：涵盖文档与OCR、视觉定位、视频理解及智能体任务的数据集。
- **DPO阶段的数据**：高度聚焦，旨在精准对齐偏好。**仅使用图像-文本和纯文本这两类数据**，利用人类对回答质量的评判来微调模型。

#### 4. **方案目标**
- **提升性能**：增强模型在跨模态推理和具体任务上的表现。
- **对齐意图**：确保模型的输出与用户的真实意图和人类普遍偏好保持一致。

**总结**：Qwen2.5-VL的后训练方案是一个精心设计的“两步走”高效流程。**SFT阶段**像“开广度”，利用海量多样化数据全面唤醒模型的多模态任务能力；而**DPO阶段**则像“调精度”，聚焦于用偏好数据对模型行为进行精细化校准。整个过程中冻结ViT的设计，在保证视觉感知能力稳定的前提下，极大地提升了训练效率。
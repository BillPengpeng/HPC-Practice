本文主要整理《Qwen3Technical Report》的主要内容。

## 1 - Abstract

### **内容概括**

该文介绍了**Qwen3**，即Qwen模型家族的最新版本。Qwen3是一个包含多种参数规模（0.6B至235B）的大型语言模型系列，涵盖密集型和MoE架构。其主要创新在于**将思考模式与非思考模式集成于统一框架**，用户无需在不同模型间切换，并可通过“思考预算”机制动态调配计算资源以平衡延迟与性能。此外，该系列通过知识迁移显著降低了训练小规模模型所需的计算成本，同时保持强劲性能。评估显示，Qwen3在代码生成、数学推理等多个任务上达到先进水平，其多语言支持也从上一代的29种大幅扩展至119种语言/方言。所有模型均基于**Apache 2.0协议开源**，以促进研究与社区发展。

---

### **要点总结**

1. **模型定位**  
   - Qwen3是Qwen模型家族的最新版本，旨在提升性能、效率与多语言能力。

2. **模型规模与架构**  
   - 包含从0.6B到235B参数的模型系列，支持密集型（Dense）与混合专家（MoE）架构。

3. **核心创新**  
   - **统一框架集成**：融合“思考模式”（用于复杂推理）与“非思考模式”（用于快速响应），支持根据查询动态切换，无需更换模型。
   - **思考预算机制**：允许用户在推理时自适应分配计算资源，根据任务复杂度平衡延迟与性能。
   - **高效知识迁移**：利用旗舰模型知识显著降低训练小规模模型所需的计算资源，同时保持竞争力。

4. **性能表现**  
   - 在代码生成、数学推理、智能体任务等多项基准测试中达到先进水平，可与更大规模的MoE模型及闭源模型竞争。
   - 多语言能力大幅提升：支持语言/方言从Qwen2.5的29种扩展至119种，增强了跨语言理解与生成能力。

5. **开源与可及性**  
   - 全系列模型基于**Apache 2.0协议**公开，支持可复现性及社区驱动的研究与开发。

## 2 - Introduction

### **整体内容概括**

本节重点阐述了Qwen3的三大核心创新：1）**将思考模式与非思考模式集成于单一模型**，用户无需切换模型；2）引入**“思考预算”** 机制，允许用户动态调控模型的推理资源以平衡性能与延迟；3）采用**高效知识蒸馏**技术，显著降低了训练小规模模型的成本。

本节还概述了其**三阶段预训练**（通用知识、STEM/编程知识增强、长上下文扩展）和**四阶段后训练**（发展推理能力、统一两种模式、通用强化学习）流程。评估结果显示，Qwen3在数学（如AIME）、编程（如LiveCodeBench）、智能体等多项核心基准测试中达到顶尖水平，其多语言支持也扩展至119种语言/方言。所有模型均基于**Apache 2.0协议开源**。

---

### **核心要点总结**

| 维度 | 要点说明 |
| :--- | :--- |
| **1. 模型定位** | Qwen3是Qwen模型家族的最新系列，旨在推动开源大模型发展，追求更高性能与效率。 |
| **2. 模型规模** | 提供全系列模型，参数范围从0.6B到235B，包含**密集（Dense）** 与**混合专家（MoE）** 两种架构。旗舰型号为 **Qwen3-235B-A22B**（总参235B，每Token激活22B）。 |
| **3. 核心创新** | **统一框架**：集成“思考”（深度推理）与“非思考”（快速响应）两种模式于单一模型。<br>**思考预算**：用户可像控制“燃料”一样，为任务分配特定的推理计算量，实现性能与延迟的灵活权衡。<br>**高效训练**：通过从大模型到小模型的**知识蒸馏**，大幅降低训练成本，使小模型亦具备强大竞争力。 |
| **4. 训练数据与流程** | **预训练数据**：约**36万亿**Token，覆盖**119种**语言/方言，利用多模态及领域模型（如Qwen2.5-Math）增强数据质量。<br>**三阶段预训练**：通用知识 → STEM/编程知识增强 → 长上下文扩展（支持32K上下文）。<br>**四阶段后训练**：链式思维微调 → 数学/编程强化学习 → 混合数据统一微调 → 通用领域强化学习。 |
| **5. 关键性能** | 在多项基准测试中达到**顶尖水平（SOTA）**：<br>- **数学**：AIME‘24得**85.7分**，AIME‘25得**81.5分**。<br>- **编程**：LiveCodeBench v5得**70.7分**，CodeForces评分**2056**。<br>- **智能体**：BFCL v3得**70.8分**。<br>- **规律**：增加“思考预算”可稳定提升模型在各种任务上的性能。 |
| **6. 开源与影响** | 全系列模型基于**Apache 2.0协议**完全开源，以促进更广泛的研究、创新与应用生态发展。 |

## 3.0 - Architecture

### **内容概括**

本节主要说明了Qwen3系列包含**6个密集模型**（从0.6B到32B参数）和**2个MoE模型**（30B-A2B和235B-A22B），其中后者为235B总参数的旗舰模型。其架构在继承Qwen2.5（使用GQA、SwiGLU、RoPE、RMSNorm等组件）的基础上，进行了两项关键改进：**移除了注意力机制中的QK-V偏置**，并**引入了QK-Norm技术**，以提升训练稳定性。

MoE模型采用**8粒度专家分割**的设计，包含128个总专家，每个令牌激活8个专家，且**不再包含共享专家**。所有模型均使用基于BBPE、词表大小为151,669的Tokenizer，并支持**长达32K或128K的上下文长度**。

---

### **要点总结**

1.  **模型系列总览**：
    *   包含**6个密集模型**（0.6B, 1.7B, 4B, 8B, 14B, 32B）和**2个MoE模型**（30B-A2B, 235B-A22B）。
    *   旗舰模型为 **Qwen3-235B-A22B**（总参235B，每Token激活22B）。

2.  **核心架构组件（继承自Qwen2.5）**：
    *   **注意力机制**：分组查询注意力（GQA）。
    *   **激活函数**：SwiGLU。
    *   **位置编码**：旋转位置编码（RoPE）。
    *   **归一化**：采用预归一化的RMSNorm。

3.  **关键架构创新（针对训练稳定性）**：
    *   **移除QK-V偏置**（在Qwen2.5中引入）。
    *   **引入QK-Norm** 到注意力机制中。

4.  **Tokenizer**：
    *   使用基于字节级字节对编码（BBPE）的Qwen Tokenizer。
    *   词表大小：**151,669**。

5.  **MoE模型设计特点**：
    *   实现**8粒度专家分割**。
    *   总专家数：128；每Token激活专家数：8。
    *   与Qwen2.5-MoE不同，**移除了共享专家**。
    *   采用全局批次负载均衡损失以促进专家稀疏性。

---

### **打印表格**

#### **表 1: Qwen3密集模型架构**

Tie Embedding​ 指的是在Transformer模型中，将输入层的词嵌入矩阵​ 与输出层（用于预测下一个词）的投影矩阵​ 绑定（共享）的机制。

| Models | Layers | Heads (Q / KV) | Tie Embedding | Context Length |
| :--- | :--- | :--- | :--- | :--- |
| Qwen3-0.6B | 28 | 16 / 8 | Yes | 32K |
| Qwen3-1.7B | 28 | 16 / 8 | Yes | 32K |
| Qwen3-4B | 36 | 32 / 8 | Yes | 128K |
| Qwen3-8B | 36 | 32 / 8 | No | 128K |
| Qwen3-14B | 40 | 40 / 8 | No | 128K |
| Qwen3-32B | 64 | 64 / 8 | No | 128K |

#### **表 2: Qwen3 MoE模型架构**

| Models | Layers | Heads (Q / KV) | # Experts (Total / Active) | Context Length |
| :--- | :--- | :--- | :--- | :--- |
| Qwen3-30B-A2B | 28 | 32 / 8 | 128 / 8 | 32K |
| Qwen3-235B-A22B | 56 | 128 / 8 | 128 / 8 | 128K |

## 3.1 - QK-V Bias 和 QK-Norm

| 特性 | QK-V Bias | QK-Norm |
| :--- | :--- | :--- |
| **基本定义** | 在计算注意力分数 `QK^T` 后添加的一个**可学习的偏置项** | 对 `Q` 和 `K` 向量进行**归一化处理**，再进行注意力计算 |
| **主要目的** | 为模型提供一种**直接修正注意力模式**的灵活手段 | **稳定训练过程**，防止注意力分数过大或过小，改善模型性能 |
| **在Qwen3中的应用** | 已被**移除** | 已被**引入**，作为一项关键架构改进 |
| **对计算的影响** | 增加少量参数，直接改变注意力分数矩阵 | 增加少量计算开销（归一化操作），从根本上调整`Q`和`K`的分布 |
| **核心思想** | **增量修正**：在现有计算基础上增加一个调整项 | **源头控制**：从输入层面确保数值稳定性 |

### 🔍 深入理解QK-Norm

Qwen3采用QK-Norm并移除QK-V Bias，主要是为了从源头上提升注意力机制的稳定性，尤其是在处理长上下文和复杂模型架构时。

1.  **工作原理**
    QK-Norm的核心是在`Q`和`K`经过线性变换后、进行注意力计算前，对它们进行**RMSNorm归一化** 。这个过程通常还包括一个可学习的缩放参数（`g_q`, `g_k`），其公式可简化为 `ĸ = g * (q / RMS(q))`。这样做相当于在计算注意力分数时，将点积运算转化为一种**受控的余弦相似度度量**，将分数范围大致限制在 `[-g, g]` 内 。

2.  **带来的优势**
    -   **稳定训练**：通过控制`Q`和`K`的幅值，能有效防止注意力分数在训练过程中出现极端值，避免Softmax函数进入梯度很小的饱和区，让训练过程更平滑 。
    -   **提升外推能力**：对于像RoPE这类位置编码，QK-Norm能减少随着序列长度增加可能带来的注意力分数波动，使模型在长上下文任务中表现更稳健 。
    -   **适配现代架构**：在与GQA等现代模型组件协同工作时，QK-Norm有助于保持不同注意力头之间尺度的一致性，提升整体效果 。

### 💎 总结

总而言之，QK-V Bias和QK-Norm代表了两种不同的优化思路。Qwen3选择引入QK-Norm并移除QK-V Bias，标志着其设计理念从“事后修正”转向了“源头控制”，这为模型在处理更复杂任务和更长上下文时提供了更好的基础。

## 4.0 - Pre-training Data

### **内容概括**

Qwen3 模型的预训练数据在 **规模、多样性和质量** 上相比前代 Qwen2.5 实现了显著提升。其核心数据集总量达到 **36万亿个词元（Token）**，并前所未有地覆盖了 **119种语言和方言**。

数据扩展主要通过三种关键技术手段实现：
1.  **利用视觉模型扩充文本**：使用 Qwen2.5-VL 模型从海量PDF类文档中识别文本，再经 Qwen2.5 模型优化，获得了数万亿额外的高质量文本词元。
2.  **利用专用模型合成数据**：通过 Qwen2.5、Qwen2.5-Math 和 Qwen2.5-Coder 等模型，合成了数万亿涵盖教科书、问答、指令和代码片段等多种格式的合成数据。
3.  **开发多语言数据注释系统**：该系统从教育价值、专业领域、安全性等多个维度，对超过 **30万亿词元** 进行了精细标注，并基于细粒度标签在小规模代理模型上进行消融实验，从而实现了**实例级（instance-level）** 的数据混合优化，而非传统的数据源或领域级优化。

---

### **要点总结**

| 维度 | 核心要点 |
| :--- | :--- |
| **1. 规模与覆盖** | - **总量**：预训练数据达 **36万亿 Token**。<br>- **语言**：支持语言/方言从 Qwen2.5 的 **29种** 大幅扩展至 **119种**，极大增强了模型的**语言覆盖**与**跨语言能力**。 |
| **2. 数据来源与扩充** | - **文档挖掘**：使用 **Qwen2.5-VL** 识别PDF类文档文本，并用 **Qwen2.5** 优化，获得数万亿高质量文本。<br>- **模型合成**：利用 **Qwen2.5系列模型**（如Math, Coder）合成数万亿涵盖代码、STEM、推理、书籍、指令等领域的多样化文本。 |
| **3. 数据处理与优化** | - **注释系统**：开发了**多语言数据注释系统**，对超过 **30万亿 Token** 进行了多维度精细标注。<br>- **优化粒度**：创新性地在**实例级**优化数据混合比例，通过在小代理模型上进行大量消融实验，实现了比传统**数据源级**或**领域级**优化更精准的数据配比。 |
| **4. 核心目的** | 通过大规模、高质量、多语言、精细优化的预训练数据，为 Qwen3 模型在**代码生成、数学推理、多语言理解**等复杂任务上取得顶尖（SOTA）性能奠定坚实基础。 |

## 4.1 - Pre-training Stage

### **内容概括**

该图片详细阐述了 Qwen3 模型采用的 **三阶段预训练流程**。这一流程旨在分步骤、有侧重地构建模型的核心能力：
1.  **通用知识奠基**：在第一阶段，模型在超 30 万亿词元、覆盖 119 种语言的数据上，以 4K 序列长度训练，掌握通用语言知识与世界知识。
2.  **推理能力强化**：第二阶段，通过将训练数据优化为更高比例的 STEM、代码、推理及合成数据，并使用约 5 万亿高质量词元继续训练，同时加速学习率衰减，以重点提升模型的推理能力。
3.  **长上下文扩展**：最后阶段，使用高质量长文本语料（包含数百亿词元）将模型上下文长度扩展至 32K。该阶段引入了 **ABF、YARN、Dual Chunk Attention (DCA)** 等关键技术，旨在实现推理时上下文长度四倍于训练长度的能力。整个过程遵循类似 Qwen2.5 的**超参数缩放定律**进行优化。

---

### **要点总结**

#### **表：Qwen3 三阶段预训练流程**

| 阶段 | 核心目标 | 训练数据与规模 | 序列长度 | 关键技术与策略 |
| :--- | :--- | :--- | :--- | :--- |
| **第一阶段 (S1)**<br>**通用阶段** | 掌握**通用语言能力**与**世界知识**。 | - **数据量**：> 30T Tokens<br>- **特点**：覆盖 **119 种**语言/方言的广泛语料。 | **4,096** tokens | 基础的大规模预训练。 |
| **第二阶段 (S2)**<br>**推理阶段** | 专项提升**STEM、代码、推理**等能力。 | - **数据量**：~ 5T Tokens<br>- **特点**：**高质量**，**高比例**的STEM、代码、推理及合成数据。 | **4,096** tokens | - 优化数据混合比例。<br>- **加速学习率衰减**。 |
| **第三阶段**<br>**长上下文阶段** | 将有效上下文长度扩展至 **32K**，并支持更长的推理外推。 | - **数据量**：数百B Tokens<br>- **特点**：高质量长文本，75%长度在 **16K-32K**，25%在 **4K-16K**。 | **32,768** tokens | 1. **ABF技术**：将RoPE基础频率从10,000提升至 **1,000,000**。<br>2. **引入YARN**：增强外推能力。<br>3. **引入Dual Chunk Attention (DCA)**：支持推理时实现**4倍**于训练长度的上下文（即最高 **128K**）。 |
| **整体训练策略** | 系统化地探索模型架构、数据、阶段与最优超参数之间的关系，并基于此为每个密集/MoE模型预测并设定最优的学习率与批处理大小策略。 | | | |

---
**核心结论**：Qwen3 通过 **“通用 → 专项强化 → 能力扩展”** 的三段式预训练，并系统应用前沿的长上下文技术，旨在构建一个知识广博、擅长推理、且能处理超长文档的通用大模型。

## 4.2 - Pre-training Evaluation 

### **内容概括**

该内容详细介绍了 **Qwen3 系列基础语言模型（预训练后，指令微调前的原始模型）** 的全面评估方案。评估旨在全面衡量模型在多个核心领域的知识和能力，共包含 **四大类、总计15个权威基准测试**。

1.  **通用知识与推理能力**：使用 MMLU、MMLU-Pro 等基准测试模型对世界常识、复杂推理和知识理解的广度与深度。
2.  **数学与STEM能力**：通过 GPQA、GSM8K 等测试，专项评估模型在数学解题、科学（物理、化学、生物等）专业问题上的推理能力。
3.  **编程能力**：利用 EvalPlus、MultiPL-E 等数据集，从代码正确性、复杂度到多语言编程（Python、C++等）进行全面评估。
4.  **多语言能力**：借助 MGSM、MMMLU 等基准，测试模型在不同语言（特别是非英语）下的数学推理和知识问答性能。

所有评估均采用 **少样本（Few-shot）或零样本（Zero-shot）** 的设定，以检验模型本身的内生能力，确保评估的客观性。

---

### **要点总结**

#### **表：Qwen3 基础模型预训练评估基准汇总**

| 评估维度 | 具体基准测试 (数据集) | 关键设置 (提示方式) | 测试重点说明 |
| :--- | :--- | :--- | :--- |
| **一、通用任务**<br>*(General Tasks)* | 1. **MMLU** (Hendrycks et al., 2021a)<br>2. **MMLU-Pro** (Wang et al., 2024)<br>3. **MMLU-redux** (Gema et al., 2024)<br>4. **BBH** (Suzgun et al., 2023)<br>5. **SuperGPQA** (Du et al., 2025) | 5-shot<br>5-shot, CoT<br>5-shot<br>3-shot, CoT<br>5-shot, CoT | 衡量涵盖57个学科的专业知识、理解与推理能力。是MMLU的升级版，包含更具挑战性的多选题。对MMLU的进一步改进测试集。评估跨多个领域的复杂推理和问题解决能力。一个更具挑战性的科学问答基准。 |
| **二、数学与STEM任务**<br>*(Math & STEM Tasks)* | 6. **GPQA** (Rein et al., 2023)<br>7. **GSM8K** (Cobbe et al., 2021)<br>8. **MATH** (Hendrycks et al., 2021b) | 5-shot, CoT<br>4-shot, CoT<br>4-shot, CoT | 评估高难度生物、物理、化学领域的博士级科学问题。测试小学水平的数学文字题解题能力。评估高中竞赛难度的数学问题解题能力。 |
| **三、编程任务**<br>*(Coding Tasks)* | 9. **EvalPlus** (Liu et al., 2023a)<br>10. **MultiPL-E** (Cassano et al., 2023)<br>11. **MBPP-3shot** (Austin et al., 2021)<br>12. **CRUXEval (CRUX-O)** (Gu et al., 2024) | 0-shot (平均分)<br>0-shot (多语言)<br>3-shot<br>1-shot | 综合评估HumanEval、MBPP及其增强版(Humaneval+, MBPP+)的代码生成质量。评估**多编程语言**（Python, C++, Java等）的代码生成能力。评估基于自然语言描述的编程问题解决能力。评估代码执行结果的推理能力。 |
| **四、多语言任务**<br>*(Multilingual Tasks)* | 13. **MGSM** (Shi et al., 2023)<br>14. **MMMLU** (OpenAI, 2024)<br>15. **INCLUDE** (Romanou et al., 2024) | 8-shot, CoT<br>5-shot<br>5-shot | 评估**多语言**（涵盖10种语言）的数学推理能力。多语言版本的MMLU，测试多语言知识。评估涵盖20种语言的常识推理能力。 |

---
**核心结论**：Qwen3 对预训练基础模型的评估体系**覆盖全面、维度清晰、基准权威**。通过这四大类15个基准，系统性地检验了模型在通用知识、专项推理（数学/STEM）、编程及多语言方面的核心能力，为衡量其“基本功”提供了扎实、可比的客观标准。

## 4.3 - Summary of Evaluation Results

### **内容概括**

本部分是对 Qwen3 系列基础模型（即未经过指令微调的预训练模型）综合评估结果的**核心结论摘要**。通过与当前顶尖的开源基础模型（如 DeepSeek-V3、Llama-4 系列）以及自身前代模型（Qwen2.5 系列）进行公平对比，评估凸显了 Qwen3 在**性能、参数效率和成本**方面的显著优势。主要结论围绕三个方面展开：旗舰 MoE 模型的领先性能、MoE 架构带来的高效性，以及密集型模型相较于前代的整体提升。

---

### **要点总结**

基于评估结果，可以总结出以下三大核心结论：

1.  **旗舰模型达到顶尖水平**
    *   与先前开源的最先进的密集型和MoE基础模型（如 DeepSeek-V3 Base, Llama-4-Maverick Base, Qwen2.5-72B-Base）相比，**Qwen3-235B-A22B-Base** 在大多数任务上表现更优，并且使用的**总参数或激活参数显著更少**。

2.  **MoE 模型效率卓越**
    *   **同代对比**：使用相同预训练数据，Qwen3 MoE 基础模型仅需激活 **1/5** 的参数，即可达到同代 Qwen3 密集型基础模型的相当性能。
    *   **代际提升**：受益于架构改进、更大规模训练数据和更先进的训练策略，Qwen3 MoE 基础模型能以**不到1/2的激活参数**和**更少的总参数**，超越前代 Qwen2.5 MoE 基础模型。
    *   **成本优势**：即使仅激活 Qwen2.5 密集型基础模型 **1/10** 的参数量，Qwen3 MoE 基础模型也能实现可比性能，这在推理和训练成本上带来了显著优势。

3.  **密集型模型整体性能提升**
    *   在更高参数规模下，Qwen3 密集型基础模型的整体性能与 Qwen2.5 基础模型相当（例如，Qwen3-1.7B/4B/8B/14B/32B-Base 分别对应 Qwen2.5-3B/7B/14B/32B/72B-Base 的性能）。
    *   尤其在 **STEM、编程和推理** 等专项基准测试中，Qwen3 密集型基础模型在更高参数规模下的性能甚至**超越了**对应的 Qwen2.5 基础模型。

## 4.4 - Qwen3-235B-A22B-Base

### **内容概括**

Qwen3-235B-A22B-Base 在**绝大多数任务上取得了最优性能**，同时凭借其 **MoE 架构和优化的训练策略**，在**参数量、激活参数量以及训练/推理成本**方面展现出显著优势，实现了**性能与效率的双重领先**。

---

### **要点总结**

结合图文信息，核心结论可总结为以下三点：

#### **1. 性能全面领先**
- **表格数据**：在评估的 **4大类、15个** 基准测试中，Qwen3-235B-A22B-Base 在 **12项** 中取得**最高分**（粗体标出），在多项任务（如 MATH, EvalPlus, MMLU-Pro）上优势显著。
- **文字佐证**：文字明确指出该模型在“大多数评估基准上获得最高性能得分”。

#### **2. 参数效率与成本优势突出**
与主要竞争对手相比，Qwen3-235B-A22B-Base 以更少的参数量实现了更强的性能：
- **对比 DeepSeek-V3 Base**：总参数仅为其 **1/3**，激活参数为其 **2/3**，却在15个基准中的 **14个** 上实现超越。
- **对比 Qwen2.5-72B-Base**：激活参数不到其 **1/3**，但性能全面超越，且得益于 MoE 架构，其**单位词元的训练和推理成本大幅降低**。
- **对比 Llama-4-Maverick-Base**：总参数约为其 **一半**，但性能在大多数任务上更优。

#### **3. 代际与架构改进显著**
- **对比前代 MoE 模型 (Qwen2.5-Plus-Base)**：在**总参数和激活参数均更少**的情况下，性能实现**显著超越**。这表明 Qwen3 在**预训练数据质量、训练策略和模型架构**（如引入QK-Norm等）上取得了实质性进步。

#### **关键数据速览表**

| 对比模型 | 架构 | 总参数 | 激活参数 | 与Qwen3-235B的核心对比结论 |
| :--- | :--- | :--- | :--- | :--- |
| **Qwen3-235B-A22B-Base** | **MoE** | **235B** | **22B** | **基准：在绝大多数任务上排名第一。** |
| DeepSeek-V3 Base | MoE | 671B | 37B | 参数多但性能弱，Qwen3效率更高。 |
| Llama-4-Maverick Base | MoE | 402B | 17B | 参数翻倍，但性能不及Qwen3。 |
| Qwen2.5-Plus-Base (前代) | MoE | 271B | 37B | 参数更多，性能全面落后，显示代际提升。 |
| Qwen2.5-72B-Base (前代) | Dense | 72B | 72B | 激活参数多3倍以上，性能仍不及，突显MoE架构成本优势。 |

## 4.5 - Qwen3-32B-Base

### **内容概括**

本部分专注于评估 Qwen3 系列中**最大的密集型基础模型 Qwen3-32B-Base**。通过与**三个规模相近的基准模型**（Qwen2.5-32B-Base, Gemma-3-27B-Base）和**两个参数规模更大、性能更强的基线模型**（Qwen2.5-72B-Base, Llama-4-Scout-Base）进行全面对比，结果表明：**Qwen3-32B-Base 不仅在同规模模型中全面领先，甚至在多项任务上超越了参数规模翻倍的旗舰模型，展现出卓越的参数效率和代际性能提升。**

---

### **要点总结**

基于表格（Table 4）数据和文字结论，核心发现可总结为以下三点：

| 对比维度 | 核心结论 | 关键证据与数据支持 |
| :--- | :--- | :--- |
| **1. 同规模对比：全面领先** | **Qwen3-32B-Base 性能显著优于所有同规模（约30B参数级）的对比模型。** | • **对比 Qwen2.5-32B-Base**：在15个评估基准中的**大多数**上表现更优，特别是在 **MMLU-Pro (65.54 vs 55.10)** 和 **SuperGPQA (39.78 vs 33.55)** 上优势巨大。<br>• **对比 Gemma-3-27B-Base**：在所有15个基准测试中**全面领先**。 |
| **2. 越级挑战：效率惊人** | **以不到一半的参数，在多项核心能力上超越前代旗舰级密集模型。** | • **对比 Qwen2.5-72B-Base (72B)**：尽管总参数仅为对方的 **44%**，Qwen3-32B-Base 在15个基准中的 **10个** 上实现了**反超**。<br>• **优势领域**：在 **编码（Coding）**、**数学（Math）** 和 **推理（Reasoning，如BBH）** 等专项任务上表现尤为出色，体现了其在高质量数据与先进训练策略下的能力飞跃。 |
| **3. 架构对比：性能碾压** | **在总参数仅为对方三分之一的情况下，性能全面大幅领先一个更强的MoE基线模型。** | • **对比 Llama-4-Scout-Base (MoE, 109B)**：Qwen3-32B-Base 的总参数仅为对方的 **29%**，但在评估的 **全部15个基准** 上均**显著领先**。<br>• **效率反思**：此结果凸显了**模型性能并非单纯由总参数量决定**，**训练数据的质量、模型架构的优化**（如引入QK-Norm）和**训练策略**同等甚至更为关键。 |

## 4.6 - Qwen3-14B-Base & Qwen3-30B-A3B-Base

### **内容概况**

本部分评估了Qwen3系列中**两个中等规模的模型**：**密集型Qwen3-14B-Base** 和 **MoE架构的Qwen3-30B-A3B-Base**。评估将它们与**规模相近的基线**（如Gemma-3-12B、Qwen2.5-14B）以及**两个参数更多或架构更强的基线**（Qwen2.5-32B-Base、Qwen2.5-Turbo-Base）进行了全面对比。结果显示，这两个模型在性能与效率方面均表现出色：**Qwen3-14B-Base实现了显著的代际提升**，而**Qwen3-30B-A3B-Base则凭借MoE架构展现了卓越的成本效益**。

---

### **要点总结**

综合文字结论与表格（Table 5）数据，可以得出以下三个核心结论：

1.  **Qwen3-14B-Base：同规模性能最强**
    *   与同为14B参数级别的 **Qwen2.5-14B-Base** 相比，**Qwen3-14B-Base在评估的全部15个基准测试中均表现更优**，尤其在**推理（MMLU-Pro, BBH）、数学（MATH）和编程（EvalPlus, MultiPL-E）** 任务上优势显著（如MMLU-Pro：61.03 vs 51.16；EvalPlus：72.23 vs 60.70）。这直接体现了Qwen3在相同架构和参数规模下的代际进步。

2.  **Qwen3-14B-Base：以小搏大，挑战更强基线**
    *   尽管总参数（14B）不到 **Qwen2.5-32B-Base**（32B）的一半，**Qwen3-14B-Base在多项任务上与之不相上下，甚至在编程和部分推理任务上实现反超**（如EvalPlus：72.23 vs 66.25；MMLU-Pro：61.03 vs 55.10）。这再次印证了Qwen3系列在数据质量和训练策略上的优势，带来了更高的**参数效率**。

3.  **Qwen3-30B-A3B-Base：高性价比的MoE典范**
    *   **性能强劲**：总参30B的MoE模型 **Qwen3-30B-A3B-Base**（激活3B），其性能全面超越激活参数多4.6倍的 **Qwen2.5-14B-Base**，并与 **Qwen3-14B-Base** 和 **Qwen2.5-32B-Base** 表现相当（在15个基准中的9个位列第一或第二）。
    *   **成本优势巨大**：其激活参数量（3B）仅为对比的MoE模型 **Qwen2.5-Turbo-Base**（激活6B）的**一半**，但性能全面领先。这意味着在**训练和推理时，它能以更低的计算成本，达到甚至超越更大激活规模模型的性能**。

#### **关键性能数据速览**

| 评估维度 (代表任务) | Qwen2.5-14B-Base (14B Dense) | Qwen3-14B-Base (14B Dense) | Qwen3-30B-A3B-Base (30B MoE, 激活3B) | 对比结论 |
| :--- | :--- | :--- | :--- | :--- |
| **通用推理 (MMLU-Pro)** | 51.16 | **61.03** | 61.49 | Qwen3模型显著领先前代 |
| **复杂推理 (BBH)** | 78.18 | 81.07 | **81.54** | Qwen3模型表现更优 |
| **数学 (MATH)** | 55.64 | **62.02** | 59.04 | 14B密集型表现突出 |
| **编程 (EvalPlus)** | 60.70 | **72.23** | 71.45 | Qwen3模型大幅提升 |
| **多语言 (MGSM)** | 74.68 | **79.20** | 79.11 | Qwen3模型全面领先 |

## 4.7 - Qwen3-8B / 4B / 1.7B / 0.6B-Base

### **内容概况**

本部分评估了Qwen3系列中**参数量相对较小（0.6B 至 8B）的密集型基础模型**。评估以**同规模的主流开源基线**（如Qwen2.5、Llama-3、Gemma-3系列）为参照，通过详尽的基准测试对比（文字总结与表格6、7、8），揭示了Qwen3小模型的核心优势：**不仅在同类模型中全面领先，更能以更少的参数，在关键任务上超越前代的更大规模模型，实现了显著的代际性能飞跃和卓越的参数效率。**

---

### **要点总结**

#### **核心结论**
1.  **全面领先**：所有Qwen3小模型（8B/4B/1.7B/0.6B-Base）在**几乎所有评估基准**上都保持了强劲性能，并**全面超越**了参数规模相近的基线模型（Qwen2.5、Llama-3、Gemma-3）。
2.  **越级挑战**：**Qwen3-8B/4B/1.7B-Base** 在大多数任务上，其性能甚至**超过了参数更多的前代模型**（如分别超过 Qwen2.5-14B/7B/3B），特别是在 **STEM（数学与科学）** 和 **编码** 相关任务上优势最为明显。
3.  **持续进步**：即使是**最小的0.6B模型**，对比前代0.5B模型也实现了**全方位的显著性能提升**，证明了技术改进在整个产品线中的有效性。

#### **分模型关键亮点**

| 模型 | 对比基线 (同规模) | 关键胜利 (超越更大模型) | 突出优势领域 |
| :--- | :--- | :--- | :--- |
| **Qwen3-8B-Base** | 全面优于 **Llama-3-8B-Base** 与 **Qwen2.5-7B-Base**。 | 在 **MMLU-Pro**、**GPQA**、**MATH**、**EvalPlus** 等多个核心任务上，**超越参数近乎翻倍的 Qwen2.5-14B-Base**。 | **复杂推理**、**STEM深度知识**、**代码生成**。 |
| **Qwen3-4B-Base** | 全面优于 **Gemma-3-4B-Base** 与 **Qwen2.5-3B-Base**。 | 在绝大多数任务上，**接近或超越 Qwen2.5-7B-Base**，以一半左右的参数实现相当性能。 | **数学推理**、**代码生成**、**多语言理解**。 |
| **Qwen3-1.7B-Base** | 显著优于 **Gemma-3-1B-Base** 与 **Qwen2.5-1.5B-Base**。 | 在所有任务上**大幅领先 Qwen2.5-1.5B-Base**，并在**数学、编码、多语言任务**上拉开巨大差距。 | **综合性能力平衡**，在极小规模下仍具备优秀的推理与生成能力。 |
| **Qwen3-0.6B-Base** | 在几乎所有任务上**显著超越前代 Qwen2.5-0.5B-Base**。 | 作为超轻量模型，在**常识推理、数学、多语言**等任务上展现了可用的基础能力。 | **参数效率的极致体现**，是边缘设备部署的强力候选。 |

#### **性能飞跃的领域**
从具体得分看，Qwen3小模型的代际提升在以下领域尤为突出：
*   **复杂推理**：在 **MMLU-Pro**、**BBH** 等基准上得分大幅提升。
*   **数学能力**：在 **MATH**、**GSM8K** 等数学解题任务上进步显著。
*   **代码生成**：在 **EvalPlus**、**MultiPL-E** 等编程基准上表现强劲。
*   **多语言理解**：在 **MGSM**（多语言数学）、**MMMLU** 等任务上持续领先。


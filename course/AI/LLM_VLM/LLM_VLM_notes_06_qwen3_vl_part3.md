本文主要整理《Qwen3-VL Technical Report》的主要内容。

## 6.0 - General Visual Question Answering

### **内容概括**

本部分系统报告了Qwen3-VL系列模型在**通用视觉问答**任务上的综合评估结果。通过对从2B到235B不同参数规模的模型变体在MMBench、RealWorldQA、MMStar和SimpleVQA等多个权威基准上的广泛测试，结果表明：该系列模型展现出了强大且极具竞争力的性能。无论是最大规模的旗舰模型，还是中等及较小规模的模型，均在各项基准上取得了领先或接近顶尖水平的成绩，并清晰地展现了性能随模型规模扩大而提升的良好可扩展性。

---

### **要点总结**

#### **1. 评估范围与方法**
*   **评估对象**：Qwen3-VL全系列模型，参数规模涵盖 **2B, 4B, 8B, 32B** 以及旗舰的 **235B-A22B (MoE)**。
*   **评估模式**：区分了 **“思考”** 和 **“指令跟随”** 两种推理模式。
*   **评测基准**：在 **MMBench, RealWorldQA, MMStar, SimpleVQA** 等多个多样化、具有挑战性的VQA基准上进行了全面测试。

#### **2. 主要性能表现**
*   **大型模型对比**：
    *   在 **思考模式** 下，**Qwen3-VL-235B-A22B-Thinking** 在 **MMStar** 基准上取得了 **78.7** 的最高分，综合性能紧随顶尖的 **Gemini-2.5-Pro (Thinking)**。
    *   在 **非思考模式** 下，**Qwen3-VL-235B-A22B-Instruct** 在 **MMBench** 和 **RealWorldQA** 上均取得了最高分，分别为 **89.3/88.9** 和 **79.2**。

*   **中等规模模型表现**：
    *   **Qwen3-VL-32B-Thinking** 在 MMBench 和 RealWorldQA 上均获得了同类最高分（分别为89.5/89.5 和 79.4）。
    *   一个有趣的发现是，在RealWorldQA基准上，**Qwen3-VL-32B-Instruct** 甚至以 **79.0** 的分数超过了其对应的“思考”变体，展现了强大的指令跟随能力。

*   **小型模型的可扩展性**：
    *   在 **8B及以下规模** 的模型中，**Qwen3-VL-8B** 在全部五个基准测试中都取得了最高性能。
    *   性能随着模型规模增大而显著提升。例如，在MMBench-EN的思考模式下，得分从 **2B模型的79.9** 提升至 **8B模型的85.3**；在MMStar上，也从2B模型的68.1提升至8B模型的75.3。这清晰地证明了该系列模型**优秀的性能可扩展性**。

#### **3. 核心结论**
Qwen3-VL系列在通用视觉问答任务上展现出了**全面而强大的能力**。其旗舰模型在多项基准上达到领先水平，而全系列模型，尤其是中小型模型，也表现出了卓越的性能和清晰的性能-规模正相关关系，验证了其架构与训练方法的有效性。

## 6.1 - Multimodal Reasoning

### **内容概括**

本部分系统报告了Qwen3-VL系列模型在一系列**多模态推理基准测试**（主要聚焦于STEM相关任务和视觉谜题）上的全面评估结果。评测覆盖了从大型旗舰模型到小型模型的整个产品线，在包括MMMU、MathVista、MathVerse、DynaMath、LogicVista、VisualPuzzles等十余个权威且具挑战性的基准上进行了测试。结果表明，无论是旗舰模型还是各规模段模型，Qwen3-VL系列均展现出了卓越且极具竞争力的推理性能，验证了其在复杂视觉-语言推理任务上的强大实力。

---

### **要点总结**

#### **1. 评估范围**
*   **评测基准**：涵盖广泛的**STEM（科学、技术、工程、数学）任务**和**视觉谜题**，包括 MMMU、MMMU-Pro、MathVision、MathVista、MathVerse、DynaMath、LogicVista、VisualPuzzles、VLMsAreBlind、ZeroBench、VisuLogic 等。
*   **评估对象**：Qwen3-VL全系列模型，区分了 **“思考”** 与 **“非思考”** 两种推理模式。

#### **2. 主要性能发现**
*   **旗舰模型（235B-A22B）表现卓越**：
    *   **非思考模式（-Instruct）** 在多个基准（如MathVista mini, MathVision, MathVerse mini, DynaMath等）上取得了**已报告的最佳结果**。
    *   **思考模式（-Thinking）** 则在MathVista mini, MathVision, MathVerse mini等基准上达到了**最先进的（SOTA）水平**。
    *   这证明了其在**复杂推理**和**高效指令跟随**两个维度上的顶级能力。

*   **中等规模模型（32B）优势显著**：
    *   Qwen3-VL-32B模型在同类模型中展现出**显著优势**，性能**持续超越**Gemini-2.5-Flash和GPT-5-mini等竞品。
    *   一个关键进步是：**中等规模的Qwen3-VL模型在推理任务上已经超越了前一代的更大模型（Qwen2.5-VL-72B）**，标志着多模态大模型领域的重要进展。
    *   新引入的**30B-A3B MoE模型**也交付了具有竞争力的结果。

*   **小规模模型（2B/4B/8B）能力突出**：
    *   **8B变体**在与GPT-5-Nano等竞品的对比中保持了**清晰的总体优势**。
    *   **4B模型**在DynaMath和VisuLogic等特定任务上取得了**最高分**，展示了其在特定领域的强劲能力。
    *   **值得注意**的是，即使是最小的**2B模型**也表现出了**强大的推理能力**，证明了该系列架构在小参数规模下的高效性。

## 6.2 - Alignment and Subjective Tasks

### **内容概括**

本部分报告了Qwen3-VL系列模型在评估**指令跟随能力和减少图像幻觉**这两项关键主观任务上的表现。通过在 **MM-MT-Bench（多轮对话评估）、HallusionBench（图像上下文推理诊断）和 MIA-Bench（复杂指令响应）** 三个代表性基准上的测试，结果显示：其旗舰模型在各项测试中 consistently 超越其他闭源模型，展现了卓越的对齐能力；同时，从32B到2B的各规模模型也均表现出色，性能衰减极小，证明了该系列模型优秀的可扩展性和综合实力。

---

### **要点总结**

#### **1. 评估目标与基准**
*   **核心评估能力**：
    *   遵循复杂用户指令的能力。
    *   减少潜在的图像级幻觉（即模型生成与图像内容不符的描述）。
*   **三大评估基准**：
    *   **MM-MT-Bench**：用于测试多模态指令调优模型的多轮对话能力（采用LLM-as-a-judge评估）。
    *   **HallusionBench**：专门诊断模型在图像上下文推理中的幻觉问题，对当前VLM挑战巨大。
    *   **MIA-Bench**：更全面的基准，用于评估模型对用户复杂指令（如带字数限制的创意写作、组合指令）的响应能力。

#### **2. 旗舰模型（235B-A22B）表现**
*   **总体领先**：在表格2中，旗舰模型 **Qwen3-VL-235B-A22B** 持续超越其他闭源模型。
*   **HallusionBench**：其 **思考版本（-Thinking）** 分别超越 **Gemini-2.5-pro、GPT-5 和 Claude opus 4.1** 达 **3.0, 1.0, 和 6.3** 分，展示了其在减少幻觉方面的顶级能力。
*   **MIA-Bench**：**Qwen3-VL-235B-A22B-Thinking** 在所有模型中取得了**整体最佳成绩**，证明了其卓越的多模态指令跟随能力。
    *   **细分任务优势**：在MIA-Bench的数学和文本子任务中，该模型分别超越 **GPT-5-high-thinking** 版本 **10.0** 分和 **5.0** 分。

#### **3. 各规模模型表现**
*   **中等规模模型（30B-A3B, 32B）**：同样展现出优势，性能超越了其他同类尺寸的模型。
*   **小规模模型系列（2B/4B/8B）**：
    *   整体表现良好。
    *   性能下降**微乎其微**，尤其是在 **MIA-Bench** 上，显示出该系列模型从大到小均保持了强大的指令跟随与对齐能力。

## 6.3 - Text Recognition and Document Understanding

### **内容概括**

本部分系统评估了Qwen3-VL系列模型在**文本识别（OCR）、文档解析、文档问答与推理**等一系列复杂文档理解任务上的综合性能。评测涵盖了从旗舰模型到小型模型的完整产品线，并在包括CC-OCR、OmniDocBench、DocVQA、InfoVQA、AI2D、ChartQA、CharXiv及MMLongBench-Doc等多个权威基准上进行了全面对比。结果表明：Qwen3-VL系列在OCR解析和长文档理解任务上达到了新的SOTA水平；其“思考”与“指令跟随”变体在不同任务上各具优势；中小型模型同样展现出强大的竞争力。此外，其OCR能力已扩展到39种语言，实现了真正的多语言实用化支持。

---

### **要点总结**

#### **1. 旗舰模型（235B-A22B）全面领先**
*   **OCR解析任务**：在CC-OCR、OmniDocBench、OCR-Bench、OCRBench_v2等聚焦和综合性OCR解析基准上，**Qwen3-VL-235B-A22B-Instruct** 模型**建立了新的最高性能（SOTA）**，并略微超越了其“思考”版本。
*   **文档问答任务**：在DocVQA、InfoVQA、AI2D、ChartQA以及CharXiv的描述子集等需要OCR与检索结合的基准上，其**Instruct**和**Thinking**版本**性能相当**，均表现优异。
*   **深度图表推理任务**：在**CharXiv的推理子集**（需要深度图表理解和多步推理）上，**“思考”版本（Thinking）超越了其“指令”版本（Instruct）**，性能仅落后于GPT-5-Thinking和Gemini-2.5-Pro-Thinking，位居前列。

#### **2. 中小型模型竞争力强劲**
*   **中等规模模型（30B-A3B, 32B）**：在大多数评测指标上，均**持续优于**同规模段的Gemini-2.5-Flash和GPT-5-mini等模型。
*   **紧凑型模型（8B, 4B, 2B）**：在OCR解析、视觉问答及综合基准套件上展现了**卓越的竞争力和性能**，证明了Qwen3-VL架构在不同模型规模下的**高效性与强大的可扩展性**。

#### **3. 长文档理解能力显著提升**
*   Qwen3-VL在此版本中**特别增强了长文档理解能力**。
*   在**MMLongBench-Doc**基准测试中，其旗舰模型**Qwen3-VL-235B-A22B**在指令/思考两种设置下分别取得了**57.0%** 和 **56.2%** 的准确率，展示了在该任务上的**先进性能**。

#### **4. 多语言OCR能力大幅扩展**
*   相较于Qwen2.5-VL支持的10种语言，Qwen3-VL将支持的**非中/英语言数量大幅扩展至39种**。
*   在内部构建的新数据集上评估，模型在测试的39种语言中，有**32种语言的准确率超过了70%**（被认为是实际可用的阈值）。这证明了其强大的OCR能力**具有广泛的跨语言适用性**，而不仅限于少数几种主要语言。

## 6.4 - 2D and 3D Grounding

### **内容概括**

本节系统性地评估了Qwen3-VL系列模型在**2D视觉定位**（如指代目标检测、开放词汇检测、计数）和**3D目标定位**（从单张图像预测物体在三维空间中的位置）两大核心空间感知任务上的性能。报告通过在一系列权威基准上与先进模型对比，展示了Qwen3-VL在该领域的领先地位：其旗舰模型在多个2D与3D基准上达到最先进水平（SOTA），且从大型到小型的全系列模型均展现出强大且均衡的竞争力，验证了其空间感知能力的卓越与可扩展性。

---

### **要点总结**

#### **1. 2D视觉定位评估**
*   **评估基准**：
    *   **RefCOCO+/g**：指代表达理解（根据语言描述定位图像中的物体）。
    *   **ODinW-13**：开放词汇目标检测（检测训练时未见过类别的物体）。
    *   **CountBench**：视觉计数（统计图像中物体的数量）。
*   **评估方法**：在ODinW-13等任务上，采用**平均精度（mAP）** 作为核心指标，并设置置信度分数为1.0以确保公平比较。
*   **主要结果**：
    *   **旗舰模型（235B-A22B）**：在2D定位与计数基准上均取得了**最先进（SOTA）的性能**。特别是在 **ODinW-13** 上达到 **48.6 mAP**，展现了其在**多目标、开放词汇物体定位**上的强大能力。
    *   **全系列模型**：详细结果（见于文中提及的Table 3/4）显示，从大型到小型的各个变体模型（如30B-A3B，32B，8B，4B，2B）在2D视觉定位任务上均表现出了**极具竞争力的性能**。

#### **2. 3D目标定位评估**
*   **评估基准**：**Omni3D**，一个包含ARKitScenes、Hypersim、SUN RGB-D等多个数据集的综合性3D检测基准。
*   **评估方法**：
    *   输入为**图像-文本对**（图像+指定物体类别的文本提示）。
    *   为与现有视觉语言模型公平比较，设置**交并比（IoU）阈值为0.15**，并报告 **mAP@0.15**，检测置信度固定为1.0。
*   **主要结果**：
    *   **能力增强**：本版本Qwen3-VL显著增强了其**3D物体定位的空间感知能力**。
    *   **旗舰模型领先**：Qwen3-VL-235B-A22B模型在Omni3D的多个数据集上**持续超越其他闭源模型**。
    *   **关键优势**：在 **SUN RGB-D** 数据集上，其 **“思考”（Thinking）** 变体比 **Gemini-2.5-Pro** 的性能高出 **5.2分**，优势明显。
    *   **小模型竞争力**：中小型变体（如30B-A3B, 32B, 8B, 4B, 2B）在3D定位任务上也展现了**非常出色的竞争力**。

## 6.5 - Fine-grained Perception

### **内容概括**

本部分评估了Qwen3-VL系列模型在**细粒度视觉感知**任务上的表现。通过在V*、HRBench-4k和HRBench-8k三个流行基准上的测试，结果显示：相比前代模型（Qwen2.5-VL-72B），Qwen3-VL系列实现了显著的性能飞跃。其旗舰模型在结合外部工具后，在三个基准上均达到了最先进的性能水平。评估还揭示了一个关键洞见：**集成外部工具所带来的性能提升，持续超过了单纯增加模型参数规模所带来的收益**。

---

### **要点总结**

#### **1. 主要评估结果**
*   **性能飞跃**：Qwen3-VL系列在细粒度视觉理解能力上，相比其前身 **Qwen2.5-VL-72B** 有**显著的提升**。
*   **SOTA表现**：**Qwen3-VL-235B-A22B** 模型在集成外部工具后，在评估的三个基准上均取得了**最先进的性能**：
    *   **V\*** 基准：得分 **93.7**
    *   **HRBench-4k** 基准：得分 **85.3**
    *   **HRBench-8k** 基准：得分 **82.3**

#### **2. 核心发现与洞见**
*   **架构与训练策略的有效性**：这一致的领先表现，凸显了Qwen3-VL在**架构改进和训练策略**上的有效性，特别是在处理**高分辨率输入**和**细微视觉差异**这些对细粒度感知至关重要的任务上。
*   **工具集成优于单纯规模扩展**：
    *   一个更令人惊讶且重要的发现是：**集成外部工具所带来的性能增益，持续超过了单纯增加模型规模（参数量）所带来的收益**。
    *   例如，在Qwen3-VL系列内部，通过添加工具，在V*基准上带来的**绝对性能提升 consistently 约为 5 个百分点**。

#### **3. 结论与方向**
这一发现强化了一个核心信念：**在多模态领域，扩展工具集成的智能体学习，是一条极具前景的发展路径。** 这意味着，未来的性能突破可能不仅依赖于模型的“大脑”（参数量和架构），还同样依赖于其有效使用“外部工具”的能力。

## 6.6 - Multi-Image Understanding

### **内容概括**

本部分评估了Qwen3-VL系列模型在**多图像理解**这一高级任务上的性能。报告指出，超越单图像对话，推进视觉语言模型处理多图像理解具有重要价值，因为这需要跨多样视觉模式进行更高层次的上下文分析，以实现更高级的识别与推理能力。为此，Qwen3-VL通过综合的跨图像模式学习技术（如多图像指代定位、视觉对应和多跳推理）进行增强。在BLINK和MuirBench两个主流多图像基准上的测试结果表明，Qwen3-VL整体优于其他领先的视觉语言大模型，其“思考”版本在MuirBench上取得了显著的领先成绩。

---

### **要点总结**

#### **1. 任务价值与技术增强**
*   **高级能力要求**：多图像理解需要模型进行**跨图像的上下文分析**，处理多样的视觉模式，从而实现更高级的识别与推理，这比单图像任务更具挑战性。
*   **技术赋能**：为提升此能力，Qwen3-VL采用了**综合的跨图像模式学习技术**进行滋养，具体包括：
    *   多图像指代定位
    *   视觉对应
    *   多跳推理

#### **2. 评估基准与核心结果**
*   **评估基准**：在两个知名的多图像理解基准上进行测试：
    *   **BLINK**
    *   **MuirBench**
*   **整体性能**：如表2所示，Qwen3-VL在多项对比中展现出**整体优势（overall superiority）**。
*   **具体模型表现**：
    *   **Qwen3-VL-235B-A22B-Instruct**（指令跟随版本）：其性能与 **Gemini-2.5-Pro** 等顶尖模型相当。
    *   **Qwen3-VL-235B-A22B-Thinking**（思考版本）：在 **MuirBench** 基准上取得了 **80.1** 的卓越分数，**超越了所有其他对比模型**。

### **核心结论**
Qwen3-VL不仅在单模态和常见的多模态任务上表现强劲，在需要**综合分析和关联多个图像信息**的复杂任务上也确立了领先地位。其“思考”版本在MuirBench上的突出表现，尤其证明了其在进行深度、连贯的跨图像推理方面具有强大能力。

## 6.7 - Embodied and Spatial Understanding

### **内容概括**

本部分报告了Qwen3-VL系列模型在**具身智能与空间理解**这一关键领域的性能。通过在 **ERQA、VSIBench、EmbSpatial、RefSpatial** 和 **RoboSpatialHome** 等一系列具有挑战性的基准上进行严格评估，并与顶尖模型对比，结果显示：Qwen3-VL在空间理解相关任务上表现出卓越的能力，其性能可与Gemini-2.5-Pro、GPT-5、Claude-Opus-4.1等顶级模型相媲美。其**旗舰模型Qwen3-VL-235B-A22B**在多个基准上取得了顶尖分数，这主要归功于其在训练中使用了高分辨率视觉数据与细粒度的空间标注，以及集成了指向、定位和时空感知数据。

---

### **要点总结**

#### **1. 评估范围**
*   **评估基准**：在**具身智能与空间理解**领域使用了一个具有挑战性的基准套件，包括：
    *   **ERQA** (Team et al., 2025)
    *   **VSIBench** (Yang et al., 2025b)
    *   **EmbSpatial** (Du et al., 2024)
    *   **RefSpatial** (Zhou et al., 2025)
    *   **RoboSpatialHome** (Song et al., 2025a)

#### **2. 核心评估结果**
*   **整体表现**：在这些基准上，模型展现了**卓越的能力**，其性能可与 **Gemini-2.5-Pro、GPT-5 和 Claude-Opus-4.1** 等顶级模型相匹敌。
*   **具体成绩（以Qwen3-VL-235B-A22B为例）**：
    *   **EmbSpatial**: **84.3**
    *   **RefSpatial**: **69.9**
    *   **RoboSpatialHome**: **73.9**
    *   **ERQA**: **52.5**
    *   **VSIBench**: **60.0**

#### **3. 能力来源分析**
*   **强大的空间理解能力**：这种成功主要源于模型在训练时使用了**高分辨率视觉数据**，并辅以**细粒度的指向、相对位置标注和问答对**，这使其获得了深刻的空间理解能力。
*   **集成的具身智能数据**：通过在训练中集成**指向、视觉定位和时空感知数据**，模型的具身智能得到了显著增强，从而在ERQA和VSIBench等基准上取得了顶级分数。

## 6.8 - Video Understanding

### **内容概括**

本节报告了Qwen3-VL系列模型在**视频理解**任务上的全面评估。得益于训练数据规模的扩大和关键架构改进（如交错的MRoPE、文本时间戳插入和时间密集型视频字幕的扩展），模型性能得到显著提升，其8B参数版本甚至能与前代72B模型竞争。通过在通用视频理解、时序定位、视频推理和长视频理解等多个权威基准上与Gemini 2.5 Pro、GPT-5等顶尖闭源模型对比，Qwen3-VL展现出强大竞争力。其旗舰模型在标准任务上与领先模型持平，在长视频任务上（凭借256K上下文）甚至实现了超越。

---

### **要点总结**

#### **1. 性能飞跃与关键改进**
*   **显著提升**：Qwen3-VL的视频理解能力因**数据扩展**和**关键架构改进**而大幅增强。
*   **核心技术**：**交错的MRoPE**、**文本时间戳的插入**以及**时间密集型视频字幕的扩展**是提升性能的关键。
*   **效率突破**：得益于上述改进，**Qwen3-VL-8B** 模型实现了与庞大的 **Qwen2.5-VL-72B** 相竞争的性能，体现了卓越的能效比。

#### **2. 全面评估与核心结果**
*   **评估范围广泛**：在**通用视频理解**、**时序视频定位**、**视频推理**和**长视频理解**四大类任务上进行了综合评测，涵盖 VideoMME、MVBench、Charades-STA、VideoMMU、LVBench、MLVU 等多个基准。
*   **整体竞争力强**：与 **Gemini 2.5 Pro、GPT-5、Claude Opus 4.1** 等顶尖闭源模型相比，Qwen3-VL 展现出**竞争力，并在多个案例中表现更优**。
*   **旗舰模型表现**：
    *   **标准任务**：**Qwen3-VL-235B-A22B-Instruct** 在标准视频理解基准上的性能与 **Gemini 2.5 Pro** 和 **GPT-5** 等领先模型**相当**。
    *   **长视频任务**：通过将上下文窗口扩展至 **256K tokens**，该模型在长视频评估任务（尤其是 **MLVU**）上**达到甚至超越了 Gemini-2.5-Pro** 的表现。

#### **3. 评估设置与技术细节**
*   **统一限制**：评估时对所有基准设定了每视频**最多2048帧**的上限，确保视频总token数不超过224K。
*   **差异化采样**：不同基准采用了不同的帧采样策略（如Charades-STA为4fps，其他多为2fps）和每帧最大token数（VideoMMU/MMVU为768，其他为640）。
*   **评估方法**：对于VideoMMU，由于基于规则的评分不够准确，采用了**基于模型的评判员**进行评估。
*   **公平性说明**：报告坦诚指出，由于资源与API限制，与竞品的对比可能无法完全公平（例如，竞品模型的输入帧数受到更大限制）。

## 6.9 - Agent

### **内容概括**

本部分评估了Qwen3-VL作为**图形用户界面智能体**的两项核心能力：**UI感知**与**在线决策**。通过分别在GUI定位任务和在线模拟环境中的测试，结果表明：Qwen3-VL-235B-A22B在跨平台的界面感知任务上达到了最先进的性能；其32B版本在在线决策评估中超越了现有基础视觉语言模型。全系列模型展现了卓越的规划、决策与反思能力，且较小的模型也具备高度竞争力。

---

### **要点总结**

#### **1. 评估框架与基准**
评估从两个互补的维度进行：
*   **UI感知能力**：通过 **GUI定位任务** 评估，使用的基准包括：
    *   ScreenSpot
    *   ScreenSpot Pro
    *   OSWorldG
*   **决策与执行能力**：通过 **在线环境评估** 测试，使用的基准包括：
    *   AndroidWorld
    *   OSWorld

#### **2. 核心评估结果**
*   **UI感知（GUI定位）**：
    *   **旗舰模型卓越**：**Qwen3-VL-235B-A22B** 在涵盖桌面、移动端和PC的多种交互界面任务中，取得了**最先进的性能**，展现了异常强大的UI感知能力。
*   **在线决策与执行**：
    *   **中等模型领先**：**Qwen3-VL-32B** 在在线评估中得分优异（OSWorld: 41, AndroidWorld: 63.7），**超越了当前现有的基础视觉语言模型**。
*   **综合智能体能力**：Qwen3-VL 整体表现出异常强大的**规划、决策和反思能力**，证明了其作为GUI智能体的高度实用性。
*   **小模型竞争力**：值得注意的是，**更小规模的Qwen3-VL模型**在这些基准测试上也展现了**高度竞争力的性能**。

### **核心结论**
Qwen3-VL不仅是一个被动的多模态理解模型，更是一个具备**主动交互能力**的智能体。其在UI元素精准感知和在线环境复杂决策两方面的优异表现，证明了它能够有效理解图形界面并执行任务，为自动化测试、辅助操作等实际应用奠定了坚实基础。

## 6.10 - Text-Centric Tasks

### **内容概括**

本部分旨在全面评估 Qwen3-VL 系列模型在处理**纯文本任务**（即“文本中心”任务）上的能力。为了进行系统化评估，研究团队将任务划分为**知识、推理、代码、对齐任务、智能体及多语言**六大类别，并采用了相应的权威基准。报告详细说明了针对不同规模与架构模型的评估参数设置，并分层次对比了旗舰模型、中等模型及小型“边缘侧”模型与当前领先的纯文本模型和思考模型的表现。核心结论是：Qwen3-VL 作为视觉语言模型，在广泛的文本任务上展现出了与顶尖纯文本模型**相媲美甚至超越的竞争力**，这验证了其强大的跨模态能力与知识蒸馏策略的有效性。

---

### **要点总结**

#### **1. 系统化的评估框架**
评估围绕六大类文本任务展开，并采用对应基准：
*   **知识**：MMLU-Pro, MMLU-Redux, GPQA, SuperGPQA
*   **推理**：AIME-25, HMMT-25, LiveBench
*   **代码**：LiveCodeBench v6, CFEval, OJBench
*   **对齐任务**：IFEval, Arena-Hard v2, Creative Writing v3, WritingBench
*   **智能体**：BFCL-v3, TAU2 系列（Retail, Airline, Telecom）
*   **多语言**：MultiIF, MMLU-ProX, INCLUDE, PolyMATH

#### **2. 差异化的评估设置**
为不同模型设定了精细化的采样超参数以确保公平和最佳表现：
*   **指令模型（235B-A22B, 32B, 30B-A3B）**：温度=0.7，top-p=0.8，top-k=20，存在惩罚=1.5。
*   **小型指令模型（8B, 4B, 2B）**：温度=1.0，top-p=1.0，top-k=40，存在惩罚=2.0。
*   **思考模型**：
    *   混合专家架构：温度=0.6，top-p=0.95，top-k=20。
    *   密集架构：温度=1.0，top-p=0.95，top-k=20，存在惩罚=1.5。
*   **输出长度**：大多数任务设为32,768/32,780 tokens，对AIME-25等高推理需求任务扩展至81,920 tokens以提供充足“思考”空间。

#### **3. 核心评估结果**
*   **旗舰模型（235B-A22B）竞争力**：
    *   **指令版本**：在知识、推理、代码等任务上与 **DeepSeek-V3、Claude Opus-4** 等领先纯文本模型**表现相当或更优**。
    *   **思考版本**：在AIME-25、LiveCodeBench v6等需要深度推理的任务上，**超越了 OpenAI-o3 和 Claude Opus-4（思考模式）**。
    *   **关键意义**：作为一个**视觉语言模型**，其文本能力达到了顶级**纯文本大语言模型**的水平，实现了视觉与文本能力的成功融合。

*   **中等模型（32B / 30B-A3B）显著提升**：
    *   与其对应的纯文本版本（Qwen3-32B, Qwen3-30B-A3B）相比，**在所有基准上均显示出显著的性能提升**。
    *   在部分推理基准上，达到了与更大规模纯文本模型**相当甚至更好的结果**。

*   **小型“边缘侧”模型（8B/4B/2B）表现优异**：
    *   Qwen3-VL-2B/4B/8B 模型整体表现令人印象深刻，**均优于其对应的纯文本基线模型**（如Qwen3-1.7B/4B/8B）。

#### **4. 核心结论与归因**
评估结果证明了 **“强到弱知识蒸馏”方法的巨大功效**。该方法使得团队能够以**显著降低的成本和精力**，构建出在文本任务上表现卓越的轻量级多模态模型。这表明Qwen3-VL不仅视觉能力强，其文本理解与推理的基础也极为牢固，是一个真正均衡且强大的通用模型。

## 6.11 - Ablation Study

### **内容概括**

1.  **第一张图（消融研究）**：通过对比实验，验证了Qwen3-VL所采用的两个核心创新——**Qwen3-ViT视觉编码器**和**DeepStack深层堆叠架构**的有效性。结果表明，Qwen3-ViT在多项基准上超越了之前广泛使用的SigLIP-2编码器；而DeepStack架构则为模型带来了全面的性能提升。
2.  **第二张图（长上下文评估）**：通过精心设计的“大海捞针”实验，评估了模型在超长视频中定位并理解特定信息的能力。结果显示，Qwen3-VL-235B-A22B-Instruct模型在处理长达30分钟（对应256K tokens）的视频时能达到100%的准确率，即使在通过YaRN技术将上下文扩展到约2小时（对应1M tokens）时，准确率仍能保持在99.5%，证明了其强大的长序列建模能力。

---

### **要点总结**

#### **1. 关键技术消融研究**
*   **Qwen3-ViT视觉编码器**：
    *   **对比对象**：与之前的先进视觉编码器 **SigLIP-2** 进行对比。
    *   **核心发现**：
        *   **预训练阶段**：在CLIP风格的零样本评估中，Qwen3-ViT在标准基准上保持了竞争力，同时在内部综合评估集 **OmniBench** 上取得了显著提升。
        *   **下游VLM阶段**：当与相同的1.7B语言模型结合时，Qwen3-ViT在多项关键视觉语言任务上**持续优于**基于SigLIP-2的模型，并在OmniBench上保持显著领先。
    *   **结论**：Qwen3-ViT作为一个强大的视觉骨干网络，其有效性和优越性得到了验证。

*   **DeepStack深层堆叠架构**：
    *   **核心发现**：配备了DeepStack的模型在**多个评估基准上均表现出性能增益**。
    *   **结论**：实验数据支撑了DeepStack架构能够有效提升模型整体性能的结论。

#### **2. 长上下文处理能力评估**
*   **评估任务**：“大海捞针”实验。将包含关键视觉信息的“针”帧插入长视频的不同位置，要求模型准确定位该帧并回答相关问题。
*   **评估设置**：视频以1帧/秒采样，动态调整分辨率以维持固定的视觉token预算。
*   **核心性能**：
    *   在长达**30分钟**（对应 **256K tokens** 上下文）的视频中，模型实现了 **100%** 的定位与回答准确率。
    *   通过 **YaRN** 位置编码扩展技术，将上下文长度推至约 **1M tokens**（约2小时视频）时，准确率仍高达 **99.5%**。
*   **结论**：该结果强有力地证明了Qwen3-VL模型**强大的长序列建模与信息提取能力**，能够高效、准确地处理超长视频输入。

## 7 - Conclusion

### **内容概括**

本部分作为全文的总结，高度概括了Qwen3-VL系列模型的**核心贡献、技术优势、应用价值与未来愿景**。文章指出，Qwen3-VL通过高质量数据迭代与架构创新（如增强的交错MRoPE、DeepStack视觉-语言对齐等），在广泛的多模态基准上取得了前所未有的性能，同时保持了强大的纯文本能力。其原生支持256K令牌的长序列推理，适用于需要高保真跨模态理解的真实世界应用。展望未来，研究团队视Qwen3-VL为连接数字与物理世界的**具身AI智能体的基础引擎**，并计划向交互感知、工具增强推理等方向拓展。团队将以Apache 2.0许可证开源整个模型家族，以推动社区创新。

---

### **要点总结**

#### **1. 核心成就与技术亮点**
*   **前沿模型系列**：Qwen3-VL是处于技术前沿的视觉-语言基础模型系列，推动了多模态理解与生成的边界。
*   **卓越性能**：通过**高质量多模态数据迭代**和**关键架构创新**（增强的交错MRoPE、DeepStack视觉-语言对齐、基于文本的时间定位等），在广泛的多模态基准上取得了**前所未有的性能**。
*   **强大文本能力**：在实现顶尖多模态性能的同时，**保持了强大的纯文本能力**。
*   **长上下文支持**：原生支持 **256K令牌的交错序列**，能够对长文档、图像序列和视频进行**鲁棒的长序列推理**，使其特别适合需要高保真跨模态理解的现实应用。
*   **灵活部署**：提供**密集（Dense）和混合专家（MoE）两种变体**，以满足不同延迟和质量要求的灵活部署。

#### **2. 未来愿景与方向**
*   **核心定位**：将Qwen3-VL定位为**具身AI智能体的基础引擎**，旨在无缝连接数字世界与物理世界。
*   **能力展望**：未来的智能体不仅能感知和推理丰富的多模态输入，还能在动态环境中执行**果断的、情境感知的行动**，与用户交互、操纵数字界面，并通过基于多模态的决策来指导机器人系统。
*   **技术拓展**：未来工作将集中于扩展Qwen3-VL的能力，方向包括：
    *   **交互式感知**
    *   **工具增强推理**
    *   **实时多模态控制**
*   **架构探索**：积极探索**统一的理解-生成架构**，利用视觉生成能力来进一步提升整体智能。
*   **开源承诺**：以 **Apache 2.0 许可证** 开源整个模型家族，旨在催化社区驱动的创新，共同迈向真正集成的、多模态AI智能体的愿景。



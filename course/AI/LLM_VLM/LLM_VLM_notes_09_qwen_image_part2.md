本文主要整理《Qwen-Image Technical Report》的主要内容。

## 5.0 - Pre-training

### 内容概况
本节阐述了 **Qwen-Image 模型预训练阶段所采用的核心训练方法——流匹配（Flow Matching）**。该方法通过常微分方程（ODEs）路径实现稳定学习，并保持与最大似然目标的理论等价性。文中以公式化的形式，清晰地描述了从**数据编码、噪声采样、条件输入、到中间状态计算及最终损失函数定义**的完整训练流程。

### 核心要点总结

1.  **训练目标：流匹配**
    *   采用**流匹配**作为预训练目标。其核心思想是学习一个**从简单分布（如高斯噪声）到复杂数据分布的概率流**。
    *   相比传统的扩散模型目标，流匹配通过常微分方程（ODEs）进行建模，通常能带来**更稳定、更高效的学习动态**。

2.  **核心训练流程**
    *   **步骤1：潜在表示编码**。将原始输入图像 `x_0` 通过**变分自编码器（VAE）的编码器 `ℰ`** 转换到潜在空间，得到 `z`。
    *   **步骤2：采样噪声**。从标准多元正态分布 `𝒩(0, I)` 中采样一个随机噪声向量 `x_1`。
    *   **步骤3：条件信息提取**。对于用户输入 `S`（文本或图文结合），通过**多模态大语言模型（MLLM）φ** 提取出语义条件向量 `h`。
    *   **步骤4：采样时间步**。从 `[0, 1]` 区间的一个对数正态分布中采样一个扩散时间步 `t`。
    *   **步骤5：计算中间状态与“速度”**。根据 **Rectified Flow** 方法，通过线性插值计算时间步 `t` 对应的中间潜在变量 `x_t` 及其理论“速度” `v_t`（公式 (1)）。
    *   **步骤6：模型训练与损失计算**。模型 `f_θ` 的任务是，给定 `x_t`、时间 `t` 和条件 `h`，预测这个“速度”。损失函数定义为模型预测的速度 `v_θ` 与真实速度 `v_t` 之间的**均方误差（MSE）**（公式 (2)）。

3.  **核心优势**
    *   **稳定性**：基于 ODE 的流匹配学习过程通常更平滑、收敛更稳定。
    *   **等价性**：在理论上，此目标与最大似然估计是等价的，确保了模型学习的有效性。

### 公式解释

**公式 (1)：中间状态与速度的计算**
```
x_t = t * x_0 + (1 - t) * x_1
v_t = dx_t / dt = x_0 - x_1
```
*   **`x_t`**：在时间步 `t` 时的**中间潜在变量**。它是由干净的图像潜在表示 `x_0` 和纯噪声 `x_1` 通过**线性插值**得到的。当 `t=0` 时，`x_t = x_1`（纯噪声）；当 `t=1` 时，`x_t = x_0`（干净数据）。这定义了一条从噪声到数据的**确定性直线路径**。
*   **`v_t`**：定义为 `x_t` 对时间 `t` 的导数，即**速度**。在这个线性插值的设定下，速度是一个**常数**：`x_0 - x_1`。它指示了从噪声 `x_1` “流向” 数据 `x_0` 所需的方向和大小。

**公式 (2)：流匹配损失函数**
```
ℒ = 𝔼_{(x_0, h)∼𝒟, x_1, t} [ || v_θ(x_t, t, h) - v_t ||^2 ]
```
*   这是预训练的**目标函数（损失）**。
*   **`v_θ(x_t, t, h)`**：这是 **Qwen-Image 模型（参数为 θ）的预测输出**。模型接收中间状态 `x_t`、时间步 `t` 和条件向量 `h` 作为输入，目标是预测出从当前状态回到干净数据所需的“速度”。
*   **`v_t`**：根据公式 (1) 计算出的**真实速度**，即 `x_0 - x_1`。
*   **`|| ... ||^2`**：计算预测速度与真实速度之间的**L2范数的平方**，即**均方误差（MSE）**。
*   **`𝔼_{(x_0, h)∼𝒟, x_1, t}`**：表示对**整个训练数据集 `𝒟` 中的样本 `(x_0, h)`**、随机采样的噪声 `x_1` 以及随机时间步 `t` 取**数学期望（平均值）**。模型训练的目标就是最小化这个平均预测误差。

**总结**：Qwen-Image 的预训练过程，本质上是**让模型学会预测一条从任意噪声点 `x_1` 回归到对应数据点 `x_0` 的直线路径的“速度”**。通过最小化这个速度预测的误差，模型最终能够学会从纯噪声出发，沿着学习到的流（由 ODE 描述）逆向推导，生成符合条件 `h` 的高质量图像。

## 5.1 - Producer-Consumer Framework

### 内容概况
本节介绍了一个受**Ray框架**启发的**生产者-消费者（Producer-Consumer）框架**。该框架的核心设计目标是解决在**大规模GPU集群**上进行训练时，**数据预处理**（I/O密集型）与**模型训练**（计算密集型）之间的耦合瓶颈问题。通过将两者**解耦**，实现了异步并行操作、高效资源利用和数据管道的动态更新。

### 要点总结
1.  **核心目标**：确保扩展到大规模GPU集群时，既能实现**高吞吐量**，又能保证**训练稳定性**。
2.  **设计理念**：将**数据预处理（生产者）** 与**模型训练（消费者）** 解耦，使两者能**异步、高效地并行运行**，并支持在训练过程中**动态更新数据管道**而无需中断训练。
3.  **生产者端（Producer）职责**：
    *   **预处理**：对原始图文数据进行**过滤、编码**（使用MLLM和VAE模型转换为潜在表示）。
    *   **组织与存储**：将处理后的数据按**分辨率分组**，存入**快速访问的缓存桶**中，并放置在**共享的、位置感知的存储**中。
4.  **消费者端（Consumer）职责**：
    *   专门部署在**GPU密集型集群**上。
    *   **专注训练**：将所有计算资源完全投入到**MMDiT模型的训练**中。
    *   **异步拉取**：每个数据并行组直接从生产者端**异步拉取**已预处理好的批次数据。
5.  **连接与通信**：
    *   通过一个**特定的HTTP传输层**连接生产者和消费者。
    *   该传输层原生支持**RPC语义**，实现了两者间的**异步、零拷贝调度**，极大提升了数据传输效率。
6.  **训练配置**：
    *   MMDiT模型的参数在消费者节点间采用 **4路张量并行** 的方式进行分布式存储和计算。

**总结**：该框架是支撑Qwen-Image模型进行高效、稳定大规模训练的**关键基础设施**。它通过精妙的解耦设计，使得数据准备和模型计算这两个阶段都能以最优效率运行，是模型得以成功训练的重要工程保障。

## 5.2 - Distributed Training Optimization

### 内容概况
本部分阐述了为应对 Qwen-Image 庞大参数量带来的内存挑战，研发团队所采用的一系列高级分布式训练策略。核心在于利用 **Megatron-LM** 框架，通过**混合并行**、**分布式优化器**等技术，在保证训练效率的同时，成功地将模型部署到大规GPU集群上进行训练。

### 核心要点总结

1.  **核心挑战与解决方案**
    *   **挑战**：模型参数量巨大，仅使用 **FSDP** 无法将模型装入单个GPU。
    *   **解决方案**：放弃单一的FSDP，转而采用专为超大模型设计的 **Megatron-LM** 训练框架。

2.  **关键技术一：混合并行策略**
    *   **策略组合**：结合 **数据并行** 与 **张量并行**，以高效利用大规模GPU集群。
    *   **实现方式**：使用 **NVIDIA Transformer-Engine** 库构建核心的 **MMDiT** 模型。该库支持**无缝且自动地切换不同程度的张量并行**，提供了极大的灵活性。
    *   **专项优化**：针对多头自注意力模块，采用了 **头并行** 策略。与传统的沿注意力头维度进行张量并行相比，此方法能**有效减少同步和通信开销**。

3.  **关键技术二：内存优化技术**
    *   **目标**：在反向传播过程中，以最小的重计算开销来缓解GPU内存压力。
    *   **尝试方案**：实验了 **分布式优化器** 和 **激活检查点** 技术。
    *   **实践发现**：**激活检查点** 会为反向传播引入**大量的计算开销**，从而可能**显著降低训练速度**。这表明在实际应用中需要在内存节省和计算效率之间做出权衡。

**总结**：Qwen-Image 的分布式训练方案是一个高度工程化的系统，它通过 **Megatron-LM 框架的混合并行** 解决了模型放不下的根本问题，并利用 **Transformer-Engine 的灵活性** 和 **头并行等专项优化** 来提升训练效率，同时对 **激活检查点** 等技术的副作用进行了实证评估。这些优化共同支撑了该超大模型的可行与高效训练。

## 5.3 - Training Strategy

### 内容概况
本部分阐述了 Qwen-Image 采用的**多阶段渐进式预训练策略**。其核心思想是：**不采用单一、固定的数据集和训练模式，而是将训练过程划分为多个阶段，在每个阶段有针对性地调整数据构成与训练目标**，从而引导模型稳健、高效地从学习基础通用视觉概念，逐步掌握复杂、高难度的生成能力。

### 核心要点总结（五大渐进式策略）
该训练策略围绕五个关键维度，实施了从“简单”到“复杂”、从“粗糙”到“精细”的渐进式演进：

1.  **分辨率渐进提升**：从低分辨率到高分辨率
    *   **路径**：`256×256` → `640×640` → `1328×1328`
    *   **目的**：使模型先掌握物体的基本结构和构图，再逐步学习捕捉**更精细的纹理、细节和颜色渐变**，从而提升生成图像的**真实感和泛化能力**。

2.  **文本渲染能力渐进集成**：从无文本图像到含文本图像
    *   **路径**：先使用**常规视觉数据集**训练通用视觉表示 → 再逐步引入**包含渲染文本的自然背景图像**
    *   **目的**：专门解决**文本渲染（特别是复杂的中文字符）能力薄弱**的通用难题。让模型在掌握基础视觉生成后，再专项学习如何将文本精准、自然地嵌入到图像中。

3.  **数据质量渐进精炼**：从大规模数据到高质量精选数据
    *   **路径**：早期使用**大规模、覆盖面广的数据**打下基础 → 后期应用**更严格的过滤机制**，逐步使用**质量更高的精选数据**
    *   **目的**：在保证模型学习到广泛视觉概念的前提下，通过后期注入高质量数据，**提升生成图像的保真度和整体美学质量**，实现从“量”到“质”的飞跃。

4.  **数据分布渐进平衡**：从非平衡分布到平衡分布
    *   **路径**：在训练过程中，逐步调整数据在**领域（如自然、设计、人物）** 和**分辨率**上的分布，使其更加均衡。
    *   **目的**：防止模型**过度拟合**到某些常见领域或分辨率，确保其在各种场景和设定下都能生成**高保真、细节丰富**的图像，从而增强**模型的泛化鲁棒性**。

5.  **训练数据渐进增强**：从真实世界数据到合成数据
    *   **路径**：以**真实世界数据**为主体 → 针对性地引入**高质量合成数据**
    *   **目的**：**弥补真实数据集的固有缺陷**，例如某些超现实风格或包含大量文本的高质量图像在现实中稀缺。合成数据可以**扩展数据分布的边界**，使模型能够处理更广泛、更多样的视觉场景。

## 6.0 - Supervised Fine-Tuning (SFT)

1.  **后训练框架结构**：Qwen-Image 的后训练明确分为两个连续阶段——**监督微调（SFT）** 和**强化学习（RL）**。
2.  **SFT阶段的核心目标**：旨在**针对性解决模型在预训练后仍存在的特定不足**，引导其生成更优质的内容。
3.  **高质量数据集构建**：
    *   **数据组织**：构建了一个**按语义类别层次化组织**的数据集。
    *   **数据标准**：对入选图像有严格的质量要求，必须满足**清晰、细节丰富、明亮且逼真（具有照片真实感）**。
    *   **标注方式**：采用了**精细的人工标注**，以确保指导信息的准确性和高质量。
4.  **最终目的**：通过上述方法，系统性地**指导模型朝着生成更具真实感和更细致细节的内容方向发展**，为后续的强化学习阶段打下坚实基础。

## 6.1 - Reinforcement Learning (RL)

### 内容概况
这四张图片内容分为两大核心部分：
1.  **图片1、2、4：强化学习策略** - 详细阐述了用于对齐和提升模型输出质量的两种强化学习算法：**直接偏好优化（DPO）** 和**组相对策略优化（GRPO）**。这包括了它们的数据准备、训练目标、数学公式以及实现采样随机性的技术细节。
2.  **图片3：图像编辑架构** - 展示了用于文本引导的图像编辑任务的具体模型架构，特别是如何通过改进的**位置编码（MS-RoPE）** 来处理“原始图像”与“目标图像”的关系。

### 核心要点总结

#### （一）强化学习策略
Qwen-Image 采用了一个**两阶段RL策略**来微调模型，使其生成结果更符合人类偏好。

1.  **阶段一：直接偏好优化**
    *   **目标**：高效、大规模地利用**离线的人类偏好数据**来调整模型。
    *   **数据准备**：针对同一提示词，生成多张图片，由标注员选出**最佳**和**最差**的样本。数据分为**有参考图**和**无参考图**两类，标注规则略有不同。
    *   **算法核心**：通过对比“获胜”图片和“失败”图片在模型预测上的差异，构建损失函数，引导模型更倾向于生成“获胜”类型的输出。

2.  **阶段二：组相对策略优化**
    *   **目标**：在DPO基础上，进行更精细的、**在线的策略优化**。
    *   **工作流程**：
        *   **分组采样**：模型一次性为一组提示生成多张图片及其完整的生成轨迹。
        *   **组内评分**：使用奖励模型为组内每张图片打分。
        *   **计算优势**：根据组内得分的均值和标准差，计算每张图片的**相对优势**，用于衡量其好坏。
        *   **策略优化**：通过优化目标函数，有倾向性地提升高优势（好）样本的生成概率，并约束新策略不要偏离原始策略（参考模型）太远。
    *   **关键改进**：为了增加探索的随机性，将原本确定性的“流匹配”采样过程，重构为引入了随机噪声项的**随机微分方程（SDE）** 过程。

#### （二）图像编辑架构
*   **核心任务**：根据输入图像和用户文本提示，生成编辑后的目标图像。
*   **架构流程**：
    1.  **双路输入编码**：将**输入图像**和**目标图像**分别通过VAE编码器转换为潜在表示。
    2.  **语义理解**：使用 **Qwen2.5-VL** 理解用户文本提示和输入图像的组合，生成语义条件。
    3.  **改进的位置编码**：在**MS-RoPE** 位置编码中，引入了额外的 **`frame` 维度**，用以明确区分和表征“输入图像”与“目标图像”的 patches，帮助模型更好地区分编辑前后的内容。

### 关键公式解释

#### **公式 (3): DPO 目标函数**
$$ \begin{cases}\text{Diff}_{\text{policy}}=(\|v_{\theta}(x_{t}^{\text{win}},h,t)-v_{t}^{\text{win}}\|_{2}^{2}-\|v_{\theta}(x_{t}^{\text{lose}},h,t)-v_{t}^{\text{lose}}\|_{2}^{2})\\\text{Diff}_{\text{ref}}=(\|v_{\text{ref}}(x_{t}^{\text{win}},h,t)-v_{t}^{\text{win}}\|_{2}^{2}-\|v_{\text{ref}}(x_{t}^{\text{lose}},h,t)-v_{t}^{\text{lose}}\|_{2}^{2})\\\mathcal{L}_{\text{DPO}}=-\mathbb{E}[\log\sigma(-\beta(\text{Diff}_{\text{policy}}-\text{Diff}_{\text{ref}}))],\end{cases} $$
*   **公式解读**：该损失函数旨在让**待优化的策略模型**学会区分“好图”和“差图”。
*   **`Diff_policy`**：策略模型对“好图”和“差图”的预测误差之差。我们希望这个值**越小越好**（即策略模型对好图的预测误差远小于对差图的误差）。
*   **`Diff_ref`**：参考模型（通常为SFT后的模型）对“好图”和“差图”的预测误差之差。作为一个**基线**。
*   **核心思想**：损失函数通过Sigmoid函数鼓励 `(Diff_policy - Diff_ref)` 为**负数**，即策略模型在区分好坏图片的能力上要**显著优于**参考模型。`β` 是一个缩放参数，控制偏好学习的强度。

#### **公式 (4) & (5): GRPO 优势函数与目标**
$$ A_{i}=\frac{R(x_{0}^{i},h)-\operatorname{mean}(\{R(x_{0}^{i},h)\}_{i=1}^{G})}{\operatorname{std}(\{R(x_{0}^{i},h)\}_{i=1}^{G})} $$
*   **公式解读**：**优势函数**。它衡量组内单张图片得分相对于本组平均水平的**标准化优势**。正值表示该图片优于组内平均水平，负值则表示劣于平均水平。这消除了绝对分数偏差的影响。

$$ \mathcal{L}_{\text{GRPO}}(\theta)=\mathbb{E}[\frac{1}{G}\sum_{i=1}^{G}\frac{1}{T}\sum_{t=0}^{T-1}\left(\min(r_{t}^{i}(\theta)A_{i},\operatorname{clip}(r_{t}^{i}(\theta),1-\epsilon,1+\epsilon)A_{i})-\beta D_{KL}(\pi_{\theta}||\pi_{\text{ref}})\right)] $$
*   **公式解读**：**GRPO总目标**。它由两部分组成：
    1.  **策略提升项**：`min(...)` 部分。`r_t^i(θ)` 是新旧策略生成某一步轨迹的概率比。该部分鼓励提升高优势 (`A_i > 0`) 轨迹的概率，同时通过**裁剪**防止单步更新过大，保证训练稳定。
    2.  **策略约束项**：`-β D_KL(...)`。KL散度惩罚项，防止优化后的新策略 `π_θ` 过度偏离原始的参考策略 `π_ref`，避免“放飞自我”导致模型崩溃。`β`是控制约束强度的系数。

#### **公式 (6) & (7): SDE 采样过程**
$$ \mathrm{d}x_{t}=\left(v_{t}+\frac{\sigma_{t}^{2}}{2t}(x_{t}+(1-t)v_{t})\right)\mathrm{d}t+\sigma_{t}\mathrm{~d} w $$
$$ x_{t+\Delta t}=x_{t}+\left[v_{\theta}(...)+\frac{\sigma_{t}^{2}}{2t}(x_{t}+(1-t)v_{\theta}(...))\right]\Delta t+\sigma_{t}\sqrt{\Delta t}\epsilon $$
*   **公式解读**：这两个公式描述了**引入随机性的采样过程**。公式(6)是连续的随机微分方程形式，公式(7)是其离散化的欧拉-丸山形式。
*   **确定性部分**：`v_θ(...)Δt` 是模型预测的“漂移”方向，即去噪的主方向。
*   **随机性部分**：`σ_t √Δt ε` 是额外添加的随机噪声（`ε ~ N(0, I)`）。`σ_t` 控制噪声强度。
*   **作用**：在采样时引入可控的随机噪声，允许模型探索更多可能的生成路径，有助于在GRPO阶段进行更充分的探索，找到更好的生成策略。

#### **公式 (8): KL散度的闭式解**
$$ D_{KL}(\pi_{\theta}||\pi_{\text{ref}})=\frac{\Delta t}{2}\left(\frac{\sigma_{t}(1-t)}{2t}+\frac{1}{\sigma_{t}}\right)^{2}||v_{\theta}(...)-v_{\text{ref}}(...)||^{2} $$
*   **公式解读**：该公式给出了在SDE采样设定下，新策略 `π_θ` 与参考策略 `π_ref` 之间KL散度的**解析表达式**。
*   **核心变量**：KL散度的大小正比于两个策略模型（优化模型 `v_θ` 和参考模型 `v_ref`）**预测速度的差异的平方** `||v_θ - v_ref||^2`。
*   **作用**：由于有了这个闭式解，在计算GRPO的损失函数时，可以高效、准确地评估策略的偏离程度，无需进行复杂的估算，使得训练更加稳定和高效。

### **总结**
Qwen-Image 的后训练阶段通过 **DPO（大规模离线偏好学习）** 和 **GRPO（小规模在线精细优化）** 的组合拳，系统性地将模型输出与人类偏好对齐。GRPO阶段创新的 **SDE采样** 和可计算的 **KL散度**，为稳定有效的在线策略优化提供了保障。同时，针对图像编辑任务专门设计的**双流架构与改进的位置编码**，确保了模型能精准理解并执行编辑指令。这些构成了其实现高质量、可控图像生成与编辑的核心技术环节。

## 6.2 - Multi-task training

### 内容概况
这部分内容阐述了 **Qwen-Image 如何从一个强大的文本到图像生成模型，扩展为一个支持“图像+文本”多模态输入的统一生成基础模型**。通过引入对图像输入的原生支持与关键架构改进，模型现在能够处理包括**指令图像编辑、新视图合成**等在内的多种生成与视觉任务。

### 核心要点总结

1.  **任务范围扩展**：
    *   基础能力从单一的 **文本到图像生成**，扩展至一系列以 **“图像+文本”为输入** 的生成任务，可统称为**通用图像编辑任务**。
    *   具体涵盖：**基于指令的图像编辑、新视图合成**，以及**深度估计**等计算机视觉任务。

2.  **核心技术机制**：
    *   **原生图像输入支持**：依托 **Qwen2.5-VL** 的多模态理解能力，模型可直接接收用户提供的图像。图像经视觉Transformer编码为图像块，并与文本标记拼接，形成统一的输入序列，使模型能理解复杂的图文混合指令。
    *   **双路视觉条件注入**：
        *   **语义条件（高层指导）**：从多模态大语言模型获取的视觉语义嵌入，增强了模型对指令的**理解与遵循能力**。
        *   **结构条件（底层约束）**：受先前工作启发，将输入图像通过VAE编码得到的**像素级潜在表示**，与带噪声的目标图像潜在表示在序列维度拼接，并输入到图像流中。这**显著提升了模型保持视觉保真度以及与用户提供图像结构一致性的能力**。
    *   **扩展位置编码**：为让模型明确区分多图像输入（如编辑中的“原图”与“目标图”），对先前提出的 **MS-RoPE** 位置编码进行了扩展，在原有的高度、宽度维度外，引入了额外的 **帧维度**。

### **总结**
通过上述技术升级，Qwen-Image 成功构建了一个**统一的多任务生成框架**。该框架不仅保留了强大的文生图能力，更通过**双路条件注入**（语义理解+结构保持）和**改进的位置编码**，使其能够精准执行以图文为输入的复杂编辑与生成任务，实现了从单一功能到通用基础的进化。

## 7.0 - Human Evaluation

### 内容概况
这两张图片系统性地展示了 **Qwen-Image 与全球顶尖闭源图像生成API进行客观、公平对比的评估框架与结果**。
1.  **第一张图（文本部分，5.1节）**：详细阐述了评估所采用的 **“AI Arena” 开源基准平台**的设计理念、运行机制、评估设置及最终排名结果。
2.  **第二张图（可视化部分，图16）**：直观展示了 **AI Arena平台的前端界面** 以及包含各大模型排名的 **“文生图Elo排行榜”**。

### 核心要点总结

#### **（一）评估平台：AI Arena**
这是一个为多模态生成模型设计的**公平、动态、开源的竞技平台**，其核心设计确保评估的客观性：
1.  **匿名盲测机制**：每轮向用户匿名展示同一提示词下两个随机模型生成的图像，用户投票选择更优者。
2.  **Elo评分系统**：根据投票结果，采用国际象棋等竞技体育中广泛使用的 **Elo算法** 动态更新全球排行榜，量化模型相对实力。
3.  **严谨的评估设置**：
    *   **提示词库**：精心策划了约 **5,000个** 涵盖多样主题、风格和摄影视角的提示词。
    *   **评估团队**：邀请了超过 **200名** 来自不同专业背景的评估者参与。
    *   **公平性保障**：为保持客观，**排除了涉及中文文本的提示**（因多数闭源API对中文支持不佳）；并通过技术手段检测和剔除作弊或无效数据。

#### **（二）竞争对手与评估规模**
*   **竞争对手**：选取了五个业界领先的闭源API作为对手，包括Google的 **Imagen 4 Ultra**、OpenAI的 **GPT Image 1 [High]**、Black Forest Labs的 **FLUX.1 Kontext [Pro]** 等。
*   **评估规模**：截至报告时，每个模型都参与了**至少10,000次**两两比较，确保了结果的统计鲁棒性。

#### **（三）核心评估结果**
在AI Arena平台的文生图Elo排行榜中（如图16所示）：
1.  **排名表现**：**Qwen-Image 作为榜单中唯一的开源模型，总体排名第三。**
2.  **分数对比**：
    *   虽落后于排名第一的 **Imagen 4 Ultra** 约30个Elo分。
    *   但相比 **GPT Image 1 [High]、FLUX.1 Kontext [Pro]** 等知名闭源模型，取得了**超过30个Elo分的显著优势**。
3.  **结论意义**：这一客观、大规模的盲测结果证明，**Qwen-Image 的性能已达到全球第一梯队水平**，为开发者、研究者和用户提供了一个兼具强大性能与实用价值的开源选择。

#### **（四）平台的开放性与未来**
*   **公众开放**：AI Arena平台已向公众开放，任何人都可以参与不同模型的比较。
*   **任务拓展**：未来计划从文生图评估，进一步扩展至**图像编辑、文生音频、文生视频、图生视频**等多模态生成任务。

### **总结**
通过构建 **AI Arena** 这一严谨的开源评估平台，Qwen-Image 团队以 **“匿名盲测”** 和 **“Elo动态排名”** 为核心，完成了与全球顶级闭源API的公平对决。评估结果表明，**Qwen-Image 作为开源模型，其生成质量与用户偏好对齐度已稳居世界前列**，不仅验证了其技术实力，也为开源社区树立了一个高性能的基准。

## 7.1 - Performance of VAE Reconstruction

### 内容概况
本节对包括Qwen-Image-VAE在内的**多种先进的图像“标记器”进行了全面的定量评估**。评估旨在衡量这些模型将图像压缩为紧凑的潜在表示，并重新解码重建图像的能力。报告通过标准指标、公平的参数计算和在不同数据集上的测试，证明了 **Qwen-Image-VAE 在重建质量和计算效率之间达到了最佳平衡**。

### 要点总结

1.  **评估对象与范围**：
    *   **评估对象**：对比了多种业界先进的VAE模型，包括专注于图像的**FLUX-VAE, Cosmos-CI-VAE, SD-3.5-VAE**，以及支持图像与视频的联合标记器**Wan2.1-VAE, Hunyuan-VAE** 和 **Qwen-Image-VAE**。
    *   **统一标准**：所有模型均在**8倍压缩率**和**16维潜在通道**的相同配置下进行比较，确保了对比的公平性。

2.  **核心评估指标**：
    *   **重建质量**：使用 **PSNR** 和 **SSIM** 两个业界公认的客观指标进行衡量。
    *   **计算效率**：引入 **“有效图像参数”** 的概念。对于联合图像-视频VAE，将其3D卷积核转换为等效的2D卷积核进行参数统计，以公平反映其在纯图像任务上的参数规模。

3.  **评估数据集**：
    *   **通用性能**：在经典的 **ImageNet-1k 验证集**（分辨率为256x256）上进行评估，衡量通用领域的重建保真度。
    *   **专项能力**：额外在一个**内部构建的、富含多种文本**（如PDF、PPT、海报、合成文本）和多语言的数据集上进行评估，专门测试模型对**小文本**的重建能力，这对于高质量的文本渲染至关重要。

4.  **关键发现与结论**：
    *   **性能领先**：如表2所示，**Qwen-Image-VAE 在所有评估指标上均达到了最优的重建性能**。
    *   **效率卓越**：在仅激活**编码器1900万参数**、**解码器2500万参数**的情况下，就实现了顶尖的重建质量。
    *   **最佳平衡**：Qwen-Image-VAE成功地在**出色的重建质量**和**高效的计算开销**之间取得了最优平衡。

**总结**：该评估系统性地证明了Qwen-Image-VAE作为图像标记器的优越性。它不仅在全域图像重建上表现最佳，还特别擅长重建富含文本的图像，同时保持了极高的参数效率。这为Qwen-Image模型整体强大的图像生成与编辑能力，尤其是其卓越的**文本渲染精度**，奠定了坚实的基础。

## 7.2 - Performance of Text-to-Image Generation

### 内容概况
这部分内容（论文第5.2.2节）系统性地评估了 Qwen-Image 在 **“文生图”** 核心任务上的能力，评估从 **“通用生成能力”** 和 **“文本渲染能力”** 两个维度展开，并在一系列公开权威基准测试中，与众多前沿模型进行了量化对比。

### 核心要点总结

#### **（一）评估框架概述**
1.  **通用生成能力评估**：使用 **DPG、GenEval、OneIG-Bench、TIIF** 四个综合基准，衡量模型在遵循提示词、组合推理、审美等方面的整体表现。
2.  **文本渲染能力评估**：专门测试模型在图像中生成可读文本的能力。
    *   **英文文本**：使用 **CVTG-2K** 基准。
    *   **中文文本**：为解决缺乏标准评测的问题，**引入了新的ChineseWord基准**（按汉字使用频率分三级）。
    *   **长文本渲染**：使用 **LongText-Bench** 基准，评估中英文长段落的渲染准确性。

#### **（二）通用生成能力表现**
1.  **DPG 基准**：
    *   **总体成绩第一**：Qwen-Image 在包含实体、属性、关系等细粒度评估的 **DPG** 基准上，取得了 **88.32** 的最高分，展现出了**卓越的综合提示词遵循能力**。
2.  **GenEval 基准**：
    *   **基础模型已领先**：Qwen-Image 的基础模型（SFT后）得分已超越当时的先进模型（如Seedream 3.0和GPT Image 1）。
    *   **强化学习后突破阈值**：经过强化学习微调后，其得分达到 **0.91**，是榜单上**首个且唯一超过0.9分的基础模型**，证明了其强大的可控生成能力。
3.  **OneIG-Bench 基准**：
    *   **中英文双料冠军**：在涵盖**对齐度、文本、推理、风格、多样性**五个维度的OneIG综合评估中，Qwen-Image 在**英文（0.539）和中文（0.505）赛道均位列第一**。
    *   **优势领域**：在 **“对齐度”** 和 **“文本”** 两个关键类别上均排名第一，印证了其精准的提示词跟随和强大的文本渲染能力。
4.  **TIIF Bench 基准**：
    *   **指令跟随能力突出**：在测试复杂指令理解与跟随的TIIF Benchmark中，Qwen-Image 总体排名**第二**，仅次于GPT Image 1，展现了顶尖的指令遵循能力。

#### **（三）文本渲染能力表现**
1.  **英文文本渲染**：
    *   在 **CVTG-2K** 基准上，Qwen-Image 的表现与最先进的图像生成模型**相当**，证明了其强大的英文文本渲染能力。
2.  **中文文本渲染**：
    *   在新提出的 **ChineseWord** 基准上，Qwen-Image 在**三个难度层级（总计8105个汉字）上均取得了最高的渲染准确率**，凸显了其在中文文本生成方面的**显著优势和独特性**。
3.  **长文本渲染**：
    *   在 **LongText-Bench** 上，Qwen-Image 在**中文长文本渲染准确率上排名第一**，在**英文长文本渲染准确率上排名第二**，综合展示了其在处理长段落文本任务上的强大实力。

### **总结**
综合以上评测，Qwen-Image 在文生图任务的**通用生成能力**和**专项文本渲染能力**上均展现出**全球领先水平**。其核心优势体现在：
1.  **综合提示跟随能力顶尖**（DPG总体第一）。
2.  **中文文本渲染能力独树一帜**（自建基准全面领先）。
3.  **长文本生成优势明显**（中英文长文本渲染均名列前茅）。
这些结果系统性地证明了 Qwen-Image 不仅在通用图像生成上达到一流，更在文本渲染，特别是**中文文本生成这一关键挑战上，树立了新的标杆**。

## 7.3 - Performance of Image Editing

### 内容概况
本节系统性地评估了经过**多任务训练**的Qwen-Image模型在**文本引导的图像编辑**以及**相关3D视觉任务**上的综合能力。评估分为两大方向，并在一系列公开基准测试中与前沿模型进行了对比。

1.  **通用图像编辑**：在 **GEdit-Bench** 和 **ImgEdit-Bench** 上评估模型的开放式指令编辑能力。
2.  **3D视觉任务**：在 **新视角合成** 和 **深度估计** 任务上评估模型的空间理解和生成能力。

### 核心要点总结

#### **（一）评估框架：两大核心任务方向**
1.  **通用图像编辑**：评估模型根据文本和视觉指令进行开放式编辑的能力，核心是**指令跟随与视觉保真度**。
2.  **3D视觉任务**：评估模型从单张图像**推断并生成连贯空间信息**的能力，测试其深层的**几何与空间理解**。

#### **（二）通用图像编辑：全面领先**
1.  **GEdit-Bench 表现（表11）**：
    *   **中英文双料冠军**：在涵盖11类真实用户指令的GEdit-Bench上，Qwen-Image在**英文全集（GEdit-Bench-EN）和中文全集（GEdit-Bench-CN）的排行榜上均位列第一**。
    *   **指标解读**：在**语义一致性（G_SC）、感知质量（G_PQ）和总分（G_O）** 三个由GPT-4.1评分的0-10分指标上全面领先，证明了其编辑结果既准确又高质量。
    *   **多语言泛化**：结果显著优于因中文能力有限而在中文榜上表现欠佳的FLUX.1等模型，凸显了其强大的多语言指令泛化能力。

2.  **ImgEdit-Bench 表现（表12）**：
    *   **综合能力第一**：在覆盖9类常见编辑任务、734个真实案例的ImgEdit-Bench上，Qwen-Image以 **4.27** 的“总体”平均分排名最高。
    *   **任务均衡优势**：在“添加”、“调整”、“替换”、“背景”等多项子任务上得分突出，特别是在 **“提取”任务上取得3.43分，显著领先于其他模型**，展现了强大的细节分离与重建能力。
    *   **顶尖竞争**：其表现与GPT Image 1 [High]等顶级闭源模型不相上下，处于同一性能梯队。

#### **（三）3D视觉任务：极具竞争力**
1.  **新视角合成表现（表13）**：
    *   **媲美专业模型**：在GSO数据集上，Qwen-Image（作为通用生成模型）的新视角合成质量（PSNR: 15.11, SSIM: 0.884, LPIPS: 0.153）**与Zero123、ImageDream等专门的新视角合成模型处于同一水平，甚至更优**。
    *   **远超同类通用模型**：其表现大幅领先于GPT Image 1、BAGEL等其他通用图像生成模型，证明了其多任务训练在空间理解上的有效性。

2.  **深度估计表现（表14）**：
    *   **多任务模型中的佼佼者**：在KITTI、NYUv2等五个零样本数据集上，Qwen-Image的深度估计精度（如AbsRel指标）**与最新的多任务生成模型（如DepthFM、Marigold）表现相当**。
    *   **与专业模型的差距**：虽然仍不及Depth Anything v2、Metric3D v2等**专精于深度估计的SOTA模型**，但这符合预期，因为后者是针对性优化的专家模型。

### **总结**
Qwen-Image的**多任务训练版本**在图像编辑及相关视觉任务上展现出**全面而强大的性能**：
1.  **作为编辑工具**：它在遵循复杂指令、保持视觉质量、处理多语言用户请求方面**达到业界顶尖水平**，综合编辑能力领先。
2.  **作为通用基础模型**：其能力成功扩展至**新视角合成、深度估计**等需要空间推理的任务，且表现极具竞争力，验证了其统一框架的**通用性和强大潜力**。
这些结果共同表明，Qwen-Image不仅是一个优秀的文生图模型，更是一个**功能全面、能力均衡的通用视觉生成与编辑基础平台**。

## 7.4 - Qualitative Results on VAE Reconstruction

1.  **评估目的与形式**：
    *   本节旨在通过**定性（视觉）对比**，而非量化指标，来直观呈现不同VAE模型在重建**文本密集型图像**时的性能差异。
    *   **Figure 17** 展示了多个模型对同一张**包含英文文本的PDF图像**的重建结果对比。

2.  **核心发现（基于文中描述）**：
    *   **Qwen-Image-VAE优势明显**：在重建结果中，短语 **“double-aspect”** 保持**清晰可辨**。
    *   **其他模型存在不足**：在其他对比模型的重建结果中，同一短语变得**难以识别或模糊不清**。

3.  **总体结论**：
    *   该定性对比**直接印证了**之前定量评估的结论：**Qwen-Image-VAE 在重建包含小文本的图像时，能够提供更精确、细节保留更好的结果**。
    *   这从视觉上证明了其解码器经过**针对性微调的有效性**，也解释了为何Qwen-Image模型整体上具备卓越的文本渲染能力。

**总结**：此部分通过一个具体的视觉案例，生动地证明了Qwen-Image-VAE在**文本细节重建精度上的显著优势**，为其在完整图像生成流程中保障高质量的文本输出提供了底层支撑。

## 7.5 - Qualitative Results on Image Generation

### 内容概况
本部分通过丰富的视觉案例，对Qwen-Image的文生图能力进行了**定性（视觉）评估与对比**。评估从**四个关键维度**展开，并将Qwen-Image与包括GPT Image 1、Seedream 3.0等在内的多个领先的开源及闭源模型进行了直观比较，旨在全面展示其生成质量的优势。

### 核心要点总结（四个维度的对比）

#### **1. 英文文本渲染**
*   **长段落渲染（图18）**：Qwen-Image对长英文段落的渲染在**视觉风格上更真实，质量更高**。
*   **字符准确性（图19）**：
    *   **上部分**：在七个不同位置渲染文本的任务中，Qwen-Image全部正确，而其他模型存在**字符缺失（如GPT Image 1）、扭曲（如Seedream 3.0）** 等问题。
    *   **下部分**：Qwen-Image不仅能成功渲染各文本段，还能生成**布局合理、视觉美观的幻灯片**。对比模型则出现**文本缺失或字符错误**。

#### **2. 中文文本渲染**
*   **传统文本生成（图20）**：在生成中文对联的场景中，Qwen-Image能**准确再现文本内容和风格**，并正确描绘房间布局。其他模型则出现**字符缺失或扭曲**，甚至无法生成正确的对联。
*   **复杂布局与场景文本（图21）**：
    *   **上部分（动漫场景）**：Qwen-Image能正确生成多个角色和店铺招牌，**完美遵循空间布局和文本渲染的提示**。Seedream 3.0在复杂布局上有困难，其他模型则无法理解复杂的文本和空间指令。
    *   **下部分（手写体）**：Qwen-Image能在复杂场景中生成**逼真且排版美观的手写文本段落**，而其他模型难以生成结构化的段落文本。

#### **3. 多对象生成**
*   **对象与风格（图22上）**：在要求生成多个指定动物并应用毛绒风格的提示下，Qwen-Image能**准确生成所有动物、保持其指定位置，并一致地应用正确风格**。GPT Image 1无法生成毛绒风格，Recraft V3和Seedream 3.0则生成了错误的动物。
*   **多语言文本与布局（图22下）**：在台球上渲染混合语言文本并排成两行的任务中，Qwen-Image**严格遵循指令**。GPT Image 1布局不佳且写错汉字，其他模型则无法正确生成大部分汉字。

#### **4. 空间关系生成**
*   **人物互动（图23第一部分）**：在描绘攀登者互动的场景中，Qwen-Image能**准确捕捉场景并表现两人间指定的互动**。而GPT Image 1、Seedream 3.0和Recraft V3则**未能完全遵循提示**，生成了错误的互动关系。
*   **精细空间关系（图23剩余部分）**：在描绘角色与鸽子、怀表与杯把之间空间关系的任务中，**只有Qwen-Image和GPT Image 1能够准确描绘**这些精细的空间关系。

### **总结**
定性评估从**文本渲染的准确性与美观度、多对象生成的忠实度与一致性、空间关系理解的精确性**三个层面，直观且有力地证明了Qwen-Image的卓越能力：
1.  **文本渲染**：尤其在**中文及复杂布局文本**上优势显著，字符准确，排版合理。
2.  **多对象生成**：能**严格遵循提示**，准确生成指定数量、属性和风格的对象。
3.  **空间关系**：在理解并可视化**对象间的复杂空间与互动关系**上表现领先。
这些案例共同表明，Qwen-Image在遵循复杂、详细的用户指令方面，**综合能力处于最前沿**。

## 7.6 - Qualitative Results on Image Editing

### 内容概况
本节通过丰富的视觉案例，对Qwen-Image的**文本引导图像编辑（TI2I）能力**进行了全面的定性评估。评估系统性地涵盖了 **五大关键编辑场景**，并将Qwen-Image与业界领先的指令编辑模型（SeedEdit 3.0， FLUX.1 Kontext [Pro]， GPT Image 1 [High]）进行了直观对比，旨在多维度展示其编辑能力的优势与特点。

### 核心要点总结（五大评估维度）

#### **1. 文本与材质编辑**
*   **复杂文本编辑**：在更改特定字母（如“H”改为“Q”）并保持原有艺术风格的场景中，**Qwen-Image和FLUX.1表现出色**，成功修改文本且维持风格；Seedream 3.0修改失败，GPT Image 1则丢失了原风格。
*   **材质生成与文本添加**：在要求添加特定文本及相关材质（如珐琅彩色玻璃艺术）的任务中，**Qwen-Image是唯一能准确生成指定材质的模型**，展现了**卓越的材质渲染和指令遵循能力**。

#### **2. 对象添加/删除/替换**
*   **对象添加（风格匹配）**：在卡通场景中添加猫和狗时，**Qwen-Image和SeedEdit 3.0**能确保新对象与画面的艺术风格协调一致；而FLUX.1在处理非写实图像时出现一致性困难。
*   **对象删除**：在从拥挤场景中移除所有人的复杂任务上，**所有模型均能准确完成**，仅在细节处理上略有差异。
*   **整体一致性**：除了GPT Image 1在编辑后常出现整体图像不一致的问题外，其他模型在保留未编辑区域方面都表现良好。

#### **3. 姿势操纵**
*   **细节保持**：在改变人物姿势时，**只有FLUX.1和Qwen-Image能够保留发丝等精细细节**。
*   **服装与场景推理**：在要求保持服装和背景稳定的姿势编辑中，**Qwen-Image表现最佳**：它能准确推理出人物穿着“开衩裙+丝绸裤”的搭配，并在新姿势中正确呈现裤子，同时完美保持背景不变。
*   **姿态与装饰一致性**：在另一案例中，Qwen-Image再次胜出，更好地保留了原始姿势和服装装饰的一致性。

#### **4. 链式编辑**
*   **复杂任务链**：评估模型在**以生成图作为上下文进行迭代编辑**的能力。
*   **案例一（从画作中提取细节）**：从中国传统画中提取鸟类并展示织物细节。**GPT Image 1和Qwen-Image能准确提取对象，且Qwen-Image在纹理细节保持上更优**；SeedEdit 3.0和FLUX.1则从第一步提示就失败了。
*   **案例二（保留结构特征的多步编辑）**：在保留船只“双开尾”结构的同时进行多步编辑。**Qwen-Image和FLUX.1能全程保留此结构，但只有Qwen-Image成功完成了添加两艘货船的完整指令链**。

#### **5. 新视角合成**
*   **空间推理能力**：评估模型从单图生成新视角的**空间与语义连贯性**。
*   **性能对比**：SeedEdit 3.0和FLUX.1在同一指令下**无法很好地完成视角旋转**。GPT Image 1在主体明确时能生成新视角，但**难以泛化到具有复杂多物体的真实场景**。
*   **Qwen-Image优势**：**只有Qwen-Image能保持全局一致性**，包括文本保真度和光照结构，在复杂编辑任务中展现了**优越的空间和语义连贯性**。

### **总结**
定性评估表明，Qwen-Image在图像编辑的**五大核心挑战上均展现出顶尖或领先的综合能力**：
1.  **精准的局部控制**：在文本、材质编辑和对象操作中，能**严格遵循指令**并保持风格与结构。
2.  **强大的空间与语义理解**：在姿势操纵和新视角合成中，表现出**出色的细节推理与全局一致性保持能力**。
3.  **可靠的迭代编辑**：在链式编辑中，能**维护上下文并完成复杂任务链**。
这些结果共同证实，经过多任务训练的Qwen-Image不仅是一个强大的图像生成器，更是一个**功能全面、控制精细、理解深入的专业级图像编辑平台**，其综合能力已达到与全球最先进闭源模型媲美甚至在某些维度超越的水平。

## 8 - Conclusion

### **内容概况**
1.  **技术成就回顾**：重申了模型在**复杂文本渲染**和**精确图像编辑**两大核心任务上取得的重大进展及其实现方法。
2.  **范式意义探讨**：从 **“图像生成”**、**“图像”生成** 和 **“视觉生成”** 三个递进的视角，深入论述了该工作对多模态基础模型未来发展方向的启示与推动作用。

### **核心要点总结**

#### **（一）技术成就回顾**
Qwen-Image 通过系统性工程实现了性能突破：
1.  **能力突破**：在**复杂文本渲染**（特别是中文）和**精准图像编辑**的一致性上取得重大进展。
2.  **实现路径**：
    *   **数据与学习策略**：通过构建**全面的数据管道**和采用**渐进式课程学习**策略，大幅提升了图像内的文本渲染能力。
    *   **训练范式与机制**：通过改进的**多任务训练范式**和**双编码机制**，显著增强了图像编辑的**语义连贯性**与**视觉保真度**。
3.  **验证结果**：在众多公开基准测试中均展示了**最先进的性能**，证明了其技术鲁棒性和广泛的现实应用潜力。

#### **（二）范式意义与未来启示**
结论从三个层面升华了Qwen-Image工作的价值：

1.  **作为“图像生成”模型：重新定义生成模型的优先级**
    *   **核心理念**：不再仅仅追求“照片真实感”或“AI美学”，而是**强调文本与图像之间的精确对齐**，尤其是挑战性的文本渲染任务。
    *   **未来展望**：强大的文本渲染能力将使未来的人机接口从**纯语言的LUI**演进为**视觉-语言的VLUI**。当大语言模型难以描述颜色、空间关系等视觉属性时，VLUI可以生成**图文并茂、富含文本的插画**，实现**结构化的视觉解释**和有效的**知识外化**。

2.  **作为“图像”生成模型：展示跨模态的强泛化能力**
    *   **超越2D**：在**3D新视角合成**任务中，作为通用模型，其在多个挑战性场景下的表现**优于专用3D模型**，展现了卓越的视角一致性。
    *   **视频生成基础**：在姿态编辑任务中，能在剧烈运动变化下保持主体身份与背景结构的**高度连贯性**，这是视频生成的关键。采用的**视频VAE**也为其向动态视觉模态的扩展奠定了基础。

3.  **作为“视觉生成”模型：推动理解与生成的融合**
    *   **三支柱愿景**：实现真正的理解-生成统一依赖于三个基础支柱：**精通理解、精通生成、精通二者的协同整合**。
    *   **填补关键空白**：Qwen-Image作为该系列首个专注视觉生成的工作，**填补了第二支柱（生成能力）的关键空白**，与擅长视觉理解的Qwen2.5-VL（第一支柱）形成互补，共同为下一代多模态AI奠定了平衡基础。
    *   **范式转变**：它**不仅是一个高性能生成模型，更代表了一种范式转变**。它挑战学界重新思考生成模型在感知、界面设计和认知建模中的角色，指向一个**生成模型真正理解图像、理解模型通过内在生成过程实现理解**的未来。

### **总结**
Qwen-Image的结论表明，其贡献**超越了技术指标的领先**。它通过**在文本渲染上确立新标准**、**在架构上为多模态预留空间**、**在理念上倡导理解与生成的融合**，为构建下一代**视觉-语言全能系统**指明了方向，标志着大规模基础模型演进的一个重要里程碑。

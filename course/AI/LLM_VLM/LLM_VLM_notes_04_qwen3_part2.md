本文主要整理《Qwen3Technical Report》的主要内容。

## 5.0 - Post-training

### **内容概括**

该流程为实现两个核心目标而设计：**1）为用户提供灵活的“思维控制”能力；2）高效生产不同规模的模型**。

为此，流程分化为**两条路径**：
1.  **旗舰模型路径**：对最大的基础模型（如235B、32B）进行**精细的四阶段训练**，旨在系统性地开发并融合“思考”与“非思考”两种模式。
2.  **轻量级模型路径**：对较小规模的基础模型，采用 **“强到弱”知识蒸馏** 技术，直接利用旗舰模型的知识进行高效训练，极大降低了开发成本与时间。

### **要点总结**

| 维度 | 核心要点 |
| :--- | :--- |
| **1. 两大设计目标** | • **思维控制**：在单一模型中集成 **“非思考”** （快速响应）与 **“思考”** （深度推理）两种模式，并允许用户通过设定 **“思考预算”** 来控制推理深度。<br>• **强到弱蒸馏**：利用大模型（教师）的知识，高效训练小模型（学生），以**大幅降低轻量级模型的训练成本与开发周期**。 |
| **2. 旗舰模型训练路径（四阶段）** | 如图1左侧所示，是一个分步递进的过程：<br>• **阶段1**：**长链思维冷启动**。使用长链思维数据进行微调，初步激发模型的推理能力。<br>• **阶段2**：**推理强化学习**。在数学、代码等专项任务上进行强化学习，强化“思考”能力。<br>• **阶段3**：**思维模式融合**。将强大的“非思考”功能整合到已具备“思考”能力的模型中，实现两种模式的统一。<br>• **阶段4**：**通用强化学习**。在广泛的通用任务上进行强化学习，进一步提升模型的整体性能和指令遵循能力。<br>**产出**：`Qwen3-235B-A22B` 与 `Qwen3-32B` 等旗舰模型。 |
| **3. 轻量级模型生产路径（蒸馏）** | 如图1右侧所示，是一个高效的直达路径：<br>• **方法**：直接从训练好的旗舰（教师）模型向各种规模的基础（学生）模型**蒸馏输出对数**。<br>• **优势**：<br>  &nbsp;&nbsp;&nbsp;- **性能更优**：实验表明，该方法能带来比单独进行四阶段训练**更高的性能**（更高的Pass@1分数）。<br>  &nbsp;&nbsp;&nbsp;- **探索性更强**：模型在采样更多输出时表现更好（更高的Pass@64分数）。<br>  &nbsp;&nbsp;&nbsp;- **效率极高**：所需GPU时间仅为完整四阶段训练方法的 **1/10**。<br>**产出**：`Qwen3-30B-A3B` 及 `14B/8B/4B/1.7B/0.6B` 等全系列轻量级模型。 |
| **4. 流程核心价值** | 该设计实现了**质量与效率的平衡**：<br>• 通过资源密集的四阶段训练，打造出具备顶级“思维控制”能力的**旗舰标杆**。<br>• 通过高效的蒸馏技术，快速、低成本地生产出性能优异的**全系列模型**，满足了不同场景下的算力与性能需求。 |

## 5.1 - Long-CoT Cold Start

### **内容概况**

本部分详细介绍了 Qwen3 后训练流程的**第一阶段：“长链式思维冷启动”**。该阶段的目标并非直接追求模型的最佳推理性能，而是通过精心构建的数据集和最小化的训练，为模型**植入基础、正确的推理模式**，为后续的强化学习阶段预留充分的优化与提升空间。

其核心是一个**严谨的两阶段（查询过滤 + 响应过滤）数据构建与筛选流程**，旨在确保用于冷启动训练的数据集由**真正需要深度、逐步推理才能解决的复杂问题**及其**高质量、逻辑连贯的解答**构成。

---

### **要点总结**

| 环节 | 核心目标 | 关键操作与标准 |
| :--- | :--- | :--- |
| **1. 数据集构建基础** | 创建一个**覆盖广泛（数学、代码、逻辑推理、通用STEM）** 且**答案可验证**的综合数据集，作为冷启动的“原料”。 | - 收集涵盖多类别的问题。<br>- 为每个问题配备**经过验证的参考答案**或**基于代码的测试用例**。 |
| **2. 查询过滤阶段** | 筛选出**真正需要链式思维（CoT）** 的复杂问题，排除简单或不可验证的问题，确保数据质量与领域平衡。 | 1. **移除不可验证的查询**：如包含多个子问题或要求通用文本生成的问题。<br>2. **排除无需CoT的问题**：使用大模型（Qwen2.5-72B-Instruct）判断，仅保留它**不借助CoT就无法正确回答**的复杂问题。<br>3. **标注与平衡领域**：为每个查询标注领域，以确保数据集的领域分布均衡。 |
| **3. 响应过滤阶段** | 为筛选后的问题，生成并筛选出**高质量、逻辑正确的链式思维响应**，作为模型学习的范例。 | 1. **生成候选响应**：为每个查询，使用QwQ-32B生成N个候选解答。<br>2. **人工介入验证**：当模型持续生成错误解答时，由人工评估准确性。<br>3. **六重严格过滤**：对通过初筛的响应，进一步剔除以下情况：<br>   &nbsp;&nbsp;• 最终答案错误。<br>   &nbsp;&nbsp;• 存在大量重复。<br>   &nbsp;&nbsp;• 明显猜测、缺乏充分推理。<br>   &nbsp;&nbsp;• 思考过程与总结内容不一致。<br>   &nbsp;&nbsp;• 语言混杂或风格不当。<br>   &nbsp;&nbsp;• 可能与预留的验证集项目过于相似。 |
| **4. 训练策略** | **最小化训练**，仅为模型植入基础推理模式，避免过度优化，为后续阶段留出提升空间。 | - 仅使用**精心筛选出的数据子集**进行训练。<br>- **严格控制训练样本数量和训练步数**。<br>- **核心目的**：打下基础，但不限制模型在后续强化学习阶段的潜力与灵活性。 |

## 5.2 - Reasoning RL

### **内容概括**

“推理强化学习”是Qwen3后训练流程的第二阶段，其核心目标是在“长链式思维冷启动”奠定的基础上，通过**强化学习（RL）** 机制，系统性地**专项提升模型在复杂推理任务（如数学、编程）上的能力**。

该阶段的关键在于使用一套**高质量、高难度、广覆盖的精选问题集**，并采用**GRPO算法**进行训练。通过优化训练策略（如大批次、多回滚）并精细控制模型的探索行为，实现了**稳定、高效且自动化**的性能提升，使模型无需人工干预即可在核心推理基准上取得显著进步。

---

### **要点总结**

| 环节 | 核心目标与做法 | 关键细节与效果 |
| :--- | :--- | :--- |
| **1. 数据构建：四大精选标准** | 构建一个专用于强化学习的高质量数据集（共 **3,995个** 查询-验证对）。 | 数据必须满足：<br>• **未在冷启动阶段使用**，确保泛化。<br>• **对冷启动模型具有可学习性**，难度适中。<br>• **尽可能具有挑战性**，以激发潜力。<br>• **覆盖广泛的子领域**，确保能力均衡。 |
| **2. 训练方法：GRPO强化学习** | 采用 **GRPO（Group Relative Policy Optimization）算法**更新模型参数，专注于优化推理策略。 | • **大批次训练 & 高回滚次数**：有助于稳定训练并提升样本效率。<br>• **离线策略训练**：进一步提高数据利用效率。<br>• **熵值控制**：通过稳定或逐步增加模型的熵（不确定性），巧妙地**平衡探索与利用**，避免模型陷入局部最优或训练崩溃。 |
| **3. 训练效果：稳定且显著提升** | 实现全自动化的稳定训练，在单次训练周期内持续提升模型性能。 | • **训练过程稳定**：无需手动调整超参数，奖励和验证性能同步提升。<br>• **性能飞跃例证**：以旗舰模型 **Qwen3-235B-A22B** 为例，经过 **170步** RL训练，其在**AIME‘24（美国数学邀请赛）** 基准上的得分从 **70.1** 大幅提升至 **85.1**。 |

## 5.3 - Thinking Mode Fusion

### **内容概括**

“思维模式融合”阶段是 Qwen3 后训练流程的第三阶段，其核心目标是**将“非思考”模式的能力无缝集成到已经具备强大“思考”能力的模型中**。通过这种融合，开发者得以在**单一模型内**，通过简单的指令标志（`/think` 或 `/no_think`）动态控制模型是否进行深度链式推理，从而无需为不同任务部署和维护多个独立模型，显著降低了**部署成本与系统复杂性**。

为实现这一目标，技术团队对第二阶段完成推理强化的模型进行了**持续的监督微调**，并设计了统一的**聊天模板**。此外，一个关键的涌现能力——**“思考预算”控制**——在此阶段得以实现，允许用户根据计算资源或时间限制，灵活地控制模型的推理深度。

---

### **要点总结**

| 维度 | 核心要点 |
| :--- | :--- |
| **1. 核心目标与价值** | • **统一模型**：在一个模型中融合“思考”与“非思考”两种行为模式。<br>• **灵活控制**：允许开发者通过简单指令动态切换模型推理模式。<br>• **降低成本**：避免了为不同模式部署独立模型带来的成本和运维复杂性。 |
| **2. 实现方法：监督微调 (SFT)** | • **数据构成**：SFT数据集由 **“思考”数据** 和 **“非思考”数据** 混合构成。<br>• **“思考”数据生成**：为确保不损失第二阶段模型的性能，使用**第二阶段模型本身**对第一阶段的查询进行**拒绝采样**来生成高质量思考数据。<br>• **“非思考”数据构建**：精心构建以覆盖**代码、数学、指令遵循、多语言、创意写作、问答、角色扮演**等广泛任务。为提升低资源语言性能，**特别增加了翻译任务的比例**。 |
| **3. 聊天模板设计** | • **指令标志**：在用户查询或系统消息中插入 **`/think`** 或 **`/no_think`** 标志来明确指定期望的模式。<br>• **响应格式**：<br>  - **思考模式**：助手响应包含 `{thinking_content}`（思考过程）和 `{response}`（最终答案）。<br>  - **非思考模式**：助手响应保留一个空的思考块，直接生成 `{response}`。<br>• **默认与多轮**：默认情况下模型处于思考模式；对于复杂多轮对话，会在用户查询中随机插入多个标志，模型以**遇到的最后一个标志为准**响应。<br>• **框架支持**：该功能已集成在 **Hugging Face 分词器的聊天模板**中，可通过参数（如 `enable_thinking=False`）便捷控制。 |
| **4. 关键衍生能力：思考预算** | • **能力涌现**：一旦模型学会了两种模式，它就**自然获得了基于不完整思考生成回答的能力**，这为实现“思考预算”控制奠定了基础。<br>• **预算控制机制**：当模型的思考内容长度达到用户定义的阈值时，可以**手动停止**思考过程，并插入特定指令，强制模型基于已完成的推理生成最终答案。<br>• **价值**：这使得用户可以根据实际需求（如延迟、计算成本）动态调配模型的推理资源，在速度与深度之间取得平衡。 |

## 5.4 - General RL

### **内容概括**

“通用强化学习”阶段是Qwen3后训练流程的第四阶段，其核心目标是**全面提升模型在多样化、开放式任务下的综合能力、稳定性与用户体验**。为实现这一目标，技术团队构建了一个**覆盖超过20个不同任务的复杂奖励系统**，并为每个任务定制了评分标准。这些任务主要针对五个核心能力的提升，并辅以三种不同类型的奖励机制提供训练信号，确保模型在广泛场景下都能可靠、有用且符合格式要求。

---

### **要点总结**

| 维度 | 核心要点 | 具体说明 |
| :--- | :--- | :--- |
| **1. 阶段目标** | **全面提升模型在通用场景下的综合表现**，包括指令遵循、用户偏好对齐、工具使用能力及在特定场景（如RAG）下的可靠性。 | 通过多任务、多奖励源的强化学习，对模型进行广泛而精细的优化。 |
| **2. 五大核心能力** | 奖励系统专门设计用于提升以下五个方面的能力： | |
| | **a. 指令遵循** | 确保模型准确理解并遵循用户在**内容、格式、长度、结构化输出**等方面的要求。 |
| | **b. 格式遵循** | 确保模型能正确响应 **`/think`** 和 **`/no_think`** 标志以切换模式，并在最终输出中一致地使用指定标记（如 **``** 和 **``**）来分隔思考过程和最终答案。 |
| | **c. 偏好对齐** | 针对开放式查询，提升模型的**有帮助性、互动性和风格**，以提供更自然、令人满意的用户体验。 |
| | **d. 智能体能力** | 训练模型通过指定接口正确调用工具。在RL训练过程中，模型可以进行完整的多轮交互循环并接收真实环境执行反馈，从而提升其在**长视野决策任务**中的性能和稳定性。 |
| | **e. 专业场景能力** | 在更专业的场景（如**检索增强生成**）中设计特定任务，通过奖励信号引导模型生成准确、符合上下文的回答，以**最小化幻觉风险**。 |
| **3. 三种奖励机制** | 为上述任务提供反馈，采用以下三类奖励： | |
| | **a. 基于规则的奖励** | 广泛用于推理RL阶段，同样适用于指令遵循、格式遵循等通用任务。设计良好的基于规则的奖励可以高精度评估模型输出的正确性，防止“奖励黑客”问题。 |
| | **b. 带参考答案的基于模型的奖励** | 为每个查询提供参考答案，并提示 **Qwen2.5-72B-Instruct** 模型基于此参考为模型响应打分。此方法处理任务更灵活，避免严格格式导致的误判。 |
| | **c. 不带参考答案的基于模型的奖励** | 利用人类偏好数据训练奖励模型，为模型响应分配标量分数。此方法不依赖参考答案，能处理更广泛的查询，有效提升模型的互动性和有帮助性。 |

## 5.5 - Strong-to-Weak Distillation

### **内容概括**

“通用强化学习”阶段是Qwen3后训练流程的第四阶段，其核心目标是**全面提升模型在多样化、开放式任务下的综合能力、稳定性与用户体验**。为实现这一目标，技术团队构建了一个**覆盖超过20个不同任务的复杂奖励系统**，并为每个任务定制了评分标准。这些任务主要针对五个核心能力的提升，并辅以三种不同类型的奖励机制提供训练信号，确保模型在广泛场景下都能可靠、有用且符合格式要求。

---

### **要点总结**

| 维度 | 核心要点 | 具体说明 |
| :--- | :--- | :--- |
| **1. 阶段目标** | **全面提升模型在通用场景下的综合表现**，包括指令遵循、用户偏好对齐、工具使用能力及在特定场景（如RAG）下的可靠性。 | 通过多任务、多奖励源的强化学习，对模型进行广泛而精细的优化。 |
| **2. 五大核心能力** | 奖励系统专门设计用于提升以下五个方面的能力： | |
| | **a. 指令遵循** | 确保模型准确理解并遵循用户在**内容、格式、长度、结构化输出**等方面的要求。 |
| | **b. 格式遵循** | 确保模型能正确响应 **`/think`** 和 **`/no_think`** 标志以切换模式，并在最终输出中一致地使用指定标记（如 **``** 和 **``**）来分隔思考过程和最终答案。 |
| | **c. 偏好对齐** | 针对开放式查询，提升模型的**有帮助性、互动性和风格**，以提供更自然、令人满意的用户体验。 |
| | **d. 智能体能力** | 训练模型通过指定接口正确调用工具。在RL训练过程中，模型可以进行完整的多轮交互循环并接收真实环境执行反馈，从而提升其在**长视野决策任务**中的性能和稳定性。 |
| | **e. 专业场景能力** | 在更专业的场景（如**检索增强生成**）中设计特定任务，通过奖励信号引导模型生成准确、符合上下文的回答，以**最小化幻觉风险**。 |
| **3. 三种奖励机制** | 为上述任务提供反馈，采用以下三类奖励： | |
| | **a. 基于规则的奖励** | 广泛用于推理RL阶段，同样适用于指令遵循、格式遵循等通用任务。设计良好的基于规则的奖励可以高精度评估模型输出的正确性，防止“奖励黑客”问题。 |
| | **b. 带参考答案的基于模型的奖励** | 为每个查询提供参考答案，并提示 **Qwen2.5-72B-Instruct** 模型基于此参考为模型响应打分。此方法处理任务更灵活，避免严格格式导致的误判。 |
| | **c. 不带参考答案的基于模型的奖励** | 利用人类偏好数据训练奖励模型，为模型响应分配标量分数。此方法不依赖参考答案，能处理更广泛的查询，有效提升模型的互动性和有帮助性。 |

## 6.0 - Post-training Evaluation

### **内容概况**

评估旨在衡量模型在 **“思考”与“非思考”** 两种模式下的综合性能，其体系覆盖了 **五大核心能力维度**，并针对不同任务设定了精细化的评估策略与超参数。

评估不仅在传统基准上进行，还引入了对齐、多语言、智能体等前沿评估任务，并针对性地为思考和创造两种输出模式配置了不同的解码超参数，以确保评估的科学性与全面性。

---

### **要点总结**

#### **五大核心评估维度**

| 评估维度 | 评估目标 | 关键基准与设置 |
| :--- | :--- | :--- |
| **1. 通用任务** | 评估模型的基础知识、理解与泛化能力。 | • **MMLU-Redux**：进阶版知识测试。<br>• **GPQA-Diamond**：高难度科学问答，**每个问题采样10次取平均准确率**。<br>• **C-Eval**：中文知识评估。<br>• **LiveBench**：动态更新的综合基准。 |
| **2. 对齐任务** | 评估模型与人类偏好、指令的契合度与创造性。 | • **指令遵循**：使用 **IFEval** 的严格提示准确率。<br>• **人类偏好**：使用 **Arena-Hard**、**AlignBench**。<br>• **写作能力**：使用 **Creative Writing V3**、**WritingBench** 评估文笔与创意。 |
| **3. 数学与文本推理** | 评估高阶逻辑与解题能力。 | • **数学**：**MATH-500**、**AIME‘24/‘25**（每年共30题，**每题采样64次取平均**）。<br>• **文本推理**：**ZebraLogic**（斑马谜题）、**AutoLogi**（自动逻辑推理）。 |
| **4. 智能体与编码** | 评估实际任务解决与编程能力。 | • **智能体**：**BFCL v3**，Qwen3模型使用FC格式，并将上下文扩展至64K进行评估。<br>• **编码**：**LiveCodeBench (v5)**，思考模式会调整提示模板以允许自由思考。<br>• **竞技编程**：**Codeforces Ratings (CodeElo)**，每个问题最多生成8次独立求解尝试。 |
| **5. 多语言任务** | 评估跨语言的理解与生成能力。 | • **指令遵循**：**Multi-IF** (8种语言)。<br>• **知识**：**INCLUDE** (44种语言，区域知识)、**MMMLU** (14种语言，通用知识)，为提升效率**仅采样原数据集的10%**。<br>• **数学**：**MT-AIME2024** (55种语言)、**PolyMath** (18种语言)。<br>• **逻辑推理**：**MLogiQA** (10种语言)。 |

#### **关键评估设置与策略**

1.  **模式区分评估**：所有基准测试均在 **“思考”** 和 **“非思考”** 两种模式下分别进行，以全面衡量模型的灵活性与可控性。
2.  **精细化超参数配置**：
    *   **思考模式**：`temperature=0.6`, `top_p=0.95`, `top_k=20`。旨在获得稳定、聚焦的推理输出。
    *   **创造/非思考模式**：`temperature=0.7`, `top_p=0.8`, `top_k=20`, `presence_penalty=1.5`。旨在鼓励更多样、更有创意的文本生成。
3.  **生成长度设置**：为两种模式均设置 **最大输出长度为32,768个词元**，仅为 **AIME‘24/‘25** 数学竞赛评估额外扩展至 **38,912个词元**，以确保模型有充足的“思考”空间解决复杂问题。
4.  **高效评估策略**：对于部分大规模多语言基准（如INCLUDE、MMMLU），采用**仅采样10%数据**的策略，在保证评估代表性的同时大幅提升效率。

## 6.1 - Summary of Evaluation Results


### **内容概况**

总结从三个层面肯定了模型的成功：**旗舰 MoE 模型达到了开源模型的顶尖水平并可与顶级闭源模型竞争**；**旗舰密集模型在推理能力上实现了代际飞跃**；**全系列轻量级模型凭借高效的蒸馏方法，在同等参数量下全面领先开源竞品**。这证明了 Qwen3 在模型架构、训练策略和知识迁移上的综合优势。

---

### **要点总结**

| 模型类别与型号 | 核心结论 | 关键证据与对比 |
| :--- | :--- | :--- |
| **1. 旗舰 MoE 模型**<br>**Qwen3-235B-A22B** | 在“思考”与“非思考”两种模式下，均展现了**开源模型中的最先进（SOTA）综合性能**，并且**与顶级闭源模型实力相当**。 | • **超越强劲开源基线**：性能优于 **DeepSeek-R1** 和 **DeepSeek-V3**。<br>• **比肩顶级闭源模型**：与 **OpenAI-o1、Gemini-2.5-Pro、GPT-4o** 等相比也极具竞争力，展现了深厚的推理能力和全面的通用能力。 |
| **2. 旗舰密集模型**<br>**Qwen3-32B** | **推理能力显著超越前代最强模型**，同时**在非思考模式下的表现也极为出色**。 | • **推理能力突破**：在大多数基准测试上超越了之前最强的推理模型 **QwQ-32B**，其推理能力与闭源模型 **OpenAI-o3-mini** 相当。<br>• **非思考模式领先**：在非思考模式下的性能超过了前代旗舰非推理密集模型 **Qwen2.5-72B-Instruct**。 |
| **3. 全系列轻量级模型**<br>**(Qwen3-30B-A3B, 14B 及更小模型)** | 在参数量相近或更少的情况下，**持续且显著地优于其他开源模型**，验证了“强到弱知识蒸馏”方法的巨大成功。 | • **参数效率卓越**：包括 MoE 模型 **Qwen3-30B-A3B** 和密集模型 **Qwen3-14B** 等在内的轻量级模型，其性能均优于参数量相当或更大的开源竞品。<br>• **方法论验证**：这一结果强有力地证明了从大模型（教师）向小模型（学生）进行知识蒸馏的 **“强到弱蒸馏”** 方法是高效且成功的。 |

## 6.2 - Qwen3-235B-A22B

### **内容概括**

该内容展示了 **Qwen3-235B-A22B 旗舰模型（指令微调后）** 在 **“思考”** 与 **“非思考”** 两种模式下，分别与当前最顶尖的 **推理型** 和 **非推理型** 模型进行全面性能对比的结果。

评估涉及涵盖**通用知识、对齐、数学推理、智能体、编码及多语言**在内的 **23个** 权威基准测试。结果表明：
1.  **思考模式**：该模型凭借其强大的推理能力，在开源模型中达到顶尖（SOTA）水平，并与顶级闭源推理模型高度竞争。
2.  **非思考模式**：该模型凭借其固有能力，同样超越了所有主要的开源竞品，并优于一个主要的闭源非推理模型。

---

### **要点总结**

#### **Qwen3-235B-A22B (思考模式) 表现**

1.  **卓越的推理性能**：在 **23个** 评估基准中，Qwen3-235B-A22B (思考) 在 **17个** 上超越了另一个顶级开源推理模型 **DeepSeek-R1**，尤其在**数学（AIME'24/25）、智能体（BFCL v3）和编码（LiveCodeBench, CodeForces）** 等需要深度推理的任务上表现突出。

2.  **具备与顶级闭源模型竞争的实力**：尽管在总参数量和激活参数量上少于部分对手，但Qwen3-235B-A22B (思考) 的性能与 **OpenAI-o1、Grok-3-Beta、Gemini-2.5-Pro** 等闭源推理模型相比**极具竞争力**，显著**缩小了开源与闭源模型在推理能力上的差距**。

#### **Qwen3-235B-A22B (非思考模式) 表现**

1.  **全面领先开源领域**：在 **23个** 评估基准中，Qwen3-235B-A22B (非思考) 在 **18个** 上超越了包括 **DeepSeek-V3、LLaMA-4-Maverick** 及前代旗舰 **Qwen2.5-72B-Instruct** 在内的所有主要开源非推理模型。

2.  **超越主流闭源模型**：在没有经过刻意“思考”增强的情况下，其固有能力已经**超过了闭源模型 GPT-4o-2024-11-20** 在相同基准中的表现。

#### **核心优势与结论**

| 对比维度 | Qwen3-235B-A22B (思考) | Qwen3-235B-A22B (非思考) | 综合结论 |
| :--- | :--- | :--- | :--- |
| **对比模型类型** | 顶尖推理模型 (如DeepSeek-R1, OpenAI-o1) | 顶尖通用/非推理模型 (如DeepSeek-V3, GPT-4o) | 成功挑战两种类型的最强模型。 |
| **参数效率** | 仅激活 **22B** 参数（约为DeepSeek-R1的60%），总参 **235B**（约为35%）。 | 使用相同的模型参数。 | 以**更低的激活和总参数量**实现了顶尖性能，体现了卓越的架构与训练效率。 |
| **性能地位** | **开源推理模型的SOTA**，并与闭源顶尖模型比肩。 | **开源非推理模型的领先者**，并超越一个主要闭源对手。 | 证明了其在**统一架构下，既能深度推理，又具备强大通用能力**的全面性与先进性。 |
| **行业意义** | 显著提升了开源模型的**推理能力上限**。 | 展现了开源模型在**通用能力上对闭源模型的追赶与超越潜力**。 | 标志着开源大模型在**性能与可控性**上达到了新的高度。 |

## 6.3 - Qwen3-32B

### **内容概括**

该部分评估了旗舰密集模型 **Qwen3-32B** 在指令微调后，于 **“思考”** 与 **“非思考”** 两种模式下的综合表现。评估选取了各自领域内的顶尖模型作为基线：
*   **思考模式**：对比了开源推理模型 **DeepSeek-R1-Distill-Llama-70B**、前代最强推理模型 **QwQ-32B** 及闭源模型 **OpenAI-o3-mini (medium)**。
*   **非思考模式**：对比了闭源模型 **GPT-4o-mini**、开源模型 **LLaMA-4-Scout** 及前代旗舰通用模型 **Qwen2.5-72B-Instruct**。

评估涵盖23个基准测试。结果表明，**Qwen3-32B 在两种模式下均展现出卓越性能，确立了其在32B参数规模下的新标杆地位**。

---

### **要点总结**

#### **核心结论**

1.  **思考模式**：**成为32B规模新的最先进推理模型**。在23个基准中的17个上超越前代最强的QwQ-32B，并能与闭源的OpenAI-o3-mini竞争，在对齐和多语言任务上表现更佳。
2.  **非思考模式**：**全面超越所有基线**。在几乎所有基准上优于对比模型，尤其在对齐、多语言和推理相关任务上对比前代72B模型有显著优势。
3.  **代际飞跃**：评估结果证明了Qwen3系列相较于Qwen2.5系列的**根本性改进**。

#### **分模式表现详述**

| 对比模式 | 核心对比与结论 | 关键数据与例证 (源自表格) |
| :--- | :--- | :--- |
| **思考模式**<br>(Qwen3-32B) | **1. 超越前代最强**：在 **17/23** 个基准上优于 **QwQ-32B**，成为新SOTA。<br>**2. 挑战闭源模型**：与 **OpenAI-o3-mini** 总体表现相当，且在**对齐**（IFEval: 93.8 vs 89.0）和**多语言**（Multi-IF: 73.0 vs 48.4）任务上优势明显。<br>**3. 参数效率高**：以**32B**参数，在多数任务上超越了**70B**参数的 **DeepSeek-R1** 蒸馏模型。 | • **通用知识**：MMLU-Redux (90.9) 和 LiveBench (74.9) 得分领先。<br>• **数学推理**：AIME‘24 (81.4) 和 AIME‘25 (72.9) 分数优异。<br>• **智能体与编码**：BFCL v3 (70.3) 表现突出。<br>• **多语言**：MT-AIME2024 (75.0) 和 MLogiQA (76.3) 得分很高。 |
| **非思考模式**<br>(Qwen3-32B) | **1. 全面领先**：在**几乎所有**基准上优于 **GPT-4o-mini、LLaMA-4-Scout 和 Qwen2.5-72B-Instruct**。<br>**2. 以小博大**：以**32B**参数，在通用任务上与**72B**参数的 **Qwen2.5-72B-Instruct** 表现相当，并在对齐、多语言、推理任务上实现反超。<br>**3. 代际优势**：显著优于参数更多的前代旗舰，证明了根本性改进。 | • **对齐能力**：Arena-Hard (8.58) 和 Creative Writing v3 (78.3) 得分大幅领先。<br>• **数学与推理**：在AIME‘24 (31.0)、ZebraLogic (29.2) 等任务上远超其他非推理基线。<br>• **编码**：CodeForces 评分 (1353) 和通过率 (71.0%) 显著高于基线。<br>• **多语言**：Multi-IF (70.7) 和 MT-AIME2024 (24.1) 表现最佳。 |

## 6.4 - Qwen3-30B-A3B & Qwen3-14B

### **内容概括**

本部分评估了Qwen3系列中**两个中型模型（一个MoE，一个密集型）在“思考”与“非思考”两种模式下的表现**。通过与各自领域内的顶尖基线模型对比，结果显示：
1.  **思考模式**：Qwen3-30B-A3B (MoE) 和 Qwen3-14B 在推理能力上，与顶尖的32B推理模型 **QwQ-32B** 高度竞争，且前者以**显著更少的激活参数和总参数**实现了可比性能。
2.  **非思考模式**：两者在绝大多数基准测试上超越了所有非推理基线，包括前代的 **Qwen2.5-32B-Instruct**，实现了**更高参数效率、更低成本的卓越性能**。

---

### **要点总结**

#### **核心结论**

1.  **思考模式表现优异**：Qwen3-30B-A3B 和 Qwen3-14B 在思考模式下，在大多数任务上与最强的32B推理模型 **QwQ-32B** 表现相当，尤其在**数学推理、逻辑推理和多语言任务**上竞争力突出。
2.  **非思考模式全面领先**：在非思考模式下，两个模型在绝大多数基准上**全面超越**了包括 **Phi-4、Gemma-3-27B-IT 和 Qwen2.5-32B-Instruct** 在内的非推理基线。
3.  **参数效率与成本优势显著**：
    *   **Qwen3-30B-A3B (思考)**：以**仅3B激活参数、30B总参数**，达到与**32B激活与总参数**的QwQ-32B相当的推理性能，激活参数量约为对方的 **1/10**。
    *   **对比前代**：两个模型以**更少的总参数和激活参数**，性能全面超越了前代32B旗舰指令模型 **Qwen2.5-32B-Instruct**。

#### **分模式表现与关键数据**

| 评估模式 | 对比基线 | 核心优势与关键例证 (基于表格数据) |
| :--- | :--- | :--- |
| **思考模式**<br>(Qwen3-30B-A3B / 14B) | **QwQ-32B**<br>(32B Dense) | • **整体相当**：在通用、对齐、数学、编码、多语言等五大类任务上，得分与QwQ-32B互有高低，整体竞争力强。<br>• **部分领先**：在**逻辑推理**（ZebraLogic: 89.5/88.5 vs 76.8）、**智能体**（BFCL v3: 69.1/70.4 vs 66.4）、**多语言指令遵循**（Multi-IF: 72.2/74.8 vs 68.3）等任务上表现更优。<br>• **效率突出**：Qwen3-30B-A3B 仅激活 **3B** 参数，约为QwQ-32B的 **1/10**。 |
| **非思考模式**<br>(Qwen3-30B-A3B / 14B) | **Phi-4, Gemma-3-27B-IT, Qwen2.5-32B-Instruct** | • **全面超越**：在23个基准中的绝大多数上，两个Qwen3模型的得分均**高于或显著高于**三个基线模型。<br>• **代际飞跃**：以**14B/30B总参数、14B/3B激活参数**，在**数学**（AIME‘24: 31.7/32.8 vs 18.8）、**逻辑推理**（AutoLogi: 82.0/81.5 vs 65.5）、**对齐**（AlignBench: 8.52/8.55 vs 7.71）等任务上大幅超越**32B总参数与激活参数**的 **Qwen2.5-32B-Instruct**。 |

#### **方法验证与总体评价**

*   **强到弱蒸馏的有效性**：Qwen3-14B和Qwen3-30B-A3B出色的思考能力，尤其是后者以极低激活成本达到顶级推理水平的表现，**强有力地验证了从大模型向小模型进行“强到弱知识蒸馏”方法的巨大成功**。
*   **模型定位**：**Qwen3-30B-A3B** 成为**高性价比MoE模型的典范**，以极低的推理成本提供顶尖性能；**Qwen3-14B** 则是**同参数规模密集型模型中的佼佼者**，性能全面超越前代及同期竞品。

## 6.5 - Qwen3-8B / 4B / 1.7B / 0.6B

### **内容概括**

本部分全面评估了Qwen3系列中面向边缘或轻量级部署的四个小规模模型（**Qwen3-8B, 4B, 1.7B, 0.6B**）在“思考”与“非思考”两种模式下的性能。

*   **Qwen3-8B/4B**：在思考模式下与更大参数（14B/32B）的蒸馏推理模型对比；在非思考模式下与同规模及前代更大参数（7B/14B）的通用指令模型对比。
*   **Qwen3-1.7B/0.6B**：在思考模式下与其他小参数（1.5B/8B）推理模型对比；在非思考模式下与同规模及前代（1.5B/3B）指令模型对比。

评估数据显示，这些轻量级模型**在绝大多数任务和两种模式下均表现出色，全面超越了作为基线的、参数相当或更多的模型**。这一结果强有力地证明了 **“强到弱知识蒸馏”** 方法的巨大成功，能够以极低的成本和努力构建高性能的轻量级Qwen3模型。

---

### **要点总结**

#### **核心结论**
1.  **全面性能领先**：Qwen3-8B/4B/1.7B/0.6B 在“思考”与“非思考”两种模式下，于**绝大多数评估基准上均超越了对比的基线模型**，包括参数更多的前代模型。
2.  **方法验证**：这一优异表现**再次证明了“强到弱蒸馏”方法的极高效率与有效性**，使得小模型能够以极低成本继承大模型的强大能力。
3.  **边缘部署价值**：这些模型为资源受限的边缘侧和设备端部署提供了**性能强大且可控**的先进选择。

#### **分模型关键表现**

| 模型 | 对比模式与基线 | 核心优势与例证 |
| :--- | :--- | :--- |
| **Qwen3-8B**<br>(8B Dense) | **思考模式** vs <br>DeepSeek-R1-Distill-Qwen-14B/32B | 以**更少参数（8B）**，在**数学**（AIME‘24: 67.3 vs 49.6）、**逻辑推理**（ZebraLogic: 84.8 vs 69.6）、**智能体**（BFCL v3: 68.1 vs 53.5）、**多语言数学**（MT-AIME2024: 65.4 vs 44.6）等核心推理任务上**大幅超越14B/32B基线**。 |
| | **非思考模式** vs <br>LLaMA-3.1-8B-Instruct, Gemma-3-12B-IT, Qwen2.5-7B/14B-Instruct | 在几乎所有任务上领先，尤其在**数学推理**（AIME‘24: 29.1 vs 22.4）、**逻辑推理**（AutoLogi: 76.5 vs 58.9）、**对齐**（AlignBench: 8.38 vs 7.77）、**编码**（CodeForces评分: 1110 vs 904）上优势明显。 |
| **Qwen3-4B**<br>(4B Dense) | **思考模式** vs <br>DeepSeek-R1-Distill-Qwen-14B/32B | 仅以**4B参数**，在**逻辑推理**（ZebraLogic: 81.0 vs 69.6）、**智能体**（BFCL v3: 65.9 vs 53.5）、**多语言**（MT-AIME2024: 60.7 vs 44.6）等任务上**显著超越32B基线**。 |
| | **非思考模式** vs <br>LLaMA-3.1-8B-Instruct, Gemma-3-12B-IT, Qwen2.5-7B/14B-Instruct | 性能全面占优，以**4B参数**在**数学**（AIME‘24: 25.0 vs 22.4）、**逻辑**（AutoLogi: 76.3 vs 58.9）、**对齐**（AlignBench: 8.10 vs 7.77）等任务上**超越12B/14B基线**。 |
| **Qwen3-1.7B**<br>(1.7B Dense) | **思考模式** vs <br>DeepSeek-R1-Distill-Qwen-1.5B/Llama-8B | 在几乎所有任务上**大幅领先1.5B基线**，并与**8B基线高度竞争**。在**指令遵循**（IFEval: 72.5 vs 59.0）、**逻辑推理**（AutoLogi: 83.2 vs 63.4）、**多语言**（MLogiQA: 56.0 vs 32.8）等任务上表现尤为出色。 |
| | **非思考模式** vs <br>Gemma-3-1B-IT, Phi-4-mini, Qwen2.5-1.5B/3B-Instruct | 在绝大多数基准上**全面领先**，以**1.7B参数**在**数学**（AIME‘24: 13.4 vs 8.1）、**逻辑**（AutoLogi: 59.8 vs 28.8）、**对齐**（AlignBench: 7.20 vs 6.49）等任务上**超越参数更多（3.8B/3.1B）的基线**。 |
| **Qwen3-0.6B**<br>(0.6B Dense) | **思考模式** vs <br>DeepSeek-R1-Distill-Qwen-1.5B | 以**不足一半的参数（0.6B）**，在**指令遵循**（IFEval: 59.2 vs 39.9）、**逻辑推理**（AutoLogi: 61.6 vs 19.1）、**智能体**（BFCL v3: 46.4 vs 14.0）等任务上**显著超越1.5B基线**。 |
| | **非思考模式** vs <br>Gemma-3-1B-IT, Phi-4-mini, Qwen2.5-1.5B/3B-Instruct | 表现极具竞争力，在**逻辑推理**（AutoLogi: 37.4 vs 28.8）、**指令遵循**（IFEval: 54.5 vs 54.5持平最优）等任务上达到或接近最优水平，展现了卓越的参数效率。 |

## 7.0 - The Effectiveness of Thinking Budget

### **内容概括**

该图表（图2）旨在验证 **Qwen3模型是否能够通过增加“思考预算”来有效提升其智能水平**。研究在数学、编程和STEM三个领域的四个高难度基准（AIME‘24、AIME‘25、LiveCodeBench v5、GPQA Diamond）上进行实验，通过调整分配给模型的“思考”token数量（1K至32K）来观察性能变化。

**核心发现是：Qwen3模型的性能与分配的思考预算呈正相关，展现出了可扩展且平滑的提升曲线。** 在所有基准测试中，“思考模式”的性能均显著且稳定地优于“非思考模式”，证明此机制能有效解锁模型的深层推理能力。

---

### **要点总结**

| 关键要点 | 详细说明与图表数据佐证 |
| :--- | :--- |
| **1. 思考模式显著优于非思考模式** | 在所有四个基准测试中，**蓝色实线（思考模式）** 的全程表现均**大幅高于红色虚线（非思考模式）**，直观证明了在复杂任务中进行深度推理的有效性和必要性。 |
| **2. 性能与思考预算呈正相关** | 随着思考预算从 **1K token** 逐步增加到 **32K token**，思考模式下的性能（Pass@1）在所有任务上均呈现出**平滑且持续的增长趋势**。这表明模型能够有效利用更多的“思考”篇幅来进行更深入、更复杂的推理。 |
| **3. 不同任务领域的提升表现** | • **数学竞赛（AIME）**：提升最为显著。以AIME‘24为例，思考预算从1K增至32K，正确率从约 **42%** 提升至约 **88%**，增幅超过一倍。<br>• **编程（LiveCodeBench）**：同样有明显提升，从约 **43%** 提升至约 **71%**。<br>• **高难度科学问答（GPQA Diamond）**：在已较高的基础上，从约 **66%** 进一步提升至约 **73%**，证明了其在专业领域的稳定增益。 |
| **4. 未来的扩展潜力** | 研究发现指出，**若将输出长度（思考预算）进一步延长至超过32K token，模型的性能有望得到进一步提升**。这表明当前观察到的提升曲线可能尚未达到平台期，更大的思考预算可能解锁更强的性能。 |
| **5. 核心机制验证** | 该实验成功验证了Qwen3设计的 **“思考预算”** 这一核心机制的有效性。它不仅是模型的一个功能开关，更是一个**可平滑调节的“性能旋钮”**，允许用户根据任务复杂度与计算成本，在推理深度（性能）与推理速度/成本之间进行灵活权衡。 |

## 7.1 - The Effectiveness and Efficiency of On-Policy Distillation

### **内容概括**

本研究旨在评估 **“在策略蒸馏”** 作为一种后训练方法的有效性与效率。通过对比从同一检查点（已完成“离策略蒸馏”的Qwen3-8B模型）出发的**“在策略蒸馏”** 与**“直接强化学习”** 两种方法，在**数学与代码**相关任务上的性能与计算成本。

**核心结论是**：“在策略蒸馏”不仅在所有评估基准上取得了**显著优于**强化学习的性能，而且所需的计算成本（GPU小时）仅为强化学习的**约十分之一**。同时，蒸馏还能有效扩展学生模型的探索潜力，提升其在多次采样下的最佳表现，而强化学习则不具备此效果。

---

### **要点总结**

| 评估维度 | 核心发现与结论 |
| :--- | :--- |
| **1. 性能表现** | **“在策略蒸馏”全面领先。**<br>在评估的六个基准（AIME‘24/‘25、MATH500、LiveCodeBench v5、MMLU-Redux、GPQA-Diamond）上，经过“在策略蒸馏”的模型（最后一行）在所有任务上的 **Pass@1分数均为最高**，显著超越了仅用强化学习（中间行）的模型。 |
| **2. 计算效率** | **“在策略蒸馏”成本极低。**<br>“在策略蒸馏”仅消耗了 **1,800个GPU小时**，而“直接强化学习”则消耗了 **17,920个GPU小时**，前者成本约为后者的 **1/10**。这证明了蒸馏是一种**高效率**的性能提升方法。 |
| **3. 模型潜力挖掘** | **蒸馏能扩展探索潜力，强化学习则不能。**<br>• **“在策略蒸馏”** 在提升单次回答准确率（Pass@1）的同时，也大幅提升了**多次采样下的最佳表现**（Pass@64）。例如在AIME‘24上，Pass@64从90.0提升至93.3。<br>• **“直接强化学习”** 虽然提升了Pass@1，但**未能改善Pass@64分数**（AIME‘24/‘25上均为不变）。这表明强化学习可能使模型输出趋于保守，而蒸馏从教师模型处学到了更丰富的解决路径，拓宽了“探索空间”，增强了推理潜力。 |
| **4. 方法对比启示** | 实验结果突出表明，在指导学生模型学习时，**利用一个更强的教师模型进行知识蒸馏**，相比于让模型在奖励信号下独自进行强化学习探索，是一条**更有效、更高效、且能保留并发展模型解决复杂问题潜力的优越路径**。 |

**关键数据例证**（来自表格）：
*   **AIME‘24任务**：蒸馏后模型得分为 **74.4**，而强化学习后为 **67.6**，GPU耗时仅为后者的约十分之一。
*   **LiveCodeBench v5任务**：蒸馏后模型得分为 **60.3**，显著高于强化学习的 **52.9**。

## 7.2 - The Effects of Thinking Mode Fusion and General RL

### **内容概括**

本研究旨在评估 Qwen3 后训练流程中 **“思维模式融合”（第3阶段）** 与 **“通用强化学习”（第4阶段）** 的有效性。研究对 Qwen3-32B 模型在完成“推理RL”（第2阶段）后的各阶段性能进行了系统评估，不仅使用了公开基准，还引入了多个内部基准以监控其他关键能力。

**核心结论是**：后两个阶段显著提升了模型的**通用能力、指令遵循和智能体功能**，并成功实现了可靠的思维模式切换。然而，对于**高度专业化的数学、代码等任务**，引入更广泛的通用任务训练会带来轻微的**性能权衡**（小幅下降），这是为了增强模型整体通用性而做出的主动选择。

---

### **要点总结**

#### **表：各训练阶段对Qwen3-32B模型能力的影响**

| 能力维度 | 评估基准 (示例) | 阶段3 (思维模式融合) 效果 | 阶段4 (通用RL) 效果 | 整体趋势与解读 |
| :--- | :--- | :--- | :--- | :--- |
| **1. 思维模式切换** | **ThinkFollow*** (内部) | 从无到有，得分 **88.7**，具备初步但非完美的模式切换能力。 | 大幅提升至 **98.9**，实现了**高度可靠的模式切换**。 | **成功**：两个阶段的核心目标之一达成，模型能准确响应用户的 `/think` 和 `/no_think` 指令。 |
| **2. 通用与指令遵循** | **LiveBench, Arena-Hard, CounterFactQA*, IFEval, Multi-IF** | 在“思考”模式下，通用对话能力、对齐能力、反事实问答、指令遵循均有显著提升（如CounterFactQA +10.9）。同时，“非思考”模式能力初步建立。 | 在**两种模式**下，上述能力得到**进一步全面增强**。 | **显著增强**：模型变得更有用、更安全、更遵循指令，综合用户体验提升。 |
| **3. 智能体与工具调用** | **BFCL v3, ToolUse*** (内部) | “思考”模式下智能体能力稳定；“非思考”模式下工具调用准确率提升（ToolUse +7.1）。 | **两种模式**下的智能体稳定性与工具调用准确性**大幅提升**（ToolUse +15.1 / +13.3）。 | **显著增强**：模型使用工具、执行多步骤任务的可靠性和准确性显著提高。 |
| **4. 知识、STEM、数学、编程** | **MMLU-Redux, GPQA, AIME‘24, LiveCodeBench** | 在“思考”模式下，此类专业化任务性能**基本持平或轻微下降**。“非思考”模式能力初步建立但较弱。 | 在“思考”模式下，性能**基本保持或继续轻微下降**（如AIME‘24从83.8降至81.4）。“非思考”模式能力略有提升。 | **性能权衡**：为了获得更强的通用性和模式切换能力，模型在高度专业化任务上的顶尖性能有**轻微牺牲**。这是开发者为追求模型**整体通用性**而接受的**设计权衡**。 |

---
**核心结论**：
1.  **模式融合成功**：阶段3成功将“非思考”模式集成到模型中，阶段4使其高度可靠。
2.  **通用能力飞跃**：阶段3和4大幅提升了模型的通用对话、指令遵循、对齐安全和智能体能力。
3.  **存在专业性能权衡**：在追求更通用、更可控的模型过程中，其解决最复杂数学和编程问题的极限能力会受到轻微影响。Qwen3团队**主动选择了接受这一权衡**，以优先打造一个能力均衡、实用性更强的通用模型。

## 7.3 - Conclusion

### **内容概括**

Qwen3作为系列最新版本，其核心创新在于集成了“思考”与“非思考”两种模式，并支持用户通过动态管理“思考预算”来平衡任务复杂度与计算资源。模型在36万亿词元、覆盖119种语言/方言的超大规模数据上预训练，并在代码生成、数学推理、智能体等一系列标准基准测试中展现出强大性能。展望未来，研究将聚焦于提升预训练数据质量与多样性、优化模型架构以支持更长上下文及高效压缩，并增加基于环境反馈的智能体强化学习资源。

---

### **要点总结**

| 维度 | 核心要点 |
| :--- | :--- |
| **1. 模型核心创新** | **1. 双模式集成**：Qwen3首次在同一框架内集成了 **“思考模式”**（用于复杂任务深度推理）和 **“非思考模式”**（用于快速响应），用户无需切换模型。<br>**2. 动态资源管理**：用户可通过 **“思考预算”** 机制，动态控制模型用于推理的计算量（Token数），实现性能与延迟/成本的灵活权衡。 |
| **2. 关键技术成就** | **1. 大规模预训练**：基于 **36万亿Token** 的高质量、多样化数据进行预训练，为模型能力奠定坚实基础。<br>**2. 卓越的多语言能力**：支持 **119种语言和方言**，具备强大的跨语言理解和生成能力。<br>**3. 全面的性能领先**：在**代码生成、数学解题、复杂推理、智能体任务**等多个领域的权威基准测试中，均取得了领先或极具竞争力的成绩。 |
| **3. 未来研究方向** | **1. 扩展预训练**：持续提升预训练数据的**质量与多样性**。<br>**2. 优化模型架构**：改进架构与训练方法，以实现**更高效的模型压缩**和**对极长上下文的有效支持**。<br>**3. 加强智能体学习**：增加计算资源投入，重点发展能够**从环境反馈中学习**的智能体强化学习系统，以构建能应对复杂、需长时间推理任务的智能体。 |

## 8.0 - Long-Context Ability

### **内容概括**

本部分（附录A.1.1）通过 **RULER 基准测试** 评估了 Qwen3 系列模型在不同上下文长度（从 4K 到 128K tokens）下的信息检索与处理能力，并与前代 Qwen2.5 模型进行了对比。评估区分了 **“非思考模式”** 与 **“思考模式”**。结果显示：
1.  **在非思考模式下**，Qwen3 模型的长上下文处理能力全面优于同规模的前代 Qwen2.5 模型。
2.  **在思考模式下**，模型的性能在所有测试长度上均出现轻微下降。研究者分析认为，对于无需复杂推理的纯检索任务，额外的“思考”内容可能不仅无益，反而会干扰信息定位与提取过程。

---

### **要点总结**

| 评估维度 | 核心发现与结论 |
| :--- | :--- |
| **1.评估设置** | • **评估基准**：RULER (Hsieh et al., 2024)，专为测试模型的长上下文信息提取与理解能力而设计。<br>• **长度外推技术**：采用了 **YARN** (Peng et al., 2023) 技术，并设置 `scaling_factor=4` 以支持更长的上下文。<br>• **思考模式设置**：为避免模型在极长输入上产生过于冗长的思考，将“思考预算”固定为 **8192个tokens**。 |
| **2.非思考模式表现** | • **全面超越前代**：在非思考模式下，**所有参数量级的 Qwen3 模型均优于同规模的 Qwen2.5-Instruct 模型**。<br>• **性能示例**：以 128K 长度为例，Qwen3-14B 得分为 **85.1**，显著高于 Qwen2.5-14B-Instruct 的 **78.1**；旗舰模型 Qwen3-235B-A22B 得分 **90.6**，也优于 Qwen2.5-72B-Instruct 的 **88.4**。 |
| **3.思考模式表现** | • **性能轻微下降**：在思考模式下，所有 Qwen3 模型在 RULER 基准上的表现**均差于其对应的非思考模式**。<br>• **原因分析**：研究团队假设，对于 RULER 这类**主要依赖信息检索而非深度推理的任务**，模型生成的“思考”内容可能**无法提供显著收益，甚至会干扰检索过程**，从而导致性能下降。 |
| **4.未来方向** | 团队已认识到思考模式在长上下文检索任务上的局限性，并承诺**将在未来版本中致力于提升思考模式下的长上下文处理能力**。 |

## 8.1 - Multilingual Ability

### **内容概括**

本部分（附录A.1.2）系统评估了Qwen3系列模型的**多语言自然语言理解能力**。评估分为两个层面：
1.  **重点语言深度评估**：在包含西班牙语、法语、阿拉伯语、日语等在内的**12种具体语言**上进行了详尽的基准测试。
2.  **广泛语言广度评估**：使用 **Belebele** 基准，在覆盖**80种语言**（按语系组织）的大范围上，对比了Qwen3与同规模**Gemma**模型及前代**Qwen2.5**模型的性能。

**核心结论是**：Qwen3在所有这些评估中均展现了**强大且全面的多语言能力**，与同规模的先进模型表现相当，并**显著超越了自己的前代模型**。

---

### **要点总结**

| 评估维度 | 核心要点 |
| :--- | :--- |
| **1. 评估范围与数据** | • **深度评估**：在**12种**具体语言（西、法、葡、意、阿、日、韩、印尼、俄、越、德、泰）上进行了多项基准测试。<br>• **广度评估**：使用 **Belebele** 基准，在**80种**支持的语言上进行测试，并排除了42种未优化的语言以聚焦核心能力。 |
| **2. 核心评估结果** | • **全面竞争力**：在12种重点语言的详细基准测试中，Qwen3系列模型**在所有被评估的基准上都取得了有竞争力的性能**，展现了强大的多语言能力。<br>• **横向对比相当**：在广泛的80种语言测试（Belebele）中，Qwen3达到了与**同参数规模的Gemma模型相当**的性能水平。<br>• **代际提升显著**：与**前代Qwen2.5模型**相比，Qwen3在Belebele基准上的表现实现了**显著的超越**。 |
| **3. 结论与意义** | 评估结果证实，Qwen3不仅在常用的主流语言上表现优异，其多语言支持能力在**广度（80种语言）和深度（具体任务性能）** 上均实现了代际飞跃，使其成为一个真正具备强大跨语言理解和生成能力的模型系列。 |

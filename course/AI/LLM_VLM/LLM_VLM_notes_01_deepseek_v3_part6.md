本文主要整理《DeepSeek-V3 Technical Report》的主要内容。

## 12.0 - Pre-Training Evaluation Benchmarks

### **内容概括**

这两张图片系统地阐述了DeepSeek-V3基础模型的**全面评估体系**。评估的核心是检验这个在多语言语料（以中英文为主）上预训练的模型，在**英语、中文及多语言**场景下的综合能力。

评估在自研的**HAI-LLM框架**内置评估系统中进行。图片将所有基准测试按任务类型进行了详细分类，并明确标注了中文（绿色单下划线）和多语言（绿色双下划线）数据集。评估覆盖了从**学科知识、语言理解、推理、问答、阅读理解、代码到数学**的广泛领域，并针对不同类型的任务采用了**基于困惑度的评估**和**基于生成的评估**两种方法，以确保评估的准确性和公平性。

### **要点总结**

| 方面 | 核心内容 |
| :--- | :--- |
| **1. 评估背景** | 模型在**多语言语料（中英文为主）** 上预训练，因此评估涵盖**英文、中文及多语言**能力。 |
| **2. 评估框架** | 基于自研的 **HAI-LLM框架** 中的内部评估系统。 |
| **3. 评估方法** | • **基于困惑度的评估**：用于**多项选择题**和部分理解类任务。<br>• **基于生成的评估**：用于**开放生成类**任务（如问答、数学、代码）。<br>• **语言建模评估**：对Pile-test使用**每字节比特数（BPB）** 指标，避免分词器差异影响，保证公平。 |
| **4. 基准范围** | **极其广泛**，共涉及8大类、超过25个知名公开基准，全面检验模型的知识、理解、推理与生成能力。 |
| **5. 语言侧重** | 除通用英文基准外，特别标注并纳入了关键的**中文基准**（如C-Eval, CMMLU, CMath等）和**多语言基准**，凸显对中文及跨语言能力的重视。 |

### **涉及的数据集介绍**

以下是按类别梳理的数据集，其中`中文`和`多语言`已按图片标注注明。

| 类别 | 数据集名称 | 简介与特点 | 评估方法 |
| :--- | :--- | :--- | :--- |
| **1. 多学科选择题** | **MMLU** (及 Redux, Pro 变体) | 涵盖57个科目的英文知识测试，衡量模型的世界知识和问题解决能力。 | 困惑度评估 |
| | **MMMU** | 大规模多学科多模态理解基准。 | 困惑度评估 |
| | `中文` **C-Eval** | 涵盖52个学科的中文知识测试，评估中文知识水平。 | 困惑度评估 |
| | `中文` **CMMLU** | 专门针对中文语言、知识、推理的评估基准。 | 困惑度评估 |
| **2. 语言理解与推理** | **HellaSwag** | 常识推理，完成句子或事件。 | 困惑度评估 |
| | **PIQA** | 物理常识推理。 | 困惑度评估 |
| | **ARC** | 科学领域的小学级选择题。 | 困惑度评估 |
| | **BBH** | 挑战性推理任务集合，测试思维链能力。 | 生成式评估 |
| **3. 闭卷问答** | **TriviaQA** | 基于维基百科的常识问答。 | 生成式评估 |
| | **NaturalQuestions** | 真实的谷歌搜索问题及答案。 | 生成式评估 |
| **4. 阅读理解** | **RACE** | 来自中学英语考试的阅读理解。 | 困惑度评估 |
| | **DROP** | 需要离散推理的阅读理解。 | 生成式评估 |
| | `中文` **C3** | 中文机器阅读理解数据集。 | 困惑度评估 |
| | `中文` **CMRC** | 中文阅读理解数据集。 | 生成式评估 |
| **5. 指代消解** | **CLUEWSC** | 中文Winograd模式挑战，测试常识与指代消解。 | 生成式评估 |
| | **WinoGrande** | 大规模的英文Winograd模式挑战。 | 困惑度评估 |
| **6. 中文理解与文化** | `中文` **CCPM** | 中文古典诗词阅读理解数据集。 | 困惑度评估 |
| **7. 数学** | **GSM8K** | 英文小学水平数学应用题。 | 生成式评估 |
| | **MATH** | 英文竞赛水平的数学题。 | 生成式评估 |
| | **MGSM** | 多语言小学数学应用题。 | 生成式评估 |
| | `中文` **CMath** | 中文数学应用题数据集。 | 生成式评估 |
| **8. 代码** | **HumanEval** | 评估Python代码生成能力的经典基准。 | 生成式评估 |
| | **MBPP** | 入门级Python编程问题。 | 生成式评估 |
| | **LiveCodeBench** | 持续更新的代码生成评估基准，反映最新趋势。 | 生成式评估 |
| | **CRUXEval** | 评估代码执行与推理能力的基准。 | 生成式评估 |
| **9. 标准化考试** | `多语言` **AGIEval** | 涵盖中英文的标准化考试（如高考、SAT、LSAT等）题目。 | 生成式评估 |
| **10. 语言建模** | **The Pile** | 大规模、多样化的文本语料库，用于评估模型的通用语言建模能力。 | 语言建模（BPB） |

**总结**：这份详尽的评估清单表明，DeepSeek-V3的评估不是为了通过“刷榜”，而是为了进行一场**严格、全面、公平的“体检”**。它尤其重视检验模型在**中文场景和复杂推理任务（数学、代码）** 上的真实能力，这与其技术报告中所展示的卓越性能是相互印证的。

## 12.0 - 基于困惑度的评估、基于生成的评估、语言建模评估分别如何评测?

### 1. 基于困惑度的评估
这种方法主要用于 **“多项选择题”或“完形填空”类任务**。

*   **核心原理**：不要求模型生成答案，而是让模型**计算每个候选选项（A/B/C/D）在给定上下文下的困惑度**。困惑度越低，代表模型认为该选项在上下文中出现的可能性越大，越“自然”或“正确”。
*   **操作方式**：对于一道选择题，模型会分别计算将每个选项填入空白后的整个序列的困惑度，然后选择困惑度最低的那个选项作为预测答案。
*   **为什么用在这里**：这种方法**高效、客观**，无需生成文本，避免了生成模型可能出现的格式错误、冗余等问题，直接评估模型对知识的“判断力”。
*   **图片中的应用**：正如第二张图所述，此方法用于评估 `HellaSwag`, `PIQA`, `WinoGrande`, `RACE`, `MMLU`系列, `ARC`, `C-Eval`, `CMMLU`, `C3`, `CCPM` 等数据集。这些基本都是给定上下文和若干选项的选择题。

### 2. 基于生成的评估
这种方法主要用于 **需要自由生成答案的开放性问题**。

*   **核心原理**：让模型像正常对话或完成任务一样，**自主生成文本序列来回答问题**。然后，将模型的生成结果与标准答案（或一组参考答案）进行比较。
*   **操作方式**：
    1.  输入问题或指令。
    2.  模型生成一段文本（代码、数学解、自由文本答案等）。
    3.  使用特定指标进行比对：
        *   **精确匹配**：如代码任务，检查生成代码是否能通过单元测试（`HumanEval`, `MBPP`）。
        *   **字符串匹配/规则提取**：如数学任务，从生成文本中提取最终答案与标准答案比较（`GSM8K`, `MATH`）。
        *   **F1分数/ROUGE**：如阅读理解任务，比较生成文本与参考答案的重合度（`DROP`, `TriviaQA`）。
*   **为什么用在这里**：这类任务（数学、代码、自由形式问答）的答案不固定，必须通过实际生成来检验模型的**推理、创造和问题解决能力**。
*   **图片中的应用**：如第二张图所述，此方法用于 `TriviaQA`, `NaturalQuestions`, `DROP`, `MATH`, `GSM8K`, `HumanEval`, `MBPP`, `LiveCodeBench`, `CRUXEval`, `BBH`, `AGIEval`, `CLUEWSC`, `CMRC`, `CMath` 等数据集。

### 3. 语言建模评估
这是一种 **评估模型最基础能力的通用方法**，用于衡量模型对自然语言分布的建模好坏。

*   **核心原理**：给定一段文本（如`The Pile`的测试集），让模型计算其**困惑度**。困惑度越低，说明模型对该文本越不感到“意外”，即其建模的语言分布与真实世界语言分布越接近。**BPB 越低，说明模型对下一个“字节”是什么越有把握，其“不确定性”或“惊讶度”越低，因此语言建模能力越强**。
*   **操作方式与关键指标（BPB）**：
    *   传统上使用**困惑度**，但它受模型**分词器**影响很大。词表大的模型，困惑度天生可能更低，这不公平。
    *   **因此，DeepSeek-V3采用“每字节比特数”作为指标**。BPB将信息论中的“比特”作为统一度量衡，**绕过了分词器的影响**，直接计算模型压缩每个原始字节（而非每个Token）所需的信息量。**BPB越低，表示模型的语言建模能力越强**。
*   **为什么用在这里**：为了在**不同词表大小、不同分词方式的模型之间进行公平比较**，评估其最核心的“理解与预测自然语言”的基本功。
*   **图片中的应用**：专门用于 `Pile-test` 数据集，并明确使用 **BPB** 指标以保证公平性。

### 总结对比

| 评估方法 | 核心思想 | 典型任务 | 关键指标 | 目的 |
| :--- | :--- | :--- | :--- | :--- |
| **基于困惑度** | **计算选项的可能性** | 多项选择题、完形填空 | 困惑度 (选择最低的) | 评估模型的知识判断与选择能力 |
| **基于生成** | **自主生成答案** | 数学解题、代码编写、开放问答 | 精确匹配、通过率、F1等 | 评估模型的推理、创造与执行能力 |
| **语言建模 (BPB)** | **计算文本的“意外程度”** | 通用文本续写 | **每字节比特数** | 公平地评估最基础的语言建模能力 |

## 12.1 - Evaluation Results

### **内容概括**

**第一部分（图片1）** 介绍了评估的背景、对象与方法。团队在内部统一的评估框架下，将DeepSeek-V3与当前顶尖的开源基座模型（包括其前代模型DeepSeek-V2-Base、中文代表Qwen2.5 72B Base以及目前参数规模最大的LLaMA-3.1 405B Base）进行了公平对比。

**第二部分（图片2，即Table 3）** 以详尽的表格形式，直观展示了上述四个模型在**架构信息**以及**英语、代码、数学、中文、多语言**五大能力维度下，超过25个具体基准测试中的量化得分。这份表格是评估结论的核心数据支撑。

**第三部分（图片3与4的文本）** 对评估结果进行了深入的解读与分析。主要结论是：DeepSeek-V3在绝大多数基准上，尤其是在**数学与代码**任务上表现最佳，全面超越了其他对比模型。此外，本部分还简要提及了支撑其卓越性能的关键原因。

**第四部分（图片4，即Table 4）** 通过消融实验，专门验证了 **“多令牌预测”策略**的有效性，表明该策略在大多数评估基准上都能持续提升模型性能。

### **要点总结**

| 方面 | 核心结论 |
| :--- | :--- |
| **1. 评估设置** | • **框架**：使用自研的**内部评估框架**，确保所有模型在**完全相同的设置**下对比。<br>• **标准**：分数差距**不超过0.3**即视为同一水平。 |
| **2. 总体表现** | **DeepSeek-V3-Base 在绝大多数评估基准上取得了最佳性能**，实质上已成为当前**最强的开源基座模型**。 |
| **3. 分项对比优势** | • **对比 DeepSeek-V2-Base**：**全面大幅领先**。得益于模型架构改进、参数量与训练Token量的扩大以及数据质量提升。<br>• **对比 Qwen2.5 72B Base**：**以约一半的激活参数量（37B vs 72B），在英语、多语言、代码和数学基准上展现出显著优势**。在中文基准上，除CMMLU外，也表现更优。<br>• **对比 LLaMA-3.1 405B Base**：**以约1/11的激活参数量（37B vs 405B），在多语言、代码和数学基准上表现更优**。在英/中文语言理解基准上，表现相当或更好，尤其在BBH、MMLU系列、DROP、C-Eval、CMMLU和CCPM等任务上突出。 |
| **4. 效率证明** | 由于高效的架构和全面的工程优化，训练效率极高：**每万亿Token仅需18万H800 GPU小时**，成本远低于训练72B或405B的稠密模型。 |
| **5. MTP策略有效性** | **消融实验（Table 4）证实**：“多令牌预测”策略在大多数评估基准上能**持续提升模型性能**，验证了其作为核心训练技术的价值。 |

### **核心结论**
DeepSeek-V3基础模型通过一系列**算法与工程的协同创新**，在激活参数量（37B）远小于竞争对手的情况下，实现了**综合性能的全面领先**。这标志着其不仅在**模型能力**上达到了新的高度，更在**训练效率与性价比**上树立了新的标杆。此次评估系统地证明了其作为“最强开源基座模型”的地位。

## 12.2 - Ablation Studies for Multi-Token Prediction

### **内容概况**

研究人员在**两种不同规模**的基线混合专家模型（小规模：总参15.7B，激活2.4B；大规模：总参228.7B，激活20.9B）上，保持其架构和训练数据不变，仅额外增加一个**1层深度的MTP模块**进行训练，形成对比模型。在推理时，MTP模块被丢弃，因此对比模型的推理成本完全相同。

表格详细对比了有/无MTP策略的模型在基础语言建模（Pile-test BPB）以及一系列下游任务（如BBH、MMLU、HumanEval、GSM8K等）上的性能表现。结果显示，**MTP策略在绝大多数评估基准上持续提升了模型性能**。

### **要点总结**

| 方面 | 核心内容 |
| :--- | :--- |
| **1. 实验设计** | **控制变量对比**：在“小规模”与“大规模”两种基线MoE模型上，**仅添加/不添加1层MTP模块**，使用相同数据训练，比较最终性能。 |
| **2. 模型配置** | • **小规模模型**：总参15.7B，激活2.4B，训练1.33T Token。<br>• **大规模模型**：总参228.7B，激活20.9B，训练540B Token。 |
| **3. 核心发现 (性能提升)** | **MTP策略带来了普遍的、一致的性能提升**：<br>• **代码能力**：提升**最为显著**。在**HumanEval**上，小模型提升6.1分（20.7→26.8），大模型提升9.2分（44.5→53.7）。<br>• **数学推理**：提升明显。在**GSM8K**上，小模型提升6.0分（25.4→31.4），大模型提升1.7分（72.3→74.0）；在**MATH**上也有稳定提升。<br>• **综合推理与知识**：在**BBH、DROP、TriviaQA**等任务上，两个规模的模型均获得提升。<br>• **语言建模**：基础**Pile-test (BPB)** 指标保持稳定，略有优化。 |
| **4. 个别波动** | 在**大规模模型的MMLU**上，性能有轻微下降（67.5→66.6），但这属于个别现象，不影响“普遍提升”的整体结论。 |
| **5. 关键结论** | **MTP是一种高效且低成本的训练增强策略**：它能在**不增加模型推理开销**的前提下，通过修改训练目标（预测未来多个Token），有效地提升模型在**代码、数学推理及多项综合任务**上的最终能力。 |

## 12.3 - Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy

### **内容概括**

研究以两种不同规模、且**纯粹依赖辅助损失来鼓励负载均衡的MoE模型作为基线**。它们的门控函数与超参数设置分别与已发布的DeepSeek-V2-Lite和DeepSeek-V2相同。

在此基线之上，研究人员**保持所有训练数据、模型架构和其他超参数完全不变**，仅进行一项关键改动：**移除所有辅助损失，并引入全新的“无辅助损失平衡策略”**。通过严格对比新策略与基线策略在多项评估基准上的性能，得出结论：**在绝大多数任务中，无辅助损失策略能持续取得更好的模型性能**。

### **要点总结**

| 方面 | 核心内容 |
| :--- | :--- |
| **1. 研究目的** | 通过控制变量的消融实验，验证 **“无辅助损失平衡策略”** 是否优于传统的、**纯粹依赖辅助损失**的负载平衡方法。 |
| **2. 实验设计 (控制变量法)** | • **基线模型**：两个不同规模的MoE模型，均使用 **Sigmoid门控 + Top-K亲和力归一化**，并**仅通过调整辅助损失的强度**来促进负载均衡。<br>• **唯一变量**：将基线的辅助损失机制，替换为论文提出的 **“无辅助损失平衡策略”**。其他一切（训练数据、总/激活参数量、架构）完全相同。 |
| **3. 模型规格** | • **小规模模型**：总参15.7B，激活2.4B，训练1.33T Token。<br>• **大规模模型**：总参228.7B，激活20.9B，训练578B Token。 |
| **4. 核心发现 (性能对比)** | 根据表格数据，**无辅助损失策略在绝大多数评估基准上表现更优**：<br>• **显著提升**：在**BBH**（大规模）、**TriviaQA**、**NaturalQuestions**、**HumanEval**（大规模）、**GSM8K**（大规模）、**MATH**（大规模）等任务上，新策略的得分**明显更高**。<br>• **稳定或微升**：在**Pile-test (BPB)**、**DROP**等任务上保持稳定或略有提升。<br>• **个别波动**：仅在**大规模模型的MMLU**上出现轻微下降（68.3→67.2），这属于个别情况，不影响整体结论。 |
| **5. 关键结论与意义** | **“无辅助损失平衡策略”被证明是一种更优的负载均衡解决方案**。它避免了为强制平衡而引入的辅助损失对模型主任务性能的潜在损害，通过在路由机制中直接嵌入动态偏差调整，实现了**更好的性能与效率权衡**。这从实验层面巩固了该策略作为DeepSeek-V3核心创新之一的有效性。 |

## 12.4 - 两种损失平衡策略的区别是什么?

这是一个非常核心的问题。DeepSeek-V3论文中提到的两种负载平衡策略，代表了混合专家模型训练中解决“专家负载不均”这一经典难题的两种不同哲学。

简单来说，它们的核心区别在于：**传统策略通过修改“损失函数”来间接引导，而DeepSeek-V3的策略通过修改“路由计算”来直接调控。**

---

### **1. 传统策略：依赖辅助损失**

这种方法在MoE模型的**损失函数**中添加一个额外的项，专门用来惩罚负载不均衡。

*   **核心机制**：
    1.  **路由计算**：根据输入和专家亲和力，按常规方式（如Softmax或Sigmoid + TopK）计算门控值，选择专家。
    2.  **计算辅助损失**：在反向传播时，计算一个额外的**负载平衡损失**。常见的形式是计算所有专家负载的**方差**或**基于门控值的某种不平衡度量**。这个损失项会鼓励模型参数朝着使专家负载更均匀的方向更新。
    3.  **总损失**：`总损失 = 主任务损失（如交叉熵） + α * 辅助损失`，其中 `α` 是一个超参数，控制平衡的强度。

*   **工作原理**：通过梯度下降，**主模型的参数（包括路由门的参数）被同时优化以完成主任务和实现负载均衡**。路由门“学习”到，如果它能把Token更均匀地分配出去，总损失就会更低。

*   **主要问题**：
    *   **目标冲突**：**主任务目标**和**负载平衡目标**可能存在根本性冲突。有些Token天生就只和少数几个专家高度相关，强行将它们路由到其他专家会**损害模型的主任务性能**。
    *   **超参数敏感**：平衡系数 `α` 需要仔细调优。`α` 太小不起作用，`α` 太大会严重干扰主任务学习。

---

### **2. DeepSeek-V3策略：无辅助损失**

这种方法**不修改损失函数**，而是通过一个**独立的、动态的反馈控制系统**来直接调整路由决策。

*   **核心机制**：
    1.  **路由决策**：为每个专家 `i` 引入一个可学习的**偏差项 `b_i`**。在决定路由时，计算 `(亲和力分数 s_i + 偏差 b_i)`，并基于这个**调整后的分数**来选择Top-K专家。
    2.  **门控值计算**：**关键一步**：在计算最终输出时，**只使用原始的亲和力分数 `s_i`** 来加权各专家的输出。偏差 `b_i` 只影响“选择谁”，不影响“输出多少”。
    3.  **动态调整**：在训练过程中，持续监控每个专家的负载（被选中的频率）。如果一个专家过载，就**降低**它的 `b_i`；如果欠载，就**提高**它的 `b_i`。这个调整过程是**独立的**，不通过主损失函数的梯度来驱动。

*   **工作原理**：它像一个**独立的流量控制系统**。偏差 `b_i` 是一个“旋钮”，专门用来调节通往专家 `i` 的“流量”（Token数量），以确保所有专家都得到充分利用。这个系统的目标**唯一且纯粹**：就是负载均衡。

*   **主要优势**：
    *   **目标解耦**：将**负载平衡**和**模型性能**两个目标完全分开。路由门仍然只根据输入和亲和力（`s_i`）来决定如何组合专家输出以最好地完成任务。平衡系统（`b_i`）则专心致志地调节流量，两者互不干扰。
    *   **避免性能损害**：由于平衡操作不影响专家输出的权重，因此**不会强迫Token去使用不相关的专家**，从而保护了模型的主任务能力。

---

### **总结对比**

| 特性 | **传统策略（辅助损失）** | **DeepSeek-V3策略（无辅助损失）** |
| :--- | :--- | :--- |
| **核心手段** | 在损失函数中添加额外项 | 在路由决策中引入动态偏差项 |
| **优化目标** | 双目标联合优化（主任务 + 平衡） | **目标解耦**（主任务归主任务，平衡归平衡） |
| **如何影响路由** | 间接，通过影响路由门参数的学习 | **直接**，通过实时调整偏差值来改变选择概率 |
| **对模型性能影响** | 可能因目标冲突而**损害性能** | 理论上**更能保护模型性能** |
| **超参数** | 需要精细调整平衡系数 `α` | 需要调整偏差更新步长 `γ`，但影响更直接、更局部 |
| **类比** | **交规 + 交警**：通过制定法律（损失函数）和惩罚（梯度）来引导司机（路由门）的行为。 | **智能红绿灯系统**：直接根据实时车流（负载）调整信号（偏差），司机（路由门）仍按自己的最佳路线行驶。 |

**结论**：DeepSeek-V3的“无辅助损失”策略是一种更优雅、更解耦的解决方案。它将复杂的多目标优化问题拆分为两个相对简单的单目标问题，通过一个独立的控制回路来实现负载均衡，从而在保证训练稳定性的同时，最大程度地减少了对模型核心能力的潜在干扰。这也是其宣称能实现“无Token丢弃”和更高训练效率的重要原因之一。

## 12.5 - Batch-Wise Load Balance VS. Sequence-Wise Load Balance

### **内容概况**

这两张图片（对应论文章节 4.5.3）系统性地探讨了 **“批量级负载平衡”** 与 **“序列级负载平衡”** 两种方法的根本区别、性能影响及工程挑战。

研究首先指出，无辅助损失平衡方法与序列级辅助损失的核心区别在于 **平衡范围**：前者是**批量级**的，后者是**序列级**的。批量级平衡的约束更为**灵活**，它不强制每个序列内部都达到平衡，从而允许模型中的专家更专注于不同的领域。这一推断通过 **Figure 9** 的热力图得到了验证：在Pile测试集的三个不同领域（英文维基百科、Github、数学）上，无辅助损失模型比基于辅助损失的模型展现出更显著的**专家专业化模式**。

为了进一步探究这种灵活性带来的性能优势，研究还额外设计并验证了一种 **“批量级辅助损失”** 。实验结果表明，在达到相似的批量级平衡水平时，批量级辅助损失(batch-wise auxiliary loss)也能取得与无辅助损失(the auxiliary-loss-free)方法**相近的优异性能**（1B和3B MoE模型的验证损失数据均支持此结论）。最后，文章客观地指出了批量级平衡方法面临的两个效率挑战及其解决方案。

### **要点总结**

| 核心维度 | 详细说明与发现 |
| :--- | :--- |
| **1. 核心对比：平衡范围** | • **序列级平衡**：强制**每个独立的输入序列内部**的专家负载均匀。<br>• **批量级平衡**：只约束**整个训练批次（包含多个序列）** 的专家负载均匀，允许单个序列内负载倾斜。 |
| **2. 核心优势：专家专业化** | 批量级平衡的**灵活性**是关键优势。它不强迫每个序列都均匀使用所有专家，从而使得专家能够根据数据中的领域模式，**自然地专注于各自擅长的特定领域**（如代码、数学、百科）。Figure 9 的热力图直观证实了这一点。 |
| **3. 实验验证：性能关联** | • **对比实验**：在16B模型上，通过热力图验证了无辅助损失（批量级）模型比基于辅助损失（序列级）模型具有更明显的专家专业化模式。<br>• **控制实验**：在1B和3B MoE模型上对比了三种方法：<br>  - **序列级辅助损失**：验证损失较高（1B: 2.258, 3B: 2.085）。<br>  - **无辅助损失方法** 与 **批量级辅助损失**：两者性能**几乎完全相同**且更优（1B: 2.253, 3B: 2.080）。<br>• **结论**：性能优势源于**批量级平衡**本身，而非特定实现方式（无辅助损失或批量级辅助损失）。 |
| **4. 面临的效率挑战与解决** | 批量级方法面临两大挑战：<br>• **挑战1**：序列内或小批次内的负载不均。<br>  **解决方案**：训练框架采用**大规模专家并行与数据并行**，确保每个微批次规模足够大，自然平滑负载。<br>• **挑战2**：推理时，由于数据分布（领域）变化导致的负载不均。<br>  **解决方案**：推理框架采用**冗余专家部署策略**（见第3.4节），动态应对热门专家请求。 |

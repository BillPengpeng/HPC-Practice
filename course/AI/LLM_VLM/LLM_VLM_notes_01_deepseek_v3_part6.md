本文主要整理《DeepSeek-V3 Technical Report》的主要内容。

## 13.0 - Supervised Fine-Tuning

### **内容概括**

这两张图片系统地阐述了DeepSeek-V3模型在完成预训练后，进入的**监督微调**阶段。这一阶段的核心目标是利用高质量的指令数据，将基础的预训练模型（Base Model）塑造成一个擅长理解和遵循用户指令的对话模型。

其工作分为两大主线：
1.  **高质量数据构建**：这是SFT成功的基石。团队为不同领域的任务定制了数据生成方法。对于**逻辑推理、数学、代码**等复杂任务，借助已具备强大“长思考链”推理能力的内部DeepSeek-R1模型来生成数据。同时，通过一套包含 **“SFT→强化学习（RL）→高质量拒绝采样”** 的专家模型训练流程，平衡了R1数据的高准确性和理想格式的清晰简洁性。对于**非推理任务**（如创意写作、角色扮演），则主要依赖DeepSeek-V2.5生成并由人工审核。
2.  **微调训练执行**：使用精心构建的、覆盖多领域的**150万条指令数据**，对DeepSeek-V3-Base模型进行了**两个周期的微调**。训练采用了余弦衰减学习率调度，并实施了**样本掩码策略**，以确保在序列打包时不同训练样本间的信息隔离。

### **要点总结**

| 模块 | 核心要点 |
| :--- | :--- |
| **1. 整体数据规模** | 构建了包含 **1.5M（150万）个实例** 的指令微调数据集，涵盖多个领域。 |
| **2. 推理数据生成** | • **来源**：利用内部**DeepSeek-R1模型**生成。<br>• **挑战**：R1数据准确率高，但存在**过度思考、格式不佳、内容冗长**的问题。<br>• **解决方案**：通过训练**领域专家模型**（SFT+RL）作为数据生成器，并结合**拒绝采样**，最终得到**既准确又清晰简洁**的训练数据。 |
| **3. 非推理数据处理** | 对于**创意写作、角色扮演、简单问答**等数据，使用 **DeepSeek-V2.5** 生成响应，并依赖**人工标注员**进行验证和修正。 |
| **4. 微调训练设置** | • **周期**：使用SFT数据集进行**2个周期**的微调。<br>• **学习率**：采用**余弦衰减**，从 **5e-6** 逐渐降至 **1e-6**。<br>• **序列打包**：将多个样本打包成单个训练序列以提高效率，但通过**样本掩码策略**确保样本间不可见，防止信息泄露。 |

## 13.1 - Reinforcement Learning Reward Model

### **内容概括**

此部分阐述了DeepSeek-V3在强化学习阶段所使用的**混合奖励模型策略**。为了确保反馈信号的**可靠性**并覆盖**多样化的任务类型**，系统并未依赖单一的奖励模型，而是并行采用了两种不同原理的奖励模型：

1.  **基于规则的奖励模型**：针对**答案具有确定性、可验证性**的任务（如数学计算、编程题）。该模型通过预设的规则（如格式匹配、代码编译与测试）自动判断答案正确与否，提供**客观、可重复、抗操纵**的奖励信号。
2.  **基于模型的奖励模型**：针对**开放性、创意性**或**缺乏唯一标准答案**的任务（如创意写作、开放式问答）。该模型本身是一个经过训练的神经网络，能够根据问题和回答的综合输入来评估回答质量。为了提升其可靠性和可解释性，训练数据中包含了导致最终评判的**思维链**。

### **要点总结**

| 奖励模型类型 | 核心机制 | 适用任务类型 | 优势与特点 |
| :--- | :--- | :--- | :--- |
| **基于规则的RM** | **自动化规则验证**。例如：<br>• **数学题**：要求模型将最终答案置于特定格式（如`\boxed{}`）中，通过规则提取并比对。<br>• **编程题**：使用编译器运行测试用例，根据通过率给出反馈。 | **答案确定、可验证的任务**：<br>• 数学计算<br>• 编程（LeetCode类）<br>• 其他有明确真值的问题 | • **可靠性高**：结果客观，不受主观判断影响。<br>• **抗操纵性强**：模型难以通过“花言巧语”骗取高分，必须给出正确答案。<br>• **无需训练**：基于预定逻辑，开发后直接应用。 |
| **基于模型的RM** | **神经网络评估**。模型以**（问题，回答）** 作为输入，输出一个代表回答质量的奖励分数。<br>• **训练基础**：从DeepSeek-V3的SFT检查点开始训练。<br>• **数据增强**：使用包含**思维链**的偏好数据进行训练，即数据不仅提供最终偏好判断，还包含了得出该判断的**推理过程**。 | **开放性或无确定答案的任务**：<br>• 创意写作<br>• 角色扮演<br>• 开放式问答<br>• 文本摘要等 | • **灵活性高**：可以处理复杂、主观的评价任务。<br>• **缓解奖励黑客**：包含思维链的训练数据使奖励模型的决策过程更透明、更符合人类价值观，降低了模型通过“投机”方式骗取高奖励的风险。<br>• **可泛化**：能够对未见过的回答类型进行评估。 |

## 13.2 - Group Relative Policy Optimization

### **内容概括**

该图片节选自论文强化学习部分，详细介绍了DeepSeek-V3所采用的 **“组相对策略优化”** 算法。该算法继承自DeepSeek-V2，其核心创新在于**摒弃了传统强化学习中需要额外训练一个与策略模型同等规模的“评论家”模型的做法**。

GRPO的核心原理是：对于一个给定的问题，从当前的策略模型中采样生成**一组** 回答，然后利用这组回答**内部**的奖励分数来互相评估，计算出每个回答的“相对优势”。优化目标是在鼓励高优势回答的同时，通过裁剪机制保证训练稳定性，并引入KL散度惩罚来防止模型过度偏离参考模型（通常为SFT模型）的行为分布。此外，在RL训练过程中融入来自编码、数学、写作等多领域的提示，被证明能更好地对齐人类偏好，并在监督微调数据有限的场景下显著提升模型在基准测试上的性能。

### **要点总结**

1.  **核心目标**：在强化学习阶段优化模型，使其输出更符合人类偏好。
2.  **关键创新**：**无需训练独立的“评论家”模型**，极大降低了计算成本和训练复杂性。
3.  **核心机制**：**组内相对评估**。通过比较同一问题下**一组输出答案的奖励分数**，来估计每个答案的相对质量（优势值）。
4.  **训练过程**：
    *   **采样**：对一个问题，用当前策略模型生成 $G$ 个答案。
    *   **评分**：用奖励模型为每个答案打分。
    *   **计算优势**：在组内标准化这些分数，得到每个答案的优势值 $A_i$。
    *   **优化**：通过最大化目标函数 $J_GRPO$ 来更新策略模型，该函数鼓励模型增加高优势答案的产生概率，并约束其不过度偏离原始行为。
5.  **领域覆盖**：使用**多领域提示**进行RL训练，以全面提升模型在各种任务上的能力。

### **公式解释**

公式定义了GRPO的完整优化目标。

**1. 核心优势函数（公式28）**
$$ A_{i}=\frac{r_{i}-\text{mean}(\{r_{1},r_{2},\cdots,r_{G}\})}{\text{std}(\{r_{1},r_{2},\cdots,r_{G}\})} $$
*   **$r_i$**：第 $i$ 个答案的原始奖励分数。
*   **$mean(...)$**：计算该组所有 $G$ 个答案奖励分数的**平均值**。
*   **$std(...)$**：计算这组分数的**标准差**。
*   **$A_i$**：第 $i$ 个答案的**优势值**。它表示该答案的奖励**高于或低于组内平均水平的程度**，并经过了标准化（以标准差为单位）。$A_i$ > 0 表示该答案优于组内平均水平；$A_i$ < 0 则表示劣于平均水平。**这一计算完全在组内进行，无需外部基准。**

**2. KL散度项（公式27）**
$$ \mathbb{D}_{KL}\left(\pi_{\theta}||\pi_{ref}\right)=\frac{\pi_{ref}(o_{i}|q)}{\pi_{\theta}(o_{i}|q)}-\log\frac{\pi_{ref}(o_{i}|q)}{\pi_{\theta}(o_{i}|q)}-1 $$
*   **$π_θ$**：正在被优化的**新策略模型**。
*   **$π_ref$**：**参考模型**，通常是监督微调后的模型，代表“安全”、“良好”的初始行为。
*   **$D_KL$**：衡量新策略模型 $π_θ$ 与参考模型 $π_ref$ 之间概率分布的差异。**最小化此项是为了防止强化学习过程“走火入魔”**，确保模型在提升奖励的同时，不会过度偏离其原有的、符合人类预期的表达方式。$β$ 是控制此惩罚项权重的超参数。

**3. 总体目标函数（公式26）**
$$ \mathcal{J}_{\text{GRPO}}(\theta)=\mathbb{E}\left[\frac{1}{G}\sum_{i=1}^{G}\left(\min\left(\frac{\pi_{\theta}(o_{i}|q)}{\pi_{\theta_{\text{old}}}(o_{i}|q)}A_{i},\text{clip}\left(\frac{\pi_{\theta}(o_{i}|q)}{\pi_{\theta_{\text{old}}}(o_{i}|q)},1-\varepsilon,1+\varepsilon\right)A_{i}\right)-\beta\mathbb{D}_{KL}\left(\pi_{\theta}||\pi_{ref}\right)\right)\right] $$
这是需要**最大化**的目标。它包含两个主要部分：
*   **策略比裁剪项**：$min( (π_θ/π_old) * A_i, clip(π_θ/π_old, 1-ε, 1+ε) * A_i )$
    *   **$π_θ/π_old$**：新策略与旧策略产生某个答案的概率比。比值>1表示新策略更倾向于该答案。
    *   如果优势 $A_i$ 为正，我们希望增加该答案的概率（即让比值增大）。
    *   **$clip$函数**：将策略比限制在 $[1-ε, 1+ε]$ 之间。这**防止了单次更新中策略发生剧变**，是保障训练稳定的关键技术。$ε$ 是超参数。
    *   **$min$操作**：在原始比率和裁剪后的比率之间取最小值，构成一个**悲观（保守）估计**，进一步稳定训练。
*   **KL惩罚项**：$-β * D_KL(π_θ||π_ref)$
    *   如前所述，此项用于约束模型不要偏离参考模型太远。

**总结**：GRPO通过巧妙的**组内相对评估**替代了传统的Critic模型，并结合**策略裁剪**和**KL约束**，形成了一个高效、稳定且节省资源的强化学习框架。这是DeepSeek-V3能够通过RL阶段进一步优化其对话能力的关键算法保障。

## 14.0 - Evaluation Settings

### **内容概括**

评估体系分为三个明确部分：
1.  **广泛且专项的评估基准**：在基础测试之外，引入了多个专门针对**指令遵循、长文本理解、深度专业知识和复杂工程/竞赛**的新基准，以检验模型在真实应用场景下的能力。
2.  **全面且强力的对比基线**：选择与当前**开源和闭源领域最强的模型系列**进行对比，确保评估的标杆意义和公信力。
3.  **细致且标准化的配置**：对每种类型的基准（如标准选择题、代码、数学竞赛）都明确了具体使用的**评估框架、提示格式、协议和参数**，以保证评估过程的严谨、可复现和结果的可比性。

### **要点总结**

| 构成部分 | 核心内容 |
| :--- | :--- |
| **1. 评估基准** | **新增四大类专项基准：** <br>• **指令遵循**：IFEval <br>• **长文本理解**：FRAMES, LongBench v2 <br>• **深层专业知识**：GPQA， SimpleQA & C-SimpleQA <br>• **复杂工程/竞赛**：SWE-Bench Verified, Aider, LiveCodeBench, Codeforces, AIME 2024, CNMO 2024 |
| **2. 对比基线** | **最强的开源与闭源模型：** <br>• **开源**：DeepSeek-V2-0506, DeepSeek-V2.5-0905, Qwen2.5-72B-Instruct, LLaMA-3.1-405B-Instruct <br>• **闭源(API评估)**：Claude-Sonnet-3.5-1022, GPT-4o-0513 |
| **3. 详细配置** | • **标准选择题**：采用 `simple-evals` 框架的提示。<br>• **MMLU-Redux**：使用 `Zero-Eval` 提示格式。<br>• **其他数据集**：遵循各自**原始评估协议与默认提示**。<br>• **代码评估**：HumanEval-Mul测8种语言；LiveCodeBench同时用CoT与非CoT方法；SWE-Bench用无代理框架；Aider用`diff`格式。<br>• **数学评估**：AIME/CNMO用温度0.7，结果取16次平均；MATH-500用贪心解码。<br>• **通用设置**：允许最大输出 **8192个令牌**，确保复杂答案的完整性。 |

### **详细描述采用的评估指标**

此部分描述中并未指定一个统一的数学公式（如准确率、F1分数），而是体现了一个**任务导向、协议优先**的评估哲学。它采用了多种符合各自领域标准的**原生评估指标**，概括如下：

| 任务类型 | 评估指标本质 | 具体实现与解读 |
| :--- | :--- | :--- |
| **指令遵循 (IFEval)** | **格式遵循准确率** | 检查模型的输出是否严格遵守指令中要求的**特定格式**（如必须包含特定关键词、以特定结构列表等）。这衡量的是“听从指挥”的精确性。 |
| **代码生成 (HumanEval, LiveCodeBench)** | **单元测试通过率** | 将模型生成的代码提交给**测试用例**运行，根据通过用例的比例评分。这是最客观、直接的代码能力衡量标准。 |
| **数学问题 (MATH, AIME, CNMO)** | **答案匹配/竞赛得分** | • **MATH**：将模型生成的**最终答案**（通常要求置于如`\boxed{}`的格式中）与标准答案比对。<br>• **AIME/CNMO**：采用相应竞赛的**标准评分规则**，并可能转换为百分比排名或标准化分数。 |
| **软件工程修复 (SWE-Bench)** | **任务解决率** | 评估模型能否根据问题描述，生成正确的代码补丁（Patch）来解决GitHub上的真实issue。通常以解决（通过所有测试）的issue比例来衡量。 |
| **长文本理解 (LongBench v2)** | **任务特异性指标** | 根据不同任务设定，可能采用**精确答案匹配、ROUGE分数（摘要）、F1分数（问答）** 等指标。核心是检验模型从长上下文中提取和利用信息的能力。 |
| **知识问答 (GPQA, SimpleQA)** | **答案正确率** | 判断模型生成的回答是否包含事实性问题的**标准答案**，通常基于字符串匹配或经过验证的判定。 |

**核心评估理念总结：**
DeepSeek-V3的评估没有试图用一个“万能指标”去概括所有能力，而是**尊重每个任务领域的标准与惯例**。它通过：
1.  **采用原数据集的标准提示和协议**，确保公平对比。
2.  **使用客观、可复现的评判方式**（如测试用例、答案匹配）。
3.  **模拟真实、复杂的应用场景**（如完整编程项目、奥林匹克数学竞赛）。
从而实现对模型**实用价值和解决复杂问题能力**的真实检验。这比单纯报告一组标准化基准的准确率更有说服力，也与其打造“经济可行的超级智能”的务实目标一脉相承。

## 14.1 - Standard Evaluation

### **内容概况**

这组图片的核心是 **Table 6**，它对DeepSeek-V3指令微调后的对话模型进行了**全方位、多维度、高标准的横向评测**。评测将其与当前**开源和闭源领域的顶尖模型**进行了直接对比，涵盖了从**架构特征、英语能力、代码/数学推理到中文理解**的广泛领域。

总体结论非常明确：**DeepSeek-V3是目前性能最佳的开源对话模型，并且在多项关键指标上与GPT-4o、Claude-3.5-Sonnet等前沿闭源模型不相上下，甚至在数学和长上下文任务中实现超越。**

### **要点总结 (基于 Table 6 和数据解读)**

| 评估维度 | **DeepSeek-V3 的核心表现与定位** |
| :--- | :--- |
| **1. 总体定位** | **最佳开源模型**，且具备与**顶尖闭源模型竞争**的实力。 |
| **2. 架构效率** | **极致的“大模型，小计算”**：<br>• **总参数**：671B (行业顶尖容量) <br>• **激活参数**：37B/Token (远低于405B的稠密模型) <br>→ 意味着**极高的推理性价比**。 |
| **3. 英语综合能力** | **顶级水平，全面均衡**：<br>• **MMLU (通识)**：88.5分，与Claude/GPT-4o/LlaMA-405B持平。<br>• **MMLU-Pro (高阶)**：接近Claude-3.5，表现优异。<br>• **GPQA (博士级)**：59.1分，**大幅领先所有其他模型**，仅次于Claude-3.5。<br>• **DROP (阅读理解)**：91.6 F1，**排名第一**，展现强大的长文档信息提取能力。 |
| **4. 代码能力** | **工程与算法双优**：<br>• **HumanEval-Mul (多语言)**：82.6分，**开源最佳**。<br>• **LiveCodeBench (最新代码题)**：40.5分 (CoT)，**全面领先**。<br>• **SWE-Bench (真实工程修复)**：42.0分，显著优于其他开源模型，**为开源代码助手树立新标杆**。 |
| **5. 数学能力** | **绝对优势，刷新记录**：<br>• **MATH-500**：90.2分，**遥遥领先**。<br>• **AIME 2024 (竞赛)**：39.2分，**超越所有基线约10%**，是非o1类模型的新SOTA。<br>• **CNMO 2024 (中文奥数)**：43.2分，**表现突出**。<br>→ 证明了其从DeepSeek-R1**蒸馏的推理能力非常有效**。 |
| **6. 中文能力** | **理解深厚，事实性知识领先**：<br>• **C-SimpleQA (中文事实)**：64.8分，**排名第一**，即使对比在更大语料(18T vs 14.8T)上训练的Qwen2.5-72B，仍以显著优势胜出。<br>• **C-Eval (中文知识)**：86.5分，与Qwen2.5-72B表现相当，均为顶尖水平。<br>• **CLUEWSC (中文推理)**：90.9分，表现优异。 |

### **详细描述采用的评估指标**

| 指标名称 (缩写) | 全称与含义 | 衡量什么 | 典型基准 |
| :--- | :--- | :--- | :--- |
| **EM** | **Exact Match** (精确匹配) | 模型生成的答案与标准答案**完全一致**才得分。极为严格的指标。 | MMLU, MMLU-Redux, C-Eval |
| **F1** | **F1 Score** (调和平均数) | 综合**精确率(Precision)**和**召回率(Recall)**的分数，常用于需要从文本中提取答案的任务（如阅读理解）。值越高，答案越完整准确。 | DROP |
| **Pass@1** | **Pass Rate at 1 sample** (单次通过率) | 模型只生成**一个**答案，评估该答案是否通过测试（如代码执行正确、数学答案正确）。直接反映**一次性解决问题的能力**。 | HumanEval, LiveCodeBench, GPQA, AIME, CNMO |
| **Acc.** | **Accuracy** (准确率) | 正确回答的数量占总问题数的比例。通用指标。 | FRAMES, LongBench v2 |
| **Correct** | **Correctness** (正确数/率) | 通常指回答正确问题的**数量**或**比例**。在SimpleQA中可能指正确回答的**数量**。 | SimpleQA, C-SimpleQA |
| **Resolved** | **Resolved Rate** (解决率) | 成功解决的任务（如修复软件Issue）占总任务数的比例。 | SWE-Bench Verified |
| **Percentile** | **Percentile Rank** (百分位数) | 在Codeforces等竞赛平台上，该分数**击败了全球多少百分比**的参赛者。 | Codeforces |

**重要评测配置说明 (图片底部注释)**：
*   **输出长度**：所有模型评估时**限制输出最大长度为8K Token**，确保测试条件一致。
*   **小样本可靠性**：对于样本数少于1000的基准（如AIME, CNMO），会采用**不同温度设置进行多次测试**，以得出稳定、可靠的最终结果，避免随机性影响排名。

## 14.2 - Open-Ended Evaluation

### **内容概况**

为了全面评估模型在实际对话中的能力，研究团队采用了业界公认的、基于大语言模型（LLM）作为评判员的评估框架 **AlpacaEval 2.0** 和 **Arena-Hard**。这两个基准测试要求模型进行自由形式的生成，并由 **GPT-4-Turbo-1106** 作为评判员，对模型的回复与基线模型的回复进行**成对比较**，从而得出胜率。

结果显示，DeepSeek-V3 在这两项极具挑战性的评测中均取得了**突破性的成绩**，尤其是在 Arena-Hard 基准上，不仅超越了所有开源模型，更达到了与顶尖闭源模型并驾齐驱的水平。

### **要点总结**

| 评估维度 | **核心发现与解读** |
| :--- | :--- |
| **1. 评估方法** | 采用 **LLM-as-a-Judge** 范式，以 **GPT-4-Turbo** 为评判员，在 **AlpacaEval 2.0**（侧重写作与简单问答）和 **Arena-Hard**（侧重复杂推理、编码与调试）上进行**成对盲评**，结果以“胜率”呈现。 |
| **2. Arena-Hard 表现 (核心成就)** | • **胜率**：**85.5%**，超越了 GPT-4o (80.4%)，与 Claude-3.5-Sonnet (85.2%) 表现相当。<br>• **里程碑意义**：这是**首个胜率超过85%的开源模型**，在最具挑战性的复杂任务评测中，**显著缩小了开源与闭源模型之间的性能差距**，为开源社区树立了新标杆。 |
| **3. AlpacaEval 2.0 表现** | • **胜率**：达到惊人的 **70.0%**，以巨大优势领先于所有对比模型（第二名 Claude-3.5-Sonnet 为 52.0%）。<br>• **自我超越**：相比前代模型 **DeepSeek-V2.5 (50.5%)**，实现了近 **20个百分点** 的飞跃式提升，证明了其迭代的有效性。 |
| **4. 综合结论** | DeepSeek-V3 在开放域对话评估中展现出**全面而卓越的能力**：<br>• **处理复杂任务**：在 Arena-Hard 上达到顶尖水平，证明其强大的**推理、编码和问题解决能力**。<br>• **完成常规任务**：在 AlpacaEval 2.0 上的压倒性优势，证明其在**写作、基础问答等日常任务上极为可靠和出色**。 |

**总结**：开放域评估是衡量对话模型“实用性”和“智能感”的终极试金石。DeepSeek-V3 在这两项权威评测中的表现，不仅验证了其作为 **“最佳开源对话模型”** 的地位，更证明它已具备与 **GPT-4o、Claude-3.5-Sonnet 等顶级闭源模型同台竞技**的实力。其高达70%的 AlpacaEval 2.0 胜率更是表明，它在处理大多数用户日常请求时，将能提供质量极高的回复。

## 14.3 - DeepSeek-V3 as a Generative Reward Model

### **内容概况**

这部分内容展示了 DeepSeek-V3 一个超越常规对话的**进阶能力**：作为**评判者（Judger）或奖励模型（Reward Model）**。

研究使用权威的评估基准 **RewardBench**，将 DeepSeek-V3 与当前最先进的闭源模型（GPT-4o 和 Claude-3.5-Sonnet 的多个版本）进行对比，衡量它们在多个维度上的判断能力。

结果显示，**DeepSeek-V3 的基础版本（单次生成）表现已与最佳闭源版本相当，而通过“多数投票”技术增强后，其综合判断能力实现了全面领先。** 基于此，团队计划利用 DeepSeek-V3 结合投票机制，为开放性问题提供自我反馈，以提升模型对齐过程的有效性和鲁棒性。

### **要点总结**

| 方面 | 核心结论 |
| :--- | :--- |
| **1. 评估定位** | 测试模型作为 **“裁判”或“奖励模型”** 的能力，即评估其判断回答质量、安全性、逻辑性的水平。 |
| **2. 评估基准** | 使用 **RewardBench**，一个专门评估模型判断能力的标准测试集。 |
| **3. 主要发现 (基础版本)** | **DeepSeek-V3 单次生成**的判断能力：<br>• **综合水平 (Average: 87.0)**：与表现最佳的 **Claude-3.5-Sonnet-1022 (88.7)** 和 **GPT-4o-0806 (86.7)** 处于同一梯队，优于其他版本。<br>• **细分维度**：在 **Chat-Hard (79.8)** 上表现尤为突出，接近最佳水平；在 **Chat (96.9)** 上达到最高分。 |
| **4. 性能增强技术** | **多数投票 (maj@6)**：通过采样6个回答并进行多数投票来聚合判断。<br>• **效果**：使 DeepSeek-V3 在所有维度上实现**显著提升**，平均分达到 **89.6**，在 **Chat-Hard、Reasoning** 等关键维度上建立领先优势。 |
| **5. 未来应用** | 计划利用这种强大的判断能力，让 DeepSeek-V3 **为自身的开放式生成提供“自我反馈”**，从而形成一个更有效的强化学习对齐循环。 |

### **详细描述评估标准（RewardBench）**

RewardBench 通过一系列精心构建的**选择题**来评估模型的判断力。模型需要根据给定的**提示（Prompt）** 和**两个候选回答（Response A & B）**，选出它认为更好的那一个。

表格中的分数代表模型判断与人类偏好**一致的比例（准确率）**，分数越高，说明模型的判断越符合人类价值观。四个维度的具体含义如下：

| 维度 | 评估重点 | 内容说明 |
| :--- | :--- | :--- |
| **Chat** | **通用对话质量** | 评估模型在**日常对话、指令遵循、有帮助性**等方面的判断能力。问题相对标准。 |
| **Chat-Hard** | **困难对话质量** | 评估模型在**更具挑战性、需要复杂理解或推理**的对话场景中的判断能力。是区分模型判断力高低的关键。 |
| **Safety** | **安全性** | 评估模型识别**有害、偏见、不安全或不道德内容**的能力。判断哪个回答更安全、无害。 |
| **Reasoning** | **推理链质量** | 评估模型判断**逐步推理过程（思维链）逻辑是否严谨、正确**的能力。关注推理过程而非最终答案。 |
| **Average** | **综合平均分** | 上述四个维度得分的平均值，反映模型整体判断能力的强弱。 |

**结论**：这项评估表明，DeepSeek-V3 不仅是一个强大的**对话生成者**，也具备成为顶尖**对话评判者**的潜力。其卓越的判断能力（尤其是经投票增强后）为构建更强大的自我迭代和对齐训练系统提供了可能，展现了其作为开源模型领军者的综合实力。

## 15.0 - Distillation from DeepSeek-R1

### **内容概括**

此图片节选自论文的讨论部分（5.4.1），通过一个消融实验（Table 9）探讨了 **从DeepSeek-R1模型进行知识蒸馏** 对提升最终模型性能的贡献与影响。

研究以 **DeepSeek-V2.5** 作为基线模型进行对比。基线模型仅在较短的思维链数据上进行训练，而其对比模型则额外使用了前述方法（通过专家检查点）生成的DeepSeek-R1蒸馏数据。

Table 9的结果清晰地证明了蒸馏数据的有效性。同时，研究揭示了一个有趣的权衡：蒸馏在显著提升性能的同时，也**大幅增加了模型响应的平均长度**。研究团队在性能与效率之间进行了精心的平衡，最终为DeepSeek-V3选择了最优的蒸馏设置。

### **要点总结**

| 方面 | 核心发现与结论 |
| :--- | :--- |
| **1. 实验设计** | 进行**消融实验**，对比 **“仅使用短思维链数据训练的DeepSeek-V2.5基线”** 与 **“额外加入DeepSeek-R1蒸馏数据的版本”**。 |
| **2. 有效性证明 (Table 9)** | 蒸馏数据带来了**显著的性能提升**：<br>• **LiveCodeBench-CoT**：Pass@1从 **31.1** 提升至 **37.4**。<br>• **MATH-500**：Pass@1从 **74.6** 大幅提升至 **83.2**。 |
| **3. 核心权衡** | **性能提升 vs. 响应长度**：<br>• 两个任务的响应**平均长度均明显增加**（LiveCodeBench从718增至783；MATH-500从769激增至1510）。<br>• 这表明蒸馏让模型学会了生成**更长、更详细的推理过程**，这通常是其性能提升的来源，但也带来了更高的计算成本。 |
| **4. 工程决策** | 为了在**模型准确性**和**计算效率**之间取得平衡，团队为DeepSeek-V3**精心选择了最优的蒸馏设置**。 |
| **5. 未来展望** | • **潜力方向**：从推理模型（如R1）进行知识蒸馏，被证明是后训练优化的一个有前途的方向。<br>• **当前局限**：当前工作仅聚焦于**数学和编码领域**的蒸馏。<br>• **广阔前景**：这种方法在需要复杂推理的其他认知任务中同样具有巨大潜力，**跨领域的探索**是未来重要研究方向。 |

**总结**：这部分内容揭示了DeepSeek-V3卓越推理能力（特别是在数学和代码上）的一个重要来源——**从DeepSeek-R1进行的知识蒸馏**。它不仅在数据上证实了该方法的效果，也坦诚地指出了其带来的“更长输出”的代价，并展现了团队在工程上所做的精细化权衡。这为整个行业优化大模型的后训练过程提供了一个极具价值的可行路径。

## 15.1 - Self-Rewarding

### **内容概括**

文章指出，在**数学、编程等有明确外部工具验证的领域**，强化学习效果显著。但对于**更广泛、主观性强的场景**（如创意写作、开放问答），构建硬编码的奖励机制不切实际。

为此，团队采用了 **“宪法人工智能”** 的方法。其核心是：**利用 DeepSeek-V3 模型自身的能力，通过多轮自我评估（投票），来生成用于指导其自身优化的奖励信号。** 这种方法产生了显著的对齐效果，大幅提升了模型在主观评估中的表现。通过引入额外的“宪法”约束，可以引导模型向特定方向优化。

### **要点总结**

| 核心方面 | 具体说明 |
| :--- | :--- |
| **1. 问题背景** | 强化学习需要奖励信号引导，但在**开放式、主观性任务**中，难以通过硬编码规则或外部工具提供可靠奖励。 |
| **2. 解决方案** | 采用 **宪法人工智能（Constitutional AI）** 框架，核心是利用模型**自身的判断力作为奖励来源**。 |
| **3. 核心机制** | **自我奖励**：让 DeepSeek-V3 生成多个回答，然后通过**自身进行投票评估**，选出更符合预期标准（如宪法原则）的回答，并将此评估结果转化为奖励信号，用于驱动模型优化。 |
| **4. 实施效果** | 产生了**显著的对齐效果**，**大幅提升了模型在主观评估中的性能**。 |
| **5. 扩展性与可控性** | 通过整合**额外的宪法输入**（如一组更具体的道德准则、安全原则或风格要求），可以引导模型向指定的方向进行优化，增强了方法的可控性。 |
| **6. 范式意义** | 确立了一种重要范式：**LLM自身 + 补充信息 = 通用奖励信号**。LLM作为通用处理器，可将各种非结构化信息转化为促进自我改进的奖励。 |
| **7. 未来方向** | 除自我奖励外，团队将继续探索其他**通用、可扩展的奖励方法**，以持续提升模型在通用场景下的能力。 |

### **详细解释：宪法人工智能**

“宪法人工智能”是由 Anthropic 公司研究人员（Bai et al., 2022）提出的一种用于对齐和提升大型语言模型安全性与有用性的方法。其核心思想是让模型根据一套明确的、高层次的 **“宪法”原则**，进行**自我批评、自我改进和自我监督**。

**1. 核心流程（通常分为两个阶段）：**
*   **监督式宪法AI**：首先，人类提供一组宪法原则（如“有帮助”、“无害”、“诚实”）。模型被要求根据这些原则，对自己或他人生成的回答进行批判和修订，生成一个“改进版”的回答。这个过程产生了高质量的偏好数据对（原始回答 vs 修订后回答）。
*   **强化学习宪法AI**：利用上一阶段产生的偏好数据对，训练一个**奖励模型**，该模型学会根据宪法原则来判断回答的好坏。然后，使用这个奖励模型通过强化学习来训练最初的策略模型，使其输出更符合宪法原则。

**2. 在DeepSeek-V3上下文中的具体应用：**
在DeepSeek-V3的工作中，团队借鉴并简化了这一思想：
*   **“宪法”**：可能是一组内化的关于“高质量、有帮助、安全”的准则，也可能被具体化为额外的指令输入。
*   **“自我奖励”**：省去了单独训练奖励模型的步骤，直接利用 **DeepSeek-V3 自身强大的评判能力**（如您在上一部分看到的 RewardBench 高分表现），通过自我投票产生偏好判断，并将此判断直接作为奖励信号。
*   **优势**：
    *   **可扩展性**：无需为每个新任务手工设计奖励函数。
    *   **一致性**：奖励标准来自于模型自身对齐过的价值观，理论上更一致。
    *   **自我迭代**：形成了“生成→评估→优化”的自我改进循环。

**总结来说**，DeepSeek-V3 采用的“自我奖励”是宪法AI思想的一种高效实践。它利用模型自身作为评判者，将复杂的、主观的人类价值观判断问题，转化为模型内部可处理的优化问题，从而在缺乏明确外部奖励的广阔领域内，实现了有效的模型对齐与能力提升。

## 15.2 - Multi-Token Prediction Evaluation

### **内容概况**

该部分（5.4.3）旨在评估 **多令牌预测（MTP）** 技术与 **推测解码** 框架结合后，在实际应用中对模型推理速度的提升效果。

核心评估指标是 **“第二令牌接受率”** ，即在推测解码中，模型额外预测的第二个令牌被主验证模型采纳的比例。评估结果显示，该接受率在不同生成主题下均稳定在**85%至90%** 的高位。这一高可靠性直接转化为显著的端到端加速效果，使 **DeepSeek-V3 的令牌生成速度提升了约1.8倍**。

### **要点总结**

| 评估维度 | 核心内容与结论 |
| :--- | :--- |
| **1. 技术组合** | **MTP（多令牌预测）** 与 **推测解码框架** 相结合。MTP负责一次预测多个未来令牌作为“草稿”，推测解码框架则高效地验证并接受这些预测。 |
| **2. 评估焦点** | **第二令牌的接受率**。这是衡量MTP预测准确性和推测解码效率的关键指标。 |
| **3. 评估结果** | 接受率稳定在 **85% - 90%** 之间。这表明MTP模块的预测具有**高度一致性和可靠性**，能够被主模型广泛采纳。 |
| **4. 性能收益** | 凭借高接受率，该技术为DeepSeek-V3带来了 **1.8倍的令牌每秒生成速度提升**。这直接转化为更快的模型响应速度，优化了用户体验。 |
| **5. 意义** | 此评估证实了MTP不仅是一种有效的**训练时辅助目标**（提升模型能力），在**推理时**也能作为高效的**草稿模型**，切实提升服务效率。 |

**总结**：该部分通过量化评估，验证了MTP技术在推理阶段的实用价值。高接受率表明其预测质量优异，而1.8倍的解码加速则体现了显著的工程收益，使DeepSeek-V3在拥有强大能力的同时，也具备了更优的推理效率。

## 16 - Conclusion, Limitations, and Future Directions

### **内容概况**

首先，论文总结了模型的**核心成就**：它是一个拥有6710亿总参数、激活370亿参数的混合专家模型，通过包括无辅助损失负载均衡、多令牌预测在内的多项创新，在仅消耗278.8万H800 GPU小时的经济成本下，训练出了性能可比肩顶尖闭源模型（如GPT-4o、Claude-3.5-Sonnet）的当前最强开源模型。

其次，论文坦诚地指出了模型存在的**主要局限性**，主要集中在**部署端对硬件资源的要求较高**，以及**推理速度仍有提升空间**。

最后，论文基于开源与长期主义的理念，明确了四个关键的**未来研究方向**，涵盖了从模型架构、训练数据、推理能力到评估体系的全面演进计划。

### **要点总结**

#### **第一部分：核心成就与结论**
1.  **模型定位**：**当前最强大的开源语言模型**，性能与领先闭源模型相当。
2.  **关键创新**：
    *   **架构**：沿用并优化了MLA和DeepSeekMoE。
    *   **训练策略**：首创**无辅助损失的负载均衡策略**和**多令牌预测训练目标**。
    *   **工程效率**：支持FP8训练及一系列精细化工程优化，实现了**极致的训练成本效益**。
    *   **后训练**：成功从DeepSeek-R1系列**蒸馏出强大的推理能力**。
3.  **经济性证明**：完整训练周期（预训练、上下文扩展、后训练）总成本仅为 **278.8万H800 GPU小时**，树立了高性能与经济性兼顾的新标杆。

#### **第二部分：当前局限性**
1.  **部署门槛**：为实现高效推理，推荐的**部署单元规模较大**，可能对小型团队构成资源负担。
2.  **性能瓶颈**：尽管生成速度已比V2快两倍以上，但**端到端推理速度仍有提升潜力**。
3.  **解决方案展望**：认为这些局限将随着**更先进硬件的发展**而得到自然缓解。

#### **第三部分：未来研究方向**
DeepSeek团队秉持开源与长期主义，承诺在以下四个战略方向持续投入：

| 方向 | 具体内容与目标 |
| :--- | :--- |
| **1. 架构演进** | • **持续优化架构**，进一步提升训练与推理效率。<br>• **追求无限上下文**的高效支持方案。<br>• **尝试突破Transformer架构的固有局限**，拓展其建模能力边界。 |
| **2. 数据扩展** | • **持续迭代训练数据的数量与质量**。<br>• **探索融入额外的训练信号源**（如多模态、强化学习反馈等）。<br>• 推动数据在**更全面的维度上扩展**，不局限于文本Token数量。 |
| **3. 深度思考能力** | • **持续探索和迭代模型的深度思考能力**。<br>• 通过**拓展推理的长度和深度**，旨在提升模型的**根本智能与复杂问题解决能力**。 |
| **4. 评估体系革新** | • **探索更全面、多维的模型评估方法**。<br>• **防止研究过程陷入对固定基准集的过度优化**，避免造成对模型能力的**误解**并影响**基础评估的客观性**。 |

## 17 - Ablation Studies for Low-Precision Training

### **内容概况**

这三张图片属于论文的附录部分，提供了对正文中提及的关键技术（**FP8训练**与**无辅助损失负载平衡**）的**补充实验验证与深度分析**。

*   **图片1 (附录 B.1)**：通过在不同规模的模型上进行对比实验，**验证了FP8混合精度训练框架的有效性和精度保证**。
*   **图片2 & 3 (附录 B.2 与 C)**：分别从**硬件实现挑战**和**模型行为可视化**的角度，对两项核心技术进行了更深入的探讨。
    *   **B.2** 解释了为何对某些张量（如激活梯度）采用“块量化”会带来挑战。
    *   **C** 通过热力图直观对比了基于辅助损失和无辅助损失的模型在专家负载模式上的差异，为后者的优势提供了证据。

### **要点总结**

#### **1. FP8训练的有效性验证 (图10)**
*   **实验设计**：在**小规模（总参16B）** 和**大规模（总参230B）** 两个MoE模型上，对比了FP8训练与标准BF16训练的损失曲线。
*   **核心结论**：
    *   在两个规模上，**FP8训练的损失曲线与BF16高度吻合**。
    *   在高精度累加和细粒度量化策略下，**FP8训练相对于BF16的相对误差始终低于0.25%**。
    *   这**实证了FP8训练框架在保证精度无损的前提下，能够实现显著的性能提升和内存节省**。

#### **2. 块量化策略的局限性与讨论**
*   **问题背景**：虽然论文提出的 **“瓦片级”量化**（如对激活使用`1x128`分组）有效，但它要求在前向和反向传播中使用不同的分组策略（`1x128` 和 `128x1`），实现较复杂。一个自然的想法是对所有张量（包括激活梯度）统一使用更简单的 **“块级”量化**（如`128x128`）。
*   **实验发现**：对计算激活梯度的 **`Dgrad`** 操作使用块量化，会导致一个16B的MoE模型在大约300B token后**训练发散**。
*   **原因分析**：激活梯度在不同token间**高度不平衡**，会引入**与特定token相关的异常值**。块量化（`128x128`）难以有效处理这种**以token为单位的异常值分布**，从而导致数值不稳定。这解释了为何需要为前向激活和反向梯度设计不同的、更精细的量化策略。

#### **3. 无辅助损失模型的专家专业化模式**
*   **实验对比**：记录并对比了**基于辅助损失**和**无辅助损失**的16B模型在Pile测试集上的专家负载情况。
*   **核心发现**：如**Figure 10**（推测为展示热力图的图）所示，**无辅助损失模型在所有层上都倾向于表现出更高的专家专业化程度**。
*   **意义**：这为正文中的论点——**无辅助损失策略通过实现批量级（而非序列级）平衡，促进了专家的自然专业化，从而带来性能提升**——提供了直观的、经验性的证据。热力图很可能显示，无辅助损失模型中，特定专家对特定类型的数据（如代码、数学）响应更集中。

### **总结**
附录部分的这些内容是对正文核心技术的**有力支撑和深化阐述**：
1.  **用实验数据证明了FP8训练的可行性**（精度损失可忽略）。
2.  **从失败案例中揭示了算法细节设计的必要性**（为何不能对所有操作都用简单的块量化）。
3.  **通过可视化展现了无辅助损失策略的底层优势**（促进专家专业化）。

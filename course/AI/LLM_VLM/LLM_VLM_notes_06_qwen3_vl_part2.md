本文主要整理《Qwen3-VL Technical Report》的主要内容。

## 5.0 - Training Recipe

### **内容概括**

本部分阐述了Qwen3-VL在预训练完成后，为进一步提升其实用性、推理能力和与人类偏好对齐而实施的 **“三阶段后训练流程”** 。该流程旨在将基础模型精炼为强大的指令跟随和问题解决工具，涵盖了**监督微调**、**知识蒸馏**和**强化学习**三个核心阶段，并针对“思考型”和“非思考型”模型变体以及长上下文场景进行了专门设计。

---

### **要点总结**

#### **1. 总体目标**
后训练是一个三阶段流程，旨在：
1.  **精炼模型**的指令跟随能力。
2.  **增强模型**的推理能力。
3.  **使模型**与人类偏好对齐。

#### **2. 三阶段流程详解**
**第一阶段：监督微调**
*   **目的**：赋予模型指令跟随能力，并激活其潜在的推理技能。
*   **实施**：分两阶段进行：
    1.  **初始阶段**：在 **32K** 的上下文长度下进行。
    2.  **扩展阶段**：将上下文窗口扩展到 **256K**，重点关注**长文档和长视频数据**。
*   **数据格式**：为满足不同需求，训练数据分为两类：
    *   **标准格式**：用于训练**非思考型**模型。
    *   **思维链格式**：用于训练**思考型**模型，该格式会显式地建模推理过程。

**第二阶段：强到弱的知识蒸馏**
*   **目的**：显著提升模型的推理能力。
*   **方法**：采用知识蒸馏技术，由一个**强大的教师模型**将其能力迁移给学生模型。
*   **关键设计**：此过程**仅使用纯文本数据**对模型的**LLM骨干部分**进行微调。
*   **效果**：该方法被证明非常有效，能同时在**纯文本任务**和**多模态任务**上显著提升模型的推理能力。

**第三阶段：强化学习**
*   **目的**：进一步提升模型性能和与人类的对齐度。
*   **分类**：此阶段分为两部分：
    1.  **推理强化学习**：专注于提升推理相关任务。
    2.  **通用强化学习**：覆盖更广泛的能力。
*   **应用范围**：在包括但不限于**数学、OCR、基础模型（grounding）和指令跟随**在内的广泛文本和多模态领域进行大规模强化学习，以改善模型更细粒度的能力。

## 5.1 - SFT Data

### **内容概括**

本部分详细阐述了 Qwen3-VL 为进行**监督微调**而构建高质量数据集的全过程。其核心目标是赋予模型处理多样、复杂现实场景的能力。为实现此目标，团队不仅从 Qwen2.5-VL 的核心能力出发进行了战略性扩展，还精心策划了一个约120万样本的、兼顾单模态与多模态、支持长上下文的数据集。最关键的是，为确保数据质量，团队实施了一个包含**查询过滤**与**响应过滤**的两阶段严格筛选流程，最终为模型的高效、安全训练提供了坚实的数据保障。

---

### **要点总结**

#### **1. 核心目标与数据构建原则**
*   **目标**：赋予模型解决广泛真实世界场景的能力，使其成为一个全面、鲁棒的多模态基础模型。
*   **构建原则**：
    *   **能力延续与扩展**：在 Qwen2.5-VL 的基础上，根据社区反馈、学术研究和实际应用，战略性地引入了新能力，如**具身智能空间推理、细粒度视觉理解、视频时空定位、数百页长文档理解**等。
    *   **基于真实用例**：围绕目标任务和真实用例，系统地从开源数据集和网络资源中精心挑选和合成样本。

#### **2. SFT数据集构成与特点**
*   **规模**：约 **1,200,000** 个样本。
*   **模态构成**：
    *   **单模态数据**：占 **1/3**，为纯文本条目。
    *   **多模态数据**：占 **2/3**，包含图像-文本和视频-文本对。
*   **多样性与复杂性**：
    *   **多语言**：超越中英文，包含多样化多语言样本。
    *   **对话形式**：模拟真实对话，包含基于**单图和多图序列**的单轮与多轮对话。
    *   **高级行为支持**：包含交错的图文示例，以支持**工具增强的图像搜索、视觉推理**等高级智能体行为。

#### **3. 分阶段训练策略**
为优化对256K超长上下文支持的计算效率，采用两阶段训练：
*   **第一阶段**：以 **32K** 令牌序列长度训练一个周期。
*   **第二阶段**：以完整 **256K** 令牌长度训练一个周期，期间混合使用全长数据和32K长度的数据。

#### **4. 严格的数据质量管控（核心）**

**第一阶段：查询过滤**
*   **目的**：确保问题指令的质量和相关性。
*   **方法**：
    1.  **验证与剔除**：利用 **Qwen2.5-VL** 识别并剔除无法验证或过于模糊的查询。
    2.  **修正**：对模糊指令进行最小化修订以提升清晰度。
    3.  **内容筛选**：系统剔除无实质内容的网络查询。
    4.  **最终评估**：评估查询的**复杂性**和**上下文相关性**，仅保留具备适当挑战性和相关性的样本。

**第二阶段：响应过滤**
*   **目的**：确保回答本身的高质量、可靠性与安全性。
*   **策略**：采用基于规则和基于模型的互补策略。
    *   **基于规则的过滤**：应用预定义启发式规则，剔除存在**重复、不完整、格式错误**等质量缺陷的响应。同时，剔除**离题或可能产生有害内容**的问答对。
    *   **基于模型的过滤**：采用源自 **Qwen2.5-VL 系列的奖励模型**进行多维度评估。
        *   **评分标准**：对回答的**正确性、完整性、清晰度、有用性**进行评分。
        *   **视觉侧重**：对于视觉任务，重点评估对**视觉信息的准确解释和利用**。
        *   **捕获微妙问题**：能发现规则方法易忽略的问题，如**不恰当的语言混合或突兀的风格转换**。

## 5.2 - Long-CoT Cold Start Data

### **内容概括**

本节阐述了为训练 **“思考型”模型** 而精心构建的核心数据集——**长链式思维（Long-CoT）冷启动数据集**。该数据集旨在激发和精炼模型的复杂推理能力，通过混合纯文本与多模态数据（约1：1比例），并特别强化**STEM（科学、技术、工程、数学）** 和**智能体工作流**任务来构建挑战。为保证数据的高质量与有效性，团队实施了一套严格的多阶段筛选协议，确保数据不仅难度足够，而且真正需要多模态理解，最终得到一个高质量的推理启动数据集。

---

### **要点总结**

#### **1. 数据集目标与构成**
*   **核心目标**：为“思考型”模型提供高质量、高难度的训练数据，以**激发和精炼其复杂、多步推理能力**。
*   **数据构成**：平衡混合**纯文本**和**视觉-语言（多模态）** 数据，比例约为1：1，以实现通用推理与多模态推理能力的均衡发展。
*   **内容重点**：
    *   **多模态部分**：除覆盖VQA、OCR、视觉定位、视频分析等传统领域外，**特别强调和丰富了STEM与智能体工作流相关的任务**。
    *   **纯文本部分**：与Qwen3的纯文本数据保持一致，包含数学、代码生成、逻辑推理和通用STEM中的挑战性问题。

#### **2. 严格的多阶段筛选协议**
为确保数据的高质量和有效性，采取了三个关键筛选步骤：

*   **难度筛选**：
    *   **标准**：仅保留那些导致基线模型**通过率较低**或**需要生成长篇、详细回答**的问题实例。
    *   **目的**：确保数据集包含对当前模型而言真正具有挑战性的问题。

*   **多模态必要性筛选**：
    *   **标准**：对于涉及数学的视觉问题，使用 **Qwen3-30B-nothink模型** 进行测试。**丢弃任何仅凭文本就能正确解决的问题**。
    *   **目的**：保证筛选后的问题**真正需要模型理解视觉信息**，而非仅依赖文本线索，从而强化模型的多模态理解能力。

*   **响应质量控制**：
    *   **与Qwen3方法对齐**：对所有生成的推理链（CoT）响应进行净化。
    *   **步骤**：
        1.  **结果过滤**：对于有多个候选答案的问题，首先**移除最终答案错误的响应**。
        2.  **质量过滤**：进一步过滤掉表现出不良模式的响应，例如**过度重复、不恰当的语言混合、或明显没有足够推理步骤的猜测性答案**。

#### **3. 最终成果**
通过这一系列严格的筛选，最终得到了一个**高质量、高挑战性**的数据集，专门用于**启动（Bootstrap）和训练具备先进多模态推理能力的“思考型”模型**。

## 5.3 - Strong-to-Weak Distillation

### **内容概括**

该方法借鉴自纯文本模型Qwen3，采用一种名为 **“强到弱蒸馏”** 的两阶段策略。该策略旨在将一个或多个强大教师模型的推理能力高效地迁移给参数更少的学生模型，通过 **Off-policy Distillation** 和 **On-policy Distillation** 两个互补的阶段，逐步引导学生模型模仿教师模型的输出分布和推理模式。

---

### **要点总结**

#### **1. 核心目标与来源**
*   **目标**：显著提升**轻量级（较小参数量）学生模型**的性能，使其在资源受限的条件下也能具备强大的推理能力。
*   **方法来源**：直接采用并适配了纯文本模型 **Qwen3** 中已验证有效的“强到弱蒸馏”流程。

#### **2. 两阶段蒸馏流程详解**
整个流程分为两个逻辑上连贯的阶段：

*   **第一阶段：Off-policy Distillation**
    *   **策略**：使用**预先收集的、由强大教师模型生成的高质量输出**作为训练数据。
    *   **过程**：将这些教师模型的响应（答案及推理过程）与学生模型需要学习的任务提示相结合，构成训练样本。
    *   **目的**：让学生模型通过“观摩”和“模仿”优秀范例，**初步建立扎实的基础推理能力**，为后续更自主的学习奠定基础。

*   **第二阶段：On-policy Distillation**
    *   **策略**：让学生模型**主动参与**，根据给定的提示自行生成响应。
    *   **过程**：
        1.  学生模型对一批提示生成自己的回答序列。
        2.  使用**同一个（或同一组）教师模型**对这些学生生成的序列进行评估，并给出“理想”的输出（logits）。
        3.  通过优化算法（如最小化KL散度），**对齐学生模型预测的输出分布与教师模型认为的理想分布**。
    *   **目的**：在离策略阶段建立的“模仿”能力基础上，进一步**精炼和校准学生模型自身的生成行为**，使其在更接近实际部署（自主生成）的场景下，也能产生与教师模型质量相近的输出。

#### **3. 核心思想与价值**
这一蒸馏流程体现了 **“先模仿，后精炼”** 的模型压缩思想：
1.  **Off-policy Distillation**提供了高质量、低噪声的“标准答案”，高效传递知识。
2.  **On-policy Distillation**则解决了学生模型在自主生成时可能出现的分布漂移问题，确保其在实际应用中的可靠性。

## 5.4 - Reasoning Reinforcement Learning

### **内容概括**

本节详细阐述了Qwen3-VL为提升模型在**复杂推理任务**上的性能，所设计的**推理强化学习**的具体实施方案。该方法覆盖了从数学、代码到视觉定位、视觉谜题等多种**可验证**的文本与多模态任务。其核心流程包含三个关键环节：
1.  **严格的数据准备**：通过初步实验筛选高质量查询，并混合任务批次。
2.  **统一的奖励系统**：为不同任务提供精确反馈，并抑制语码转换。
3.  **高效的RL算法**：采用SAPO算法进行策略优化，确保训练稳定有效。

---

### **要点总结**

#### **1. 总体设计原则**
*   **任务范围**：在广泛的**文本和多模态任务**上进行强化学习，涵盖数学、编程、逻辑推理、视觉定位和视觉谜题。
*   **核心要求**：所有任务的解决方案都必须是**可通过规则或代码执行器进行确定性验证**的，为强化学习提供明确的奖励信号。

#### **2. 数据准备**
为确保训练数据的有效性与挑战性，实施了多阶段的严格筛选：
*   **初步查询筛选**：
    1.  从开源和自有来源整理数据，进行严格的预处理和人工标注。
    2.  使用最强的视觉语言模型（Qwen3-VL-235B-A22B）为每个多模态查询生成16个响应。
    3.  **丢弃所有响应都错误的查询**，确保查询本身具有可学性。
*   **数据源优选**：针对每个任务进行初步的RL实验，**识别并剔除改进潜力有限的数据源**。
*   **最终数据集构建**：
    *   得到约3万个覆盖多种任务的RL查询。
    *   在训练每个模型时，为所有查询采样16个响应，并**过滤掉通过率超过90%的简单查询**。
    *   将不同任务的数据集**打乱并混合**以构建混合任务批次，通过大量前期实验确定每个任务的**固定样本比例**，以保证训练平衡。

#### **3. Reward系统**
设计了一个统一的框架来提供精准的奖励反馈：
*   **统一框架**：提供共享的基础设施（数据预处理、工具函数、奖励管理器），但**核心奖励逻辑针对每个任务单独实现**。
*   **格式引导**：使用**任务特定的格式提示**来引导模型输出所需格式，**不依赖显式的“格式正确”奖励**。
*   **抑制语码转换**：当响应的语言与提示语言不匹配时，**施加惩罚**，以鼓励语言一致性。

#### **4. RL算法**
*   采用 **SAPO**算法进行RL训练。这是一种**平滑自适应的策略梯度方法**。
*   **优势**：该方法在多样的文本和多模态任务上，以及在不同模型规模和架构上，都能带来**一致的性能提升**。

## 5.5 - General Reinforcement Learning

### **内容概括**

本节阐述了在专门的“推理强化学习”之后，旨在全面提升模型**通用能力与交互质量**的“通用强化学习”阶段。该阶段采用**多任务强化学习范式**，在一个综合任务集上优化模型，其核心目标聚焦于两个维度：**精准的指令遵循**与**符合人类偏好的对齐**。同时，此阶段还承担了**纠正错误知识**、**抑制不良行为**的关键任务。为实现这些复杂目标，系统设计了一个**混合奖励系统**，结合了基于规则的精确反馈和基于模型的细粒度评估，以灵活、高效地引导模型学习。

---

### **要点总结**

#### **1. 核心目标与范式**
*   **目标**：增强模型的**泛化能力**和**操作鲁棒性**，使其成为一个更可靠、更易用的助手。
*   **范式**：采用**多任务强化学习**。奖励函数基于监督微调阶段涵盖的广泛任务来定义，包括视觉问答、图像描述、OCR、文档解析、视觉定位、视频时间识别等。

#### **2. 优化的两个核心维度**
*   **指令遵循（Instruction Following）**：
    *   评估模型对**明确用户指令**的遵从程度。
    *   包括处理复杂的**内容、格式、布局约束**，以及生成和处理**结构化输出**的能力。
*   **偏好对齐（Preference Alignment）**：
    *   针对开放式或主观性查询，使模型的输出与**人类偏好**保持一致。
    *   优化方向包括提升**交互体验、事实准确性**和**风格得体性**，以营造更自然、吸引人的交互。

#### **3. 作为纠正与抑制机制**
*   **纠正错误知识**：
    *   **问题**：模型在SFT阶段可能习得了一些顽固但错误的知识先验。
    *   **方案**：设计专门的、可验证的任务来“触发”这些特定错误，并引入正确知识进行**针对性纠正**。
*   **抑制不良行为**：
    *   **问题**：模型可能残留如**不恰当的语言混合、过度重复、格式错误**等行为。
    *   **挑战**：这些问题在通用数据中样本稀少，直接使用RL纠正效率低下。
    *   **方案**：为此阶段专门策划一个数据集，**集中包含那些已知会引发不良行为的提示**，从而能够施加高频、针对性的惩罚，有效抑制这些错误。

#### **4. 混合奖励系统**
为上述复杂目标提供有效反馈，结合两种互补方法：
*   **基于规则的奖励**：
    *   **适用场景**：对于有可验证标准答案的任务，如格式遵循。
    *   **特点**：提供**明确、高精度的反馈**，使模型快速学习正确模式，并能有效防止模型“钻空子”。
*   **基于模型的奖励**：
    *   **实现**：使用 **Qwen2.5-VL-72B-Instruct** 等强大模型作为“评判员”。
    *   **工作方式**：评判员将模型生成的响应与参考答案对比，在**多个评估轴向上**进行评分。
    *   **特点**：为评估**开放、复杂**的任务提供了**卓越的灵活性**，能有效减少因格式或措辞不同而误判优质响应的“假阴性”情况。

## 5.6 - Thinking with Images

### **内容概括**

本节阐述了Qwen3-VL如何通过一种**两阶段的训练范式**，赋予模型“看图思考”的智能体能力。该工作借鉴了视觉智能体领域的先进思想，旨在让模型能够自主规划并使用工具（如视觉定位）来分步骤解决复杂的视觉任务。第一阶段利用**合成冷启动数据集**训练一个较小的代理模型；第二阶段则**蒸馏**第一阶段模型的能力，生成更大规模的数据集来训练最终的Qwen3-VL模型。整个训练过程都辅以**多轮、工具集成的强化学习（RL）**，并设计了三个互补的奖励信号来引导模型进行稳健、恰当的推理。

---

### **要点总结**

#### **1. 核心目标与灵感来源**
*   **目标**：赋予Qwen3-VL类似智能体的能力，使其能够通过 **“思考→行动→分析反馈→回答”** 的多轮交互流程解决复杂视觉问题。
*   **灵感**：借鉴了近期在“图像思考”领域的多项重要研究成果。

#### **2. 两阶段训练范式**
*   **第一阶段：冷启动与代理模型训练**
    1.  **数据合成**：构建一个约1万样本的冷启动数据集，主要包含简单的两轮视觉问答任务（如属性检测）。
    2.  **监督微调**：在 **Qwen2.5-VL-32B** 模型上进行SFT，训练其模仿视觉智能体的行为模式。
    3.  **强化学习**：应用**多轮工具集成RL**来进一步提升其推理能力。

*   **第二阶段：知识蒸馏与最终模型训练**
    1.  **数据蒸馏**：利用第一阶段训练好的视觉智能体（Qwen2.5-VL-32B），生成一个规模更大（约12万样本）、任务更多样化的多轮智能体交互数据集。
    2.  **最终训练**：将这个混合了蒸馏数据和合成数据的数据集，用于Qwen3-VL的后训练，采用与第一阶段类似的SFT和工具集成RL流程。

#### **3. 多轮工具集成强化学习**
*   **流程一致性**：两个阶段的RL流程基本相同，主要区别在于底层数据。
*   **三项互补的奖励信号**：
    1.  **答案准确性奖励**：使用 **Qwen3-32B** 评估**最终答案是否正确**。
    2.  **多轮推理奖励**：使用 **Qwen2.5-VL-72B** 评估助手是否**正确解读了工具或环境的反馈**，并通过连贯、逐步的推理得出答案。
    3.  **工具调用奖励**：通过比较**实际工具调用次数**与**专家预估的目标次数**（由Qwen2.5-VL-72B根据任务复杂度离线确定），来鼓励模型进行恰当的工具使用。

#### **4. 关键发现与对策**
*   **早期问题**：实验发现，模型存在“退化”倾向——无论任务需求如何，都倾向于**只调用一次工具**来“骗取”前两项奖励（答案准确性和多轮推理）。
*   **解决方案**：**显式地引入“工具调用奖励”**，以鼓励模型根据任务复杂性进行自适应的工具探索，从而确保推理过程的完整性和合理性。

## 5.7 - Infrastructure

### **内容概括**

本部分介绍了支撑Qwen3-VL系列模型训练与部署的**核心基础设施和工程框架**。报告指出，模型的预训练在**阿里云PAI-Lingjun AI计算服务**上进行，并采用基于**Megatron-LM的混合并行策略**，实现了在高达10,000个GPU规模下的高效扩展。在本地部署与评估阶段，则选用**vLLM**和**SGLang**作为后端框架，分别提供了高吞吐量推理和复杂提示处理的稳定高效能力。

---

### **要点总结**

#### **1. 训练基础设施**
*   **计算平台**：在**阿里云PAI-Lingjun AI计算服务**上完成训练。该平台专为AI、高性能计算等计算密集型场景设计，提供所需的高性能算力。
*   **并行训练策略**：
    *   **框架基础**：基于 **Megatron-LM** 框架构建。
    *   **策略组合**：采用混合并行策略，综合了：
        *   **张量并行（TP）**
        *   **流水线并行（PP）**
        *   **上下文并行（CP）**
        *   **专家并行（EP）**
        *   **ZeRO-1数据并行（DP）**
    *   **设计目标**：实现**模型规模、计算负载和通信开销**之间的精细平衡。
    *   **扩展能力**：该配置保障了高硬件利用率，维持了高吞吐量与低通信延迟，**支持扩展至高达10,000个GPU的规模**。

#### **2. 部署与评估基础设施**
*   **本地部署策略**：基于 **vLLM** 或 **SGLang** 后端进行部署和性能评估。
*   **vLLM 后端**：
    *   **核心技术**：利用 **PagedAttention（分页注意力）** 机制。
    *   **核心优势**：实现**高效的内存管理**和**高吞吐量的推理**。
*   **SGLang 后端**：
    *   **核心优势**：擅长**结构化生成**和**处理复杂的提示（prompt）**。
*   **整体价值**：这两个后端共同为模型提供了**稳定、高效且灵活**的推理与评估能力。


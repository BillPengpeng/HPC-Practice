本文主要整理《DeepSeek-V3 Technical Report》的主要内容。

## 3.2 - Auxiliary-Loss-Free Load Balancing

### **内容概括**

其核心创新在于：**为每个专家引入一个动态可调的偏差项**。在决定每个 Token 应路由至哪些专家时，系统会将这个偏差项加到原始的亲和力分数上，以此来影响路由决策。训练过程中，系统持续监控各专家的负载情况：对**过载**专家调低其偏差，对**欠载**专家调高其偏差，从而实现专家使用率的动态平衡。关键在于，这个偏差**仅用于路由选择**，而在计算最终专家输出的权重时，仍使用原始的亲和力分数，从而在维持模型表达能力和实现负载均衡之间取得了更优的权衡。

### **要点总结**

1.  **问题背景**：MoE模型中，专家负载不均衡会导致**路由崩溃**（部分专家被过度使用，部分被闲置）和**计算效率下降**（在专家并行场景下尤为严重）。
2.  **传统方案**：普遍依赖**辅助损失**来鼓励负载均衡，但过强的辅助损失会直接**损害模型的主任务性能**。
3.  **本方案核心**：提出一种**无需辅助损失**的负载均衡策略，通过引入一个**动态调整的专家偏差**来引导路由。
4.  **工作原理**：
    *   **路由决策**：使用 `亲和力分数 + 专家偏差` 来决定 Top-K 专家。
    *   **输出计算**：计算专家输出的权重（门控值）时，**仅使用原始的亲和力分数**，偏差不参与。
    *   **动态调整**：根据专家在每一步训练的负载情况（过载或欠载），以固定步长 $γ$ 反向调整其偏差值。
5.  **核心优势**：实现了**负载均衡与模型性能之间更好的权衡**，避免了辅助损失对模型能力的潜在伤害。

### **公式解释**

图片中的核心公式定义了如何利用偏差项进行路由选择：

**公式 (16) - 基于偏差调整的 Top-K 路由**
$$
g_{i,t}^{\prime}=\begin{cases}s_{i,t}, & s_{i,t}+b_{i} \in \text{Topk}(\{s_{j,t}+b_{j}|1\leqslant j\leqslant N_{r}\}, K_{r}), \\
0, & \text{otherwise}.
\end{cases}
$$

*   **公式目的**：确定第 $t$ 个 Token 是否应路由给第 $i$ 个专家，并给出其原始权重。
*   **符号解释**：
    *   $g_{i,t}'$：第 $i$ 个专家在第 $t$ 步的**原始门控值**（在后续会归一化为最终权重）。
    *   $s_{i,t}$：第 $i$ 个专家在第 $t$ 步的**原始亲和力分数**（由公式15的Sigmoid计算得出）。
    *   $b_i$：第 $i$ 个专家的**可学习偏差项**，是本方法的核心。
    *   $N_r$：路由专家的总数。
    *   $K_r$：每个 Token 激活的路由专家数量（Top-K的值）。
    *   $Topk(...)$：从集合中选出值最大的 $K_r$ 个元素的函数。
*   **计算逻辑**：
    1.  **评分调整**：为每个专家的原始亲和力分数 $s_{i,t}$ 加上其对应的偏差 $b_i$，得到调整后的分数 $(s_{i,t} + b_i)$。
    2.  **竞争选择**：所有专家的调整后分数组成集合 ${s_{j,t} + b_j}$。系统从中选出分数最高的 $K_r$ 个专家。
    3.  **条件赋值**：
        *   **如果** 第 $i$ 个专家的调整后分数 $(s_{i,t} + b_i)$ 位于这 Top-K 集合内，则它被选中。此时，$g_{i,t}'$ 被赋予其**原始亲和力分数** $s_{i,t}$。
        *   **否则**，该专家未被选中，$g_{i,t}' = 0$。
*   **关键点**：
    *   偏差 $b_i$ **仅用于路由决策**（决定谁进入Top-K）。
    *   决定专家最终输出贡献的权重基础，仍然是**原始的、未加偏差的亲和力分数** $s_{i,t}$。这保证了模型能力不受平衡操作的影响。
    *   偏差 $b_i$ 本身在训练中根据专家负载动态调整：过载则 $b_i$ 减小，降低其被选中的概率；欠载则 $b_i$ 增大，增加其被选中的概率。

## 3.2 - 负载情况判断

DeepSeek-V3中判断专家**过载（overloaded）** 或**欠载（underloaded）** 的逻辑，其核心是基于在**每个训练步骤（step）的整个批次（batch）** 上，对**每个专家被激活（即被选入Top-K）的频率**进行实时监控和统计。

具体判断逻辑可推断如下：

### **1. 核心指标：专家的“负载”**
*   **负载的定义**：在一个训练步骤中，对于一个给定的专家，其**负载**可以理解为在该批次所有输入Token中，有多少个Token将它选入了自己的Top-K专家集合。简单说，就是**该专家在这个批次中被选中的总次数**。

### **2. 判断过载与欠载的基准**
系统需要判断一个专家的负载是“过高”还是“过低”，这需要一个**参考基准值**。虽然没有在图中明确给出公式，但业界常见的做法通常基于以下计算：
*   **理想平均负载**：`期望负载 = （批次中的总Token数 × 每个Token激活的专家数 K_r） / 路由专家总数 N_r`
    *   这个值代表了在**完全均匀分配**的理想情况下，每个专家平均应该处理的Token数量。
*   **判断逻辑**：
    *   **过载专家**：该专家在当前步骤的实际负载**持续**或**显著**高于这个理想平均负载。
    *   **欠载专家**：该专家在当前步骤的实际负载**持续**或**显著**低于这个理想平均负载。

### **3. 动态调整机制（如图所示）**
判断的目的是为了驱动对偏差项 $ b_i $ 的动态调整：
1.  **监控**：在每个训练步骤的前向传播过程中，系统记录下每个路由专家被选中的次数（负载）。
2.  **判断**：在步骤结束时，根据上述负载统计与基准的比较，判定每个专家处于“过载”还是“欠载”状态。
3.  **调整**：
    *   如果专家 $i$ 被判定为 **过载**，则将其偏差项 $b_i$ **减少** 一个固定的步长 $γ$（超参数，偏差更新速度）。
    *   如果专家 $i$ 被判定为 **欠载**，则将其偏差项 $b_i$ **增加** $γ$。

### **总结判断过程**
| 步骤 | 关键操作 | 说明 |
| :--- | :--- | :--- |
| **前向计算** | 统计负载 | 对于当前批次，记录每个专家被Top-K选中的总次数。 |
| **步骤结束时** | 计算与比较 | 将每个专家的实际负载与预期的平均负载（或其他阈值）进行比较。 |
| **步骤结束时** | 状态判定 | 判定该专家是 **过载**（负载过高）还是 **欠载**（负载过低）。 |
| **步骤结束时** | 更新偏差 | 根据判定结果，按规则增加或减少该专家的偏差项 $b_i$。 |

**简单来说**：判断的依据就是**看每个专家在当前一批数据里“活”干得太多还是太少**。干得太多（过载），下次就给它“减点分”（降低b_i），让它被选中的概率变小；干得太少（欠载），就给它“加点分”（提高b_i），鼓励更多Token选择它。通过这种持续的微调，最终实现专家间工作量的均衡分配。

## 3.3 - Complementary Sequence-Wise Auxiliary Loss

### **内容概括**

这段文字描述的是DeepSeek-V3模型训练中一个**辅助性的、精细化的正则化技术**。

尽管DeepSeek-V3的核心策略是前述的 **“无辅助损失负载均衡”**，该策略能有效平衡整个训练批次（batch）中所有专家的使用率。但作者们发现，这仍不能完全避免在**单个输入序列内部**出现极端不平衡的情况，即某个序列的所有Token可能都集中在少数几个专家上，导致序列内的专家利用率极低。

为了解决这个更细粒度的问题，论文提出并采用了一个**互补的序列级平衡损失**。这个损失的计算以单个序列为单位，其目标是鼓励模型在**每个独立的序列内**也能相对均衡地使用各个专家。值得注意的是，为了避免这个额外的损失函数干扰模型的主要学习目标，其权重系数α被设置为一个**极小的值**，因此它仅作为一种温和的、预防性的正则化手段。

### **要点总结**

1.  **定位与目的**：这是一个**补充性**的技术，旨在**防止单个序列内部的专家使用出现极端不平衡**，是对核心的“无辅助损失负载均衡”策略的补充和完善。
2.  **核心思想**：通过引入一个极低权重的辅助损失函数，对模型施加一个轻微的约束，鼓励其每个序列的Token能更均匀地“咨询”不同的路由专家。
3.  **关键特性**：
    *   **序列级别**：损失的计算基于单个序列，而不是整个批次。
    *   **极小权重**：平衡因子α被赋予一个**极小的值**，确保该损失仅作为微调，绝不会主导训练过程。
    *   **互补性**：它与主要的无辅助损失策略协同工作，分别从“批次全局”和“序列局部”两个层面优化负载均衡。

### **公式解释**

图片中给出了该损失的计算过程，其设计非常精巧，旨在度量单个序列内专家使用的均衡程度。

**公式 (17) - 总体平衡损失**
$$\mathcal{L}_{\text{Bal}}=\alpha\sum_{i=1}^{N_{r}}f_{i}P_{i}$$
*   **解读**：这是最终加到总损失函数中的平衡损失项。它对所有 \(N_r\) 个路由专家求和。乘积 \(f_i P_i\) 度量了专家 $i$ 在序列中**实际被使用的频率**与其**平均被选中的倾向性**之间的关系。


**公式 (18) - 专家使用频率**
$$f_{i}=\frac{N_{r}}{K_{r}T}\sum_{t=1}^{T}1\left(s_{i,t}\in\text{Topk}(\{s_{j,t}|1\leqslant j\leqslant N_{r}\},K_{r})\right)$$
*   **解读**：计算专家 $i$ 在**当前长度为T的序列**中的**实际使用频率**。
    *   $1(·)$ 是指示函数，当专家 $i$ 在第 $t$ 个Token的Top-K集合中时值为1，否则为0。
    *   $∑$ 求和后，得到专家 $i$ 在该序列中被选中的总次数。
    *   乘以 $N_r/(K_r T)$ 是为了进行**归一化**。$K_r T$ 可以理解为序列中总的“专家槽位”数。$f_i$ 的值越接近1，表示该专家的使用率越接近全局平均期望。

**公式 (19) - 归一化亲和力分数**
$$s_{i,t}^{\prime}=\frac{s_{i,t}}{\sum_{j=1}^{N_{r}}s_{j,t}}$$
*   **解读**：将原始亲和力分数 $s_{i,t}$ 在**当前序列的当前Token层面**进行归一化，得到 $s_{i,t}'$。这可以理解为，在第 $t$ 个Token处，专家 $i$ **相对于所有其他专家**的“相对吸引力”或“被选中的倾向性概率”。

**公式 (20) - 专家平均激活概率**
$$P_{i}=\frac{1}{T}\sum_{t=1}^{T}s_{i,t}^{\prime}$$
*   **解读**：计算专家 $i$ 在整个序列中**平均的、相对的被选中倾向性**。它是对序列中所有Token位置上的归一化亲和力分数 $s_{i,t}'$ 求平均。

## 3.4 - Node-Limited Routing

### **内容概括**

其核心思想是：对每个输入Token可以路由到的**物理节点数量施加一个硬性上限（M个）**，而不是允许它自由地发送到网络中的任何专家。具体的选择逻辑是，系统会为每个节点计算其内部所有专家的“最佳部分”的亲和力分数之和，然后根据这个总和来挑选出最佳的M个节点。通过这种**通信受限的路由机制**，结合精心设计的训练框架，系统能够将大部分必要的通信时间与计算过程重叠起来，从而近乎实现**完全的计算-通信重叠**，极大地提升了超大规模MoE模型的训练效率。

---

### **要点总结**

1.  **问题起源**：在分布式MoE训练中，Token需要根据路由结果被发送到存放对应专家的不同计算节点上，这个过程会产生大量**跨节点通信**，成为训练效率的主要瓶颈。
2.  **技术目标**：**显著限制和降低训练过程中的通信开销**。
3.  **核心机制**：
    *   **节点数量限制**：设定一个上限值 $M$，确保每个Token最多只被发送到 $M$ 个物理节点。
    *   **智能节点选择**：选择节点时，并非随机，而是基于一个聚合分数：对于一个候选节点，取出其内部部署的所有专家中，**亲和力分数最高的前 $K_r/M$ 个分数**，并将它们**求和**。这个总和代表了该节点对于当前Token的“潜在价值”。系统为所有节点计算这个值，并选择总分最高的 $M$ 个节点。
    *   **计算-通信重叠**：由于通信模式受到限制且可预测，训练框架能够提前调度数据传输，使其与GPU计算同时进行，从而隐藏了大部分通信延迟。
4.  **技术关联**：此技术是DeepSeek-V2中“设备限制路由”思想的延续与演进，专门针对超大规模训练集群进行了优化。
5.  **最终效果**：这是实现DeepSeek-V3**极致训练效率**（如前文提到的仅消耗278.8万GPU小时）的关键系统级优化之一，使得训练如此庞大的模型在经济上变得可行。

### **核心优势与原理总结**

| 方面 | 传统MoE分布式训练 | DeepSeek-V3的节点限制路由 | 带来的好处 |
| :--- | :--- | :--- | :--- |
| **通信模式** | **全连接式**：每个Token可能被路由到集群中任何节点上的专家，通信目标不可预测。 | **受限制式**：每个Token最多只访问$M$个节点，通信模式规整、可预测。 | **大幅减少通信量**，为优化调度奠定基础。 |
| **选择逻辑** | 基于**单个专家**的亲和力分数选择专家，专家所属节点是随结果决定的。 | 基于**节点级聚合分数**选择节点，节点内再选专家。更符合分布式系统的物理约束。 | **通信决策更高效**，优先保证最有价值的节点参与计算。 |
| **系统优化** | 计算与通信争抢资源，通信延迟直接增加训练耗时。 | 框架可实现**近乎完全的计算-通信重叠**，将通信时间“隐藏”起来。 | **极大提升训练吞吐率**，降低总体训练时间与成本。 |

## 3.5 - No Token-Dropping

### **内容概括**

这张图片强调了DeepSeek-V3模型在**训练和推理全流程中的一个重要优势：不丢弃任何Token**。

在许多混合专家模型中，为了保证处理效率（尤其是在专家负载不均时），常常会采用“Token丢弃”策略，即忽略或跳过对某些Token的路由计算。然而，DeepSeek-V3得益于其**有效的负载均衡策略**，在完整的训练过程中始终保持了良好的专家负载平衡。因此，**在训练阶段，模型无需丢弃任何Token**，确保了训练数据的完整性和模型学习的全面性。

此外，团队还通过实施**特定的部署策略**来保障推理时的负载平衡。因此，**在推理/服务阶段，DeepSeek-V3同样不会丢弃任何Token**，从而保证了每次预测都基于完整的输入信息，提升了输出的质量和可靠性。

---

### **要点总结**

**核心结论：DeepSeek-V3 在训练和推理中均实现了“零Token丢弃”。**

**支撑要点：**
1.  **训练阶段不丢Token**：
    *   **根本原因**：模型采用了**有效的负载均衡策略**（如前文详解的“无辅助损失负载均衡”与“互补序列级辅助损失”）。
    *   **带来的效果**：专家负载始终保持良好平衡，因此**无需**通过丢弃Token来缓解负载压力，保障了训练数据的充分利用。

2.  **推理阶段不丢Token**：
    *   **关键措施**：除了模型自身的负载平衡能力，团队还设计了**特定的部署策略**（如前文提到的“节点限制路由”等系统级优化）。
    *   **带来的效果**：在服务时也能维持高效的负载分配，从而**无需**在推理时丢弃Token，确保了每次生成都基于完整的上下文。

**意义总结：**
“零Token丢弃”是DeepSeek-V3设计先进性和高效性的一个综合体现。它意味着：
*   **更高的数据效用**：在训练和推理中均使用了100%的输入信息，无信息损失。
*   **更稳定的性能**：避免了因丢弃Token可能带来的输出质量波动或偏差。
*   **端到端的优化**：这一优势是模型算法（负载均衡策略）与系统工程（部署策略）协同设计、共同优化的结果。

## 4.0 - Multi-Token Prediction

![Multi-Token Prediction](https://ucc.alicdn.com/pic/developer-ecology/6ibaby6qg4ku4_5adf018e2c5942808d6dfee342942330.png?x-oss-process=image%2Fresize%2Cw_1400%2Cm_lfit%2Fformat%2Cwebp)

### **内容概括**

这样做主要有两个目的：一是**增密训练信号**，让模型从每个训练位置获取更丰富的监督信息，从而提高数据使用效率；二是**促使模型进行更长远的前瞻性规划**，使其构建的中间表示能更好地服务于对未来多个令牌的预测，从而潜在地提升模型的理解和生成能力。

DeepSeek-V3的实现方式与Gloeckle等人的原始方案存在关键区别：后者使用**并行的独立输出头**来同时预测多个未来令牌，而DeepSeek-V3则采用**顺序预测**的方式，并在每个预测深度都保持完整的因果链关系。这种设计选择旨在更好地利用和维持语言建模中的序列因果结构。

### **要点总结**

1.  **技术名称与来源**：多令牌预测，灵感来源于Gloeckle等人（2024）的工作。
2.  **核心思想**：改变标准语言模型（仅预测下一个令牌）的训练目标，让模型在每个位置都学习预测未来的多个令牌。
3.  **主要目的**：
    *   **提高数据效率**：每个训练位置提供更多监督信号，让模型学习得更高效。
    *   **增强表示能力**：迫使模型的内部表示（隐藏状态）为未来的多个预测步骤做好“规划”，从而学习到更具前瞻性和结构化的特征。
4.  **关键实现差异（DeepSeek-V3的创新点）**：
    *   **Gloeckle等人的方法**：**并行预测**。使用多个独立的输出头，同时预测第t+1, t+2, ..., t+D个令牌。
    *   **DeepSeek-V3的方法**：**顺序预测**。保持因果链，先基于当前位置预测t+1，然后用某种方式整合此预测来预测t+2，依此类推。这种方式更符合自回归生成的本质。

## 4.1 - MTP Modules

### **内容概括**

为了实现预测未来D个额外令牌的目标，模型设计了 **D个顺序连接的MTP模块**。每个模块（第k个）负责预测第 **i+k** 个未来令牌，其核心工作流程为：

1.  **输入组合**：对于当前位置 $i$，模块将 **主模型（或前一模块）在第 $i$ 个位置的表示 $h_i^{k-1}$** 与 **未来目标令牌 $t_{i+k}$ 的嵌入向量** 进行组合。
2.  **特征融合**：通过一个**线性投影矩阵 $M_k$** 将上述两个向量的归一化表示进行拼接和融合，生成中间表示 $h_i^{k}$。
3.  **特征转换**：将融合后的表示输入一个**独立的Transformer块 $TRM_k$** 进行深度处理，得到当前预测深度的最终表示 $h_i^k$。
4.  **令牌预测**：最后，将此表示输入一个**共享的输出头**，计算出词汇表上关于第 $i+k$ 个令牌的预测概率分布。

整个设计的关键在于：**嵌入层和输出头与主模型共享**，以保持统一的词汇空间；同时**每个深度有独立的Transformer块和投影矩阵**，以学习不同预测步长的特定特征。这种方法在结构上**保持了预测的因果链**（类似EAGLE模型），但其根本目的是**改进训练过程**，而非用于推理加速的推测解码。

---

### **要点总结**

1.  **模块架构**：
    *   使用 **D个顺序的MTP模块** 来预测D个未来令牌。
    *   第k个MTP模块包含：**共享嵌入层、共享输出头、独立的Transformer块 $TRM_k$、独立的投影矩阵 $M_k$**。

2.  **核心流程（三个核心公式）**：
    *   **组合与投影（公式21）**：将前一深度当前位置的表示 $h_i^{k-1}$ 与未来目标令牌 $t_{i+k}$ 的嵌入进行RMSNorm归一化后拼接，并通过矩阵 $M_k$ 投影融合。
      $$ \mathbf{h}_{i}^{\prime k}=M_{k}\left[\text{RMSNorm}(\mathbf{h}_{i}^{k-1}) ;\text{RMSNorm}(\text{Emb}(t_{i+k}))\right] $$
    *   **特征深度处理（公式22）**：将融合后的特征 $h_i^{k}$ 送入当前深度的独立Transformer块进行处理，得到该深度最终的上下文表示 $h_i^k$。
      $$ \mathbf{h}_{1:T-k}^{k}=\text{TRM}_{k}(\mathbf{h}_{1:T-k}^{\prime k}) $$
    *   **令牌预测（公式23）**：使用与主模型**共享的输出头**，将表示 $h_i^k$ 映射为词汇表上的概率分布，预测令牌 $t_{i+k}$。
      $$ P_{i+k+1}^{k}=\text{OutHead}(\mathbf{h}_{i}^{k}) $$

3.  **共享机制**：
    *   **嵌入层共享**：所有MTP模块的嵌入层与主模型相同，确保令牌表示一致性。
    *   **输出头共享**：所有MTP模块的输出头与主模型相同，确保预测空间一致性。

4.  **设计原则与对比**：
    *   **原则**：严格保持预测的**因果链**，即预测第k个未来令牌时，依赖于对前序位置的表示。
    *   **与EAGLE的区别**：结构相似，但**目标不同**。EAGLE主要用于推理时的**推测解码**以加速生成，而DeepSeek的MTP**专用于训练阶段**，旨在通过提供更密集的监督信号来提升模型能力。

5.  **输入与输出范围**：
    *   对于长度为 $T$ 的序列，在位置 $i$ 预测第 $k$ 个未来令牌时，Transformer块处理的输入序列为 $h_{1:T-k}^{k}$，确保了操作的合法性（不依赖未来信息）。

## 4.2 - MTP Training Objective

### **内容概括**

核心逻辑分为两步：
1.  **计算每个预测深度的损失**：针对第 $k$ 个MTP模块（负责预测第 $k$ 个未来令牌），计算一个标准的**交叉熵损失**。该损失衡量的是：从序列的特定起始位置开始，该模块在所有后续位置上对目标未来令牌的预测准确度。
2.  **聚合所有深度的损失**：将所有 $D$ 个预测深度（即D个MTP模块）计算出的损失进行平均，并乘以一个可调节的**权重因子 $λ$**，最终形成一个整体的、额外的训练目标 $ℒ_MTP$。

这个额外的损失项 $ℒ_MTP$ 将与语言模型主任务的标准损失（如下一个令牌预测损失）相结合，共同指导模型的训练。

---

### **要点总结**

#### **1. 单个深度的损失（公式24）**
$$ \mathcal{L}_{\text{MTP}}^{k} = -\frac{1}{T}\sum_{i=2+k}^{T+1}\log P_{i}^{k}[t_{i}] $$

*   **目的**：衡量第 $k$ 个MTP模块的预测质量。
*   **计算方式**：采用**交叉熵损失**。
*   **关键细节**：
    *   **求和范围**：$i$ 从 $2+k$ 到 $T+1$。这意味着在预测第 $k$ 个未来令牌时，计算损失是从当前序列的第 $(2+k)$ 个位置开始的。这确保了预测的**因果性**，即模型在位置 $i$ 预测 $t_{i}$ 时，只依赖于 $i$ 及之前的输入，不依赖未来信息。
    *   **$P_i^k[t_i]$**：表示由第 $k$ 个MTP模块给出的、在位置 $i$ 的表示上预测出的、对应真实令牌 $t_i$ 的概率。
    *   **平均**：对所有有效位置 ($T - (2+k) + 1$ 个位置) 的损失求和后，除以整个序列长度 $T$ 进行归一化，使得不同深度和序列长度的损失具有可比性。

#### **2. 整体MTP损失（公式25）**
$$ \mathcal{L}_{\text{MTP}} = \frac{\lambda}{D}\sum_{k=1}^{D}\mathcal{L}_{\text{MTP}}^{k} $$

*   **目的**：将多令牌预测的所有监督信号整合为一个单一的、可优化的目标。
*   **计算方式**：
    1.  **平均**：将所有 $D$ 个深度的损失 $ℒ_MTP^k$ 求和后除以 $D$，得到平均多令牌预测损失。
    2.  **加权**：乘以一个**权重因子 $λ$**。这是一个重要的超参数，用于控制多令牌预测目标在总损失中的相对重要性。$λ$ 通常被设置为一个小于1的值（例如0.1），以确保主要的学习目标（下一个令牌预测）仍占主导地位，而MTP作为辅助目标提供额外的、更丰富的训练信号。

### **核心意义总结**

| 方面 | 说明 |
| :--- | :--- |
| **训练机制** | **多任务学习**：模型同时学习“预测下一个令牌”的主任务和“预测未来多个令牌”的辅助任务，从同一份数据中获取更密集的监督信号。 |
| **设计目标** | **增强表示能力**：迫使模型的中间隐藏状态（$h_i$）不仅包含预测 $t_{i+1}$ 的信息，还要为预测 $t_{i+2}$， $t_{i+3}$， ... 等做好“规划”，从而学习到更具前瞻性和结构化的特征。 |
| **实现保障** | **因果性**：损失计算中严谨的求和范围确保了整个训练过程完全遵循自回归模型的因果约束。 |
| **控制灵活性** | **权重因子λ**：提供了调节辅助任务强度的“旋钮”，使研究团队能在增强能力与保持主任务焦点之间取得平衡。 |

## 4.3 - MTP in Inference

### **内容概括**

1.  **主要方案（直接弃用）**：由于 MTP 策略的核心目标是通过训练提升主模型自身的性能，因此在推理时，**完全可以丢弃所有 MTP 模块**。主模型自身已经具备了增强后的能力，可以独立、正常地运行，不会影响基础功能。
2.  **附加方案（改造利用）**：作为一个额外的、有价值的选项，这些已经训练好的 MTP 模块**可以被重新用于“推测解码”**。这是一种推理加速技术，可以利用轻量级的 MTP 模块快速草拟多个未来令牌，再由主模型进行验证，从而减少总体生成延迟。

---

### **要点总结**

| 要点 | 具体说明 |
| :--- | :--- |
| **1. 核心目的与定位** | MTP 主要是一个**训练阶段的技术**，旨在通过提供更密集的监督信号来**提升主模型（基座模型）的性能**。 |
| **2. 推理时的默认策略** | **直接弃用**。在模型部署和服务时，**可以移除所有 MTP 模块**，仅保留主模型。这不会影响模型的核心生成能力，证明了性能增益已固化在主模型参数中。 |
| **3. 推理时的可选优化** | **改造用于推测解码**。训练好的 MTP 模块可以作为**轻量级草稿模型**，在推理时与主模型配合，执行推测解码，以**降低生成过程的延迟（Latency）**，实现加速。 |
| **4. 设计优势** | 体现了**灵活性与实用性**：<br>• **无额外部署负担**：主模型可独立运行，不强制增加推理成本。<br>• **提供性能增益选项**：如需进一步优化推理速度，可利用现有模块进行加速，物尽其用。 |


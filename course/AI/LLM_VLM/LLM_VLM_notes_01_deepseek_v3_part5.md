本文主要整理《DeepSeek-V3 Technical Report》的主要内容。

## 9.0 - Inference and Deployment

### **内容概括**

部署基于与训练相同的 **H800 GPU集群**，充分利用其高速互联特性：节点内GPU通过**NVLink**互联，集群内所有GPU通过**InfiniBand（IB）** 网络全互联。

为了在线上服务中**同时满足低延迟的服务级别目标（SLO）和高吞吐量的要求**，团队采用了业界先进的 **“分离预填充与解码阶段”** 的部署策略。该策略将一次生成请求的计算清晰地划分为两个不同的阶段，并可能进行差异化的资源调度与优化。

### **要点总结**

| 方面 | 具体内容 | 目的与意义 |
| :--- | :--- | :--- |
| **1. 硬件基础设施** | 部署在 **H800集群** 上，网络拓扑与训练时一致：<br>• **节点内**：NVLink高速互联。<br>• **集群内**：InfiniBand全互联。 | 为推理提供高带宽、低延迟的通信保障，尤其有利于MoE模型所需的专家间数据交换。 |
| **2. 核心部署策略** | **分离预填充阶段与解码阶段**。 | **解决核心矛盾**：同时优化**SLO**和高**吞吐量**。<br>• **预填充**：处理用户的整个输入提示（Prompt），计算量大，可并行，对延迟有一定容忍度，适合批量处理以提高吞吐。<br>• **解码**：逐个生成输出令牌（Token），计算量相对小但次数多，对延迟极其敏感（直接影响用户体验）。 |
| **3. 策略优势** | 允许对两个阶段进行**差异化的资源分配和优化**：<br>• 可为解码阶段预留专用、低负载的资源以确保低延迟。<br>• 预填充阶段可进行大规模的动态批处理以提高硬件利用率。 | 在统一的硬件平台上，实现**服务质量（延迟）与资源效率（吞吐）的最佳平衡**，是生产级大模型服务的标准且关键的做法。 |

## 9.1 - Prefilling

### **内容概括**

预填充阶段负责处理用户输入的整个提示（Prompt），其设计目标是**在保证效率的前提下最大化系统吞吐量**。

为实现这一目标，部署采用了多层并行的复合策略：
1.  **最小部署单元**：由**4个节点**（共**128个GPU**）构成。
2.  **注意力部分并行**：采用**4路张量并行** 结合 **8路数据并行**。较小的TP维度（4）有效限制了张量并行带来的通信开销。
3.  **MoE部分并行**：采用**32路专家并行**，确保每个专家都能处理足够大的批次尺寸，从而提升计算效率。其跨节点通信流程（先IB，后NVLink转发）与训练时保持一致。
4.  **特殊优化**：对浅层的稠密MLP层使用**1路张量并行**，以节省不必要的通信。

### **要点总结**

| 优化维度 | 具体策略 | 核心目的与效果 |
| :--- | :--- | :--- |
| **1. 硬件与并行基础** | • **最小单元**：4节点，128 GPU。<br>• **注意力部分**：TP4 + DP8。<br>• **MoE部分**：EP32。<br>• **通信路径**：跨节点用IB，节点内用NVLink。 | • 奠定高吞吐基础。<br>• **平衡计算与通信**：小TP减少开销，大EP提升计算效率。<br>• 复用训练期优化，降低工程复杂性。 |
| **2. 负载均衡策略** | **冗余专家部署**：<br>• 根据在线统计，定期（如10分钟）检测高负载专家。<br>• 复制高负载专家，将其冗余部署在不同GPU上。<br>• 每台GPU除原有8个专家外，额外托管**1个冗余专家**（共9个）。<br>• 在节点内精心重排专家分布，以平衡负载。 | • **解决MoE推理核心难题**：防止热门专家成为瓶颈。<br>• **实现GPU间负载均衡**，最大化集群整体利用率。<br>• **动态自适应**，能响应请求分布的变化。 |
| **3. 微批次与通信隐藏** | **同时处理两个相似计算量的微批次**：<br>让一个微批次的**注意力/MoE计算**与另一个微批次的**All-to-All通信**操作相互重叠。 | • **隐藏通信延迟**，提升硬件利用率。<br>• 进一步提高系统吞吐量。 |
| **4. 前瞻性探索策略** | **动态冗余策略**：<br>• 每台GPU物理部署更多专家（如16个）。<br>• 每次推理时，根据实时计算的**全局最优路由方案**，仅激活其中的一部分（如9个）。 | • 在计算开销可忽略（因预填充计算量大）的前提下，**追求极致的负载均衡与资源灵活性**。<br>• 代表了未来MoE服务部署的潜在进化方向。 |

## 9.2 - Decoding

### **内容概括**

这两张图片系统地阐述了DeepSeek-V3推理服务中**解码阶段**的部署架构、核心挑战与优化技术。解码阶段负责模型的自回归逐令牌生成，其核心设计目标是**在保证极低延迟的前提下，尽可能提升系统吞吐量**。

为实现这一目标，系统采用了大规模的部署单元和精细的资源调度：
*   **最小部署单元**：**40个节点，共320个GPU**，规模远大于预填充阶段。
*   **并行策略**：注意力部分结合**张量并行**、**序列并行**与**数据并行**；MoE部分则采用大规模的**专家并行**，确保每个GPU仅承载一个专家，以最大化单个专家的处理效率。
*   **核心挑战与均衡策略**：面对MoE负载不均问题，系统定期根据线上统计确定**冗余专家集**，由64个专用GPU承载，并正在探索更灵活的**动态冗余策略**。
*   **性能优化**：为隐藏通信开销并提升吞吐，系统探索了**双微批次重叠处理**技术：将一个微批次的**注意力计算**与另一个微批次的 **`DISPATCH`、`MoE`、`COMBINE`** 过程重叠，充分利用了解码阶段注意力耗时较长的特点。

### **要点总结**

| 维度 | 核心策略 | 目的与效果 |
| :--- | :--- | :--- |
| **1. 部署规模与专家路由** | • **最小单元**：40节点，320 GPU。<br>• **路由策略**：每个Token选择**9个专家**，其中包括**1个必定被选的共享专家**（被视作高负载专家）。 | • 大规模部署为高并发、低延迟服务提供资源基础。<br>• 共享专家的固定选择简化了路由决策，确保了基础能力的稳定提供。 |
| **2. 并行架构设计** | • **注意力部分**：`TP4` + `SP` + `DP80`。<br>• **MoE部分**：`EP320`，**每GPU仅1个专家**。<br>• **通信优化**：`DISPATCH/COMBINE` 通过 **IB点对点直连** 和 **IBGDA技术** 实现超低延迟。 | • `TP/SP`处理注意力计算，`DP`处理大量并发请求。<br>• 极致的专家并行度（EP320）匹配了解码阶段专家计算轻量化的特点，减少了单个GPU的资源争抢。<br>• 专用高速通信技术是保障低延迟的关键。 |
| **3. 负载均衡机制** | • **定期静态冗余**：基于在线负载统计，定期（如几分钟）在**64个专用GPU**上确定并部署冗余的热门专家副本。<br>• **探索动态冗余**：研究更灵活的算法，以在每次推理时动态计算全局最优路由方案（即动态选择激活哪些专家）。 | • 静态冗余有效缓解了专家负载不均，是当前生产就绪的方案。<br>• 动态冗余代表了前沿探索，旨在实现极致的负载均衡和资源利用率，但其算法与内核融合的开销需要精心优化。 |
| **4. 吞吐量优化技术** | **双微批次计算-通信重叠**：<br>利用解码阶段**注意力计算占主导**的特点，让一个微批次的“注意力计算”与另一个微批次的“通信+MoE计算+结果组合”过程**重叠执行**。 | • **隐藏All-to-All通信延迟**。<br>• 提升GPU整体利用率，从而在保证低延迟的同时，**显著提高系统吞吐量**。由于每个专家批次小，MoE部分计算轻，仅需分配少量SM即可，不会拖慢关键的注意力计算。 |

## 10.0 - Communication Hardware

### **内容概括**

本节阐述了DeepSeek-V3在**实现计算与通信重叠以隐藏延迟方面**所取得的效果，**分析了当前基于GPU流式多处理器实现此功能的局限性**，并在此基础上提出了对**未来理想通信硬件的展望**。

当前，团队通过精细调度，利用部分昂贵的流式多处理器执行通信任务，成功将计算与通信重叠，显著降低了对纯通信带宽的依赖。然而，这种方法占用了本应用于核心计算的硬件资源（如H800 GPU中宝贵的SM），限制了整体计算吞吐量，并造成了张量核心的闲置。

因此，文章呼吁硬件供应商未来能开发一种专用的**协处理器**，将文中列举的几类繁重通信任务从通用计算单元中彻底卸载。同时，该硬件应能为计算单元提供一个**统一的网络视图**，简化跨异构网络（如IB与NVLink）的编程复杂性。

### **要点总结**

| 方面 | 核心内容 |
| :--- | :--- |
| **1. 当前成就与核心策略** | **实现了计算与通信的重叠**，成功隐藏了通信延迟，降低了对绝对通信带宽的依赖。 |
| **2. 现有方案的代价与局限** | • **资源占用**：依赖**昂贵的流式多处理器**来执行通信任务（如在H800上分配了20/132个SM）。<br>• **核心矛盾**：这**限制了计算吞吐量**，并导致**张量核心利用率不足**，因为本应用于计算的SM被通信任务占用。 |
| **3. 当前SM承担的通信任务** | 主要包括：<br>• **域间数据转发**：在IB网络与NVLink域之间转发并聚合数据。<br>• **缓冲区数据传输**：在RDMA缓冲区与计算缓冲区之间搬运数据。<br>• **归约操作执行**：执行All-to-All通信中“组合”阶段的归约运算。<br>• **细粒度内存管理**：管理跨IB/NVLink域、面向多个专家的分块数据传输中的内存布局。 |
| **4. 对未来硬件的愿景** | • **硬件卸载**：希望未来能有类似**NVIDIA SHARP**的**专用协处理器**（GPU协处理器或网络协处理器），将上述通信任务从SM中完全卸载。<br>• **网络统一**：从计算单元视角，**统一IB（跨节点）和NVLink（节点内）网络**，提供一个单一、简化的接口。<br>• **简化编程**：通过该统一接口，计算单元能轻松地通过提交基于简单原语（如读、写、组播、归约）的请求，来完成跨整个统一网络域的操作。 |

## 10.1 - Higher FP8 GEMM Accumulation Precision in Tensor Cores

### **内容概括**

本节指出了当前（Hopper架构）**NVIDIA Tensor Core在执行FP8 GEMM运算时存在的硬件级精度限制**，并介绍了当前的一种软件缓解方案及其局限性。

文档首先点明问题：在Hopper架构的Tensor Core中，FP8矩阵乘加运算的**累加精度被限制在大约14位**。其具体流程是，在对齐32个尾数乘积后，仅使用每个乘积的最高14位进行加法运算，并将结果以14位精度存入累加寄存器。这种限制在大规模累加（K维度大）时会引入不可忽略的数值误差。

为部分解决此问题，文档提及了一种**通过在CUDA Core中使用FP32精度寄存器来累积128次FP8乘法的结果**的软件实现方法。该方法虽有助于实现成功的FP8训练，但被明确指出这仅仅是针对Hopper架构硬件缺陷的一种妥协方案。文档最后呼吁，未来的芯片需要采用更高精度的累加器来从根本上解决此问题。

### **要点总结**

| 要点 | 具体说明 |
| :--- | :--- |
| **1. 核心问题** | **Hopper架构Tensor Core的FP8 GEMM累加器精度有限**，仅使用约**14位**进行中间结果的加法和存储，导致高精度损失。 |
| **2. 问题根源** | 硬件设计如此：在对齐指数后，**仅截取每个尾数乘积的最高14位用于累加**，超出的低位直接被舍弃。 |
| **3. 当前解决方案** | 一种软件层面的部分缓解方案：在Tensor Core完成小规模乘加后，**将128次乘法的累积结果转移到CUDA Core的FP32寄存器中进行高精度累加**。 |
| **4. 方案定位** | 该方案被定义为针对**硬件缺陷的妥协**，虽有效但非根本解决之道。 |
| **5. 未来期望** | **下一代芯片需在硬件层面提供更高精度的累加器**，以从根本上保障FP8计算的数值精度。 |

### **与您之前阅读内容的关联**

此图片内容恰好为DeepSeek-V3论文中 **“提升至CUDA Core的高精度累加”** 一节提供了**底层的硬件原理说明和背景**。
*   **问题对应**：论文中指出的“约14位累加精度”限制，其根源正是本图片所描述的Hopper Tensor Core的硬件设计。
*   **方案对应**：论文中描述的将部分和转移至CUDA Core用FP32累加的方法，正是本图片提到的软件缓解方案的具体工程实现。
*   **观点一致**：两者都认为当前方案是妥协，并隐含了对未来硬件改进的期待。

## 10.2 - Support for Tile- and Block-Wise Quantization

### **内容概括**

该图片从硬件支持层面，指出了当前GPU在实现如DeepSeek-V3所采用的**瓦片级（Tile-wise）和块级（Block-wise）细粒度量化**时所面临的瓶颈，并提出了对未来芯片的改进建议。

图片指出，**当前的GPU仅原生支持张量级（per-tensor）量化**，即整个张量使用单一的缩放因子。这无法直接支持更先进的细粒度量化。在现有实现中，当累积达到 `Nc` 间隔时，部分结果需要从**张量核心（Tensor Cores）** 复制到 **CUDA核心**，在那里乘以缩放因子并进行高精度（FP32）累加。尽管结合高精度累加策略减轻了反量化的开销，但**张量核心与CUDA核心之间频繁的数据移动**，仍然严重限制了整体的计算效率。

因此，图片建议，未来的芯片应通过在**张量核心内部**支持接收缩放因子并实现**带组缩放的矩阵乘加（MMA）运算**，来原生支持细粒度量化。这样，从部分和累加到反量化的整个计算流程都可以在张量核心内部高效完成，从而避免昂贵的数据移动，大幅提升计算效率。

### **要点总结**

| 方面 | 当前状况与挑战 | 未来愿景与建议 |
| :--- | :--- | :--- |
| **1. 硬件支持现状** | GPU仅原生支持**张量级量化**，缺乏对**瓦片/块级（细粒度）量化**的硬件加速。 | 未来芯片的**张量核心应能直接接收和处理分组缩放因子**，实现对细粒度量化的原生支持。 |
| **2. 现有实现瓶颈** | 为了实现细粒度量化，必须在`Nc`间隔点，将部分结果从**Tensor Cores** 复制到 **CUDA Cores** 进行缩放与高精度累加。 | 应将**整个部分和累加及反量化过程完全集成在张量核心内部**完成，直到产出最终结果。 |
| **3. 核心性能限制** | **频繁的Tensor Core与CUDA Core间的数据移动**成为主要性能瓶颈，限制了计算效率的进一步提升。 | 通过上述集成，**彻底消除这种低效的数据移动**，让张量核心能够持续、高效地工作。 |
| **4. 关键技术特性** | 计算流被人为分割，涉及不同处理单元（专用计算单元 vs 通用计算单元）间的协同。 | 张量核心需要支持 **“带组缩放的矩阵乘加（MMA with group scaling）”** 操作，这是实现集成的关键。 |

### **深层解读：揭示系统性瓶颈**

此图片内容与您先前讨论的 **“细粒度量化”** 和 **“高精度累加”** 技术紧密相连，并揭示了其**在现有硬件上实现的根本性约束**：
*   **细粒度量化**是一项先进的算法，但受限于硬件，其实现被迫采用了“计算-搬运-再计算”的低效模式。
*   **高精度累加**是解决Tensor Core内部精度缺陷的必要方案，但同样加剧了数据搬运的开销。

## 10.3 - Support for Online Quantization

### **内容概括**

**核心问题**在于流程割裂导致的内存带宽浪费：当前的实现需要先从高带宽内存中读取BF16格式的激活值，在计算单元中量化为FP8，再将量化后的FP8值写回HBM，最后为了执行矩阵乘累加又需要重新读取这些FP8数据。这个“读-转-写-再读”的过程造成了**冗余的内存读写操作**，严重制约了性能。

为此，图片从三个层面提出了硬件优化建议：1）通过**指令融合**将量化与数据搬运合并；2）增加**细粒度类型转换指令**以优化特定计算图融合；3）探索**近内存计算**架构，从根本上减少片外数据访问。这些建议旨在将量化操作“隐身”在数据流动的过程中，从而释放内存带宽和计算潜力。

### **要点总结**

| 层面 | 问题 / 现状 | 优化建议与愿景 |
| :--- | :--- | :--- |
| **根本问题** | **冗余内存访问**：支持在线量化需要 **“读取(BF16)→量化(FP8)→写回(HBM)→再次读取(FP8)”** 的流程，导致**内存带宽成为瓶颈**。 | 从芯片架构和指令集层面进行优化，**将量化过程与数据移动过程深度融合甚至消除**。 |
| **建议一：指令与搬运融合** | 量化（类型转换）与数据搬运（通过TMA）是**两个独立的操作**。 | **将FP8类型转换与TMA内存访问融合为单一操作**，使量化能在数据从全局内存传输到共享内存的过程中完成，避免额外读写。 |
| **建议二：细粒度指令支持** | 缺乏高效的细粒度类型转换指令，阻碍了像“层归一化+量化”这类计算的算子融合优化。 | **支持Warp级别的类型转换指令**，以便更好地将归一化等操作与量化转换融合，提升局部计算效率。 |
| **建议三：架构级革新** | 数据必须在GPU计算核心和HBM之间来回移动，量化计算占用核心资源并消耗带宽。 | **采用近内存计算**：将量化计算逻辑放置在HBM内存附近。数据从HBM读入GPU芯片时，**直接以FP8格式转换并传输**，预计可减少约**50%的片外内存访问**。 |

## 10.4 - Support for Transposed GEMM Operations

### **内容概括**

该图片指出，在当前的GPU架构下，将**矩阵转置操作**与核心的**GEMM（通用矩阵乘法）运算**高效地融合在一起非常困难且流程繁琐，这成为量化工作流中的一个性能瓶颈。

文中以DeepSeek-V3的工作流程为例说明了这个问题：在前向传播时，激活值被量化为 `1x128` 的FP8瓦片并存储。然而，在反向传播阶段，为了计算梯度，需要将这些矩阵从内存中读出、进行反量化、执行转置、再重新量化为 `128x1` 的瓦片，最后写回高带宽内存（HBM）。这一系列操作产生了大量冗余的内存访问。

因此，文章建议未来的芯片应在执行矩阵乘加（MMA）操作之前，支持**从共享内存中直接以转置的方式读取矩阵**（对于训练和推理中都需要的精度）。结合之前提出的 **“FP8格式转换与TMA访问相融合”** 的优化，这一增强将能显著简化和加速整个量化工作流程。

### **要点总结**

| 层面 | 当前问题与挑战 | 未来硬件优化建议 |
| :--- | :--- | :--- |
| **核心问题** | **矩阵转置与GEMM融合困难**：当前的硬件和软件流程无法高效地将这两个操作合并，导致需要多次显式的内存读写和格式转换。 | **支持直接的转置读取**：使芯片能够在执行MMA前，直接从共享内存中读取已转置的矩阵数据，省去显式的转置操作和关联的内存搬运。 |
| **具体工作流瓶颈** | 在反向传播中，处理先前缓存的 `1x128` FP8激活瓦片时，流程为：<br>**读取(HBM) → 反量化 → 转置 → 重新量化(为128x1) → 写回(HBM)**。<br>这是一个**内存密集型**的繁琐过程。 | 理想的新工作流：<br>通过融合的 **“TMA访问+格式转换”** 指令，直接从全局内存读取BF16数据并在线量化为FP8瓦片进入共享内存；随后，MMA单元能**直接以转置布局**从共享内存读取数据进行计算。 |
| **优化本质** | 将本应紧密耦合的**数据重排（转置）** 与**核心计算（GEMM）** 在硬件层面解耦，迫使在软件层面进行低效的显式处理。 | 在内存子系统（共享内存）与计算单元（Tensor Core）的接口层面，**增加对数据布局的灵活感知和支持**，实现计算友好的数据供给。 |
| **协同效应** | 单独解决转置或量化的问题，都只能带来局部优化。 | 将 **“转置读取”** 与 **“在线量化融合”** 两项建议结合，能**从数据搬运的源头到计算的核心，系统性地重塑和简化整个数据通路**，释放最大性能潜力。 |

## 11.0 - Pre-Training Data Construction

### **内容概括**

此章节详细阐述了 DeepSeek-V3 预训练数据集的构建方法，在 DeepSeek-V2 的基础上进行了多方面的优化与改进。

核心优化包括：**提升数学与编程数据的比例**以增强模型在 STEM 领域的推理能力；**扩展多语言覆盖范围**，使其不局限于中英文；以及**精炼数据处理流水线**，在保持多样性的同时减少冗余。

在技术实现上，模型采用了 **“文档打包”** 技术以保证数据完整性（但未引入跨样本注意力掩码），并引入了 **“填空中间”** 策略来提升模型基于上下文理解与生成的能力。分词器方面，采用了改进的**字节级BPE分词器**，并通过随机拆分特定标记来**缓解因标点与换行符合并引入的“标记边界偏置”问题**。最终，用于预训练的数据集总计包含 **14.8万亿个高质量、多样化的 Token**。

### **要点总结**

| 优化方面 | 具体策略与内容 | 目的与意义 |
| :--- | :--- | :--- |
| **1. 数据构成与质量** | • **提升比例**：增加**数学和编程**样本的占比。<br>• **扩展语言**：覆盖**超越中英文**的更多语言。<br>• **精炼流水线**：减少冗余，保持多样性。 | 构建**高质量、多样化**的语料库，针对性增强模型的**推理能力**与**多语言泛化能力**。 |
| **2. 训练策略** | • **文档打包**：借鉴文献方法，保持文档完整性。<br>• **填空中间**：采用 **PSM框架**，以10%的概率在文档级别应用FIM策略，将文档结构化为 `<前缀><掩码><后缀><中间内容>` 的格式。 | • **文档打包**：确保模型学习到完整、连贯的上下文。<br>• **FIM策略**：赋予模型根据任意上下文（前缀和后缀）预测中间内容的能力，**增强其代码补全和文本填充的推理技能**，且不损害其标准的语言建模能力。 |
| **3. 分词器** | • **算法**：采用 **字节级BPE**，词表扩展至 **128K**。<br>• **优化**：改进预分词器和训练数据，提升多语言压缩效率。<br>• **解决偏置**：引入合并标点与换行符的标记后，通过**随机拆分此类组合标记**的方式训练模型，以缓解“标记边界偏置”。 | • **大词表与字节级**：平衡压缩效率与泛化能力，更好地处理多语言和生僻词。<br>• **缓解边界偏置**：确保模型在处理**未以换行符结尾的多行提示**（如少样本评估提示）时，不会因分词方式而产生性能偏差，提升评估的公平性与鲁棒性。 |
| **4. 数据规模** | 最终构建的预训练语料包含 **14.8T (万亿) Token**。 | 为训练一个拥有 671B 参数的 MoE 模型提供了充足、高质量的数据基础。 |

### **数据结构详解**
图片中的结构 `<|fim_begin|>f_pre<|fim_hole|>f_suf<|fim_end|>f_middle<|eos_token|>` 可以拆解为：

1.  **`<|fim_begin|>`**
    *   **含义**：**“Fill-in-the-Middle Begin”** 的标记。它向模型宣告：一个特殊的“填空”训练样本开始了。

2.  **`f_pre`**
    *   **含义**：**前缀（Prefix）**。这是模型在生成时可以看见的**第一部分上下文**。

3.  **`<|fim_hole|>`**
    *   **含义**：**“空洞”标记**。它象征着这里有一个缺失的部分，直接提示模型：“请注意，后面提供的信息（后缀）是这段文本的结尾，你需要根据开头和结尾来补全中间。”

4.  **`f_suf`**
    *   **含义**：**后缀（Suffix）**。这是模型在生成时可以看见的**第二部分上下文**，即文本的结尾部分。

5.  **`<|fim_end|>`**
    *   **含义**：**“Fill-in-the-Middle End”** 的标记。它告诉模型：“关于任务描述的上下文（前缀+后缀）已经提供完毕。”

6.  **`f_middle`**
    *   **含义**：**中间部分（Middle）**。这就是模型需要学习预测的**目标内容**。在训练时，模型的任务就是根据 `f_pre` 和 `f_suf` 来生成 `f_middle`。

7.  **`<|eos_token|>`**
    *   **含义**：**句子结束标记**。标志整个训练样本的终结。

## 11.1 - Model Hyper-Parameters

### **内容概括**

这段文字详细列出了DeepSeek-V3核心模型架构的各项关键超参数。模型主体是一个61层的Transformer，隐藏维度为7168。其核心创新组件——**多头潜在注意力** 和 **DeepSeekMoE**——的参数被具体设定：MLA使用128个头，并设定了KV压缩、查询压缩及解耦查询/键的维度；除前三层外，所有前馈网络均被MoE层替代，每层包含1个共享专家和256个路由专家，每个Token激活其中6个，并确保最多发送至4个节点。此外，模型采用了多Token预测（深度D=1）等增强训练效率的技术。在此配置下，模型总参数量达6710亿，但每个Token实际仅激活370亿参数，实现了容量与效率的平衡。

### **要点总结**

以下为模型关键配置的详细列表：

| 模块/组件 | 参数名称 | 设定值 | 说明与意义 |
| :--- | :--- | :--- | :--- |
| **整体架构** | Transformer层数 | 61 | 模型的深度。 |
| | 隐藏维度 | 7168 | 每层主干特征的维度。 |
| | 参数初始化标准差 | 0.006 | 所有可学习参数的随机初始化范围。 |
| **多头潜在注意力** | 注意力头数 | 128 | MLA中的总头数。 |
| | 每头维度 | 128 | 每个注意力头的特征维度。 |
| | KV压缩维度 | 512 | 键和值向量被压缩到的潜在空间维度，用于减少KV缓存。 |
| | 查询压缩维度 | 1536 | 查询向量被压缩到的潜在空间维度。 |
| | 解耦查询/键维度 | 64 | 独立的、携带旋转位置编码的查询和键分量的维度。 |
| **混合专家** | MoE替换范围 | 除第1-3层外的所有FFN | 模型的主要计算密度由MoE层贡献。 |
| | 每层专家构成 | 1个共享专家 + 256个路由专家 | 共享专家处理通用特征，路由专家处理特定特征。 |
| | 专家中间维度 | 2048 | 每个专家前馈网络内部层的维度。 |
| | 每Token激活专家数 | 6 | 每个Token选择并激活的Top-K路由专家数量。 |
| | 每Token最多发送节点数 | 4 | 跨节点专家并行中，单个Token被路由到的物理节点上限，以优化通信。 |
| **训练目标** | 多Token预测深度 | 1 | 除预测下一个Token外，额外预测未来一个Token，以增强训练信号。 |
| **其他技术** | 附加RMSNorm与缩放因子 | 已采用 | 在压缩潜在向量后使用RMSNorm，并在宽度瓶颈处乘以缩放因子，以稳定训练。 |
| **规模统计** | 总参数量 | 671B (6710亿) | 模型所有参数的总和，体现模型总体容量。 |
| | 每Token激活参数量 | 37B (370亿) | 处理每个Token时实际参与计算的前向传播参数，决定推理成本与效率。 |

**总结**：此配置表清晰地展现了DeepSeek-V3作为一个**高效的混合专家模型**的设计精髓：通过**MLA**节省KV缓存，通过**MoE**在保持极高总容量（671B）的同时，将实际计算量控制在较低水平（37B/Token）。各项参数（如激活6个专家、发送至最多4个节点）均体现了其在模型性能、计算效率与分布式通信开销之间取得的精密平衡。

## 11.2 - Training Hyper-Parameters

### **内容概括**

第一张图片详细列出了 **DeepSeek-V3 模型的训练超参数**，涵盖了优化器设置、学习率调度、批次大小策略以及模型并行与路由相关的关键配置。这些参数共同构成了其稳定、高效训练14.8万亿Token的基础。

第二张图片展示了 **DeepSeek-V3 在“大海捞针”测试中对128K长上下文处理能力的压力评估结果**。热力图显示，模型在各种上下文长度和文档深度下均表现稳定且优异，验证了其扩展上下文窗口技术的有效性。

### **要点总结**

#### **1. 训练超参数 (第一张图)**
*   **优化器**: AdamW (`β1=0.9, β2=0.95`, 权重衰减 `0.1`)。
*   **学习率调度 (复杂的三段式)**:
    1.  **热身**: 前2K步从0线性增至 `2.2e-4`。
    2.  **平台期**: 保持 `2.2e-4` 直至10T Token。
    3.  **衰减期**: 4.3T Token内余弦衰减至 `2.2e-5`。
    4.  **收尾**: 最后500B Token中，先保持 `2.2e-5` (333B)，再切换至 `7.3e-6` (167B)。
*   **其他关键设置**:
    *   **梯度裁剪**: 阈值设为 `1.0`。
    *   **批次大小**: 从 `3072` 逐步增至 `15360`，并在后期保持。
    *   **并行与路由**: 采用管道并行；专家均匀部署在8节点64 GPU上；每个Token最多路由至4个节点 (`M=4`)。
    *   **损失权重**:
        *   负载平衡偏差更新速度 `γ`: 前14.3T Token为 `0.001`，后500B为 `0.0` (停止调整)。
        *   序列内平衡损失权重 `α`: 设为极小的 `0.0001`，仅防极端不平衡。
        *   多令牌预测损失权重 `λ`: 前10T Token为 `0.3`，后4.8T降为 `0.1`。

#### **2. 长上下文能力评估 (第二张图)**
*   **测试方法**: “大海捞针”测试，用于评估模型在长上下文中精确检索信息的能力。
*   **评估维度**: 横轴为**上下文长度** (2K 至 128K)，纵轴为**文档深度百分比** (0% 到 100%)，颜色代表性能分数 (1-10分)。
*   **核心结论**: 热力图颜色分布**均匀且整体为高分段**，表明 **DeepSeek-V3 在从短到超长（128K）的各种上下文窗口下，均能稳定、可靠地提取信息，性能无明显衰减**。这直接证明了其长上下文扩展技术的成功。

## 11.3 - Long Context Extension

### **内容概括**

该图片详细描述了DeepSeek-V3模型在完成预训练后，为支持超长文本输入而进行的 **“长上下文扩展”训练方法**。此方法延续了DeepSeek-V2的策略，基于 **YaRN技术**，通过**两个独立的额外训练阶段**，分步将模型的上下文处理能力从**4K逐步扩展到32K，最终达到128K**。

整个扩展过程的核心在于保持YaRN的关键超参数不变，仅调整**序列长度**和**批次大小**以适应不同阶段的训练需求，并将**学习率**维持在预训练结束时的低水平。经过此两阶段扩展训练并完成后续的监督微调后，DeepSeek-V3不仅具备了处理128K长度输入的能力，更在著名的 **“大海捞针”测试中，表现出了跨越不同上下文长度的优异鲁棒性**。

### **要点总结**

| 方面 | 具体内容与参数 | 目的与意义 |
| :--- | :--- | :--- |
| **1. 核心技术基础** | 采用与DeepSeek-V2类似的 **YaRN** 方法进行扩展。 | 延续已验证有效的长上下文扩展技术路线，确保技术可靠性。 |
| **2. 训练阶段设计** | 分为**两个阶段**，每阶段各**1000步**：<br>• **第一阶段**：从 4K → **32K**<br>• **第二阶段**：从 32K → **128K** | **渐进式扩展**，避免直接从短上下文跳至超长上下文导致的训练不稳定或灾难性遗忘。 |
| **3. 关键YaRN配置** | • 应用范围：**仅作用于解耦的共享键 \( k^R_i \)**。<br>• 超参数：尺度 \( s=40 \), \( \alpha=1 \), \( \beta=32 \)。<br>• 缩放因子：\( \sqrt{t} = 0.1 \ln s + 1 \)。 | **保持扩展过程的一致性**，所有参数在两个阶段中完全相同，简化训练流程。 |
| **4. 阶段参数调整** | • **第一阶段**：序列长度 **32K**，批次大小 **1920**。<br>• **第二阶段**：序列长度 **128K**，批次大小 **480**。 | 适应不同长度下的显存占用与训练效率。批次随序列长度增加而减少，是标准的权衡策略。 |
| **5. 学习率设置** | 两个阶段的学习率均设为 **\( 7.3 \times 10^{-6} \)**。 | 与**预训练最终阶段的学习率完全一致**。这确保了扩展训练是在模型已经收敛的稳定基础上进行的**精细微调**，而不是重新学习，有效保护了预训练获得的能力。 |
| **6. 最终能力验证** | 扩展并经过监督微调后，模型通过了 **“Needle In A Haystack”测试**。 | **实证检验**：证明DeepSeek-V3不仅支持128K的**理论长度**，更具备在全文任意位置**精确检索和利用信息**的**实际能力**，且性能在不同长度下表现稳定。 |


本文主要整理《DeepSeek-V3 Technical Report》的主要内容。

## 11.0 - Data Construction

### **内容概括**

此章节详细阐述了 DeepSeek-V3 预训练数据集的构建方法，在 DeepSeek-V2 的基础上进行了多方面的优化与改进。

核心优化包括：**提升数学与编程数据的比例**以增强模型在 STEM 领域的推理能力；**扩展多语言覆盖范围**，使其不局限于中英文；以及**精炼数据处理流水线**，在保持多样性的同时减少冗余。

在技术实现上，模型采用了 **“文档打包”** 技术以保证数据完整性（但未引入跨样本注意力掩码），并引入了 **“填空中间”** 策略来提升模型基于上下文理解与生成的能力。分词器方面，采用了改进的**字节级BPE分词器**，并通过随机拆分特定标记来**缓解因标点与换行符合并引入的“标记边界偏置”问题**。最终，用于预训练的数据集总计包含 **14.8万亿个高质量、多样化的 Token**。

### **要点总结**

| 优化方面 | 具体策略与内容 | 目的与意义 |
| :--- | :--- | :--- |
| **1. 数据构成与质量** | • **提升比例**：增加**数学和编程**样本的占比。<br>• **扩展语言**：覆盖**超越中英文**的更多语言。<br>• **精炼流水线**：减少冗余，保持多样性。 | 构建**高质量、多样化**的语料库，针对性增强模型的**推理能力**与**多语言泛化能力**。 |
| **2. 训练策略** | • **文档打包**：借鉴文献方法，保持文档完整性。<br>• **填空中间**：采用 **PSM框架**，以10%的概率在文档级别应用FIM策略，将文档结构化为 `<前缀><掩码><后缀><中间内容>` 的格式。 | • **文档打包**：确保模型学习到完整、连贯的上下文。<br>• **FIM策略**：赋予模型根据任意上下文（前缀和后缀）预测中间内容的能力，**增强其代码补全和文本填充的推理技能**，且不损害其标准的语言建模能力。 |
| **3. 分词器** | • **算法**：采用 **字节级BPE**，词表扩展至 **128K**。<br>• **优化**：改进预分词器和训练数据，提升多语言压缩效率。<br>• **解决偏置**：引入合并标点与换行符的标记后，通过**随机拆分此类组合标记**的方式训练模型，以缓解“标记边界偏置”。 | • **大词表与字节级**：平衡压缩效率与泛化能力，更好地处理多语言和生僻词。<br>• **缓解边界偏置**：确保模型在处理**未以换行符结尾的多行提示**（如少样本评估提示）时，不会因分词方式而产生性能偏差，提升评估的公平性与鲁棒性。 |
| **4. 数据规模** | 最终构建的预训练语料包含 **14.8T (万亿) Token**。 | 为训练一个拥有 671B 参数的 MoE 模型提供了充足、高质量的数据基础。 |

### **数据结构详解**
图片中的结构 `<|fim_begin|>f_pre<|fim_hole|>f_suf<|fim_end|>f_middle<|eos_token|>` 可以拆解为：

1.  **`<|fim_begin|>`**
    *   **含义**：**“Fill-in-the-Middle Begin”** 的标记。它向模型宣告：一个特殊的“填空”训练样本开始了。

2.  **`f_pre`**
    *   **含义**：**前缀（Prefix）**。这是模型在生成时可以看见的**第一部分上下文**。

3.  **`<|fim_hole|>`**
    *   **含义**：**“空洞”标记**。它象征着这里有一个缺失的部分，直接提示模型：“请注意，后面提供的信息（后缀）是这段文本的结尾，你需要根据开头和结尾来补全中间。”

4.  **`f_suf`**
    *   **含义**：**后缀（Suffix）**。这是模型在生成时可以看见的**第二部分上下文**，即文本的结尾部分。

5.  **`<|fim_end|>`**
    *   **含义**：**“Fill-in-the-Middle End”** 的标记。它告诉模型：“关于任务描述的上下文（前缀+后缀）已经提供完毕。”

6.  **`f_middle`**
    *   **含义**：**中间部分（Middle）**。这就是模型需要学习预测的**目标内容**。在训练时，模型的任务就是根据 `f_pre` 和 `f_suf` 来生成 `f_middle`。

7.  **`<|eos_token|>`**
    *   **含义**：**句子结束标记**。标志整个训练样本的终结。

## 11.1 - Model Hyper-Parameters

### **内容概括**

这段文字详细列出了DeepSeek-V3核心模型架构的各项关键超参数。模型主体是一个61层的Transformer，隐藏维度为7168。其核心创新组件——**多头潜在注意力** 和 **DeepSeekMoE**——的参数被具体设定：MLA使用128个头，并设定了KV压缩、查询压缩及解耦查询/键的维度；除前三层外，所有前馈网络均被MoE层替代，每层包含1个共享专家和256个路由专家，每个Token激活其中6个，并确保最多发送至4个节点。此外，模型采用了多Token预测（深度D=1）等增强训练效率的技术。在此配置下，模型总参数量达6710亿，但每个Token实际仅激活370亿参数，实现了容量与效率的平衡。

### **要点总结**

以下为模型关键配置的详细列表：

| 模块/组件 | 参数名称 | 设定值 | 说明与意义 |
| :--- | :--- | :--- | :--- |
| **整体架构** | Transformer层数 | 61 | 模型的深度。 |
| | 隐藏维度 | 7168 | 每层主干特征的维度。 |
| | 参数初始化标准差 | 0.006 | 所有可学习参数的随机初始化范围。 |
| **多头潜在注意力** | 注意力头数 | 128 | MLA中的总头数。 |
| | 每头维度 | 128 | 每个注意力头的特征维度。 |
| | KV压缩维度 | 512 | 键和值向量被压缩到的潜在空间维度，用于减少KV缓存。 |
| | 查询压缩维度 | 1536 | 查询向量被压缩到的潜在空间维度。 |
| | 解耦查询/键维度 | 64 | 独立的、携带旋转位置编码的查询和键分量的维度。 |
| **混合专家** | MoE替换范围 | 除第1-3层外的所有FFN | 模型的主要计算密度由MoE层贡献。 |
| | 每层专家构成 | 1个共享专家 + 256个路由专家 | 共享专家处理通用特征，路由专家处理特定特征。 |
| | 专家中间维度 | 2048 | 每个专家前馈网络内部层的维度。 |
| | 每Token激活专家数 | 6 | 每个Token选择并激活的Top-K路由专家数量。 |
| | 每Token最多发送节点数 | 4 | 跨节点专家并行中，单个Token被路由到的物理节点上限，以优化通信。 |
| **训练目标** | 多Token预测深度 | 1 | 除预测下一个Token外，额外预测未来一个Token，以增强训练信号。 |
| **其他技术** | 附加RMSNorm与缩放因子 | 已采用 | 在压缩潜在向量后使用RMSNorm，并在宽度瓶颈处乘以缩放因子，以稳定训练。 |
| **规模统计** | 总参数量 | 671B (6710亿) | 模型所有参数的总和，体现模型总体容量。 |
| | 每Token激活参数量 | 37B (370亿) | 处理每个Token时实际参与计算的前向传播参数，决定推理成本与效率。 |

**总结**：此配置表清晰地展现了DeepSeek-V3作为一个**高效的混合专家模型**的设计精髓：通过**MLA**节省KV缓存，通过**MoE**在保持极高总容量（671B）的同时，将实际计算量控制在较低水平（37B/Token）。各项参数（如激活6个专家、发送至最多4个节点）均体现了其在模型性能、计算效率与分布式通信开销之间取得的精密平衡。

## 11.2 - Training Hyper-Parameters

### **内容概括**

第一张图片详细列出了 **DeepSeek-V3 模型的训练超参数**，涵盖了优化器设置、学习率调度、批次大小策略以及模型并行与路由相关的关键配置。这些参数共同构成了其稳定、高效训练14.8万亿Token的基础。

第二张图片展示了 **DeepSeek-V3 在“大海捞针”测试中对128K长上下文处理能力的压力评估结果**。热力图显示，模型在各种上下文长度和文档深度下均表现稳定且优异，验证了其扩展上下文窗口技术的有效性。

### **要点总结**

#### **1. 训练超参数 (第一张图)**
*   **优化器**: AdamW (`β1=0.9, β2=0.95`, 权重衰减 `0.1`)。
*   **学习率调度 (复杂的三段式)**:
    1.  **热身**: 前2K步从0线性增至 `2.2e-4`。
    2.  **平台期**: 保持 `2.2e-4` 直至10T Token。
    3.  **衰减期**: 4.3T Token内余弦衰减至 `2.2e-5`。
    4.  **收尾**: 最后500B Token中，先保持 `2.2e-5` (333B)，再切换至 `7.3e-6` (167B)。
*   **其他关键设置**:
    *   **梯度裁剪**: 阈值设为 `1.0`。
    *   **批次大小**: 从 `3072` 逐步增至 `15360`，并在后期保持。
    *   **并行与路由**: 采用管道并行；专家均匀部署在8节点64 GPU上；每个Token最多路由至4个节点 (`M=4`)。
    *   **损失权重**:
        *   负载平衡偏差更新速度 `γ`: 前14.3T Token为 `0.001`，后500B为 `0.0` (停止调整)。
        *   序列内平衡损失权重 `α`: 设为极小的 `0.0001`，仅防极端不平衡。
        *   多令牌预测损失权重 `λ`: 前10T Token为 `0.3`，后4.8T降为 `0.1`。

#### **2. 长上下文能力评估 (第二张图)**
*   **测试方法**: “大海捞针”测试，用于评估模型在长上下文中精确检索信息的能力。
*   **评估维度**: 横轴为**上下文长度** (2K 至 128K)，纵轴为**文档深度百分比** (0% 到 100%)，颜色代表性能分数 (1-10分)。
*   **核心结论**: 热力图颜色分布**均匀且整体为高分段**，表明 **DeepSeek-V3 在从短到超长（128K）的各种上下文窗口下，均能稳定、可靠地提取信息，性能无明显衰减**。这直接证明了其长上下文扩展技术的成功。

## 11.3 - Long Context Extension

### **内容概括**

该图片详细描述了DeepSeek-V3模型在完成预训练后，为支持超长文本输入而进行的 **“长上下文扩展”训练方法**。此方法延续了DeepSeek-V2的策略，基于 **YaRN技术**，通过**两个独立的额外训练阶段**，分步将模型的上下文处理能力从**4K逐步扩展到32K，最终达到128K**。

整个扩展过程的核心在于保持YaRN的关键超参数不变，仅调整**序列长度**和**批次大小**以适应不同阶段的训练需求，并将**学习率**维持在预训练结束时的低水平。经过此两阶段扩展训练并完成后续的监督微调后，DeepSeek-V3不仅具备了处理128K长度输入的能力，更在著名的 **“大海捞针”测试中，表现出了跨越不同上下文长度的优异鲁棒性**。

### **要点总结**

| 方面 | 具体内容与参数 | 目的与意义 |
| :--- | :--- | :--- |
| **1. 核心技术基础** | 采用与DeepSeek-V2类似的 **YaRN** 方法进行扩展。 | 延续已验证有效的长上下文扩展技术路线，确保技术可靠性。 |
| **2. 训练阶段设计** | 分为**两个阶段**，每阶段各**1000步**：<br>• **第一阶段**：从 4K → **32K**<br>• **第二阶段**：从 32K → **128K** | **渐进式扩展**，避免直接从短上下文跳至超长上下文导致的训练不稳定或灾难性遗忘。 |
| **3. 关键YaRN配置** | • 应用范围：**仅作用于解耦的共享键 \( k^R_i \)**。<br>• 超参数：尺度 \( s=40 \), \( \alpha=1 \), \( \beta=32 \)。<br>• 缩放因子：\( \sqrt{t} = 0.1 \ln s + 1 \)。 | **保持扩展过程的一致性**，所有参数在两个阶段中完全相同，简化训练流程。 |
| **4. 阶段参数调整** | • **第一阶段**：序列长度 **32K**，批次大小 **1920**。<br>• **第二阶段**：序列长度 **128K**，批次大小 **480**。 | 适应不同长度下的显存占用与训练效率。批次随序列长度增加而减少，是标准的权衡策略。 |
| **5. 学习率设置** | 两个阶段的学习率均设为 **\( 7.3 \times 10^{-6} \)**。 | 与**预训练最终阶段的学习率完全一致**。这确保了扩展训练是在模型已经收敛的稳定基础上进行的**精细微调**，而不是重新学习，有效保护了预训练获得的能力。 |
| **6. 最终能力验证** | 扩展并经过监督微调后，模型通过了 **“Needle In A Haystack”测试**。 | **实证检验**：证明DeepSeek-V3不仅支持128K的**理论长度**，更具备在全文任意位置**精确检索和利用信息**的**实际能力**，且性能在不同长度下表现稳定。 |

## 12.0 - Evaluation Benchmarks

### **内容概括**

这两张图片系统地阐述了DeepSeek-V3基础模型的**全面评估体系**。评估的核心是检验这个在多语言语料（以中英文为主）上预训练的模型，在**英语、中文及多语言**场景下的综合能力。

评估在自研的**HAI-LLM框架**内置评估系统中进行。图片将所有基准测试按任务类型进行了详细分类，并明确标注了中文（绿色单下划线）和多语言（绿色双下划线）数据集。评估覆盖了从**学科知识、语言理解、推理、问答、阅读理解、代码到数学**的广泛领域，并针对不同类型的任务采用了**基于困惑度的评估**和**基于生成的评估**两种方法，以确保评估的准确性和公平性。

### **要点总结**

| 方面 | 核心内容 |
| :--- | :--- |
| **1. 评估背景** | 模型在**多语言语料（中英文为主）** 上预训练，因此评估涵盖**英文、中文及多语言**能力。 |
| **2. 评估框架** | 基于自研的 **HAI-LLM框架** 中的内部评估系统。 |
| **3. 评估方法** | • **基于困惑度的评估**：用于**多项选择题**和部分理解类任务。<br>• **基于生成的评估**：用于**开放生成类**任务（如问答、数学、代码）。<br>• **语言建模评估**：对Pile-test使用**每字节比特数（BPB）** 指标，避免分词器差异影响，保证公平。 |
| **4. 基准范围** | **极其广泛**，共涉及8大类、超过25个知名公开基准，全面检验模型的知识、理解、推理与生成能力。 |
| **5. 语言侧重** | 除通用英文基准外，特别标注并纳入了关键的**中文基准**（如C-Eval, CMMLU, CMath等）和**多语言基准**，凸显对中文及跨语言能力的重视。 |

### **涉及的数据集介绍**

以下是按类别梳理的数据集，其中`中文`和`多语言`已按图片标注注明。

| 类别 | 数据集名称 | 简介与特点 | 评估方法 |
| :--- | :--- | :--- | :--- |
| **1. 多学科选择题** | **MMLU** (及 Redux, Pro 变体) | 涵盖57个科目的英文知识测试，衡量模型的世界知识和问题解决能力。 | 困惑度评估 |
| | **MMMU** | 大规模多学科多模态理解基准。 | 困惑度评估 |
| | `中文` **C-Eval** | 涵盖52个学科的中文知识测试，评估中文知识水平。 | 困惑度评估 |
| | `中文` **CMMLU** | 专门针对中文语言、知识、推理的评估基准。 | 困惑度评估 |
| **2. 语言理解与推理** | **HellaSwag** | 常识推理，完成句子或事件。 | 困惑度评估 |
| | **PIQA** | 物理常识推理。 | 困惑度评估 |
| | **ARC** | 科学领域的小学级选择题。 | 困惑度评估 |
| | **BBH** | 挑战性推理任务集合，测试思维链能力。 | 生成式评估 |
| **3. 闭卷问答** | **TriviaQA** | 基于维基百科的常识问答。 | 生成式评估 |
| | **NaturalQuestions** | 真实的谷歌搜索问题及答案。 | 生成式评估 |
| **4. 阅读理解** | **RACE** | 来自中学英语考试的阅读理解。 | 困惑度评估 |
| | **DROP** | 需要离散推理的阅读理解。 | 生成式评估 |
| | `中文` **C3** | 中文机器阅读理解数据集。 | 困惑度评估 |
| | `中文` **CMRC** | 中文阅读理解数据集。 | 生成式评估 |
| **5. 指代消解** | **CLUEWSC** | 中文Winograd模式挑战，测试常识与指代消解。 | 生成式评估 |
| | **WinoGrande** | 大规模的英文Winograd模式挑战。 | 困惑度评估 |
| **6. 中文理解与文化** | `中文` **CCPM** | 中文古典诗词阅读理解数据集。 | 困惑度评估 |
| **7. 数学** | **GSM8K** | 英文小学水平数学应用题。 | 生成式评估 |
| | **MATH** | 英文竞赛水平的数学题。 | 生成式评估 |
| | **MGSM** | 多语言小学数学应用题。 | 生成式评估 |
| | `中文` **CMath** | 中文数学应用题数据集。 | 生成式评估 |
| **8. 代码** | **HumanEval** | 评估Python代码生成能力的经典基准。 | 生成式评估 |
| | **MBPP** | 入门级Python编程问题。 | 生成式评估 |
| | **LiveCodeBench** | 持续更新的代码生成评估基准，反映最新趋势。 | 生成式评估 |
| | **CRUXEval** | 评估代码执行与推理能力的基准。 | 生成式评估 |
| **9. 标准化考试** | `多语言` **AGIEval** | 涵盖中英文的标准化考试（如高考、SAT、LSAT等）题目。 | 生成式评估 |
| **10. 语言建模** | **The Pile** | 大规模、多样化的文本语料库，用于评估模型的通用语言建模能力。 | 语言建模（BPB） |

**总结**：这份详尽的评估清单表明，DeepSeek-V3的评估不是为了通过“刷榜”，而是为了进行一场**严格、全面、公平的“体检”**。它尤其重视检验模型在**中文场景和复杂推理任务（数学、代码）** 上的真实能力，这与其技术报告中所展示的卓越性能是相互印证的。

## 12.0 - 基于困惑度的评估、基于生成的评估、语言建模评估分别如何评测?

### 1. 基于困惑度的评估
这种方法主要用于 **“多项选择题”或“完形填空”类任务**。

*   **核心原理**：不要求模型生成答案，而是让模型**计算每个候选选项（A/B/C/D）在给定上下文下的困惑度**。困惑度越低，代表模型认为该选项在上下文中出现的可能性越大，越“自然”或“正确”。
*   **操作方式**：对于一道选择题，模型会分别计算将每个选项填入空白后的整个序列的困惑度，然后选择困惑度最低的那个选项作为预测答案。
*   **为什么用在这里**：这种方法**高效、客观**，无需生成文本，避免了生成模型可能出现的格式错误、冗余等问题，直接评估模型对知识的“判断力”。
*   **图片中的应用**：正如第二张图所述，此方法用于评估 `HellaSwag`, `PIQA`, `WinoGrande`, `RACE`, `MMLU`系列, `ARC`, `C-Eval`, `CMMLU`, `C3`, `CCPM` 等数据集。这些基本都是给定上下文和若干选项的选择题。

### 2. 基于生成的评估
这种方法主要用于 **需要自由生成答案的开放性问题**。

*   **核心原理**：让模型像正常对话或完成任务一样，**自主生成文本序列来回答问题**。然后，将模型的生成结果与标准答案（或一组参考答案）进行比较。
*   **操作方式**：
    1.  输入问题或指令。
    2.  模型生成一段文本（代码、数学解、自由文本答案等）。
    3.  使用特定指标进行比对：
        *   **精确匹配**：如代码任务，检查生成代码是否能通过单元测试（`HumanEval`, `MBPP`）。
        *   **字符串匹配/规则提取**：如数学任务，从生成文本中提取最终答案与标准答案比较（`GSM8K`, `MATH`）。
        *   **F1分数/ROUGE**：如阅读理解任务，比较生成文本与参考答案的重合度（`DROP`, `TriviaQA`）。
*   **为什么用在这里**：这类任务（数学、代码、自由形式问答）的答案不固定，必须通过实际生成来检验模型的**推理、创造和问题解决能力**。
*   **图片中的应用**：如第二张图所述，此方法用于 `TriviaQA`, `NaturalQuestions`, `DROP`, `MATH`, `GSM8K`, `HumanEval`, `MBPP`, `LiveCodeBench`, `CRUXEval`, `BBH`, `AGIEval`, `CLUEWSC`, `CMRC`, `CMath` 等数据集。

### 3. 语言建模评估
这是一种 **评估模型最基础能力的通用方法**，用于衡量模型对自然语言分布的建模好坏。

*   **核心原理**：给定一段文本（如`The Pile`的测试集），让模型计算其**困惑度**。困惑度越低，说明模型对该文本越不感到“意外”，即其建模的语言分布与真实世界语言分布越接近。**BPB 越低，说明模型对下一个“字节”是什么越有把握，其“不确定性”或“惊讶度”越低，因此语言建模能力越强**。
*   **操作方式与关键指标（BPB）**：
    *   传统上使用**困惑度**，但它受模型**分词器**影响很大。词表大的模型，困惑度天生可能更低，这不公平。
    *   **因此，DeepSeek-V3采用“每字节比特数”作为指标**。BPB将信息论中的“比特”作为统一度量衡，**绕过了分词器的影响**，直接计算模型压缩每个原始字节（而非每个Token）所需的信息量。**BPB越低，表示模型的语言建模能力越强**。
*   **为什么用在这里**：为了在**不同词表大小、不同分词方式的模型之间进行公平比较**，评估其最核心的“理解与预测自然语言”的基本功。
*   **图片中的应用**：专门用于 `Pile-test` 数据集，并明确使用 **BPB** 指标以保证公平性。

### 总结对比

| 评估方法 | 核心思想 | 典型任务 | 关键指标 | 目的 |
| :--- | :--- | :--- | :--- | :--- |
| **基于困惑度** | **计算选项的可能性** | 多项选择题、完形填空 | 困惑度 (选择最低的) | 评估模型的知识判断与选择能力 |
| **基于生成** | **自主生成答案** | 数学解题、代码编写、开放问答 | 精确匹配、通过率、F1等 | 评估模型的推理、创造与执行能力 |
| **语言建模 (BPB)** | **计算文本的“意外程度”** | 通用文本续写 | **每字节比特数** | 公平地评估最基础的语言建模能力 |

## 12.1 - Evaluation Results

### **内容概括**

**第一部分（图片1）** 介绍了评估的背景、对象与方法。团队在内部统一的评估框架下，将DeepSeek-V3与当前顶尖的开源基座模型（包括其前代模型DeepSeek-V2-Base、中文代表Qwen2.5 72B Base以及目前参数规模最大的LLaMA-3.1 405B Base）进行了公平对比。

**第二部分（图片2，即Table 3）** 以详尽的表格形式，直观展示了上述四个模型在**架构信息**以及**英语、代码、数学、中文、多语言**五大能力维度下，超过25个具体基准测试中的量化得分。这份表格是评估结论的核心数据支撑。

**第三部分（图片3与4的文本）** 对评估结果进行了深入的解读与分析。主要结论是：DeepSeek-V3在绝大多数基准上，尤其是在**数学与代码**任务上表现最佳，全面超越了其他对比模型。此外，本部分还简要提及了支撑其卓越性能的关键原因。

**第四部分（图片4，即Table 4）** 通过消融实验，专门验证了 **“多令牌预测”策略**的有效性，表明该策略在大多数评估基准上都能持续提升模型性能。

### **要点总结**

| 方面 | 核心结论 |
| :--- | :--- |
| **1. 评估设置** | • **框架**：使用自研的**内部评估框架**，确保所有模型在**完全相同的设置**下对比。<br>• **标准**：分数差距**不超过0.3**即视为同一水平。 |
| **2. 总体表现** | **DeepSeek-V3-Base 在绝大多数评估基准上取得了最佳性能**，实质上已成为当前**最强的开源基座模型**。 |
| **3. 分项对比优势** | • **对比 DeepSeek-V2-Base**：**全面大幅领先**。得益于模型架构改进、参数量与训练Token量的扩大以及数据质量提升。<br>• **对比 Qwen2.5 72B Base**：**以约一半的激活参数量（37B vs 72B），在英语、多语言、代码和数学基准上展现出显著优势**。在中文基准上，除CMMLU外，也表现更优。<br>• **对比 LLaMA-3.1 405B Base**：**以约1/11的激活参数量（37B vs 405B），在多语言、代码和数学基准上表现更优**。在英/中文语言理解基准上，表现相当或更好，尤其在BBH、MMLU系列、DROP、C-Eval、CMMLU和CCPM等任务上突出。 |
| **4. 效率证明** | 由于高效的架构和全面的工程优化，训练效率极高：**每万亿Token仅需18万H800 GPU小时**，成本远低于训练72B或405B的稠密模型。 |
| **5. MTP策略有效性** | **消融实验（Table 4）证实**：“多令牌预测”策略在大多数评估基准上能**持续提升模型性能**，验证了其作为核心训练技术的价值。 |

### **核心结论**
DeepSeek-V3基础模型通过一系列**算法与工程的协同创新**，在激活参数量（37B）远小于竞争对手的情况下，实现了**综合性能的全面领先**。这标志着其不仅在**模型能力**上达到了新的高度，更在**训练效率与性价比**上树立了新的标杆。此次评估系统地证明了其作为“最强开源基座模型”的地位。

## 12.2 - Ablation Studies for Multi-Token Prediction

### **内容概况**

研究人员在**两种不同规模**的基线混合专家模型（小规模：总参15.7B，激活2.4B；大规模：总参228.7B，激活20.9B）上，保持其架构和训练数据不变，仅额外增加一个**1层深度的MTP模块**进行训练，形成对比模型。在推理时，MTP模块被丢弃，因此对比模型的推理成本完全相同。

表格详细对比了有/无MTP策略的模型在基础语言建模（Pile-test BPB）以及一系列下游任务（如BBH、MMLU、HumanEval、GSM8K等）上的性能表现。结果显示，**MTP策略在绝大多数评估基准上持续提升了模型性能**。

### **要点总结**

| 方面 | 核心内容 |
| :--- | :--- |
| **1. 实验设计** | **控制变量对比**：在“小规模”与“大规模”两种基线MoE模型上，**仅添加/不添加1层MTP模块**，使用相同数据训练，比较最终性能。 |
| **2. 模型配置** | • **小规模模型**：总参15.7B，激活2.4B，训练1.33T Token。<br>• **大规模模型**：总参228.7B，激活20.9B，训练540B Token。 |
| **3. 核心发现 (性能提升)** | **MTP策略带来了普遍的、一致的性能提升**：<br>• **代码能力**：提升**最为显著**。在**HumanEval**上，小模型提升6.1分（20.7→26.8），大模型提升9.2分（44.5→53.7）。<br>• **数学推理**：提升明显。在**GSM8K**上，小模型提升6.0分（25.4→31.4），大模型提升1.7分（72.3→74.0）；在**MATH**上也有稳定提升。<br>• **综合推理与知识**：在**BBH、DROP、TriviaQA**等任务上，两个规模的模型均获得提升。<br>• **语言建模**：基础**Pile-test (BPB)** 指标保持稳定，略有优化。 |
| **4. 个别波动** | 在**大规模模型的MMLU**上，性能有轻微下降（67.5→66.6），但这属于个别现象，不影响“普遍提升”的整体结论。 |
| **5. 关键结论** | **MTP是一种高效且低成本的训练增强策略**：它能在**不增加模型推理开销**的前提下，通过修改训练目标（预测未来多个Token），有效地提升模型在**代码、数学推理及多项综合任务**上的最终能力。 |

## 12.3 - Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy

### **内容概括**

研究以两种不同规模、且**纯粹依赖辅助损失来鼓励负载均衡的MoE模型作为基线**。它们的门控函数与超参数设置分别与已发布的DeepSeek-V2-Lite和DeepSeek-V2相同。

在此基线之上，研究人员**保持所有训练数据、模型架构和其他超参数完全不变**，仅进行一项关键改动：**移除所有辅助损失，并引入全新的“无辅助损失平衡策略”**。通过严格对比新策略与基线策略在多项评估基准上的性能，得出结论：**在绝大多数任务中，无辅助损失策略能持续取得更好的模型性能**。

### **要点总结**

| 方面 | 核心内容 |
| :--- | :--- |
| **1. 研究目的** | 通过控制变量的消融实验，验证 **“无辅助损失平衡策略”** 是否优于传统的、**纯粹依赖辅助损失**的负载平衡方法。 |
| **2. 实验设计 (控制变量法)** | • **基线模型**：两个不同规模的MoE模型，均使用 **Sigmoid门控 + Top-K亲和力归一化**，并**仅通过调整辅助损失的强度**来促进负载均衡。<br>• **唯一变量**：将基线的辅助损失机制，替换为论文提出的 **“无辅助损失平衡策略”**。其他一切（训练数据、总/激活参数量、架构）完全相同。 |
| **3. 模型规格** | • **小规模模型**：总参15.7B，激活2.4B，训练1.33T Token。<br>• **大规模模型**：总参228.7B，激活20.9B，训练578B Token。 |
| **4. 核心发现 (性能对比)** | 根据表格数据，**无辅助损失策略在绝大多数评估基准上表现更优**：<br>• **显著提升**：在**BBH**（大规模）、**TriviaQA**、**NaturalQuestions**、**HumanEval**（大规模）、**GSM8K**（大规模）、**MATH**（大规模）等任务上，新策略的得分**明显更高**。<br>• **稳定或微升**：在**Pile-test (BPB)**、**DROP**等任务上保持稳定或略有提升。<br>• **个别波动**：仅在**大规模模型的MMLU**上出现轻微下降（68.3→67.2），这属于个别情况，不影响整体结论。 |
| **5. 关键结论与意义** | **“无辅助损失平衡策略”被证明是一种更优的负载均衡解决方案**。它避免了为强制平衡而引入的辅助损失对模型主任务性能的潜在损害，通过在路由机制中直接嵌入动态偏差调整，实现了**更好的性能与效率权衡**。这从实验层面巩固了该策略作为DeepSeek-V3核心创新之一的有效性。 |

## 12.4 - 两种损失平衡策略的区别是什么?

这是一个非常核心的问题。DeepSeek-V3论文中提到的两种负载平衡策略，代表了混合专家模型训练中解决“专家负载不均”这一经典难题的两种不同哲学。

简单来说，它们的核心区别在于：**传统策略通过修改“损失函数”来间接引导，而DeepSeek-V3的策略通过修改“路由计算”来直接调控。**

---

### **1. 传统策略：依赖辅助损失**

这种方法在MoE模型的**损失函数**中添加一个额外的项，专门用来惩罚负载不均衡。

*   **核心机制**：
    1.  **路由计算**：根据输入和专家亲和力，按常规方式（如Softmax或Sigmoid + TopK）计算门控值，选择专家。
    2.  **计算辅助损失**：在反向传播时，计算一个额外的**负载平衡损失**。常见的形式是计算所有专家负载的**方差**或**基于门控值的某种不平衡度量**。这个损失项会鼓励模型参数朝着使专家负载更均匀的方向更新。
    3.  **总损失**：`总损失 = 主任务损失（如交叉熵） + α * 辅助损失`，其中 `α` 是一个超参数，控制平衡的强度。

*   **工作原理**：通过梯度下降，**主模型的参数（包括路由门的参数）被同时优化以完成主任务和实现负载均衡**。路由门“学习”到，如果它能把Token更均匀地分配出去，总损失就会更低。

*   **主要问题**：
    *   **目标冲突**：**主任务目标**和**负载平衡目标**可能存在根本性冲突。有些Token天生就只和少数几个专家高度相关，强行将它们路由到其他专家会**损害模型的主任务性能**。
    *   **超参数敏感**：平衡系数 `α` 需要仔细调优。`α` 太小不起作用，`α` 太大会严重干扰主任务学习。

---

### **2. DeepSeek-V3策略：无辅助损失**

这种方法**不修改损失函数**，而是通过一个**独立的、动态的反馈控制系统**来直接调整路由决策。

*   **核心机制**：
    1.  **路由决策**：为每个专家 `i` 引入一个可学习的**偏差项 `b_i`**。在决定路由时，计算 `(亲和力分数 s_i + 偏差 b_i)`，并基于这个**调整后的分数**来选择Top-K专家。
    2.  **门控值计算**：**关键一步**：在计算最终输出时，**只使用原始的亲和力分数 `s_i`** 来加权各专家的输出。偏差 `b_i` 只影响“选择谁”，不影响“输出多少”。
    3.  **动态调整**：在训练过程中，持续监控每个专家的负载（被选中的频率）。如果一个专家过载，就**降低**它的 `b_i`；如果欠载，就**提高**它的 `b_i`。这个调整过程是**独立的**，不通过主损失函数的梯度来驱动。

*   **工作原理**：它像一个**独立的流量控制系统**。偏差 `b_i` 是一个“旋钮”，专门用来调节通往专家 `i` 的“流量”（Token数量），以确保所有专家都得到充分利用。这个系统的目标**唯一且纯粹**：就是负载均衡。

*   **主要优势**：
    *   **目标解耦**：将**负载平衡**和**模型性能**两个目标完全分开。路由门仍然只根据输入和亲和力（`s_i`）来决定如何组合专家输出以最好地完成任务。平衡系统（`b_i`）则专心致志地调节流量，两者互不干扰。
    *   **避免性能损害**：由于平衡操作不影响专家输出的权重，因此**不会强迫Token去使用不相关的专家**，从而保护了模型的主任务能力。

---

### **总结对比**

| 特性 | **传统策略（辅助损失）** | **DeepSeek-V3策略（无辅助损失）** |
| :--- | :--- | :--- |
| **核心手段** | 在损失函数中添加额外项 | 在路由决策中引入动态偏差项 |
| **优化目标** | 双目标联合优化（主任务 + 平衡） | **目标解耦**（主任务归主任务，平衡归平衡） |
| **如何影响路由** | 间接，通过影响路由门参数的学习 | **直接**，通过实时调整偏差值来改变选择概率 |
| **对模型性能影响** | 可能因目标冲突而**损害性能** | 理论上**更能保护模型性能** |
| **超参数** | 需要精细调整平衡系数 `α` | 需要调整偏差更新步长 `γ`，但影响更直接、更局部 |
| **类比** | **交规 + 交警**：通过制定法律（损失函数）和惩罚（梯度）来引导司机（路由门）的行为。 | **智能红绿灯系统**：直接根据实时车流（负载）调整信号（偏差），司机（路由门）仍按自己的最佳路线行驶。 |

**结论**：DeepSeek-V3的“无辅助损失”策略是一种更优雅、更解耦的解决方案。它将复杂的多目标优化问题拆分为两个相对简单的单目标问题，通过一个独立的控制回路来实现负载均衡，从而在保证训练稳定性的同时，最大程度地减少了对模型核心能力的潜在干扰。这也是其宣称能实现“无Token丢弃”和更高训练效率的重要原因之一。

## 12.5 - Batch-Wise Load Balance VS. Sequence-Wise Load Balance

### **内容概况**

这两张图片（对应论文章节 4.5.3）系统性地探讨了 **“批量级负载平衡”** 与 **“序列级负载平衡”** 两种方法的根本区别、性能影响及工程挑战。

研究首先指出，无辅助损失平衡方法与序列级辅助损失的核心区别在于 **平衡范围**：前者是**批量级**的，后者是**序列级**的。批量级平衡的约束更为**灵活**，它不强制每个序列内部都达到平衡，从而允许模型中的专家更专注于不同的领域。这一推断通过 **Figure 9** 的热力图得到了验证：在Pile测试集的三个不同领域（英文维基百科、Github、数学）上，无辅助损失模型比基于辅助损失的模型展现出更显著的**专家专业化模式**。

为了进一步探究这种灵活性带来的性能优势，研究还额外设计并验证了一种 **“批量级辅助损失”** 。实验结果表明，在达到相似的批量级平衡水平时，批量级辅助损失(batch-wise auxiliary loss)也能取得与无辅助损失(the auxiliary-loss-free)方法**相近的优异性能**（1B和3B MoE模型的验证损失数据均支持此结论）。最后，文章客观地指出了批量级平衡方法面临的两个效率挑战及其解决方案。

### **要点总结**

| 核心维度 | 详细说明与发现 |
| :--- | :--- |
| **1. 核心对比：平衡范围** | • **序列级平衡**：强制**每个独立的输入序列内部**的专家负载均匀。<br>• **批量级平衡**：只约束**整个训练批次（包含多个序列）** 的专家负载均匀，允许单个序列内负载倾斜。 |
| **2. 核心优势：专家专业化** | 批量级平衡的**灵活性**是关键优势。它不强迫每个序列都均匀使用所有专家，从而使得专家能够根据数据中的领域模式，**自然地专注于各自擅长的特定领域**（如代码、数学、百科）。Figure 9 的热力图直观证实了这一点。 |
| **3. 实验验证：性能关联** | • **对比实验**：在16B模型上，通过热力图验证了无辅助损失（批量级）模型比基于辅助损失（序列级）模型具有更明显的专家专业化模式。<br>• **控制实验**：在1B和3B MoE模型上对比了三种方法：<br>  - **序列级辅助损失**：验证损失较高（1B: 2.258, 3B: 2.085）。<br>  - **无辅助损失方法** 与 **批量级辅助损失**：两者性能**几乎完全相同**且更优（1B: 2.253, 3B: 2.080）。<br>• **结论**：性能优势源于**批量级平衡**本身，而非特定实现方式（无辅助损失或批量级辅助损失）。 |
| **4. 面临的效率挑战与解决** | 批量级方法面临两大挑战：<br>• **挑战1**：序列内或小批次内的负载不均。<br>  **解决方案**：训练框架采用**大规模专家并行与数据并行**，确保每个微批次规模足够大，自然平滑负载。<br>• **挑战2**：推理时，由于数据分布（领域）变化导致的负载不均。<br>  **解决方案**：推理框架采用**冗余专家部署策略**（见第3.4节），动态应对热门专家请求。 |

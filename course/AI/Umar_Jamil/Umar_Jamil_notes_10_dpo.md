本文主要整理dpo的主要内容。

## 1 - The Bradley-Terry model

### 内容概括

该幻灯片介绍了 **Bradley-Terry 模型**，其核心作用是将基于人类反馈的偏好数据（如“哪个答案更好”）转化为可量化的分数（奖励值）。模型通过一个特定的概率公式，计算在给定问题下，一个“获胜”回答优于一个“失败”回答的可能性。幻灯片通过一个包含问题、优质回答和劣质回答的表格提供了具体示例。最后指出，为了训练一个能正确排序回答的奖励模型，可以采用**最大似然估计** 方法来优化模型参数，从而最大化正确排序的概率。

### 要点总结

1.  **模型目的**：解决如何将主观的“偏好”数据客观地量化为“分数”或“奖励”，这是基于人类反馈进行强化学习等任务中的关键步骤。
2.  **核心公式**：模型定义了偏好概率公式：
    `P(优质回答 > 劣质回答) = exp(奖励值_优质) / [exp(奖励值_优质) + exp(奖励值_劣质)]`
    *   该公式将两个回答的奖励值差异，转化为一个介于0到1之间的概率，表示优质回答胜出的可能性。
3.  **数据示例**：通过表格展示了模型的应用场景：
    *   **输入**：一个问题或提示。
    *   **对比数据**：一对标记为“优质/获胜”和“劣质/失败”的回答。
4.  **优化方法**：采用**最大似然估计** 来训练奖励模型。
    *   **目标**：找到最优的奖励函数参数，使得模型认为数据中所有“优质回答优于劣质回答”事件的**联合概率最大化**。
    *   **结果**：训练得到的奖励模型能够更准确地区分和排序回答的质量。

## 2 - Deriving the loss function of the reward model

### **内容概括**

本图片详细演示了如何从 **Bradley-Terry 模型**的概率公式出发，推导出用于训练奖励模型的**损失函数**。推导过程的核心目标是：将模型计算“优质回答优于劣质回答”概率的原始表达式，化简为更简洁、实用的 **Sigmoid 函数**形式。

推导完成后，图片展示了最终的标准损失函数（即 DPO 论文中的公式2），它是一个**负对数似然损失**，其目标是通过优化模型参数，最大化在训练数据中观察到偏好顺序的概率。

### **要点总结**

1.  **推导起点：Bradley-Terry 模型**
    *   概率公式为：`P(y_w > y_l) = e^(r(x, y_w)) / [e^(r(x, y_w)) + e^(r(x, y_l))]`
    *   其中 `r(x, y)` 是奖励模型给出的分数，`y_w` 和 `y_l` 分别代表优质（获胜）和劣质（失败）回答。

2.  **核心数学变换：化简为 Sigmoid 函数**
    *   通过一系列的代数运算（主要是分子分母同除以 `e^(r(x, y_w))`），将复杂的分数形式化简为：
        `σ(r(x, y_w) - r(x, y_l))`
    *   这里的 `σ` 就是 **Sigmoid 函数** `σ(z) = 1 / (1 + e^(-z))`。
    *   这个变换非常关键，它将偏好概率直接表示为**两个回答奖励值之差**的函数，使模型训练变得更直观和高效。

3.  **最终损失函数：负对数似然**
    *   为了使用**最大似然估计**进行训练，需要构建损失函数。目标是**最大化**所有训练样本中 `P(y_w > y_l)` 的联合概率。
    *   在机器学习中，通常通过最小化**负对数似然**来实现。因此，最终的损失函数 `L` 为：
        `L = - E[ log( σ(r(x, y_w) - r(x, y_l)) ) ]`
    *   **负号的意义**：因为优化器通常设计为最小化损失，而我们需要最大化概率（似然），所以加上负号将对数似然转化为最小化问题。

4.  **逻辑关联**
    *   本次推导的损失函数，正是为了**优化**前文所介绍的 Bradley-Terry 模型。通过最小化这个损失，可以训练奖励模型 `r_φ`，使其给出的分数能够准确反映人类偏好数据中的排序关系。

**一句话总结：这张图揭示了如何将 Bradley-Terry 偏好概率模型，通过数学推导转化为一个基于 Sigmoid 函数和奖励值之差、可直接用于梯度下降优化的标准损失函数。**

## 3 - The optimal policy

### **内容概括**

本图节选自对DPO论文的解读，其核心目的是解决一个关键难题：在强化学习从人类反馈中学习（RLHF）中，通过奖励函数来优化策略模型时，会面临一个**计算上不可行**的瓶颈。

1.  **提出经典解**：首先给出了在给定奖励函数 $ r(x, y) $ 和参考策略 $ \pi_{\text{ref}} $ 时，通过最大化奖励并最小化与参考策略差异（KL散度）的约束优化问题，其**理论上的最优解**公式。
2.  **指出核心障碍**：然而，这个解中的归一化项 $ Z(x) $ 需要对所有可能的输出 $ y $ 求和，这在语言模型巨大生成空间的情况下是**计算上不可行的**，因为无法穷举所有回答。
3.  **逆向推导**：既然正向求解不可行，文档转换思路，进行了一个“**思想实验**”：假设我们**已经知道**了那个最优策略 $ \pi^* $，那么它所隐含的奖励函数应该是什么样的？
4.  **揭示等价关系**：通过数学推导，得出了奖励函数可以通过最优策略和参考策略的**概率比**来表达。这为后续绕过奖励模型、直接利用偏好数据优化策略模型（即DPO）奠定了理论基础。

---

### **公式推导解释**

推导过程分为两步：先写出已知的“最优策略解”，再从其出发反向推导出奖励函数。

#### **第一步：已知的正向关系**

首先，文档给出了RLHF中一个标准问题的解：寻找一个策略 $ \pi $，使其在最大化期望奖励的同时，不过分偏离原始的参考策略 $ \pi_{\text{ref}} $。这个解就是：

$$
\pi_{r}(y|x) = \frac{1}{Z(x)} \pi_{\text{ref}}(y|x) \exp\left( \frac{1}{\beta} r(x, y) \right)
$$

**参数说明：**
*   $ \pi_{r}(y|x) $：我们希望得到的、与奖励函数 $ r $ 对应的最优策略。
*   $ \pi_{\text{ref}}(y|x) $：初始的参考模型（例如SFT微调后的模型）。
*   $ r(x, y) $：奖励模型，用于评估在提示 $ x $ 下回答 $ y $ 的好坏。
*   $ \beta $：控制参数，调节对奖励的追求与对参考策略的保守程度之间的平衡。
*   $ Z(x) $：**归一化常数/配分函数**，作用是确保 $ \pi_{r} $ 对所有 $ y $ 的概率之和为1。其定义为：
    $$
    Z(x) = \sum_{y} \pi_{\text{ref}}(y|x) \exp\left( \frac{1}{\beta} r(x, y) \right)
    $$
    **关键**：计算 $ Z(x) $ 需要对模型所有可能的输出 $ y $ 进行求和，这在词汇表巨大、序列长度可变的情况下是**不可能完成**的。这就是正向求解的瓶颈。

#### **第二步：从最优策略反向推导奖励函数（核心推导）**

既然正向计算 $ Z(x) $ 不可行，文档转而思考一个反向问题：假设我们**已经有了**那个最终的最优策略 $ \pi^* $（即 $ \pi_{r} $），那么它所对应的奖励函数 $ r(x, y) $ 该如何表示？

推导从第一步的正向关系等式出发，通过取对数并进行代数变换完成。

1.  **对最优策略公式两边取对数：**
    $$
    \log \pi^*(y|x) = \log \left[ \frac{1}{Z(x)} \pi_{\text{ref}}(y|x) \exp\left( \frac{1}{\beta} r(x, y) \right) \right]
    $$

2.  **利用对数性质展开（$ \log(ab) = \log a + \log b $）：**
    $$
    \log \pi^*(y|x) = \log \pi_{\text{ref}}(y|x) + \log \left[ \exp\left( \frac{1}{\beta} r(x, y) \right) \right] + \log \left[ \frac{1}{Z(x)} \right]
    $$

3.  **简化各项：**
    *   $ \log \left[ \exp\left( \frac{1}{\beta} r(x, y) \right) \right] = \frac{1}{\beta} r(x, y) $
    *   $ \log \left[ \frac{1}{Z(x)} \right] = -\log Z(x) $
    代入后得到：
    $$
    \log \pi^*(y|x) = \log \pi_{\text{ref}}(y|x) - \log Z(x) + \frac{1}{\beta} r(x, y)
    $$

4.  **整理方程，解出 $ r(x, y) $：**
    将含有 $ r(x, y) $ 的项移到一边：
    $$
    \frac{1}{\beta} r(x, y) = \log \pi^*(y|x) - \log \pi_{\text{ref}}(y|x) + \log Z(x)
    $$

5.  **两边同时乘以 $ \beta $，得到最终形式：**
    $$
    r(x, y) = \beta \log \frac{\pi^*(y|x)}{\pi_{\text{ref}}(y|x)} + \beta \log Z(x)
    $$

**推导结论的意义：**
这个推导得出了一个关键洞察：**最优策略 $ \pi^* $ 所对应的奖励函数 $ r(x, y) $，可以由最优策略与参考策略的概率比的对数来线性表示（加上一个仅与提示 $ x $ 有关、与具体回答 $ y $ 无关的偏移项 $ \beta \log Z(x) $）。**

这个等式意味着，如果我们能直接得到或学习到一个策略 $ \pi^* $，我们就**无需再显式地训练一个独立的奖励模型**。这为DPO算法奠定了基础：DPO通过直接在偏好数据上优化策略模型，让其逼近这个隐含的最优策略 $ \pi^* $，从而巧妙地绕过了训练奖励模型和计算不可行的 $ Z(x) $ 这两大难题。

## 4 - … remember the Bradley-Terry model?

### **内容概括**

本幻灯片以前文推导的**Bradley-Terry偏好概率模型**和**最优策略所隐含的奖励函数表达式**为基础，进行了一次**关键的代入与化简**，从而绕过了奖励模型。

1.  **起点回顾**：首先回顾了Bradley-Terry模型，并引出一个关键数学变换，即该模型的概率可以用奖励值之差的**Sigmoid函数**表示：`P(y_w > y_l) = σ(r(x, y_w) - r(x, y_l))`。
2.  **代入关键表达式**：将上一环节推导出的奖励函数表达式 `r(x, y) = β log(π*/π_ref) + β log Z(x)` 代入到上述概率公式中。
3.  **精妙的抵消**：代入后，奖励函数中复杂且难以计算的归一化项 `β log Z(x)` 在计算 `(r(x, y_w) - r(x, y_l))` 时**被完全抵消**。这是整个推导的精华所在，它消去了计算上不可行的部分。
4.  **得到可计算的损失函数**：经过化简，偏好概率完全由**当前待优化的策略模型 `π_θ`** 和**参考策略 `π_ref`** 的概率比决定。为了最大化优质回答被选中的概率，我们只需最小化其负对数似然，由此便得到了**DPO损失函数 `L_DPO`**。
5.  **揭示本质**：最终的结论是，DPO方法**不再优化一个独立的奖励模型**，而是**直接优化策略模型本身**（这个策略模型本身就编码了最优奖励函数的信息），从而提供了一种高效、优雅的解决方案。

---

### **要点总结**

1.  **核心技巧：代入与抵消**
    *   将奖励函数的理论解 `r(x, y)` 代入 Bradley-Terry 概率公式。
    *   在计算奖励值之差 `r(x, y_w) - r(x, y_l)` 时，两个奖励函数表达式中都包含的 `β log Z(x)` 项**恰好相减归零**。这个抵消移除了整个训练过程中最大的计算障碍。

2.  **最终的DPO损失函数**
    *   经过抵消和化简，偏好概率简化为：
        `P(y_w > y_l) = σ( β log[π_θ(y_w|x)/π_ref(y_w|x)] - β log[π_θ(y_l|x)/π_ref(y_l|x)] )`
    *   其对应的负对数似然损失 `L_DPO` 即为图中最终的公式。该损失函数**仅依赖于**：
        *   **可优化的策略 `π_θ`**
        *   **固定的参考策略 `π_ref`**
        *   **偏好数据集 `D`**
        *   **超参数 `β`**

3.  **方法的重大意义**
    *   **绕过奖励建模**：不再需要训练一个独立的、复杂的奖励模型，避免了奖励模型拟合错误和偏差的风险。
    *   **计算高效**：损失函数中的所有项（策略模型的输出概率）都可以直接、高效地计算，使得训练像标准的监督微调一样稳定和直接。
    *   **端到端优化**：直接调整语言模型的策略参数 `θ`，使其生成的回答 `π_θ(y|x)` 的概率分布能够完美吻合人类偏好数据中体现的排序关系。

**一句话总结：这张图通过精妙的代数代入与抵消，将基于奖励模型的复杂优化问题，转化为一个可以直接对策略模型进行梯度下降的、简洁优雅的损失函数，这正是DPO算法的核心创新与实现方式。**

## 5 - Calculating log probabilities

### **内容概括**

这三张材料共同解决了DPO算法实现中的一个关键实践问题：如何具体计算损失函数 $L_{DPO}$ 中所需的策略模型和参考模型的对数概率 $\log \pi(y|x)$。

1.  **第一张图：问题提出与公式回顾**
    *   **核心任务**：明确要计算 $L_{DPO}$ 损失，必须能计算出给定模型（$\pi_\theta$ 或 $\pi_{ref}$）在输入 $x$ 下生成特定序列 $y$ 的概率的对数值。
    *   **公式锚点**：再次给出了完整的DPO损失函数公式，明确了我们需要计算的核心项是 $\log \frac{\pi_{\theta}(y_{w} \mid x)}{\pi_{\text{ref}}(y_{w} \mid x)}$ 这类对数概率比。
    *   **参数说明**：补充了超参数 $\beta$（温度参数）的典型取值范围（0.1-0.5）及其意义（控制与参考模型的偏离程度，$\beta \to 0$ 时忽略参考模型）。

2.  **第二张图：计算流程示意图解**
    *   **流程演示**：以一个具体的生成案例（“Shanghai is in China”）为例，形象化地展示了**自回归语言模型**计算序列概率的标准过程。
    *   **核心步骤**：展示了从输入词元（Token）开始，经过Transformer层得到隐藏状态，再通过线性投影层得到每个词元的Logits，最后经过 **`log_softmax`** 函数得到每个位置所有可能词元的对数概率。
    *   **关键操作**：为了得到模型生成整个序列 $y$ 的对数概率 $\log \pi(y|x)$，需要**在每一个生成步骤上，选取真实下一个词元（Ground Truth Next Token）所对应的对数概率**，并将整个序列的这些概率值**求和**。

3.  **第三张图：代码实现详解**
    *   **函数定义**：展示了一个名为 `get_batch_logps` 的静态方法，它正是实现上述计算逻辑的PyTorch代码。
    *   **核心操作**：
        *   **`log_softmax`**：对模型输出的logits进行处理，得到归一化的对数概率分布。
        *   **`torch.gather`**：根据标签（即真实的下一个词元索引），从 `log_softmax` 后的张量中精准地“收集”出每个位置对应词元的对数概率。图中橙色箭头清晰地标注了这一对应关系。
        *   **掩码处理**：使用注意力掩码（`attention_mask`）来忽略填充部分（padding tokens）的无效计算。
        *   **求和或平均**：对提取出的序列对数概率进行求和（得到整个序列的联合对数概率）或平均（可选），作为函数的返回值。

---

### **要点总结**

1.  **计算目标明确**：DPO损失的计算，最终归结为高效、准确地计算 **$\log \pi_{\theta}(y|x)$** 和 **$\log \pi_{ref}(y|x)$**。两者计算方式完全相同，只是输入的模型不同。

2.  **理论基础：序列概率的分解**：
    *   对于自回归语言模型，生成一个序列的概率等于**每一步生成正确下一个词元的条件概率的连乘**。
    *   取对数后，连乘变为**求和**：$\log \pi(y|x) = \sum_{t=1}^{T} \log p(y_t | x, y_{<t})$。
    *   这正是第二张图流程和第三张图代码中“求和”操作的数学依据。

3.  **实现关键两步**：
    *   **第一步：获取每个位置的对数概率分布**。通过对模型输出的logits应用 **`log_softmax`** 实现，这得到了每个位置上所有词元的“评分”（对数概率）。
    *   **第二步：提取特定路径的概率**。使用 **`torch.gather`** 函数，根据真实的词元序列（标签），从第一步得到的分布中，像用索引查表一样，提取出序列每一步实际发生的对数概率。

4.  **工程化细节**：
    *   **批次处理**：代码是批量化操作的，可以同时为多个训练样本 $(x, y_w, y_l)$ 计算对数概率。
    *   **掩码重要性**：必须使用注意力掩码来确保计算只针对有效词元进行，避免填充符干扰损失计算。
    *   **参考模型固定**：在训练中，参考模型 $\pi_{ref}$ 的参数是冻结的（不更新），仅用于前向传播计算概率，这确保了正则化项的稳定性。

**总结来说，这三张材料清晰地勾勒出了DPO算法从理论损失函数到实际训练代码的桥梁：通过标准语言模型的前向传播获取logits，经过 `log_softmax` 和 `gather` 操作提取序列对数概率，最后将这些值代入 $L_{DPO}$ 公式进行梯度优化。**

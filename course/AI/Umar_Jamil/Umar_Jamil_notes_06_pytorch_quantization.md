本文主要整理pytorch-quantization的主要内容。

## 1 - What is quantization?

**1. 待解决的问题（量化技术的动因）**
*   **模型庞大**：现代深度神经网络（如LLaMA 2）拥有数十亿参数。
*   **存储与内存压力大**：以32位浮点数存储，仅70亿参数的模型就需约28GB空间，推理时需全部加载至内存，远超普通PC或手机的承载能力。
*   **计算效率低**：计算机处理浮点数运算的速度远慢于整数运算。

**2. 解决方案：量化的原理与效果**
*   **核心思想**：通过降低表示每个参数所需的位数来“压缩”模型，通常是将**浮点数转换为整数**。
*   **关键说明**：量化并非对浮点数进行简单的截断或四舍五入，而是一套有数学依据的映射方法。
*   **直接效果**：能将一个10GB的模型压缩至1GB以下（具体取决于量化类型），并因操作更小的数据类型而**加速计算**。

**3. 量化的主要优势**
*   **降低内存消耗**：使大模型能在智能手机等资源受限的设备上加载和运行，至关重要。
*   **减少推理时间**：使用更简单的数据类型（如整数）进行计算，速度更快。
*   **节省能源**：由于总体计算量减少，执行推理所消耗的能耗也随之降低。

## 2 - How are integers represented in the CPU (or GPU)?

### **1. 整数在硬件中的表示（图1）**
*   **固定位数**：计算机使用固定长度的比特位（如8, 16, 32, 64位）来表示所有数据。`n` 位可以表示 `2^n` 个不同的数字。
*   **二进制**：整数以二进制形式存储。例如，二进制 `110` 对应十进制 `1×2² + 1×2¹ + 0×2⁰ = 6`。
*   **补码**：现代CPU普遍使用**二进制补码**来表示有符号整数：
    *   最高位（首位）表示符号（0为正，1为负）。
    *   正数的其余位表示其绝对值。
    *   负数的其余位是其绝对值的补码。
    *   这种表示法的优势之一是能**唯一地表示数字0**。

### **2. Python处理超大整数的机制与限制（图2）**
*   **大数算术**：Python（CPython解释器）使用 **`BigNum`** 机制处理任意大的整数。它将数字存储为以 `2^30` 为基数的数字数组。
*   **软件实现**：这种能力是解释器提供的软件功能，**并非CPU/GPU的硬件原生支持**。
*   **硬件加速的限制**：当需要利用**CUDA等硬件进行加速计算**时，必须使用硬件支持的固定格式数字（通常是32位整数或浮点数）。Python的灵活大数表示在此场景下无法直接获得硬件加速。
*   **示例**：数字 `2^9999` 在Python内部被存储为一个包含334个元素的数组（大部分为0），而不是一个单一的硬件单元。

### **3. 浮点数的标准表示（图3）**
*   **十进制类比**：浮点数可以类比为包含基数负幂次的十进制小数（如 `85.612 = 8×10¹ + 5×10⁰ + 6×10⁻¹ ...`）。
*   **IEEE-754标准**：这是定义浮点数表示（尤其是32位单精度）的通用国际标准。
*   **浮点数结构**：一个浮点数（以32位为例）由三部分组成：
    1.  **符号位**：1位，表示正负。
    2.  **指数位**：若干位，表示阶码（经过偏移处理）。
    3.  **尾数位**：若干位，表示有效数字的小数部分（隐含前导1）。
*   **计算方式**：浮点数的值 = `(-1)^符号位 × (1.尾数) × 2^(指数 - 偏移量)`。图中以二进制 `0 01111100 01000000000000000000000` 为例，其值为 `+1.25 × 2^(-3) = 0.15625`。
*   **不同精度**：除了32位单精度，现代GPU也广泛支持精度更低、占用空间更小的**16位半精度浮点数**，这在高性能计算和AI中尤为重要。

## 3 - Asymmetric vs Symmetric quantization

### **1. 核心目标与前提**
*   **共同目标**：将高精度浮点数（如FP32）映射到低比特整数（如INT8）表示，以减少模型存储和计算开销。
*   **关键参数**：`n` 表示量化后的比特位数（例如 `n=8`）。
*   **核心步骤**：均包含**量化**（浮点→整数）和**反量化**（整数→浮点）两个过程。

### **2. 非对称量化**
*   **表示范围**：
    *   **原始浮点数范围**：`[β, α]`，其中 β 是最小值，α 是最大值。范围不对称于零点（如图中 `[-44.93， 43.31]`）。
    *   **量化后整数范围**：`[0, 2^n - 1]`（如8比特时为 `[0, 255]`）。
*   **核心参数**：
    1.  **比例因子**：`s = (α - β) / (2^n - 1)`。决定每个整数步长对应的浮点数值。
    2.  **零点**：`z = round(-β / s)`。一个整数偏移量，用于将浮点数的真实零点映射到整数域。
*   **量化与反量化公式**：
    *   **量化**：`x_q = clamp(round(x_f / s) + z; 0; 2^n - 1)`
    *   **反量化**：`x_f = s * (x_q - z)`
*   **优势与适用场景**：
    *   **优势**：能更精确地匹配原始数据的实际分布范围，减少因范围浪费带来的精度损失。尤其适合数据分布明显不关于零点对称的情况。
    *   **代价**：计算过程中需要处理零点 `z`，在硬件加速时可能引入额外的开销。

### **3. 对称量化**
*   **表示范围**：
    *   **原始浮点数范围**：`[-α, α]`，其中 α 是绝对值的最大值。范围对称于零点（如图中 `[-44.93， +44.93]`）。
    *   **量化后整数范围**：`[-(2^{n-1} - 1)， 2^{n-1} - 1]`（如8比特时为 `[-127， 127]`）。
*   **核心参数**：
    *   **比例因子**：`s = α / (2^{n-1} - 1)`。由最大绝对值决定。
    *   **无零点**：对称量化的零点固定为整数0。
*   **量化与反量化公式**：
    *   **量化**：`x_q = clamp(round(x_f / s); -(2^{n-1} - 1); 2^{n-1} - 1)`
    *   **反量化**：`x_f = s * x_q`
*   **优势与适用场景**：
    *   **优势**：计算极其简单高效，**无需处理零点偏移**。在GPU等硬件上进行整数矩阵运算时，这种简化能带来显著的性能提升。
    *   **代价**：如果原始数据分布不对称，则会浪费一部分整数表示范围（例如，全部为正的数据，将有一半的整数值 `[-127， -1]` 未被使用），可能略微增加精度损失。

### **4. 两种方法的核心对比**
| 特性 | **非对称量化** | **对称量化** |
| :--- | :--- | :--- |
| **数据范围** | 适配任意范围 `[β, α]` | 强制为对称范围 `[-α， α]` |
| **关键参数** | **比例因子 `s`** 和 **零点 `z`** | **仅比例因子 `s`** |
| **量化公式** | `x_q = round(x_f / s) + z` | `x_q = round(x_f / s)` |
| **计算复杂度** | 较高（需处理`z`） | **较低**（无`z`） |
| **精度潜力** | **较高**（能充分利用表示范围） | 可能较低（可能浪费表示范围） |
| **典型应用** | 对精度要求极高的场景，或数据分布严重偏斜时 | 追求极致推理速度的场景，尤其是硬件加速 |

**总结来说**：**非对称量化以稍复杂的计算换取更高的精度保真度；而对称量化则通过简化计算（牺牲零点对齐）来最大化硬件执行效率。** 在实际的模型部署中，选择哪种方案取决于对精度损失和推理延迟的具体要求。

## 4.0 - Applying quantization: integer case

### **1. 核心流程与公式**
*   **流程目标**：实现完全使用**整数运算**的神经网络推理，以提升速度并降低功耗。
*   **核心操作**：图片中心给出了量化推理的**核心数学表达式** `Y_q = XW + B`。
    *   **`X`**：量化的输入（激活值）。
    *   **`W`**：量化的权重。
    *   **`B`**：量化的偏置。
    *   **`Y_q`**：计算得到的**量化后的输出**（整数形式）。

### **2. 各组成部分的量化方式**
*   **输入**：
    *   有时被称为“激活”。
    *   量化方式有两种：
        1.  **动态量化**：在推理时“实时”根据输入数据计算量化参数。
        2.  **静态量化（使用观察器）**：通过预先观察典型输入数据（校准）来确定固定的量化参数。
*   **权重**：
    *   通常在模型转换或训练后**静态量化**，参数固定。
*   **偏置**：
    *   为确保累加精度，通常被量化为 **`int32`** 格式（比`int8`权重和激活拥有更高的数值范围和精度）。

### **3. 关键挑战与解决方案：输出的反量化**
*   **核心问题**：输出 `Y_q` 是整数运算的结果，如何将其恢复为有意义的浮点数（反量化）？我们并未事先计算输出 `Y` 的量化参数（比例 `s` 和零点 `z`）。
*   **解决方案：校准**
    *   **做法**：使用少量有代表性的输入数据运行几次量化后的模型，**观察**输出 `Y` 的典型数值分布。
    *   **目的**：根据观察到的分布，计算出用于输出反量化的**比例因子 `s`** 和**零点 `z`**。
    *   **归属**：这个过程属于 **“训练后量化”** 的一个关键步骤。

### **4. 核心优势**
*   **纯整数算术**：图中明确强调 **“Perform all operations using integer arithmetic”**。这是量化的主要收益之一，因为整数运算在硬件上远比浮点运算高效、快速且节能。

## 4.1 - Low-precision matrix multiplication

### **内容概况**
这张图片是此前量化应用流程的技术延伸，**聚焦于硬件执行层面**，解释了在量化推理中，**GPU如何利用其专用硬件单元（MAC模块）来加速核心的整数矩阵运算**。它以神经网络线性层的基础计算 `XW + B` 为例，通过一个清晰的乘累加（MAC）单元流程图，展示了从低精度输入到整数结果产生的微观过程，是整个量化技术栈中连接算法与硬件的关键环节。

---

### **要点总结**

#### **1. 核心计算目标**
*   **任务**：完成量化后线性层的前向传播计算，即 `Y_q = XW + B`。
*   **形式**：此时，输入 `X`、权重 `W` 和输出 `Y_q` 通常为`int8`，而偏置 `B` 通常为`int32`。

#### **2. 硬件加速机制：乘累加单元**
*   **MAC模块**：GPU内置的专用物理计算单元，核心功能是高效执行**乘法**和**累加**操作。
*   **工作原理（对应流程图）**：
    1.  **取数**：从输入矩阵 `X` 的一行和权重矩阵 `W` 的一列，**逐一取出对应元素**。
    2.  **相乘**：将取出的一对`int8`元素送入乘法器，得到中间乘积（图中`C₁,₁`等）。
    3.  **累加**：将中间乘积结果累加到一个**累加器寄存器**中。**累加器通常有更高的位宽（如int32）**，以防止在多轮累加过程中溢出。
    4.  **初始化**：累加器的初始值被设置为对应偏置 `B` 的`int32`值。
*   **最终输出**：当一行与一列的所有对应元素都完成乘累加后，累加器中的值就是输出矩阵 `Y_q` 中一个元素的`int32`中间结果。

#### **3. 大规模并行实现**
*   **核心策略**：GPU会启动**海量的MAC模块并行工作**。
*   **任务分配**：每个MAC模块（或一组模块）负责计算输出矩阵 `Y_q` 中的一个或一小块元素。
*   **性能来源**：正是这种大规模的硬件并行性，使得低精度矩阵乘法能获得远超通用CPU的极速性能。

#### **4. 关键细节与扩展**
*   **偏置的位宽**：明确指出偏置 `B` 通常被量化为`int32`。这是因为多个`int8`乘积累加的结果范围很大，需要更宽的位宽来容纳，从而保证计算的数值精度。
*   **数学原理参考**：推荐了**Google的GEMM库**作为深入了解底层数学（如如何处理量化尺度因子和零点）的优质资源，这为从理论到工程实现提供了桥梁。

---

### **总结**
量化（从FP32到INT8等）不仅是为了压缩模型，更是为了**将计算映射到GPU等硬件最擅长的、高度并行的低精度整数乘累加操作上**。**MAC单元是执行这一操作的基础硬件单元，而大规模的并行化是其实现加速的根本原因。** 这解释了为何量化能带来显著的推理速度提升。

## 4.2 - Quantization range: how to choose [𝛼, 𝛽]

### **整体内容概括**
这两张图片从 **“工程实践”** 和 **“理论优化”** 两个互补的视角，系统阐述了在模型量化中确定关键参数——**量化范围 [α, β]** 的不同策略。
*   **第一张图**：以**可视化对比**为核心，通过一个包含异常值的具体数据序列，生动演示了 **“最小-最大值法”** 和 **“百分位法”** 这两种基础方法的原理、效果及在处理异常值时的巨大差异。其结论直观：**百分位法通过牺牲极端异常值的精度，换取了主体数据更高的量化保真度。**
*   **第二张图**：从**优化目标**出发，介绍了两种更高级、更理论化的策略：**均方误差最小化**和**交叉熵最小化**。这些方法旨在通过数学优化，使量化后的张量在特定评估指标下最接近原始张量，适用于对精度有更高要求的复杂场景。

---

### **核心要点总结**

#### **1. 基础方法：处理异常值的直观策略**
这类方法直接基于待量化张量 `V` 的数值分布进行计算。
*   **最小-最大值法**
    *   **做法**：直接取张量的最大值和最小值作为范围边界：`α = max(V)`, `β = min(V)`。
    *   **优点**：确保覆盖所有数据点，无信息在理论上被完全截断。
    *   **缺点**：**对异常值极度敏感**。一个极端值会迫使范围被拉宽，导致用于表示绝大多数正常值的整数区间被压缩，从而**显著增大整体量化误差**（如图中正常值38.48的误差变大）。
*   **百分位法**
    *   **做法**：不采用绝对最大/最小值，而是使用分布的某个百分位数（如99.9%和0.1%）作为范围边界。
    *   **优点**：**有效抵抗异常值干扰**。通过“截断”极端值，将宝贵的表示精度集中用于主体数据分布，从而**显著降低大多数数据的量化误差**（如图中除1000外的其他值还原度很高）。
    *   **缺点**：异常值本身会产生巨大量化误差。这通常是可以接受的，因为异常值本身可能就是噪声或无意义的。

#### **2. 优化方法：基于特定目标的精细策略**
这类方法将范围选择定义为一个数学优化问题，以寻找特定评价指标下的最优解。
*   **均方误差最小化**
    *   **优化目标**：寻找使原始张量 `V` 与反量化后张量之间的**均方误差**最小的 `[α, β]`。
    *   **核心思想**：追求整体数值上的最小偏差，是一种最通用的精度保持策略。
    *   **求解方法**：由于问题可能非凸，常使用**网格搜索**等数值方法进行求解。
*   **交叉熵最小化**
    *   **适用场景**：专门用于张量中**数值的重要性不同**的情况。典型例子是大语言模型中的**Softmax层输出**（代表概率分布）。
    *   **优化目标**：寻找使原始分布与量化后分布之间的**交叉熵**最小的 `[α, β]`。
    *   **核心思想**：对于推理策略（如贪婪解码、Top-P采样）而言，**保持概率值之间的相对顺序（尤其是最大值的顺序）比精确还原每个数值更重要**。最小化交叉熵正是为了在量化后最好地保持这种分布特性。

### **方法选择总结**
| 方法 | 核心原理 | 优点 | 缺点/适用场景 |
| :--- | :--- | :--- | :--- |
| **最小-最大值法** | 覆盖全部数据范围 | 简单直接，确保无数据被截断 | 对异常值敏感，通常导致最差的整体精度 |
| **百分位法** | 截断分布两端极端值 | 鲁棒性强，能大幅提升主体数据精度 | 异常值误差大，是实践中最常用的启发式方法 |
| **MSE最小化** | 最小化数值平方误差 | 理论最优的整体数值保真度 | 计算成本较高，需网格搜索 |
| **交叉熵最小化** | 最小化分布差异 | 在保持概率分布顺序上最优 | 专用于Softmax等概率输出层 |

**总而言之**，选择量化范围需要在**实现复杂性**、**计算开销**和**精度目标**之间进行权衡。**百分位法**因其简单有效成为广泛使用的默认方法，而在对精度有极致要求的场景下，则会采用基于**MSE**或**交叉熵**的优化方法。

## 4.3 - Quantization granularity

### **1. 核心概念：什么是“量化粒度”？**
*   **定义**：指在量化过程中，**共享同一套量化参数**（如比例因子 `s`、零点 `z` 或剪切范围 `[α, β]`）的数据单元的大小。
*   **粒度越粗**：共享参数的单位越大（如整个层），计算越简单，但可能损失精度。
*   **粒度越细**：共享参数的单位越小（如每个通道），精度潜力越高，但计算复杂度和存储开销越大。

### **2. 两种量化策略的对比**

| 特性 | **逐层量化** | **逐通道量化** |
| :--- | :--- | :--- |
| **共享单位** | 整个网络层（Layer） | 层的每个输出通道（Channel） |
| **量化参数** | 全层**一套**参数（1个 `s`, 1个 `z`） | **每个通道一套**独立参数（N个 `s`, N个 `z`） |
| **精度** | **较低**。强制不同分布的数据通道使用相同范围，容易因“一刀切”而导致某些通道的数值被过度挤压，损失精度。 | **较高**。能适配每个通道独特的数据分布，更充分地利用有限的整数表示空间，保真度更好。 |
| **计算与存储开销** | **低**。计算时只需处理一组参数，内存占用小，硬件实现简单高效。 | **高**。需要为每个通道存储和应用不同的参数，计算复杂度更高，内存访问模式也可能更复杂。 |
| **适用场景** | 对计算效率、内存和功耗有极致要求的边缘设备部署。 | 对精度有较高要求，且算力资源相对充裕的场景（如服务器端推理）。 |

### **3. 核心结论与工程权衡**
*   **核心权衡**：**逐层量化用精度换取效率；逐通道量化用效率换取精度。**
*   **选择依据**：在实际部署中，选择哪种粒度取决于目标硬件的能力和对模型精度损失的具体要求。现代量化工具通常允许混合使用不同粒度（例如，对敏感性高的权重使用逐通道量化，对激活值使用逐层量化），以在精度和效率之间取得最佳平衡。

**总而言之**，这张图阐明了量化不仅仅是选择比特位数，**确定“在多大范围内共享量化参数”是影响最终模型性能与效率的关键设计决策。**

## 5 - Quantization Aware Training (QAT)

### **1. QAT的核心思想与流程**
*   **目标**：获得一个在部署时经过低比特（如INT8）量化后，精度损失最小的模型。
*   **方法**：不是在训练**完成**后再量化模型（训练后量化），而是在模型**训练**或**微调**阶段，就模拟量化带来的影响。
*   **关键操作**：在模型的计算图中，为权重和激活值插入 **“假”的量化与反量化模块**。
    *   **前向传播**：这些模块模拟真实的量化过程（舍入、截断），将高精度数值转换为低精度再转换回来，让损失函数“看到”量化后的效果。
    *   **反向传播**：通过特殊的梯度近似方法（STE）更新权重，使权重学会在存在量化噪声的情况下仍然有效。

### **2. QAT的关键技术：直通估计器**
*   **核心挑战**：量化操作（如舍入操作、截断到范围）的导数为零或不存在，无法直接进行反向传播。
*   **解决方案**：采用**直通估计器**（STE）进行梯度近似。
    *   **前向传播**：正常执行量化操作 `x_q = Quantize(x)`。
    *   **反向传播**：**忽略量化函数本身**，假设 `Quantize(x)` 的导数恒为1（当`x`在量化范围内时）。即，直接将损失对 `x_q` 的梯度 `∂L/∂x_q` 传递给输入 `x`（`∂L/∂x ≈ ∂L/∂x_q`）。
    *   **作用**：这使得梯度可以穿透量化模块，从而更新前面的网络层参数。虽然这是一种近似，但在实践中被证明非常有效。

### **3. QAT为何有效：损失曲面的优化**
*   **训练后量化的问题**：模型在训练时收敛到的高精度浮点空间最优点，在量化后可能恰好位于一个对数值扰动（即量化噪声）非常敏感的“陡峭峡谷”中，微小的权重变化就会导致输出和损失剧烈波动。
*   **QAT的效果**：通过在训练中引入模拟的量化噪声，QAT主动地“扰动”权重，并引导优化器去寻找一个对这类扰动**不敏感**的、更平坦的**最优区域**。
    *   如第三张图所示，经过QAT微调后，损失函数的极小值点从原来的尖峰（对量化敏感）移动到了一个宽阔平坦的谷底（对量化鲁棒）。
    *   这样，当模型真正被量化部署时，其性能表现会更加稳定，精度损失显著降低。

### **4. 总结**
**量化感知训练**是一种“先苦后甜”的模型优化策略。它通过在训练阶段主动引入并适应量化噪声，利用**直通估计器**解决梯度问题，最终引导模型收敛到一个对量化操作**鲁棒性更强**的参数空间。相比训练后量化，QAT通常能获得更高的最终精度，但代价是需要额外的训练或微调时间与计算资源。它是追求极致部署精度时的关键手段。
本文主要整理Stable Diffusion implemented from scratch in PyTorch的主要内容。

## 4.0 - encoder源码分析

### **整体架构概述**

这个编码器是**变分自编码器** 的编码器部分，用于将输入图像（如512×512×3）压缩到一个**低维潜在空间**（如64×64×4）。

### **关键设计目标**
1. **下采样**：将图像尺寸缩小8倍（512→64）。
2. **提取特征**：从RGB图像提取高级语义特征。
3. **输出分布参数**：输出均值和对数方差，定义潜在分布。

---

### **网络架构详解**

#### **1. 输入输出维度**
```python
# 输入
x: (Batch_Size, 3, Height, Width)  # 如 (1, 3, 512, 512)
noise: (Batch_Size, 4, Height/8, Width/8)  # 如 (1, 4, 64, 64)

# 输出
x: (Batch_Size, 4, Height/8, Width/8)  # 潜在表示 z
```

#### **2. 三层下采样结构**
网络通过三次2倍下采样，实现8倍总下采样率：

| 阶段 | 通道数 | 特征图大小 | 关键层 |
|------|--------|------------|--------|
| 初始 | 3→128 | 512×512 | 初始卷积 |
| 阶段1 | 128 | 512×512 | 2个残差块 |
| ↓下采样1 | 128→256 | 256×256 | 步长2卷积 |
| 阶段2 | 256 | 256×256 | 2个残差块 |
| ↓下采样2 | 256→512 | 128×128 | 步长2卷积 |
| 阶段3 | 512 | 128×128 | 2个残差块 |
| ↓下采样3 | 512 | 64×64 | 步长2卷积 |
| 阶段4 | 512 | 64×64 | 3个残差块 + 注意力 |

#### **3. 关键组件解释**

**a) 残差块 `VAE_ResidualBlock`**
- **作用**：增强梯度流动，防止梯度消失
- **结构**：类似ResNet的残差连接，包含卷积、归一化、激活
- **位置**：在每个分辨率层级都有多个残差块，加深网络深度

**b) 注意力块 `VAE_AttentionBlock`**
- **作用**：捕捉长距离依赖，让不同位置的特征可以相互影响
- **原理**：类似Transformer的自注意力机制
- **位置**：只在最低分辨率（64×64）使用，因为计算成本高

**c) 特殊填充处理**
```python
# 对步长为2的卷积进行非对称填充
if getattr(module, 'stride', None) == (2, 2):
    x = F.pad(x, (0, 1, 0, 1))  # 只在右下角填充
```
- **原因**：普通对称填充可能导致下采样时特征对齐问题
- **效果**：确保下采样后的特征图尺寸精确减半

---

### **VAE 核心原理实现**

#### **1. 输出分布参数**
```python
# 最终卷积输出8通道
nn.Conv2d(512, 8, kernel_size=3, padding=1)
# 分为均值和方差
mean, log_variance = torch.chunk(x, 2, dim=1)  # 各4通道
```
- 前4通道：均值 `μ`（Batch_Size, 4, H/8, W/8）
- 后4通道：对数方差 `log σ²`（Batch_Size, 4, H/8, W/8）

#### **2. 数值稳定性处理**
```python
log_variance = torch.clamp(log_variance, -30, 20)
variance = log_variance.exp()
stdev = variance.sqrt()
```
- **截断**：限制 `log σ²` 在[-30, 20]之间，防止数值溢出
- **转换**：`log σ² → σ² → σ`，得到标准差

#### **3. 重参数化技巧**
```python
x = mean + stdev * noise
```
- **`noise`**：外部传入的 `N(0, I)` 标准正态噪声
- **原理**：通过确定性变换从 `N(0, I)` 得到 `N(μ, σ²I)` 的样本
- **梯度可导**：梯度可以通过 `μ` 和 `σ` 回传

#### **4. 缩放因子**
```python
x *= 0.18215
```
- **作用**：将潜在变量缩放到合适范围
- **来源**：Stable Diffusion官方实现的经验值
- **目的**：匹配扩散模型训练时的数据分布

---

### **与标准VAE的区别**

#### **1. 条件编码**
```python
def forward(self, x, noise):  # 接收额外噪声参数
```
- 传统VAE：在forward内部采样噪声
- 此实现：外部传入噪声，便于控制（如固定种子重现结果）

#### **2. 输出设计**
- 输出不是单个潜在编码，而是**整个潜在特征图**
- 保留了空间结构（64×64网格），每个位置对应输入图像的一个区域

#### **3. 高级特征提取**
- 使用**残差连接**和**注意力机制**，比简单VAE更强大
- 能捕捉更复杂的图像语义信息

---

### **在Stable Diffusion中的作用**

#### **训练阶段**
1. 图像 `x` → 编码器 → 潜在表示 `z`
2. 在潜在空间 `z` 上添加扩散噪声
3. 训练U-Net在潜在空间去噪

#### **推理阶段**
1. 扩散模型在潜在空间生成 `z`
2. `z` → VAE解码器 → 生成图像
3. **注意**：编码器在推理时**只用于图像到潜编码的转换**（如img2img），不用于文本生成图像

#### **为什么高效？**
- 原始空间：512×512×3 = 786,432 维度
- 潜在空间：64×64×4 = 16,384 维度
- **计算量减少约98%**

## 4.1 - decoder源码分析

### **整体架构概述**

这个解码器是VAE的解码部分，负责将**低维潜在表示**（如64×64×4）**重建回原始图像**（如512×512×3）。它是编码器的**对称逆过程**。

### **关键设计目标**
1. **上采样**：将潜在空间尺寸放大8倍（64→512）。
2. **特征重建**：从高级语义特征重建具体像素。
3. **保持一致性**：确保重建图像与输入图像在语义上一致。

---

### **核心组件详解**

#### **1. VAE_ResidualBlock（残差块）**

这是解码器的**基本构建块**，与编码器中的残差块对称。

```python
class VAE_ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        # 核心结构：GroupNorm + SiLU + Conv
        self.groupnorm_1 = nn.GroupNorm(32, in_channels)
        self.conv_1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
        # 第二层
        self.groupnorm_2 = nn.GroupNorm(32, out_channels)
        self.conv_2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)
```

**工作原理**：
1. **输入归一化**：`GroupNorm(32, in_channels)` 将通道分为32组进行归一化，比BatchNorm更稳定
2. **激活函数**：`SiLU`（Sigmoid Linear Unit）是Swish激活，`x * sigmoid(x)`，平衡线性和非线性
3. **特征变换**：3×3卷积进行特征提取
4. **残差连接**：`x + self.residual_layer(residue)` 确保梯度流动

**为什么用残差块**：
- 防止深度网络中的梯度消失/爆炸
- 允许网络学习残差映射，而非完整映射，更容易优化
- 在解码器中帮助保持细节信息

#### **2. VAE_AttentionBlock（注意力块）**

```python
class VAE_AttentionBlock(nn.Module):
    def __init__(self, channels):
        self.groupnorm = nn.GroupNorm(32, channels)
        self.attention = SelfAttention(1, channels)  # 1个头
```

**工作原理**：
1. **重塑张量**：将4D特征图 `(B, C, H, W)` 转换为3D序列 `(B, H*W, C)`
2. **自注意力**：让每个位置的特征关注所有其他位置
3. **恢复形状**：转换回原始4D格式
4. **残差连接**：`x += residue`

**为什么在解码器中使用注意力**：
- **全局一致性**：确保生成图像的各个部分协调一致
- **长距离依赖**：处理图像中相隔较远但语义相关的部分
- **只在最低分辨率使用**：64×64时计算成本可接受

---

### **解码器主网络架构**

解码器采用**渐进上采样**结构，分三步从64×64恢复到512×512。

#### **1. 初始处理**
```python
# 步骤1：缩放恢复
x /= 0.18215  # 逆缩放，抵消编码器的缩放

# 步骤2：通道扩展
nn.Conv2d(4, 512, kernel_size=3, padding=1)  # 4通道→512通道
```

#### **2. 三层上采样流程**

| 阶段 | 输入尺寸 | 输出尺寸 | 通道变化 | 关键操作 |
|------|----------|----------|----------|----------|
| **阶段1** | 64×64 | 64×64 | 512→512 | 5个残差块 + 1个注意力块 |
| ↑上采样1 | 64×64 | 128×128 | 512→512 | `Upsample(scale_factor=2)` |
| **阶段2** | 128×128 | 128×128 | 512→512 | 3个残差块 |
| ↑上采样2 | 128×128 | 256×256 | 512→256 | `Upsample` + 残差块降维 |
| **阶段3** | 256×256 | 256×256 | 256→256 | 3个残差块 |
| ↑上采样3 | 256×256 | 512×512 | 256→128 | `Upsample` + 残差块降维 |
| **最终处理** | 512×512 | 512×512 | 128→3 | GroupNorm + SiLU + Conv |

#### **3. 上采样方法**
```python
nn.Upsample(scale_factor=2)  # 最邻近插值上采样
```
- **简单高效**：只是重复像素值
- **配合卷积**：上采样后立即用3×3卷积平滑和细化特征
- **与编码器对称**：编码器用步长2卷积下采样，解码器用上采样恢复

---

### **与编码器的对称性**

解码器设计**严格对称**于编码器，这是VAE的标准架构。

| 编码器操作 | 解码器对应操作 | 目的 |
|-----------|---------------|------|
| 下采样卷积（stride=2） | 上采样（Upsample） | 尺寸变换 |
| 通道增加（128→256→512） | 通道减少（512→256→128→3） | 特征压缩/展开 |
| 残差块堆叠 | 残差块堆叠 | 特征提取/重建 |
| 注意力块（64×64） | 注意力块（64×64） | 全局一致性 |
| 输出8通道（μ和logσ²） | 输入4通道（潜在z） | 概率分布↔采样 |

**为什么需要对称**：
- 确保信息无损重建的理论基础
- 训练更稳定，收敛更快
- 潜在空间结构更合理

---

### **关键数值处理**

#### **1. 缩放因子处理**
```python
# 编码器中的缩放
x *= 0.18215

# 解码器中的逆缩放
x /= 0.18215
```
- **0.18215**是经验值，来自Stable Diffusion原始论文
- **作用**：将潜在变量方差调整到接近1，使扩散过程更稳定
- **原理**：假设图像数据经过编码后近似服从标准正态分布

#### **2. 激活函数选择**
```python
F.silu(x)  # 等价于 x * torch.sigmoid(x)
```
- **Swish/SiLU**：平滑的非线性，在深度网络中表现优于ReLU
- **无梯度消失**：负值区域有微小梯度，避免神经元"死亡"
- **在归一化后使用**：GroupNorm → SiLU 是标准模式

#### **3. 归一化策略**
```python
nn.GroupNorm(32, channels)  # 分为32组
```
- **与BatchNorm对比**：不依赖批量大小，对批次大小不敏感
- **与LayerNorm对比**：更灵活，允许通道间的一定相关性
- **32组**：经验值，在通道数512时每组16个通道

---

### **在Stable Diffusion工作流中的角色**

#### **训练阶段**
1. 图像 `x` → 编码器 → 潜在 `z`
2. 潜在 `z` → 解码器 → 重建图像 `x'`
3. 最小化 `MSE(x, x') + KL散度`

#### **推理阶段（文本生成图像）**
1. 扩散模型在潜在空间生成 `z`
2. `z` → 解码器 → 最终图像
3. **这是唯一用到解码器的时候**

#### **图像重建质量**
- VAE的重建是**有损的**
- 丢失高频细节，保留语义内容
- 对扩散模型足够：扩散在潜在空间进行，细节由扩散模型"想象"补充

## 4.2 - attention源码分析

这是**Stable Diffusion 中核心的注意力机制实现**，包含**自注意力**和**交叉注意力**。这两种注意力机制是现代扩散模型（特别是条件生成模型）的**关键组件**。下面我将详细解释它们的原理、区别和在扩散模型中的作用。

### **整体概述**

#### **注意力机制在扩散模型中的作用**
- **自注意力**：让图像潜在表示的不同位置之间相互关注，建立**全局上下文关系**
- **交叉注意力**：将**文本条件**注入到图像生成过程中，实现**文本引导的图像生成**

---

### **自注意力机制详解**

自注意力让序列中的每个元素都能关注到序列的所有其他元素，建立全局依赖关系。

#### **1. 核心数学原理**

注意力机制的核心公式：
```
Attention(Q, K, V) = softmax(QKᵀ/√d_k) V
```

#### **2. 代码实现解析**

```python
class SelfAttention(nn.Module):
    def __init__(self, n_heads, d_embed, in_proj_bias=True, out_proj_bias=True):
        super().__init__()
        # 合并的Q、K、V投影矩阵
        self.in_proj = nn.Linear(d_embed, 3 * d_embed, bias=in_proj_bias)
        # 输出投影矩阵
        self.out_proj = nn.Linear(d_embed, d_embed, bias=out_proj_bias)
        self.n_heads = n_heads
        self.d_head = d_embed // n_heads
```

**多头注意力机制**：
- 将输入特征分为多个"头"，每个头学习不同的注意力模式
- 多头并行计算，最后合并结果
- 增强模型的表达能力和泛化能力

#### **3. 前向传播步骤**

```python
def forward(self, x, causal_mask=False):
    # 1. 输入投影：生成Q、K、V
    q, k, v = self.in_proj(x).chunk(3, dim=-1)
    
    # 2. 重塑为多头格式
    # (Batch, Seq_Len, Dim) -> (Batch, n_heads, Seq_Len, d_head)
    q = q.view(...).transpose(1, 2)
    k = k.view(...).transpose(1, 2)
    v = v.view(...).transpose(1, 2)
    
    # 3. 计算注意力权重
    # Q @ Kᵀ，形状：(Batch, n_heads, Seq_Len, Seq_Len)
    weight = q @ k.transpose(-1, -2)
    
    # 4. 因果掩码（用于自回归生成）
    if causal_mask:
        mask = torch.ones_like(weight, dtype=torch.bool).triu(1)
        weight.masked_fill_(mask, -torch.inf)
    
    # 5. 缩放和softmax
    weight /= math.sqrt(self.d_head)  # 防止梯度消失
    weight = F.softmax(weight, dim=-1)
    
    # 6. 应用注意力权重
    output = weight @ v
    
    # 7. 合并多头输出
    output = output.transpose(1, 2).reshape(input_shape)
    
    # 8. 输出投影
    output = self.out_proj(output)
    
    return output
```

#### **4. 因果掩码的作用**

```python
if causal_mask:
    mask = torch.ones_like(weight, dtype=torch.bool).triu(1)
    weight.masked_fill_(mask, -torch.inf)
```
- 创建上三角矩阵，对角线及以下为False，以上为True
- 将上三角区域（未来位置）填充为负无穷
- 经过softmax后，这些位置的概率变为0
- 确保当前位置只能关注到**过去和当前**的位置
- 在**自回归生成**（如语言模型）中必需

---

### **交叉注意力机制详解**

交叉注意力用于**将一种模态的信息注入到另一种模态**，在扩散模型中用于**文本条件引导图像生成**。

#### **1. 与自注意力的关键区别**

| 维度 | 自注意力 | 交叉注意力 |
|------|----------|------------|
| **输入来源** | 单个输入序列 | 两个输入序列 |
| **Q来源** | 输入序列本身 | 查询序列（如潜在表示） |
| **K, V来源** | 输入序列本身 | 上下文序列（如文本嵌入） |
| **应用场景** | 序列内部关系 | 跨模态对齐 |

#### **2. 代码实现解析**

```python
class CrossAttention(nn.Module):
    def __init__(self, n_heads, d_embed, d_cross, in_proj_bias=True, out_proj_bias=True):
        super().__init__()
        # 分别定义Q、K、V的投影
        self.q_proj   = nn.Linear(d_embed, d_embed, bias=in_proj_bias)   # 用于查询
        self.k_proj   = nn.Linear(d_cross, d_embed, bias=in_proj_bias)   # 用于键
        self.v_proj   = nn.Linear(d_cross, d_embed, bias=in_proj_bias)   # 用于值
        self.out_proj = nn.Linear(d_embed, d_embed, bias=out_proj_bias)
        self.n_heads = n_heads
        self.d_head = d_embed // n_heads
```

**为什么分开投影**：
- 自注意力：Q、K、V来自同一空间，可合并投影
- 交叉注意力：Q来自图像潜在空间，K、V来自文本嵌入空间，维度不同

#### **3. 在扩散模型中的具体应用**

在Stable Diffusion中：
- **查询(Q)**：图像潜在表示 `(Batch_Size, 4096, 320)`，4096=64×64
- **键和值(K, V)**：文本嵌入 `(Batch_Size, 77, 768)`
- 77是CLIP文本编码器的最大序列长度
- 768是CLIP的文本嵌入维度

**维度对齐**：
```python
# 将文本嵌入投影到与图像潜在表示相同的维度
k = self.k_proj(y)  # (Batch_Size, 77, 768) -> (Batch_Size, 77, 320)
v = self.v_proj(y)  # (Batch_Size, 77, 768) -> (Batch_Size, 77, 320)
```

---

### **在扩散模型U-Net中的位置**

#### **1. 注意力模块的集成**

在扩散模型的U-Net中，注意力模块通常与残差块结合：

```python
# 简化版的扩散模型块
class DiffusionBlock(nn.Module):
    def __init__(self):
        # 自注意力：图像内部关系
        self.self_attn = SelfAttention(n_heads=8, d_embed=320)
        # 交叉注意力：文本条件注入
        self.cross_attn = CrossAttention(n_heads=8, d_embed=320, d_cross=768)
        
    def forward(self, x, context, t):
        # x: 图像潜在表示
        # context: 文本嵌入
        # t: 时间步嵌入
        
        # 1. 自注意力
        x = x + self.self_attn(x)  # 残差连接
        
        # 2. 交叉注意力
        x = x + self.cross_attn(x, context)  # 注入文本条件
        
        return x
```

#### **2. 特征维度变化**

```
输入图像: 512×512×3
VAE编码: 64×64×4
U-Net中: 64×64×320 (d_embed=320)
注意力: 将64×64=4096视为序列长度
```

**多头注意力的维度分解**：
```python
# 假设 n_heads = 8, d_embed = 320
d_head = 320 // 8 = 40
# 每个头处理40维特征
```

---

### **关键技术细节**

#### **1. 缩放点积注意力**

```python
weight /= math.sqrt(self.d_head)
```
- 除以`√d_k`防止点积值过大
- 点积值过大会导致softmax梯度消失
- 稳定训练过程

#### **2. 残差连接**

在实际使用中，注意力模块通常与残差连接结合：
```python
# 标准模式
output = x + self.attention(self.norm(x))
```

#### **3. 线性投影的作用**

**输入投影**：
- 将输入映射到Q、K、V空间
- 允许模型学习如何提取相关信息

**输出投影**：
- 整合多头注意力的结果
- 可以调整输出维度
- 通常包含可学习的偏置

#### **4. 因果掩码的选择**

在扩散模型中：
- **自注意力**：通常**不使用**因果掩码，因为图像生成不是自回归的
- 图像的所有位置可以同时生成
- 但某些变体（如MaskGIT）会使用掩码

---

### **在扩散模型中的具体作用**

#### **1. 自注意力的作用**

```python
# 在图像潜在空间中
# 输入: (B, 4096, 320)  [4096 = 64×64]
# 输出: (B, 4096, 320)

# 注意力权重: (B, 8, 4096, 4096)
```
- 让图像的每个"像素"（潜在空间中的点）关注其他所有"像素"
- 建立**长距离依赖**，如"天空"和"地面"的关系
- 确保生成图像的**全局一致性**

#### **2. 交叉注意力的作用**

```python
# 查询: 图像潜在表示 (B, 4096, 320)
# 键/值: 文本嵌入 (B, 77, 768)
# 投影后: 文本嵌入 -> (B, 77, 320)
# 输出: (B, 4096, 320)
```

**工作原理**：
1. 计算图像位置与文本标记的相似度
2. 用文本信息"增强"相关的图像区域
3. 实现**文本到图像的语义对齐**

**例如**：
- 文本："一只戴墨镜的柯基犬"
- 注意力机制会让"眼睛"区域关注"墨镜"文本标记
- 让"狗"区域关注"柯基犬"文本标记

---

### **总结：注意力机制在扩散模型中的重要性**

#### **1. 为什么扩散模型需要注意力？**

- **卷积的局限性**：局部感受野，难以建模长距离依赖
- **全局一致性**：生成图像需要整体协调
- **条件生成**：需要精确对齐文本和图像

#### **2. 自注意力 vs 交叉注意力**

| 特征 | 自注意力 | 交叉注意力 |
|------|----------|------------|
| **目的** | 图像内部一致性 | 文本-图像对齐 |
| **输入** | 单模态（图像） | 双模态（图像+文本） |
| **复杂度** | O(N²) | O(N×M)，M=文本长度 |
| **在SD中** | 空间自注意力 | 文本交叉注意力 |

#### **3. 实现的关键点**

1. **多头设计**：并行学习多种注意力模式
2. **缩放点积**：稳定训练，防止梯度消失
3. **残差连接**：确保梯度流动，训练深度网络
4. **线性投影**：灵活的特征变换
5. **因果掩码可选**：根据任务需求选择

#### **4. 在Stable Diffusion中的实际配置**

```python
# 典型的配置
self_attn = SelfAttention(
    n_heads=8,          # 8个注意力头
    d_embed=320,        # 特征维度
    in_proj_bias=True,  # 使用偏置
    out_proj_bias=True
)

cross_attn = CrossAttention(
    n_heads=8,          # 8个注意力头
    d_embed=320,        # 图像特征维度
    d_cross=768,        # 文本特征维度
    in_proj_bias=True,
    out_proj_bias=True
)
```

**结论**：自注意力和交叉注意力是**扩散模型能够生成高质量、语义一致图像的核心技术**。自注意力确保图像内部的一致性，交叉注意力实现精确的文本控制。这两种机制共同使Stable Diffusion等模型能够理解复杂提示并生成相应的图像。
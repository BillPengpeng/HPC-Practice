æœ¬æ–‡ä¸»è¦æ•´ç†pytorch-quantizationçš„ä¸»è¦å†…å®¹ã€‚

## 6 - quantization_from_scratch

è¿™æ®µä»£ç å®ç°äº†ä¸¤ç§å¸¸è§çš„ç¥ç»ç½‘ç»œé‡åŒ–æ–¹æ³•ï¼š**éå¯¹ç§°é‡åŒ–**å’Œ**å¯¹ç§°é‡åŒ–**ã€‚ä¸‹é¢æˆ‘å°†é€éƒ¨åˆ†è§£é‡Šå…¶å·¥ä½œåŸç†ã€‚

### ğŸ”§ è¾…åŠ©å‡½æ•°ï¼šclamp

```python
def clamp(params_q: np.array, lower_bound: int, upper_bound: int) -> np.array:
    params_q[params_q < lower_bound] = lower_bound
    params_q[params_q > upper_bound] = upper_bound
    return params_q
```
è¿™ä¸ªå‡½æ•°ç¡®ä¿é‡åŒ–åçš„å€¼è½åœ¨æŒ‡å®šèŒƒå›´å†…ï¼š
- **åŠŸèƒ½**ï¼šå°†æ•°ç»„ä¸­å°äºä¸‹é™çš„å€¼è®¾ä¸ºä¸‹é™ï¼Œå¤§äºä¸Šé™çš„å€¼è®¾ä¸ºä¸Šé™
- **ç”¨é€”**ï¼šé˜²æ­¢é‡åŒ–æº¢å‡ºï¼Œç¡®ä¿æ‰€æœ‰å€¼éƒ½åœ¨æœ‰æ•ˆçš„æ•´æ•°è¡¨ç¤ºèŒƒå›´å†…

### âš–ï¸ éå¯¹ç§°é‡åŒ–

```python
def asymmetric_quantization(params: np.array, bits: int) -> Tuple[np.array, float, int]:
    alpha = np.max(params)  # æœ€å¤§å€¼
    beta = np.min(params)   # æœ€å°å€¼
    scale = (alpha - beta) / (2**bits-1)  # ç¼©æ”¾å› å­
    zero = -1*np.round(beta / scale)       # é›¶ç‚¹åç§»
    lower_bound, upper_bound = 0, 2**bits-1  # 8æ¯”ç‰¹èŒƒå›´[0, 255]
    
    quantized = clamp(np.round(params / scale + zero), lower_bound, upper_bound).astype(np.int32)
    return quantized, scale, zero
```

**å·¥ä½œåŸç†**ï¼š
- **ç¼©æ”¾å› å­è®¡ç®—**ï¼š`scale = (æœ€å¤§å€¼ - æœ€å°å€¼) / (2^bits - 1)`ï¼Œå°†æµ®ç‚¹èŒƒå›´æ˜ å°„åˆ°æ•´æ•°èŒƒå›´
- **é›¶ç‚¹åç§»**ï¼š`zero = -round(æœ€å°å€¼ / scale)`ï¼Œå°†æµ®ç‚¹é›¶ç‚¹å¯¹é½åˆ°æ•´æ•°é›¶ç‚¹
- **é‡åŒ–å…¬å¼**ï¼š`quantized = round(æµ®ç‚¹æ•° / scale + zero)`

**é€‚ç”¨åœºæ™¯**ï¼šæ•°æ®åˆ†å¸ƒä¸å¯¹ç§°æ—¶æ•ˆæœæ›´å¥½

### âš–ï¸ å¯¹ç§°é‡åŒ–

```python
def symmetric_quantization(params: np.array, bits: int) -> Tuple[np.array, float]:
    alpha = np.max(np.abs(params))  # æœ€å¤§ç»å¯¹å€¼
    scale = alpha / (2**(bits-1)-1)  # ç¼©æ”¾å› å­
    lower_bound = -2**(bits-1)       # æœ‰ç¬¦å·æ•´æ•°ä¸‹é™
    upper_bound = 2**(bits-1)-1       # æœ‰ç¬¦å·æ•´æ•°ä¸Šé™
    
    quantized = clamp(np.round(params / scale), lower_bound, upper_bound).astype(np.int32)
    return quantized, scale
```

**å·¥ä½œåŸç†**ï¼š
- **åŸºäºå¯¹ç§°èŒƒå›´**ï¼šä½¿ç”¨æœ€å¤§ç»å¯¹å€¼ç¡®å®šèŒƒå›´ `[-Î±, Î±]`
- **é‡åŒ–å…¬å¼**ï¼š`quantized = round(æµ®ç‚¹æ•° / scale)`ï¼Œæ— éœ€é›¶ç‚¹åç§»
- **æ•´æ•°èŒƒå›´**ï¼š8æ¯”ç‰¹æ—¶ä¸º `[-128, 127]` æˆ– `[-127, 127]`

**ä¼˜åŠ¿**ï¼šè®¡ç®—æ›´ç®€å•ï¼Œç¡¬ä»¶å®ç°æ›´é«˜æ•ˆ

### ğŸ”„ åé‡åŒ–å‡½æ•°

```python
def asymmetric_dequantize(params_q: np.array, scale: float, zero: int) -> np.array:
    return (params_q - zero) * scale

def symmetric_dequantize(params_q: np.array, scale: float) -> np.array:
    return params_q * scale
```

**åŠŸèƒ½**ï¼šå°†é‡åŒ–åçš„æ•´æ•°æ¢å¤ä¸ºæµ®ç‚¹æ•°ï¼Œç”¨äºæ¨ç†è®¡ç®—

### ğŸ“Š é‡åŒ–è¯¯å·®è¯„ä¼°

```python
def quantization_error(params: np.array, params_q: np.array):
    return np.mean((params - params_q)**2)
```

**ç”¨é€”**ï¼šè®¡ç®—åŸå§‹æµ®ç‚¹æ•°ä¸åé‡åŒ–åæ•°å€¼çš„å‡æ–¹è¯¯å·®ï¼Œè¯„ä¼°é‡åŒ–è´¨é‡

### ğŸš€ å®é™…è°ƒç”¨ç¤ºä¾‹

```python
(asymmetric_q, asymmetric_scale, asymmetric_zero) = asymmetric_quantization(params, 8)
(symmetric_q, symmetric_scale) = symmetric_quantization(params, 8)
```

**æ‰§è¡Œæµç¨‹**ï¼š
1. å¯¹è¾“å…¥å‚æ•° `params` åˆ†åˆ«è¿›è¡Œéå¯¹ç§°å’Œå¯¹ç§°é‡åŒ–
2. è¿”å›é‡åŒ–åçš„æ•´æ•°æ•°ç»„ã€ç¼©æ”¾å› å­å’Œé›¶ç‚¹ï¼ˆéå¯¹ç§°é‡åŒ–ï¼‰
3. 8æ¯”ç‰¹é‡åŒ–å°†32ä½æµ®ç‚¹æ•°å‹ç¼©ä¸º8ä½æ•´æ•°ï¼Œå‡å°‘75%å­˜å‚¨ç©ºé—´

### ğŸ’¡ æ ¸å¿ƒåŒºåˆ«æ€»ç»“

| ç‰¹æ€§ | éå¯¹ç§°é‡åŒ– | å¯¹ç§°é‡åŒ– |
|------|------------|----------|
| **èŒƒå›´æ˜ å°„** | `[Î², Î±]` â†’ `[0, 2^bits-1]` | `[-Î±, Î±]` â†’ `[-2^(bits-1), 2^(bits-1)-1]` |
| **é›¶ç‚¹åç§»** | éœ€è¦ | ä¸éœ€è¦ |
| **è®¡ç®—å¤æ‚åº¦** | è¾ƒé«˜ | è¾ƒä½ |
| **é€‚ç”¨åœºæ™¯** | æ•°æ®åˆ†å¸ƒä¸å¯¹ç§° | æ•°æ®åˆ†å¸ƒå¯¹ç§°æˆ–æ¥è¿‘å¯¹ç§° |

## 7 - quantization_compare_minmax_percentile

### ğŸ“Š éå¯¹ç§°é‡åŒ–å‡½æ•°ï¼ˆç™¾åˆ†ä½æ•°æ³•ï¼‰

**æ”¹è¿›ç‚¹**ï¼šä½¿ç”¨ç™¾åˆ†ä½æ•°æ›¿ä»£æœ€å°/æœ€å¤§å€¼ï¼Œå‡å°‘å¼‚å¸¸å€¼å¯¹é‡åŒ–èŒƒå›´çš„å¹²æ‰°ï¼Œæé«˜ä¸»ä½“æ•°æ®çš„ç²¾åº¦ã€‚

```python
def asymmetric_quantization_percentile(params: np.array, bits: int, percentile: float = 99.99) -> Tuple[np.array, float, int]:
    alpha = np.percentile(params, percentile)       # ä¸Šç™¾åˆ†ä½æ•°ï¼ˆå¦‚99.99%ï¼‰
    beta = np.percentile(params, 100 - percentile)  # ä¸‹ç™¾åˆ†ä½æ•°ï¼ˆå¦‚0.01%ï¼‰
    scale = (alpha - beta) / (2**bits - 1)
    zero = -1 * np.round(beta / scale)
    lower_bound, upper_bound = 0, 2**bits - 1
    quantized = clamp(np.round(params / scale + zero), lower_bound, upper_bound).astype(np.int32)
    return quantized, scale, zero
```
- **ç™¾åˆ†ä½æ•°çš„ä¼˜åŠ¿**ï¼šä¾‹å¦‚ï¼Œ`percentile=99.99` ä¼šå¿½ç•¥åˆ†å¸ƒä¸­æœ€é«˜0.01%å’Œæœ€ä½0.01%çš„æç«¯å€¼ï¼Œä½¿ç¼©æ”¾å› å­æ›´è´´åˆä¸»ä½“æ•°æ®åˆ†å¸ƒï¼Œé™ä½å¼‚å¸¸å€¼å¼•èµ·çš„é‡åŒ–è¯¯å·®ã€‚
- **é€‚ç”¨åœºæ™¯**ï¼šå½“è¾“å…¥æ•°æ®åŒ…å«æ˜¾è‘—ç¦»ç¾¤ç‚¹æ—¶ï¼ˆå¦‚æ¨¡å‹æ¿€æ´»å€¼ï¼‰ï¼Œè¿™ç§æ–¹æ³•é€šå¸¸æ¯”æœ€å°-æœ€å¤§å€¼æ³•æ›´é²æ£’ã€‚

## 8 - post_training_quantization

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„PyTorchè®­ç»ƒåé™æ€é‡åŒ–ï¼ˆPost-Training Static Quantizationï¼‰å®ç°ä»£ç ã€‚

### 1. é‡åŒ–ç½‘ç»œå®šä¹‰

```python
class QuantizedVerySimpleNet(nn.Module):
    def __init__(self, hidden_size_1=100, hidden_size_2=100):
        super(QuantizedVerySimpleNet,self).__init__()
        self.quant = torch.quantization.QuantStub()    # é‡åŒ–å…¥å£
        self.linear1 = nn.Linear(28*28, hidden_size_1) 
        self.linear2 = nn.Linear(hidden_size_1, hidden_size_2) 
        self.linear3 = nn.Linear(hidden_size_2, 10)
        self.relu = nn.ReLU()
        self.dequant = torch.quantization.DeQuantStub()  # åé‡åŒ–å‡ºå£

    def forward(self, img):
        x = img.view(-1, 28*28)
        x = x.contiguous()    # ç¡®ä¿å¼ é‡å†…å­˜è¿ç»­
        x = self.quant(x)      # å°†è¾“å…¥é‡åŒ–ä¸ºint8
        x = self.relu(self.linear1(x))
        x = self.relu(self.linear2(x))
        x = self.linear3(x)
        x = self.dequant(x)    # å°†è¾“å‡ºåé‡åŒ–ä¸ºfloat32
        return x
```

- **QuantStub()**: åœ¨æ¨ç†æ—¶ä¼šå°†float32è¾“å…¥è½¬æ¢ä¸ºint8
- **DeQuantStub()**: åœ¨è¾“å‡ºå‰å°†int8è½¬æ¢å›float32ï¼Œä¾¿äºåç»­å¤„ç†
- **contiguous()**: ç¡®ä¿å¼ é‡åœ¨å†…å­˜ä¸­è¿ç»­å­˜å‚¨ï¼Œé¿å…æŸäº›é‡åŒ–æ“ä½œå‡ºé”™

### 2. æ¨¡å‹åˆå§‹åŒ–ä¸æƒé‡å¤åˆ¶

```python
device = "cpu"
net_quantized = QuantizedVerySimpleNet().to(device)
# Copy weights from unquantized model
net_quantized.load_state_dict(net.state_dict())
net_quantized.eval()
```

è¿™éƒ¨åˆ†ä»£ç å°†é¢„è®­ç»ƒå¥½çš„æµ®ç‚¹æ¨¡å‹æƒé‡åŠ è½½åˆ°é‡åŒ–æ¨¡å‹ä¸­ï¼Œä¸ºåç»­çš„é‡åŒ–åšå‡†å¤‡ã€‚

### 3. é‡åŒ–é…ç½®ä¸å‡†å¤‡é˜¶æ®µ

```python
net_quantized.qconfig = torch.ao.quantization.default_qconfig
net_quantized = torch.ao.quantization.prepare(net_quantized)  # æ’å…¥è§‚å¯Ÿå™¨
```

- **qconfigè®¾ç½®**: ä½¿ç”¨é»˜è®¤é‡åŒ–é…ç½®ï¼ŒæŒ‡å®šå¦‚ä½•é‡åŒ–æ¿€æ´»å€¼å’Œæƒé‡
- **prepare()**: åœ¨æ¨¡å‹ä¸­æ’å…¥**è§‚å¯Ÿå™¨ï¼ˆObserverï¼‰**ï¼Œç”¨äºåœ¨æ ¡å‡†é˜¶æ®µæ”¶é›†å¼ é‡çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚æœ€å°å€¼ã€æœ€å¤§å€¼ï¼‰

### 4. æ¨¡å‹è½¬æ¢ï¼ˆå®é™…é‡åŒ–ï¼‰

```python
net_quantized = torch.ao.quantization.convert(net_quantized)
```

è¿™æ˜¯æœ€å…³é”®çš„ä¸€æ­¥ï¼Œæ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
1. ä½¿ç”¨è§‚å¯Ÿå™¨æ”¶é›†çš„ç»Ÿè®¡ä¿¡æ¯è®¡ç®—**scaleï¼ˆç¼©æ”¾å› å­ï¼‰**å’Œ**zero_pointï¼ˆé›¶ç‚¹ï¼‰**
2. å°†æƒé‡ä»float32é‡åŒ–ä¸ºint8
3. å°†è§‚å¯Ÿå™¨æ›¿æ¢ä¸ºå®é™…çš„é‡åŒ–æ“ä½œ
4. ç”ŸæˆçœŸæ­£æ‰§è¡Œä½ç²¾åº¦è®¡ç®—çš„é‡åŒ–æ¨¡å‹

### 5. é‡åŒ–åŸç†ä¸ä¼˜åŠ¿

é‡åŒ–åŸºäºçº¿æ€§æ˜ å°„ï¼š`quantized_value = round(float_value / scale) + zero_point`

## 9 - quantization_aware_training

è¿™æ®µä»£ç æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨PyTorchå®ç°**é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰**ï¼Œå®Œæ•´å±•ç¤ºäº†ä»æ¨¡å‹å®šä¹‰ã€QATå‡†å¤‡ã€è®­ç»ƒåˆ°æœ€ç»ˆè½¬æ¢çš„æ•´ä¸ªæµç¨‹ã€‚

### ğŸ§  æ¨¡å‹å®šä¹‰ä¸é‡åŒ–å­˜æ ¹

```python
class VerySimpleNet(nn.Module):
    def __init__(self, hidden_size_1=100, hidden_size_2=100):
        super(VerySimpleNet,self).__init__()
        self.quant = torch.quantization.QuantStub()    # é‡åŒ–å…¥å£
        self.linear1 = nn.Linear(28*28, hidden_size_1) 
        self.linear2 = nn.Linear(hidden_size_1, hidden_size_2) 
        self.linear3 = nn.Linear(hidden_size_2, 10)
        self.relu = nn.ReLU()
        self.dequant = torch.quantization.DeQuantStub()  # åé‡åŒ–å‡ºå£

    def forward(self, img):
        x = img.view(-1, 28*28)
        x = self.quant(x)      # å°†è¾“å…¥é‡åŒ–ä¸ºint8
        x = self.relu(self.linear1(x))
        x = self.relu(self.linear2(x))
        x = self.linear3(x)
        x = self.dequant(x)    # å°†è¾“å‡ºåé‡åŒ–ä¸ºfloat32
        return x
```
- **QuantStubä¸DeQuantStub**ï¼šè¿™ä¸¤ä¸ªå­˜æ ¹åˆ†åˆ«æ ‡è®°äº†æ¨¡å‹çš„**é‡åŒ–èµ·ç‚¹**å’Œ**åé‡åŒ–ç»ˆç‚¹**ã€‚åœ¨å‰å‘ä¼ æ’­ä¸­ï¼Œå®ƒä»¬ä¼šåœ¨è®­ç»ƒé˜¶æ®µæ¨¡æ‹Ÿé‡åŒ–è¿‡ç¨‹ï¼ˆç§°ä¸ºä¼ªé‡åŒ–ï¼‰ï¼Œå³å¯¹æ•°æ®æ‰§è¡Œé‡åŒ–å†ç«‹å³åé‡åŒ–ï¼Œä»¥å¼•å…¥é‡åŒ–è¯¯å·®ï¼Œä½†ä¿æŒæµ®ç‚¹è®¡ç®—ã€‚
- **ç½‘ç»œç»“æ„**ï¼šè¿™æ˜¯ä¸€ä¸ªç®€å•çš„å…¨è¿æ¥ç½‘ç»œï¼Œé€‚ç”¨äºMNISTæ•°æ®é›†ï¼ˆè¾“å…¥å°ºå¯¸28Ã—28ï¼Œè¾“å‡º10ç±»ï¼‰ã€‚

### âš™ï¸ é‡åŒ–é…ç½®ä¸QATå‡†å¤‡

```python
net = VerySimpleNet().to(device)  
net.qconfig = torch.ao.quantization.default_qconfig  # è®¾ç½®é‡åŒ–é…ç½®
net.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
net_quantized = torch.ao.quantization.prepare_qat(net)  # å‡†å¤‡QAT
```
- **qconfig**ï¼šæ­¤ä¸º**é‡åŒ–é…ç½®æ–‡ä»¶**ï¼Œå®ƒå†³å®šäº†å¦‚ä½•é‡åŒ–æƒé‡å’Œæ¿€æ´»å€¼ï¼ˆå¦‚å¯¹ç§°/éå¯¹ç§°é‡åŒ–ã€é‡åŒ–ä½æ•°ç­‰ï¼‰ã€‚`default_qconfig` æ˜¯PyTorchæä¾›çš„é»˜è®¤é…ç½®ï¼Œé€šå¸¸é’ˆå¯¹x86 CPUï¼ˆä½¿ç”¨`fbgemm`åç«¯ï¼‰æˆ–ARM CPUï¼ˆä½¿ç”¨`qnnpack`åç«¯ï¼‰è¿›è¡Œä¼˜åŒ–ã€‚
- **prepare_qat()**ï¼šè¿™æ˜¯QATçš„æ ¸å¿ƒå‡†å¤‡æ­¥éª¤ã€‚å®ƒä¼šï¼š
    - åœ¨ç½‘ç»œä¸­æ’å…¥**ä¼ªé‡åŒ–èŠ‚ç‚¹**ï¼Œè¿™äº›èŠ‚ç‚¹åœ¨å‰å‘ä¼ æ’­æ—¶æ¨¡æ‹ŸINT8é‡åŒ–çš„èˆå…¥å’Œæˆªæ–­è¯¯å·®ã€‚
    - æ›¿æ¢ç‰¹å®šçš„æ¨¡å—ï¼ˆå¦‚`nn.Linear`ï¼‰ä¸ºæ”¯æŒé‡åŒ–æ„ŸçŸ¥è®­ç»ƒçš„ç‰ˆæœ¬ã€‚
    - è®¾ç½®**è§‚æµ‹å™¨**ï¼Œç”¨äºåœ¨æ ¡å‡†é˜¶æ®µæ”¶é›†å¼ é‡çš„æ•°å€¼èŒƒå›´ï¼ˆmin/maxï¼‰ï¼Œä»è€Œè®¡ç®—ç¼©æ”¾å› å­ï¼ˆscaleï¼‰å’Œé›¶ç‚¹ï¼ˆzero_pointï¼‰ã€‚

### ğŸ‹ï¸ QATè®­ç»ƒè¿‡ç¨‹

```python
def train(train_loader, net, epochs=5, total_iterations_limit=None):
    cross_el = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)
    # ... è®­ç»ƒå¾ªç¯ ...
    for data in train_loader:
        x, y = data
        optimizer.zero_grad()
        output = net(x.view(-1, 28*28))  # å‰å‘ä¼ æ’­ï¼ˆåŒ…å«ä¼ªé‡åŒ–ï¼‰
        loss = cross_el(output, y)
        loss.backward()  # åå‘ä¼ æ’­
        optimizer.step()  # æ›´æ–°æƒé‡
```
- **å…³é”®ç‰¹æ€§**ï¼šåœ¨QATæ¨¡å¼ä¸‹ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸æ ‡å‡†è®­ç»ƒç±»ä¼¼ï¼Œä½†å‰å‘ä¼ æ’­ä¸­åŒ…å«äº†**ä¼ªé‡åŒ–æ“ä½œ**ã€‚è¿™æ„å‘³ç€æ¨¡å‹æƒé‡åœ¨åå‘ä¼ æ’­å’Œæ›´æ–°æ—¶ï¼Œèƒ½â€œæ„ŸçŸ¥â€åˆ°é‡åŒ–å¸¦æ¥çš„ç²¾åº¦æŸå¤±ï¼Œä»è€Œå­¦ä¹ è°ƒæ•´ä»¥é€‚åº”ä½ç²¾åº¦è¡¨ç¤ºï¼Œè¿™é€šå¸¸æ¯”è®­ç»ƒåé‡åŒ–ï¼ˆPTQï¼‰è·å¾—æ›´å¥½çš„ç²¾åº¦ã€‚
- **ç›®çš„**ï¼šé€šè¿‡è®­ç»ƒè®©æ¨¡å‹æƒé‡**é€‚åº”é‡åŒ–å™ªå£°**ï¼Œæ‰¾åˆ°å¯¹é‡åŒ–ä¸æ•æ„Ÿçš„å¹³å¦æœ€ä¼˜åŒºåŸŸï¼Œä½¿å¾—æœ€ç»ˆè½¬æ¢ä¸ºçœŸæ­£INT8æ¨¡å‹æ—¶ç²¾åº¦æŸå¤±æœ€å°ã€‚

### ğŸ”„ æ¨¡å‹è½¬æ¢ä¸è¯„ä¼°

```python
net_quantized.eval()  # åˆ‡æ¢ä¸ºè¯„ä¼°æ¨¡å¼
net_quantized = torch.ao.quantization.convert(net_quantized)  # è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
```
- **convert()**ï¼šè¿™æ˜¯QATæµç¨‹çš„æœ€åä¸€æ­¥ã€‚å®ƒä¼šï¼š
    - ç§»é™¤è®­ç»ƒæ—¶æ’å…¥çš„ä¼ªé‡åŒ–èŠ‚ç‚¹ã€‚
    - å°†FP32æƒé‡**æ°¸ä¹…è½¬æ¢ä¸ºINT8**ï¼ˆä½¿ç”¨è®­ç»ƒå’Œæ ¡å‡†è¿‡ç¨‹ä¸­ç¡®å®šçš„é‡åŒ–å‚æ•°ï¼‰ã€‚
    - å°†æ¨¡å—æ›¿æ¢ä¸ºçœŸæ­£çš„é‡åŒ–å®ç°ï¼Œç”Ÿæˆä¸€ä¸ª**å¯ç”¨äºé«˜æ•ˆæ¨ç†çš„INT8æ¨¡å‹**ã€‚
- **æ¨¡å‹å¤§å°**ï¼š`print_size_of_model`å‡½æ•°å±•ç¤ºäº†é‡åŒ–åçš„æ¨¡å‹å¤§å°ï¼ŒINT8æ¨¡å‹ç›¸æ¯”åŸå§‹FP32æ¨¡å‹**é€šå¸¸å¯å‡å°‘çº¦75%çš„å­˜å‚¨ç©ºé—´**ã€‚

### ğŸ’ æ ¸å¿ƒæ€»ç»“

è¿™æ®µä»£ç å®Œæ•´å®ç°äº†é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰çš„æ ¸å¿ƒæµç¨‹ï¼š**å‡†å¤‡æ¨¡å‹ â†’ é…ç½®é‡åŒ– â†’ æ’å…¥ä¼ªé‡åŒ–èŠ‚ç‚¹å¹¶è®­ç»ƒ â†’ è½¬æ¢ä¸ºæœ€ç»ˆINT8æ¨¡å‹**ã€‚QATé€šè¿‡åœ¨è®­ç»ƒä¸­æ¨¡æ‹Ÿé‡åŒ–è¯¯å·®ï¼Œè®©æ¨¡å‹è‡ªé€‚åº”è°ƒæ•´ï¼Œæ˜¯å¹³è¡¡æ¨¡å‹ç²¾åº¦ä¸æ¨ç†æ•ˆç‡çš„æœ‰æ•ˆæ–¹æ³•ï¼Œç‰¹åˆ«é€‚åˆå¯¹ç²¾åº¦è¦æ±‚è¾ƒé«˜çš„éƒ¨ç½²åœºæ™¯ã€‚ä¸è®­ç»ƒåé‡åŒ–ï¼ˆPTQï¼‰ç›¸æ¯”ï¼ŒQATé€šå¸¸ç²¾åº¦æ›´é«˜ï¼Œä½†éœ€è¦é¢å¤–çš„è®­ç»ƒæ—¶é—´ã€‚
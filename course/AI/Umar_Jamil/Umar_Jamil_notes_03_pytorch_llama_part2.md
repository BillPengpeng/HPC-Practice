æœ¬æ–‡ä¸»è¦æ•´ç†LLaMA from scratchçš„ä¸»è¦å†…å®¹ã€‚

## 10 - apply_rotary_embeddings

è¿™ä»½ä»£ç å®ç°çš„æ˜¯Transformerä¸­é‡è¦çš„**æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰**ã€‚å®ƒé€šè¿‡æ—‹è½¬æ“ä½œå°†ä½ç½®ä¿¡æ¯å·§å¦™åœ°æ³¨å…¥åˆ°æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ„ŸçŸ¥åºåˆ—ä¸­å…ƒç´ çš„ç›¸å¯¹ä½ç½®ã€‚

### ğŸ”§ é¢„è®¡ç®—é¢‘ç‡ï¼š`precompute_theta_pos_frequencies`

è¿™ä¸ªå‡½æ•°çš„ç›®æ ‡æ˜¯é¢„å…ˆè®¡ç®—å¥½æ‰€æœ‰å¯èƒ½ä½ç½®å¯¹åº”çš„æ—‹è½¬è§’åº¦ï¼ˆä»¥å¤æ•°å½¢å¼è¡¨ç¤ºï¼‰ï¼Œä¾›åç»­ä½¿ç”¨ã€‚

```python
def precompute_theta_pos_frequencies(head_dim: int, seq_len: int, device: str, theta: float = 10000.0):
    # 1. ç»´åº¦æ£€æŸ¥ï¼šRoPEè¦æ±‚ç‰¹å¾ç»´åº¦ä¸ºå¶æ•°ï¼Œå› ä¸ºéœ€è¦å°†ç»´åº¦ä¸¤ä¸¤é…å¯¹ä»¥è¡¨ç¤ºå¤æ•°ã€‚
    assert head_dim % 2 == 0, "Dimension must be divisible by 2"

    # 2. è®¡ç®—åŸºç¡€é¢‘ç‡ (theta_i)
    # ç”Ÿæˆåºåˆ— [0, 2, 4, ..., head_dim-2]ï¼Œå¯¹åº”å…¬å¼ä¸­çš„ (i-1)ï¼Œiä»1å¼€å§‹
    theta_numerator = torch.arange(0, head_dim, 2).float()
    # æ ¸å¿ƒå…¬å¼ï¼štheta_i = 10000^(-2(i-1)/dim)
    # ç»“æœ theta çš„å½¢çŠ¶ä¸º (head_dim / 2)
    theta = 1.0 / (theta ** (theta_numerator / head_dim)).to(device)

    # 3. ç”Ÿæˆä½ç½®ç´¢å¼• (m)
    # ç”Ÿæˆåºåˆ— [0, 1, 2, ..., seq_len-1]ï¼Œä»£è¡¨åºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®
    m = torch.arange(seq_len, device=device)

    # 4. è®¡ç®—å¤–ç§¯ï¼Œå¾—åˆ°æ¯ä¸ªä½ç½®æ¯ä¸ªç»´åº¦å¯¹åº”çš„è§’åº¦
    # å°†ä½ç½®å‘é‡må’Œé¢‘ç‡å‘é‡thetaè¿›è¡Œå¤–ç§¯è¿ç®—
    # ç»“æœ freqs çš„å½¢çŠ¶ä¸º (Seq_Len, Head_Dim / 2)
    # å…¶ä¸­ freqs[m, i] è¡¨ç¤ºåœ¨ä½ç½®mã€ç¬¬iä¸ªç»´åº¦å¯¹ä¸Šçš„æ—‹è½¬è§’åº¦
    freqs = torch.outer(m, theta).float()

    # 5. è½¬æ¢ä¸ºå¤æ•°å½¢å¼ï¼ˆæåæ ‡è¡¨ç¤ºï¼‰
    # torch.polar æ ¹æ®æåæ ‡åˆ›å»ºå¤æ•°ï¼šæ¨¡é•¿ï¼ˆè¿™é‡Œè®¾ä¸º1ï¼‰å’Œè§’åº¦ï¼ˆfreqsï¼‰
    # å…¶ç‰©ç†æ„ä¹‰æ˜¯ï¼šå¤æ•° e^(i * freqs[m, i])ï¼Œå³æ—‹è½¬freqs[m, i]å¼§åº¦
    freqs_complex = torch.polar(torch.ones_like(freqs), freqs)
    return freqs_complex
```
**å…³é”®ç‚¹**ï¼šè¯¥å‡½æ•°æœ€ç»ˆè¿”å›ä¸€ä¸ªå¤æ•°å¼ é‡ `freqs_complex`ï¼Œå…¶ç»´åº¦ä¸º `(åºåˆ—é•¿åº¦, å¤´ç»´åº¦/2)`ã€‚è¿™ä¸ªå¼ é‡ç¼–ç äº†ä¸åŒä½ç½®ã€ä¸åŒç‰¹å¾ç»´åº¦ä¸Šæ‰€éœ€çš„æ—‹è½¬é‡ã€‚

### âš™ï¸ åº”ç”¨æ—‹è½¬åµŒå…¥ï¼š`apply_rotary_embeddings`

è¿™ä¸ªå‡½æ•°åˆ©ç”¨é¢„è®¡ç®—å¥½çš„å¤æ•°æ—‹è½¬å› å­ï¼Œå¯¹æŸ¥è¯¢ï¼ˆQueryï¼‰å’Œé”®ï¼ˆKeyï¼‰å‘é‡è¿›è¡Œå®é™…çš„æ—‹è½¬æ“ä½œã€‚

```python
def apply_rotary_embeddings(x: torch.Tensor, freqs_complex: torch.Tensor, device: str):
    # 1. å°†è¾“å…¥å‘é‡è½¬æ¢ä¸ºå¤æ•°å½¢å¼
    # x çš„å½¢çŠ¶ä¸º (Batch, Seq_Len, Num_Heads, Head_Dim)
    # å°†æœ€åHead_Dimç»´åº¦ä¸¤ä¸¤åˆ†ç»„ (x1, x2), (x3, x4), ...ï¼Œæ¯ç»„è§†ä¸ºä¸€ä¸ªå¤æ•° x1 + i*x2
    # é‡å¡‘åå½¢çŠ¶å˜ä¸º (B, Seq_Len, H, Head_Dim/2, 2)ï¼Œå†é€šè¿‡view_as_complexå˜ä¸º (B, Seq_Len, H, Head_Dim/2)
    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))

    # 2. è°ƒæ•´é¢‘ç‡å¼ é‡çš„å½¢çŠ¶ä»¥æ”¯æŒå¹¿æ’­
    # freqs_complex åˆå§‹å½¢çŠ¶ä¸º (Seq_Len, Head_Dim/2)
    # åœ¨æ‰¹æ¬¡ç»´åº¦å’Œå¤´ç»´åº¦ä¸Šå¢åŠ ç»´åº¦ï¼Œå˜ä¸º (1, Seq_Len, 1, Head_Dim/2)
    # è¿™ä½¿å¾—å®ƒå¯ä»¥ä¸x_complex (B, Seq_Len, H, Head_Dim/2) è¿›è¡Œé€å…ƒç´ è¿ç®—
    freqs_complex = freqs_complex.unsqueeze(0).unsqueeze(2)

    # 3. æ‰§è¡Œæ—‹è½¬æ“ä½œï¼ˆå¤æ•°ä¹˜æ³•ï¼‰
    # åœ¨å¤æ•°åŸŸï¼Œä¸¤ä¸ªå¤æ•°ç›¸ä¹˜ç­‰ä»·äºåœ¨æåæ ‡ä¸‹å°†å®ƒä»¬çš„æ¨¡é•¿ç›¸ä¹˜ã€è§’åº¦ç›¸åŠ ã€‚
    # è¿™é‡Œæ¨¡é•¿ä¸º1ï¼Œæ‰€ä»¥å¤æ•°ä¹˜æ³•ä»…å®ç°æ—‹è½¬ï¼š (a+bi) * (cosÎ¸ + i*sinÎ¸) = (a*cosÎ¸ - b*sinÎ¸) + i*(a*sinÎ¸ + b*cosÎ¸)
    # ç»“æœ x_rotated çš„å½¢çŠ¶ä¸º (B, Seq_Len, H, Head_Dim/2)
    x_rotated = x_complex * freqs_complex

    # 4. å°†æ—‹è½¬åçš„å¤æ•°å‘é‡è½¬æ¢å›å®æ•°è¡¨ç¤º
    # view_as_real å°†å¤æ•°è½¬æ¢å›å®æ•°å¯¹ï¼Œå½¢çŠ¶å˜ä¸º (B, Seq_Len, H, Head_Dim/2, 2)
    # æœ€åreshapeå›åŸå§‹è¾“å…¥xçš„å½¢çŠ¶ (B, Seq_Len, H, Head_Dim)
    x_out = torch.view_as_real(x_rotated)
    x_out = x_out.reshape(*x.shape)

    return x_out.type_as(x).to(device)
```
**å…³é”®ç‚¹**ï¼šå¤æ•°ä¹˜æ³• `x_complex * freqs_complex` æ˜¯å®ç°æ—‹è½¬çš„æ ¸å¿ƒã€‚å®ƒå°†ä½ç½®ä¿¡æ¯ç¼–ç ä¸ºå‘é‡åœ¨ç‰¹å¾ç©ºé—´ä¸­çš„æ—‹è½¬ï¼Œä»è€Œåœ¨è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°æ—¶ï¼Œç‚¹ç§¯ç»“æœä»…ä¾èµ–äºè¯å…ƒçš„ç›¸å¯¹ä½ç½®å·® `m-n`ã€‚

### ğŸ’ æ ¸å¿ƒä»·å€¼ä¸æ€»ç»“

1.  **ç›¸å¯¹ä½ç½®ç¼–ç **ï¼šRoPEçš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äºï¼Œå®ƒé€šè¿‡æ—‹è½¬ä½¿æ³¨æ„åŠ›åˆ†æ•° `<f_q(q_m, m), f_k(k_n, n)>` çš„ç»“æœä»…ä¾èµ–äºè¯å…ƒå†…å®¹ `(q, k)` å’Œå®ƒä»¬çš„ç›¸å¯¹ä½ç½® `(m-n)`ï¼Œè€Œéç»å¯¹ä½ç½® `m` å’Œ `n`ã€‚è¿™æ›´ç¬¦åˆè¯­è¨€çš„å†…åœ¨é€»è¾‘ã€‚
2.  **è¿œç¨‹è¡°å‡æ€§**ï¼šéšç€ç›¸å¯¹è·ç¦» `|m-n|` çš„å¢å¤§ï¼Œæ—‹è½¬è§’åº¦å·®å¢å¤§ï¼Œå†…ç§¯çš„æœŸæœ›å€¼ä¼šè¡°å‡ï¼Œè¿™ç¬¦åˆè‡ªç„¶è¯­è¨€ä¸­è·ç¦»è¶Šè¿œçš„è¯å…³è”æ€§å¯èƒ½è¶Šå¼±çš„å…ˆéªŒã€‚
3.  **å®ç°ä¼˜é›…é«˜æ•ˆ**ï¼šåˆ©ç”¨å¤æ•°å’ŒçŸ©é˜µè¿ç®—ï¼Œå®ç°ç®€æ´ä¸”æ˜“äºåœ¨GPUä¸Šå¹¶è¡Œè®¡ç®—ï¼Œæˆä¸ºLLaMAã€Mistralç­‰ä¼—å¤šä¸»æµå¤§è¯­è¨€æ¨¡å‹çš„æ ‡å‡†é…ç½®ã€‚

## 11 - SelfAttention

è¿™ä»½ä»£ç å®ç°äº†ä¸€ä¸ª**æ”¯æŒåˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›** å¹¶é›†æˆäº†**æ—‹è½¬ä½ç½®ç¼–ç ** å’Œ**KVç¼“å­˜** çš„ç°ä»£åŒ–è‡ªæ³¨æ„åŠ›æ¨¡å—ã€‚å®ƒå¹¿æ³›ç”¨äºLLaMAç­‰å¤§å‹è¯­è¨€æ¨¡å‹çš„è‡ªå›å½’è§£ç æ¨ç†ã€‚

### ğŸ”§ åˆå§‹åŒ–å‚æ•°è§£æ

åˆå§‹åŒ–å‡½æ•°å®šä¹‰äº†æ¨¡å‹çš„æ ¸å¿ƒç»“æ„å‚æ•°å’ŒæŠ•å½±å±‚ï¼š

```python
def __init__(self, args: ModelArgs):
    super().__init__()
    
    # å¤´æ•°è®¾ç½®ï¼šæŸ¥è¯¢å¤´ä¸é”®å€¼å¤´å¯ä»¥ä¸åŒï¼Œæ”¯æŒåˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›(GQA)
    self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads
    self.n_heads_q = args.n_heads  # æŸ¥è¯¢å¤´æ•°é‡
    self.n_rep = self.n_heads_q // self.n_kv_heads  # æ¯ä¸ªé”®å€¼å¤´é‡å¤ä½¿ç”¨çš„æ¬¡æ•°
    
    # è®¡ç®—æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦
    self.head_dim = args.dim // args.n_heads
    
    # çº¿æ€§æŠ•å½±å±‚ï¼šå°†è¾“å…¥æ˜ å°„åˆ°Qã€Kã€Vç©ºé—´
    self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)
    self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)
    self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)
    self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)  # è¾“å‡ºæŠ•å½±
    
    # KVç¼“å­˜ï¼šå­˜å‚¨å†å²çš„é”®å€¼å¯¹ï¼Œé¿å…é‡å¤è®¡ç®—
    self.cache_k = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim))
    self.cache_v = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim))
```

**å…³é”®è®¾è®¡è§£æ**ï¼š
- **åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›**ï¼šå½“ `n_kv_heads` < `n_heads_q` æ—¶ï¼Œå¤šä¸ªæŸ¥è¯¢å¤´å…±äº«åŒä¸€ç»„é”®å€¼å¤´ï¼Œå¤§å¹…å‡å°‘å†…å­˜å ç”¨å’Œè®¡ç®—é‡
- **KVç¼“å­˜æœºåˆ¶**ï¼šåœ¨è‡ªå›å½’ç”Ÿæˆä¸­ç¼“å­˜å†å²é”®å€¼å¯¹ï¼Œæ¯ä¸ªæ–°tokenåªéœ€è®¡ç®—å½“å‰æ­¥çš„Qã€Kã€V
- **æ— åç½®çº¿æ€§å±‚**ï¼šè¿™æ˜¯Transformerçš„å¸¸è§è®¾è®¡é€‰æ‹©ï¼Œä¸ºäº†è®­ç»ƒç¨³å®šæ€§

### âš™ï¸ å‰å‘ä¼ æ’­æµç¨‹è¯¦è§£

å‰å‘ä¼ æ’­æ˜¯è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒï¼ŒåŒ…å«ä»¥ä¸‹å…³é”®æ­¥éª¤ï¼š

#### **1. çº¿æ€§æŠ•å½±ä¸å¼ é‡é‡å¡‘**
```python
# å°†è¾“å…¥æŠ•å½±åˆ°Qã€Kã€Vç©ºé—´
xq = self.wq(x)  # [batch_size, seq_len, n_heads_q * head_dim]
xk = self.wk(x)  # [batch_size, seq_len, n_kv_heads * head_dim]  
xv = self.wv(x)  # [batch_size, seq_len, n_kv_heads * head_dim]

# é‡å¡‘å¼ é‡å½¢çŠ¶ï¼Œåˆ†ç¦»å‡ºæ³¨æ„åŠ›å¤´ç»´åº¦
xq = xq.view(batch_size, seq_len, self.n_heads_q, self.head_dim)
xk = xk.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)
xv = xv.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)
```

#### **2. åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç **
```python
# å¯¹Qå’ŒKåº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç 
xq = apply_rotary_embeddings(xq, freqs_complex, device=x.device)
xk = apply_rotary_embeddings(xk, freqs_complex, device=x.device)
```
**ä½œç”¨**ï¼šé€šè¿‡æ—‹è½¬æ“ä½œå°†ç»å¯¹ä½ç½®ä¿¡æ¯ä»¥ç›¸å¯¹ä½ç½®çš„æ–¹å¼ç¼–ç åˆ°æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿæ„ŸçŸ¥tokené—´çš„ç›¸å¯¹è·ç¦»ã€‚

#### **3. KVç¼“å­˜æ›´æ–°ä¸å¤ç”¨**
```python
# å°†å½“å‰tokençš„Kã€Vå­˜å…¥ç¼“å­˜
self.cache_k[:batch_size, start_pos : start_pos + seq_len] = xk
self.cache_v[:batch_size, start_pos : start_pos + seq_len] = xv

# ä»ç¼“å­˜ä¸­è¯»å–æ‰€æœ‰å†å²Kã€Vï¼ˆåŒ…æ‹¬å½“å‰tokenï¼‰
keys = self.cache_k[:batch_size, :start_pos + seq_len]    # [batch_size, seq_len_kv, n_kv_heads, head_dim]
values = self.cache_v[:batch_size, :start_pos + seq_len] # [batch_size, seq_len_kv, n_kv_heads, head_dim]
```
**è®¾è®¡ä¼˜åŠ¿**ï¼šç¼“å­˜æœºåˆ¶ä½¿æ¨¡å‹åœ¨ç”Ÿæˆæ¯ä¸ªæ–°tokenæ—¶åªéœ€è®¡ç®—å½“å‰æ­¥çš„æ³¨æ„åŠ›ï¼Œæå¤§æå‡æ¨ç†æ•ˆç‡ã€‚

#### **4. åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›å®ç°**
```python
# é‡å¤é”®å€¼å¤´ä»¥åŒ¹é…æŸ¥è¯¢å¤´æ•°é‡ï¼ˆåˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›çš„æ ¸å¿ƒï¼‰
keys = repeat_kv(keys, self.n_rep)    # [batch_size, seq_len_kv, n_heads_q, head_dim]
values = repeat_kv(values, self.n_rep) # [batch_size, seq_len_kv, n_heads_q, head_dim]
```
**åŸç†**ï¼šå½“ `n_rep` > 1 æ—¶ï¼Œæ¯ä¸ªé”®å€¼å¤´è¢«å¤šä¸ªæŸ¥è¯¢å¤´å…±äº«ï¼Œåœ¨ä¿æŒæ¨¡å‹è¡¨è¾¾èƒ½åŠ›çš„åŒæ—¶æ˜¾è‘—å‡å°‘å†…å­˜å ç”¨ã€‚

#### **5. æ³¨æ„åŠ›è®¡ç®—æ ¸å¿ƒ**
```python
# è°ƒæ•´ç»´åº¦é¡ºåºä»¥è¿›è¡ŒçŸ©é˜µä¹˜æ³•
xq = xq.transpose(1, 2)    # [batch_size, n_heads_q, seq_len, head_dim]
keys = keys.transpose(1, 2)    # [batch_size, n_heads_q, seq_len_kv, head_dim]
values = values.transpose(1, 2) # [batch_size, n_heads_q, seq_len_kv, head_dim]

# è®¡ç®—ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›åˆ†æ•°
scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)

# åº”ç”¨softmaxå¾—åˆ°æ³¨æ„åŠ›æƒé‡
scores = F.softmax(scores.float(), dim=-1).type_as(xq)

# åŠ æƒæ±‚å’Œï¼šæ³¨æ„åŠ›æƒé‡ä¸Valueç›¸ä¹˜
output = torch.matmul(scores, values)  # [batch_size, n_heads_q, seq_len, head_dim]
```
**æ•°å­¦åŸç†**ï¼šè¿™ä¸€ç³»åˆ—æ“ä½œå®ç°äº†æ ‡å‡†çš„ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…¶ä¸­Queryå’ŒKeyçš„ç‚¹ç§¯è¡¡é‡ç›¸å…³æ€§ï¼Œsoftmaxå½’ä¸€åŒ–åä½œä¸ºæƒé‡å¯¹Valueè¿›è¡ŒåŠ æƒæ±‚å’Œã€‚

#### **6. è¾“å‡ºæŠ•å½±ä¸å½¢çŠ¶æ¢å¤**
```python
# åˆå¹¶å¤šå¤´è¾“å‡ºå¹¶é€šè¿‡çº¿æ€§æŠ•å½±
output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)
return self.wo(output)  # [batch_size, seq_len, dim]
```
**ç›®çš„**ï¼šå°†å¤šä¸ªæ³¨æ„åŠ›å¤´çš„è¾“å‡ºåˆå¹¶ï¼Œå¹¶é€šè¿‡çº¿æ€§å˜æ¢æ¢å¤åŸå§‹ç»´åº¦ï¼Œä»¥ä¾¿åç»­ç½‘ç»œå±‚å¤„ç†ã€‚

### ğŸ’¡ æ ¸å¿ƒæœºåˆ¶æ€»ç»“

è¿™ä¸ªè‡ªæ³¨æ„åŠ›æ¨¡å—é€šè¿‡ä¸‰é¡¹å…³é”®æŠ€æœ¯ä¼˜åŒ–äº†å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†æ•ˆç‡ï¼š

1.  **åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›**ï¼šé€šè¿‡è®©å¤šä¸ªæŸ¥è¯¢å¤´å…±äº«é”®å€¼å¤´ï¼Œåœ¨åŸºæœ¬ä¿æŒæ¨¡å‹è´¨é‡çš„åŒæ—¶ï¼Œ**æ˜¾è‘—å‡å°‘äº†KVç¼“å­˜çš„å¤§å°å’Œå†…å­˜å¸¦å®½éœ€æ±‚**
2.  **æ—‹è½¬ä½ç½®ç¼–ç **ï¼šä»¥ç›¸å¯¹ä½ç½®ç¼–ç çš„æ–¹å¼æä¾›ä½ç½®ä¿¡æ¯ï¼Œ**æ›´å¥½åœ°å¤„ç†é•¿åºåˆ—å’Œå¤–æ¨ä»»åŠ¡**
3.  **KVç¼“å­˜æœºåˆ¶**ï¼šåœ¨è‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹ä¸­é¿å…é‡å¤è®¡ç®—ï¼Œ**æå¤§æå‡æ¨ç†é€Ÿåº¦**

## 12 - FeedForward

è¿™ä»½ä»£ç å®ç°äº†ä¸€ä¸ªåŸºäº **SwiGLU æ¿€æ´»å‡½æ•°** çš„ç°ä»£å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFFNï¼‰æ¨¡å—ï¼Œå®ƒæ˜¯ Transformer æ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯ LLaMA ç­‰å…ˆè¿›å¤§è¯­è¨€æ¨¡å‹ï¼‰ä¸­çš„æ ¸å¿ƒç»„ä»¶ä¹‹ä¸€ã€‚ä¸‹é¢æˆ‘å°†ä»æ•´ä½“æ¦‚å†µã€å…³é”®è¦ç‚¹ã€é€è¡Œè§£é‡Šå’Œè®¾è®¡å¯¹æ¯”å››ä¸ªæ–¹é¢è¿›è¡Œè§£è¯»ã€‚

### å†…å®¹æ¦‚å†µ
è¯¥ `FeedForward` ç±»æ˜¯ä¸€ä¸ª**ä¸“ä¸ºç°ä»£å¤§è¯­è¨€æ¨¡å‹ä¼˜åŒ–**çš„å‰é¦ˆç½‘ç»œå±‚ã€‚å®ƒæ²¡æœ‰é‡‡ç”¨åŸå§‹ Transformer ä¸­ç®€å•çš„â€œçº¿æ€§å˜æ¢ â†’ ReLU â†’ çº¿æ€§å˜æ¢â€ç»“æ„ï¼Œè€Œæ˜¯ä½¿ç”¨äº†æ›´å…ˆè¿›çš„ **SwiGLUï¼ˆSiLU é—¨æ§çº¿æ€§å•å…ƒï¼‰** æ¶æ„ã€‚å…¶è®¾è®¡ç›®æ ‡æ˜¯åœ¨ä¿æŒæ¨¡å‹è¡¨è¾¾åŠ›çš„åŒæ—¶ï¼Œå®ç°æ›´é«˜çš„å‚æ•°æ•ˆç‡å’Œæ›´å¥½çš„è®­ç»ƒç¨³å®šæ€§ã€‚

### è¦ç‚¹æ€»ç»“
1.  **æ ¸å¿ƒæ¶æ„ï¼šSwiGLU é—¨æ§æœºåˆ¶**
    -   è¯¥ FFN çš„æ ¸å¿ƒæ˜¯ **SwiGLU æ¿€æ´»å‡½æ•°**ï¼Œå…¶å…¬å¼ä¸º `SwiGLU(x) = SiLU(xW) âŠ— xV`ï¼Œå…¶ä¸­ `âŠ—` æ˜¯é€å…ƒç´ ç›¸ä¹˜ã€‚
    -   ä¸ä»…ä½¿ç”¨ä¸€ä¸ªæƒé‡çŸ©é˜µçš„ä¼ ç»Ÿ FFN ä¸åŒï¼ŒSwiGLU ä½¿ç”¨**ä¸‰ä¸ªæƒé‡çŸ©é˜µ** (`w1`, `w2`, `w3`) æ¥å®ç°ä¸€ä¸ªé—¨æ§ç»“æ„ã€‚`SiLU(self.w1(x))` ä½œä¸ºâ€œé—¨â€æ¥æ§åˆ¶ä¿¡æ¯æµï¼Œ`self.w3(x)` æä¾›åŸå§‹ä¿¡æ¯ï¼ŒäºŒè€…ç›¸ä¹˜å®ç°æ›´ç²¾ç»†çš„ç‰¹å¾è°ƒæ§ã€‚

2.  **éšè—å±‚ç»´åº¦çš„ç²¾å¿ƒè®¡ç®—**
    -   `hidden_dim` çš„è®¡ç®—æ˜¯æ¨¡å‹æ•ˆç‡çš„å…³é”®ã€‚å®ƒéµå¾ª LLaMA è®ºæ–‡çš„è®¾è®¡ï¼š
        -   å…ˆæ‰©å±•åˆ° `4 * dim`ï¼ˆåŸºç¡€æ‰©å±•ï¼‰
        -   ç„¶åç¼©æ”¾ `int(2 * hidden_dim / 3)`ï¼ˆä¸ºä¸‰çŸ©é˜µç»“æ„ä¿æŒå‚æ•°æ€»é‡å‡è¡¡ï¼‰
        -   æœ€åå¯¹é½åˆ° `args.multiple_of` çš„æ•´æ•°å€ï¼ˆä¼˜åŒ–ç¡¬ä»¶è®¡ç®—æ•ˆç‡ï¼‰ã€‚
    -   å¦‚æœæä¾›äº† `ffn_dim_multiplier`ï¼Œè¿˜ä¼šè¿›è¡Œè¿›ä¸€æ­¥ç¼©æ”¾ï¼Œè¿™ä¸ºæ¨¡å‹å®¹é‡è°ƒæ•´æä¾›äº†çµæ´»æ€§ã€‚

3.  **æ— åç½®ï¼ˆBiasï¼‰çš„çº¿æ€§å±‚**
    -   æ‰€æœ‰çº¿æ€§å±‚ (`w1`, `w2`, `w3`) éƒ½è®¾ç½® `bias=False`ã€‚è¿™æ˜¯ç°ä»£å¤§æ¨¡å‹çš„å¸¸è§åšæ³•ï¼Œæœ‰åŠ©äºç®€åŒ–æ¨¡å‹ã€æé«˜è®­ç»ƒç¨³å®šæ€§ï¼Œå¹¶ä¸”å¯¹æœ€ç»ˆæ€§èƒ½å½±å“ç”šå¾®ã€‚

4.  **åœ¨ Transformer ä¸­çš„è§’è‰²**
    -   åœ¨ Transformer å—ä¸­ï¼ŒFFN ä½äºè‡ªæ³¨æ„åŠ›å±‚ä¹‹åã€‚å®ƒçš„æ ¸å¿ƒä½œç”¨æ˜¯**æä¾›å¼ºå¤§çš„éçº¿æ€§å˜æ¢èƒ½åŠ›**ã€‚
    -   è‡ªæ³¨æ„åŠ›æœºåˆ¶è´Ÿè´£èšåˆå…¨å±€ä¿¡æ¯ï¼ˆå†³å®šâ€œå…³æ³¨ä»€ä¹ˆâ€ï¼‰ï¼Œè€Œ FFN åˆ™åƒä¸€ä¸ªâ€œä¸“å®¶å¤„ç†å™¨â€ï¼Œå¯¹æ¯ä¸ªä½ç½®çš„è¡¨ç¤ºè¿›è¡Œæ·±åº¦åŠ å·¥å’Œå˜æ¢ï¼Œå†³å®šâ€œå¦‚ä½•å¤„ç†â€è¿™äº›ä¿¡æ¯ã€‚

### ä»£ç è§£é‡Š

```python
class FeedForward(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()

        # 1. è®¡ç®—éšè—å±‚ç»´åº¦
        hidden_dim = 4 * args.dim  # åŸºç¡€æ‰©å±•ï¼šé€šå¸¸æ‰©å±•4å€
        hidden_dim = int(2 * hidden_dim /  ï¸)  # å‚æ•°å¹³è¡¡ï¼šå› ä½¿ç”¨3ä¸ªçŸ©é˜µï¼Œéœ€è°ƒæ•´ä»¥æ§åˆ¶æ€»å‚æ•°é‡
        if args.ffn_dim_multiplier is not None:
            hidden_dim = int(args.ffn_dim_multiplier * hidden_dim)  # çµæ´»æ€§ï¼šå…è®¸å¤–éƒ¨ç¼©æ”¾å› å­
        # ç¡¬ä»¶ä¼˜åŒ–ï¼šå°†ç»´åº¦å¯¹é½åˆ°multiple_ofçš„å€æ•°ï¼Œæå‡è®¡ç®—æ•ˆç‡ï¼ˆå¦‚GPUå†…å­˜å¯¹é½ï¼‰
        hidden_dim = args.multiple_of * ((hidden_dim + args.multiple_of - 1) // args.multiple_of)

        # 2. åˆå§‹åŒ–ä¸‰ä¸ªçº¿æ€§å±‚ï¼ˆæ— åç½®ï¼‰
        self.w1 = nn.Linear(args.dim, hidden_dim, bias=False)  # å‡ç»´æŠ•å½±ï¼ˆç”¨äºé—¨æ§ï¼‰
        self.w2 = nn.Linear(hidden_dim, args.dim, bias=False)  # é™ç»´æŠ•å½±ï¼ˆè¾“å‡ºæŠ•å½±ï¼‰
        self.w3 = nn.Linear(args.dim, hidden_dim, bias=False)  # å‡ç»´æŠ•å½±ï¼ˆç”¨äºå€¼è·¯å¾„ï¼‰

    def forward(self, x: torch.Tensor):
        # SwiGLUå‰å‘ä¼ æ’­
        swish = F.silu(self.w1(x))  # å¯¹w1çš„æŠ•å½±ç»“æœåº”ç”¨SiLUæ¿€æ´»å‡½æ•°ï¼Œä½œä¸ºâ€œé—¨â€
        x_V = self.w3(x)            # ç‹¬ç«‹è®¡ç®—å€¼è·¯å¾„
        x = swish * x_V             # æ ¸å¿ƒé—¨æ§æ“ä½œï¼šç”¨é—¨æ§åˆ¶å€¼è·¯å¾„çš„ä¿¡æ¯æµ
        x = self.w2(x)               # æœ€ç»ˆæŠ•å½±å›åŸå§‹ç»´åº¦ï¼Œä¸æ¨¡å—è¾“å…¥ç»´åº¦åŒ¹é…
        return x
```

### å¯¹æ¯”ï¼šä¸åŸå§‹Transformer FFNåŠSwiGLUçš„ä¼˜åŠ¿

| ç‰¹æ€§ | åŸå§‹Transformer FFN | **æœ¬ä»£ç  (SwiGLU FFN)** |
| :--- | :--- | :--- |
| **ç»“æ„** | `Linear -> ReLU -> Linear`ï¼ˆä¸¤çŸ©é˜µï¼‰ | `(SiLU(Linear) * Linear) -> Linear`ï¼ˆä¸‰çŸ©é˜µï¼‰ |
| **å‚æ•°é‡** | ~ \(2 \times d \times 4d\) = \(8d^2\) | é€šè¿‡è°ƒæ•´ä¸­é—´ç»´åº¦ï¼Œæ€»å‚æ•°é‡ä¸åŸå§‹FFNå¤§è‡´ç›¸å½“ |
| **æ ¸å¿ƒæ“ä½œ** | ReLUæ¿€æ´»ï¼ˆç®€å•çš„å•è·¯å¾„éçº¿æ€§å˜æ¢ï¼‰ | **é—¨æ§æœºåˆ¶**ï¼ˆåŒè·¯å¾„äº¤äº’ï¼Œæ›´ç²¾ç»†çš„ç‰¹å¾æ§åˆ¶ï¼‰ |
| **ä¼˜åŠ¿** | ç»“æ„ç®€å•ï¼Œå¼•å…¥éçº¿æ€§ | **æ›´å¼ºçš„è¡¨ç°åŠ›**å’Œ**æ›´ä¼˜çš„è®­ç»ƒåŠ¨æ€**ï¼Œæˆä¸ºLLaMAã€Gemmaç­‰ç°ä»£å¤§æ¨¡å‹çš„æ ‡å‡†é€‰æ‹© |

æ€»ä¹‹ï¼Œè¿™ä¸ª `FeedForward` æ¨¡å—ä½“ç°äº†å¤§è¯­è¨€æ¨¡å‹åœ¨åŸºç¡€ç»„ä»¶ä¸Šçš„ç²¾å¿ƒä¼˜åŒ–ã€‚é€šè¿‡é‡‡ç”¨ **SwiGLU é—¨æ§ç»“æ„**å’Œ**ç²¾ç¡®çš„ç»´åº¦è®¡ç®—**ï¼Œå®ƒåœ¨æ¨¡å‹å®¹é‡å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´å–å¾—äº†å‡ºè‰²çš„å¹³è¡¡ã€‚

## 13 - LLaMA Transformer

è¿™æ˜¯ä¸€ä¸ªåŸºäºç°ä»£æ¶æ„ï¼ˆå¦‚LLaMAï¼‰å®ç°çš„Transformerç¼–ç å™¨æ¨¡å‹ï¼Œå®ƒé‡‡ç”¨**ä»…è§£ç å™¨ï¼ˆDecoder-onlyï¼‰** çš„è‡ªå›å½’æ¶æ„ï¼Œä¸“ä¸ºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡è®¾è®¡ã€‚

### ğŸ§± æ ¸å¿ƒç»„ä»¶æ¦‚è§ˆ

| ç»„ä»¶ | ç±»å | æ ¸å¿ƒåŠŸèƒ½ | å…³é”®ç‰¹å¾ |
| :--- | :--- | :--- | :--- |
| **è¯åµŒå…¥å±‚** | `nn.Embedding` | å°†è¾“å…¥çš„è¯ç´¢å¼•è½¬æ¢ä¸ºå¯†é›†å‘é‡ã€‚ | å°†ç¦»æ•£çš„token IDæ˜ å°„åˆ°è¿ç»­å‘é‡ç©ºé—´ã€‚ |
| **ç¼–ç å™¨å—** | `EncoderBlock` | æ ¸å¿ƒå¤„ç†å•å…ƒï¼ŒåŒ…å«è‡ªæ³¨æ„åŠ›å’Œå‰é¦ˆç½‘ç»œã€‚ | ä½¿ç”¨**RMSNorm**è¿›è¡Œå½’ä¸€åŒ–ï¼Œé‡‡ç”¨**æ®‹å·®è¿æ¥**ã€‚ |
| **è‡ªæ³¨æ„åŠ›** | `SelfAttention` | è®¡ç®—åºåˆ—ä¸­æ¯ä¸ªtokenä¸å…¶ä»–tokençš„å…³è”æƒé‡ã€‚ | é›†æˆ**æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰** å’Œ**KVç¼“å­˜**ã€‚ |
| **å‰é¦ˆç½‘ç»œ** | `FeedForward` | å¯¹æ¯ä¸ªä½ç½®çš„è¡¨ç¤ºè¿›è¡Œéçº¿æ€§å˜æ¢ã€‚ | é‡‡ç”¨**SwiGLU**æ¿€æ´»å‡½æ•°ï¼Œå¢å¼ºéçº¿æ€§èƒ½åŠ›ã€‚ |
| **æœ€ç»ˆå½’ä¸€åŒ–** | `RMSNorm` | å¯¹æœ€åä¸€å±‚æ‰€æœ‰ç¼–ç å™¨å—çš„è¾“å‡ºè¿›è¡Œå½’ä¸€åŒ–ã€‚ | ç¨³å®šè®­ç»ƒï¼Œä¸ºè¾“å‡ºå±‚åšå‡†å¤‡ã€‚ |
| **è¾“å‡ºå±‚** | `nn.Linear` | å°†æœ€ç»ˆè¡¨ç¤ºæŠ•å½±å›è¯æ±‡è¡¨å¤§å°ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªè¯ã€‚ | å°†éšè—å±‚ç»´åº¦ `args.dim` æ˜ å°„åˆ° `vocab_size`ã€‚ |

### ğŸ” å…³é”®ä»£ç è¯¦è§£

#### 1. `EncoderBlock`ï¼šç¼–ç å™¨æ ¸å¿ƒå—
`EncoderBlock` æ˜¯æ¨¡å‹çš„æ ¸å¿ƒæ„å»ºå—ï¼Œå®ƒé¡ºåºåœ°åº”ç”¨è‡ªæ³¨æ„åŠ›å’Œå‰é¦ˆç½‘ç»œã€‚

-   **å½’ä¸€åŒ–ç­–ç•¥ï¼ˆRMSNormï¼‰**ï¼šä¸åŸå§‹Transformerä¸åŒï¼Œæ­¤å®ç°åœ¨æ³¨æ„åŠ›å±‚å’Œå‰é¦ˆå±‚**ä¹‹å‰**è¿›è¡Œå½’ä¸€åŒ–ï¼ˆPre-Normalizationï¼‰ã€‚è¿™ç§ç­–ç•¥é€šå¸¸æœ‰åŠ©äºè®­ç»ƒæ›´æ·±çš„ç½‘ç»œï¼Œæå‡ç¨³å®šæ€§ã€‚
-   **æ®‹å·®è¿æ¥**ï¼šæ¯ä¸ªå­å±‚ï¼ˆæ³¨æ„åŠ›ã€å‰é¦ˆç½‘ç»œï¼‰çš„è¾“å‡ºéƒ½ä¼šä¸è¾“å…¥ç›¸åŠ ï¼Œå½¢æˆæ®‹å·®è¿æ¥ã€‚è¿™æœ‰åŠ©äºç¼“è§£æ·±åº¦ç½‘ç»œä¸­çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚

```python
class EncoderBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        # æ³¨æ„åŠ›æœºåˆ¶ä¸å‰ç½®å½’ä¸€åŒ–
        self.attention = SelfAttention(args)
        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps) # Pre-Norm

        # å‰é¦ˆç½‘ç»œä¸å‰ç½®å½’ä¸€åŒ–
        self.feed_forward = FeedForward(args)
        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps) # Pre-Norm
    
    def forward(self, x: torch.Tensor, start_pos: int, freqs_complex: torch.Tensor):
        # 1. æ³¨æ„åŠ›å­å±‚: Norm -> Attention -> æ®‹å·®è¿æ¥
        h = x + self.attention.forward(
            self.attention_norm(x), start_pos, freqs_complex
        )
        # 2. å‰é¦ˆç½‘ç»œå­å±‚: Norm -> FFN -> æ®‹å·®è¿æ¥
        out = h + self.feed_forward.forward(self.ffn_norm(h))
        return out
```

#### 2. `Transformer`ï¼šæ•´ä½“æ¨¡å‹æ¶æ„
`Transformer` ç±»å°†è¯åµŒå…¥ã€å¤šä¸ªç¼–ç å™¨å—ä»¥åŠè¾“å‡ºå±‚ç»„åˆåœ¨ä¸€èµ·ï¼Œæ„æˆå®Œæ•´çš„æ¨¡å‹ã€‚

-   **ä½ç½®ç¼–ç **ï¼šæ¨¡å‹ä½¿ç”¨**æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰**ï¼Œé€šè¿‡ `freqs_complex` åœ¨æ³¨æ„åŠ›è®¡ç®—ä¸­æ³¨å…¥ä½ç½®ä¿¡æ¯ã€‚è¿™ç§æ–¹å¼èƒ½è®©æ¨¡å‹æ›´å¥½åœ°ç†è§£tokené—´çš„ç›¸å¯¹ä½ç½®ã€‚
-   **è‡ªå›å½’æ¨ç†**ï¼š`forward` å‡½æ•°è®¾è®¡ä¸ºä¸€æ¬¡å¤„ç†ä¸€ä¸ªtokenï¼ˆ`seq_len == 1`ï¼‰ï¼Œå¹¶åˆ©ç”¨ `start_pos` å‚æ•°ç®¡ç†KVç¼“å­˜ï¼Œè¿™æ˜¯å…¸å‹çš„é«˜æ•ˆè‡ªå›å½’æ–‡æœ¬ç”Ÿæˆæ–¹å¼ã€‚

```python
class Transformer(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        # è¯åµŒå…¥å±‚
        self.tok_embeddings = nn.Embedding(self.vocab_size, args.dim)

        # å †å Nä¸ªç¼–ç å™¨å—
        self.layers = nn.ModuleList([EncoderBlock(args) for _ in range(args.n_layers)])

        # æœ€ç»ˆè¾“å‡ºå‰çš„å½’ä¸€åŒ–åŠçº¿æ€§æŠ•å½±å±‚
        self.norm = RMSNorm(args.dim, eps=args.norm_eps)
        self.output = nn.Linear(args.dim, self.vocab_size, bias=False)

        # é¢„è®¡ç®—æ—‹è½¬ä½ç½®ç¼–ç æ‰€éœ€çš„é¢‘ç‡å‚æ•°
        self.freqs_complex = precompute_theta_pos_frequencies(...)

    def forward(self, tokens: torch.Tensor, start_pos: int):
        # 1. è¯åµŒå…¥
        h = self.tok_embeddings(tokens)

        # 2. è·å–å½“å‰ä½ç½®å¯¹åº”çš„æ—‹è½¬ä½ç½®ç¼–ç 
        freqs_complex = self.freqs_complex[start_pos:start_pos + seq_len]

        # 3. é€å±‚é€šè¿‡ç¼–ç å™¨å—
        for layer in self.layers:
            h = layer(h, start_pos, freqs_complex)

        # 4. æœ€ç»ˆå½’ä¸€åŒ–å¹¶æŠ•å½±åˆ°è¯æ±‡è¡¨ç©ºé—´ï¼Œå¾—åˆ°æ¯ä¸ªè¯çš„é¢„æµ‹åˆ†æ•°
        h = self.norm(h)
        output = self.output(h).float()
        return output
```

### ğŸ’¡ è®¾è®¡äº®ç‚¹æ€»ç»“

è¿™ä¸ªTransformerå®ç°ä½“ç°äº†ç°ä»£å¤§è¯­è¨€æ¨¡å‹çš„å‡ é¡¹å…³é”®ä¼˜åŒ–æŠ€æœ¯ï¼š

1.  **æ•ˆç‡ä¼˜åŒ–**ï¼šé€šè¿‡**KVç¼“å­˜**å’Œ**æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰**ï¼Œæå¤§æå‡äº†è‡ªå›å½’ç”Ÿæˆçš„æ¨ç†æ•ˆç‡ã€‚
2.  **ç»“æ„ä¼˜åŒ–**ï¼šé‡‡ç”¨**Pre-LN**ç»“æ„å’Œ**RMSNorm**ï¼Œæœ‰åŠ©äºè®­ç»ƒæ›´æ·±ã€æ›´ç¨³å®šçš„ç½‘ç»œã€‚
3.  **ç»„ä»¶ç°ä»£åŒ–**ï¼šä½¿ç”¨**SwiGLU**æ¿€æ´»å‡½æ•°ç­‰æ”¹è¿›ç»„ä»¶ï¼Œå¢å¼ºäº†æ¨¡å‹çš„éçº¿æ€§è¡¨è¾¾èƒ½åŠ›ã€‚

## 14 - LLaMAæ¨ç†

### ğŸ§± æ ¸å¿ƒç±»ä¸åˆå§‹åŒ–

è¿™ä¸ª `LLaMA` ç±»æ˜¯ä¸€ä¸ª**å¤–å£³**ï¼Œå®ƒå¹¶ä¸ç›´æ¥å®šä¹‰ç½‘ç»œç»“æ„ï¼Œè€Œæ˜¯å°†**æ¨¡å‹**ã€**åˆ†è¯å™¨**å’Œ**é…ç½®å‚æ•°**ç»„åˆåœ¨ä¸€èµ·ï¼Œå¹¶æä¾›æ¨¡å‹æ„å»ºä¸æ–‡æœ¬ç”Ÿæˆçš„ä¾¿æ·æ–¹æ³•ã€‚

*   **`__init__` æ–¹æ³•**ï¼šéå¸¸ç®€å•ï¼Œåªæ˜¯å°†ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ä¿å­˜ä¸ºç±»çš„å±æ€§ã€‚
    *   `self.model`: æ ¸å¿ƒçš„ `Transformer` æ¨¡å‹ï¼Œè´Ÿè´£æ‰€æœ‰çš„è®¡ç®—ã€‚
    *   `self.tokenizer`: `SentencePieceProcessor` åˆ†è¯å™¨ï¼Œè´Ÿè´£åœ¨æ–‡æœ¬å’Œä»¤ç‰ŒIDä¹‹é—´è¿›è¡Œè½¬æ¢ã€‚
    *   `self.args`: æ¨¡å‹çš„é…ç½®å‚æ•°ã€‚

### âš™ï¸ æ¨¡å‹æ„å»ºï¼š`build` é™æ€æ–¹æ³•

`build` æ–¹æ³•æ˜¯æ ¸å¿ƒï¼Œå®ƒè´Ÿè´£**ä»å¤´å¼€å§‹ç»„è£…ä¸€ä¸ªLLaMAæ¨¡å‹å®ä¾‹**ï¼Œå¹¶å¯é€‰åœ°åŠ è½½é¢„è®­ç»ƒçš„æƒé‡ã€‚å…¶å·¥ä½œæµç¨‹å¦‚ä¸‹ï¼š

```mermaid
flowchart TD
    A[å¼€å§‹æ„å»ºæ¨¡å‹] --> B{æ˜¯å¦åŠ è½½é¢„è®­ç»ƒæ¨¡å‹?};
    B -- æ˜¯ --> C[åŠ è½½æƒé‡æ–‡ä»¶<br>.pth];
    B -- å¦ --> D[ä»…ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡];
    C --> D;
    
    D --> E[åŠ è½½æ¨¡å‹å‚æ•°<br>params.json];
    E --> F[åˆå§‹åŒ–åˆ†è¯å™¨<br>SentencePieceProcessor];
    F --> G[æ ¹æ®è®¾å¤‡è®¾ç½®å¼ é‡ç±»å‹];
    G --> H[å®ä¾‹åŒ–æ ¸å¿ƒTransformeræ¨¡å‹];
    
    H --> I{æ˜¯å¦åŠ è½½äº†é¢„è®­ç»ƒæƒé‡?};
    I -- æ˜¯ --> J[å°†æƒé‡åŠ è½½åˆ°æ¨¡å‹ä¸­];
    I -- å¦ --> K[æ¨¡å‹ä¿æŒéšæœºåˆå§‹åŒ–];
    J --> L[è¿”å›ç»„è£…å¥½çš„LLaMAç±»å®ä¾‹];
    K --> L;
```

**å…³é”®æ­¥éª¤è§£æ**ï¼š

1.  **åŠ è½½æ£€æŸ¥ç‚¹**ï¼šå¦‚æœ `load_model` ä¸º `True`ï¼Œåˆ™ä» `checkpoints_dir` ç›®å½•åŠ è½½ç¬¬ä¸€ä¸ª `.pth` æƒé‡æ–‡ä»¶ã€‚`params.json` æ–‡ä»¶åˆ™åŒ…å«äº†æ¨¡å‹çš„ç»“æ„è¶…å‚æ•°ï¼ˆå¦‚å±‚æ•°ã€ç»´åº¦ç­‰ï¼‰ã€‚
2.  **åˆå§‹åŒ–ç»„ä»¶**ï¼š
    *   **åˆ†è¯å™¨**ï¼šä½¿ç”¨ `SentencePieceProcessor` åŠ è½½è¯è¡¨æ–‡ä»¶ï¼Œå¹¶å°†è¯è¡¨å¤§å°è®¾ç½®åˆ° `model_args` ä¸­ã€‚
    *   **æ¨¡å‹**ï¼šæ ¹æ®é…ç½®å®ä¾‹åŒ– `Transformer` æ¨¡å‹ã€‚ä»£ç ä¸­ `torch.set_default_tensor_type` çš„è®¾ç½®æ˜¯ä¸ºäº†**èŠ‚çœæ˜¾å­˜**ï¼Œä½¿ç”¨åŠç²¾åº¦ï¼ˆ`HalfTensor` å³ `float16`ï¼‰æˆ–åŠæµ®ç‚¹ç²¾åº¦ï¼ˆ`BFloat16Tensor`ï¼‰è¿›è¡Œè®¡ç®—ã€‚
3.  **åŠ è½½æƒé‡**ï¼šåœ¨åŠ è½½é¢„è®­ç»ƒæƒé‡æ—¶ï¼Œæœ‰ä¸€è¡Œå…³é”®æ“ä½œ `del checkpoint['rope.freqs']`ã€‚è¿™æ˜¯å› ä¸º `rope.freqs`ï¼ˆæ—‹è½¬ä½ç½®ç¼–ç çš„é¢‘ç‡ï¼‰é€šå¸¸æ˜¯åœ¨æ¨¡å‹åˆå§‹åŒ–æ—¶**åŠ¨æ€é¢„è®¡ç®—**çš„ï¼Œè€Œééœ€è¦ä¿å­˜çš„æ¨¡å‹å‚æ•°ï¼Œå› æ­¤éœ€è¦ä»åŠ è½½çš„æ£€æŸ¥ç‚¹ä¸­ç§»é™¤ä»¥é¿å…æŠ¥é”™ã€‚

### ğŸ’¬ æ–‡æœ¬ç”Ÿæˆï¼š`text_completion` æ–¹æ³•

è¿™ä¸ªæ–¹æ³•å®ç°äº†**è‡ªå›å½’ç”Ÿæˆ**ï¼Œå³æ ¹æ®ç»™å®šçš„æç¤ºï¼ˆpromptï¼‰ï¼Œæ¨¡å‹é€ä¸ªç”Ÿæˆåç»­çš„ tokenã€‚å…¶æ ¸å¿ƒå¾ªç¯å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

```mermaid
flowchart LR
    A[è¾“å…¥æç¤º Prompts] --> B[åˆ†è¯ä¸ç¼–ç <br>Prompt Tokens];
    B --> C[åˆå§‹åŒ–ç”Ÿæˆåºåˆ—<br>Tokens Tensor];
    
    C --> D[å½“å‰ä½ç½®cur_pos=1];
    D --> E{å¾ªç¯: æ˜¯å¦è¾¾åˆ°<br>æ€»é•¿åº¦æˆ–å…¨éƒ¨ç”Ÿæˆç»“æŸ?};
    E -- æ˜¯ --> F[è·³å‡ºå¾ªç¯];
    E -- å¦ --> G[æ¨¡å‹å‰å‘è®¡ç®—<br>è¾“å…¥: tokens up to cur_pos];
    G --> H[åº”ç”¨æ¸©åº¦è°ƒèŠ‚å’ŒTop-pé‡‡æ ·<br>å¾—åˆ°ä¸‹ä¸€ä¸ªä»¤ç‰Œ];
    H --> I[å°†æ–°ä»¤ç‰Œå¡«å…¥åºåˆ—];
    I --> J[æ›´æ–°ç”Ÿæˆç»“æŸçŠ¶æ€EOS];
    J --> K[cur_pos += 1];
    K --> D;
    
    F --> L[è§£ç ç”Ÿæˆçš„ä»¤ç‰Œåºåˆ—ä¸ºæ–‡æœ¬];
    L --> M[è¿”å›ç»“æœ];
```

**å…³é”®æ­¥éª¤è§£æ**ï¼š

1.  **å‡†å¤‡è¾“å…¥**ï¼šå°†æ–‡æœ¬æç¤ºè½¬æ¢ä¸º token ID åºåˆ—ï¼Œå¹¶å¡«å……åˆ°ä¸€ä¸ªå›ºå®šå¤§å°çš„å¼ é‡ `tokens` ä¸­ï¼Œåˆå§‹éƒ¨åˆ†ä¸º promptï¼Œå…¶ä½™ä¸ºå¡«å……ç¬¦ï¼ˆ`pad_id`ï¼‰ã€‚
2.  **è‡ªå›å½’å¾ªç¯**ï¼šè¿™æ˜¯ç”Ÿæˆçš„**æ ¸å¿ƒ**ã€‚åœ¨æ¯ä¸€ä¸ªç”Ÿæˆæ­¥ï¼ˆ`cur_pos`ï¼‰ï¼š
    *   **å‰å‘è®¡ç®—**ï¼šå°†å½“å‰åºåˆ—ï¼ˆä»å¼€å§‹åˆ° `cur_pos-1`ï¼‰è¾“å…¥æ¨¡å‹ï¼Œå¾—åˆ°ä¸‹ä¸€ä¸ª token çš„é¢„æµ‹é€»è¾‘å€¼ï¼ˆlogitsï¼‰ã€‚è¿™é‡Œæ¨¡å‹å†…éƒ¨ä¼šä½¿ç”¨ **KVç¼“å­˜** æ¥é¿å…é‡å¤è®¡ç®—ï¼Œæå‡æ•ˆç‡ã€‚
    *   **é‡‡æ ·ä¸‹ä¸€ä¸ª token**ï¼š
        *   **æ¸©åº¦è°ƒèŠ‚**ï¼š`temperature` å‚æ•°æ§åˆ¶ç”Ÿæˆçš„éšæœºæ€§ã€‚æ¸©åº¦è¶Šé«˜ï¼ˆ>1ï¼‰ï¼Œåˆ†å¸ƒè¶Šå¹³ç¼“ï¼Œè¾“å‡ºè¶Šéšæœºã€æœ‰åˆ›æ„ï¼›æ¸©åº¦è¶Šä½ï¼ˆ<1ï¼‰ï¼Œåˆ†å¸ƒè¶Šå°–é”ï¼Œè¾“å‡ºè¶Šç¡®å®šï¼›æ¸©åº¦ä¸º0åˆ™é€€åŒ–ä¸ºè´ªå¿ƒæœç´¢ï¼ˆæ€»æ˜¯é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„tokenï¼‰ã€‚
        *   **Top-p é‡‡æ ·**ï¼šè°ƒç”¨ `_sample_top_p` æ–¹æ³•ï¼Œåˆç§°**æ ¸é‡‡æ ·**ã€‚å®ƒä»ç´¯ç§¯æ¦‚ç‡è¶…è¿‡ `p` çš„æœ€å° token é›†åˆä¸­éšæœºæŠ½æ ·ï¼Œèƒ½åŠ¨æ€æ§åˆ¶å€™é€‰é›†çš„å¤§å°ï¼Œæœ‰æ•ˆé¿å…ç”Ÿæˆä½è´¨é‡ tokenï¼Œæ˜¯ä¿è¯ç”Ÿæˆè´¨é‡çš„å…³é”®æŠ€æœ¯ã€‚
    *   **æ›´æ–°åºåˆ—**ï¼šå°†é‡‡æ ·å¾—åˆ°çš„æ–° token å¡«å…¥åºåˆ—çš„ `cur_pos` ä½ç½®ã€‚è¿™é‡Œæœ‰ä¸€ä¸ªå·§å¦™è®¾è®¡ï¼š`torch.where(prompt_tokens_mask[:, cur_pos], tokens[:, cur_pos], next_token)`ã€‚åœ¨ prompt èŒƒå›´å†…ï¼Œå®ƒç›´æ¥ä½¿ç”¨çœŸå®çš„ prompt token è€Œéæ¨¡å‹è¾“å‡ºï¼Œè¿™ç¡®ä¿äº† prompt è¢«æ­£ç¡®ä¼ é€’ï¼Œä¹Ÿé¿å…äº†åœ¨ prompt ä½ç½®è¿›è¡Œæ— æ•ˆçš„é‡‡æ ·ã€‚
3.  **å¤„ç†è¾“å‡º**ï¼šå¾ªç¯ç»“æŸåï¼Œå°†æ¯ä¸ªåºåˆ—ä¸­çš„ token ID è½¬æ¢å›æ–‡æœ¬ã€‚å¦‚æœç”Ÿæˆäº†ç»“æŸç¬¦ï¼ˆ`eos_id`ï¼‰ï¼Œåˆ™æå‰æˆªæ–­ã€‚

### ğŸ¯ è¾…åŠ©æ–¹æ³•ï¼š`_sample_top_p`

è¿™ä¸ªæ–¹æ³•æ˜¯ Top-p é‡‡æ ·çš„å…·ä½“å®ç°ï¼Œå…¶æµç¨‹å¦‚ä¸‹ï¼š
1.  **æ’åº**ï¼šå¯¹æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œé™åºæ’åºï¼Œå¾—åˆ°æœ‰åºçš„æ¦‚ç‡å€¼å’Œå¯¹åº”çš„ç´¢å¼•ã€‚
2.  **è®¡ç®—ç´¯ç§¯æ¦‚ç‡**ï¼šè®¡ç®—æ’åºåæ¦‚ç‡çš„ç´¯ç§¯å’Œã€‚
3.  **åˆ›å»ºæ©ç **ï¼šæ‰¾åˆ°ç¬¬ä¸€ä¸ªç´¯ç§¯æ¦‚ç‡å¤§äº `p` çš„ä½ç½®ï¼Œå¹¶å°†å…¶ä¹‹åçš„æ‰€æœ‰ token æ©è”½ï¼ˆæ¦‚ç‡ç½®é›¶ï¼‰ã€‚
4.  **é‡æ–°å½’ä¸€åŒ–**ï¼šå¯¹å‰©ä½™ token çš„æ¦‚ç‡è¿›è¡Œé‡æ–°å½’ä¸€åŒ–ï¼Œä½¿å…¶å’Œä¸º1ã€‚
5.  **é‡‡æ ·**ï¼šä»æ–°çš„åˆ†å¸ƒä¸­è¿›è¡Œå¤šé¡¹å¼é‡‡æ ·ï¼Œå¾—åˆ°ä¸‹ä¸€ä¸ª tokenã€‚


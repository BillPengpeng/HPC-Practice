本文主要整理pytorch-lora的主要内容。

## 1 - Problems with fine-tuning

### **内容概况**

三张图片围绕**神经网络的工作机制**及其**模型微调（Fine-Tuning）** 展开，逐步从基本原理过渡到实践中的挑战：

1. **第一张图**：以图示化方式直观解释了**神经网络的基本工作原理**，包括前向传播、损失计算与反向传播过程。
2. **第二张图**：在神经网络结构基础上引入**微调（Fine-Tuning）** 的概念，说明其是通过新数据训练预训练模型以适应特定任务的方法。
3. **第三张图**：重点列出**微调在实际应用中面临的三个主要问题**，涉及计算成本、存储开销与多模型切换的负担。

---

### **要点总结**

#### **1. 神经网络的基本工作流程（图1）**
- **结构**：包含输入层、隐藏层（可有多层）和输出层，神经元通过权重连接。
- **过程**：
  - **前向传播**：数据从输入层经隐藏层流向输出层，产生预测结果。
  - **损失计算**：比较预测输出与目标值，通过损失函数量化误差。
  - **反向传播**：将损失误差反向传递回网络，逐层调整权重以最小化误差。

#### **2. 微调（Fine-Tuning）的定义与作用（图2）**
- **本质**：在**预训练模型**（如已在大规模数据上训练好的网络）基础上，使用**特定任务的新数据**进行进一步训练。
- **目的**：提升模型在**该特定任务**上的性能，使其更适应新领域或细分需求。
- **举例**：对已掌握多种编程语言的LLM进行微调，使其专门适用于某种新的SQL方言。

#### **3. 微调面临的主要问题（图3）**
- **计算成本高昂**：微调需更新整个网络的参数，对于GPT等大型语言模型，普通用户难以承担训练所需算力。
- **存储开销大**：
  - 每个微调检查点需保存整个模型的权重，占用大量磁盘空间。
  - 若同时保存优化器状态（通常需要），存储压力进一步增大。
- **多模型切换效率低**：
  - 当存在多个针对不同任务微调的模型（如分别用于SQL和JavaScript代码辅助）时，切换需重新加载全部权重，过程缓慢且资源消耗大。

## 2.0 - Introducing LoRA

### **内容概况**

两张图片共同介绍了 **LoRA（Low-Rank Adaptation，低秩自适应）** 这一高效微调大模型的技术。其核心逻辑如下：

- **图1**：通过图示和公式展示 **LoRA 的基本原理与结构**，说明它是如何在不更新原始预训练权重的情况下，通过训练两个小型低秩矩阵来实现模型微调的。
- **图2**：列举了使用 LoRA 进行微调所带来的 **四项主要优势**，从参数效率、存储成本、计算速度和模型切换便捷性等方面进行了量化说明。

---

### **核心要点总结**

#### **1. LoRA 的基本工作原理（图1）**
- **核心思想**：**冻结**庞大的预训练权重矩阵 **`W`**（图中蓝色部分），不直接更新它。
- **关键结构**：在 `W` 的旁路引入 **两个可训练的小型低秩矩阵 `A` 和 `B`**（图中红色部分）。
    - 矩阵 `A` 的维度为 `R^(d x r)`
    - 矩阵 `B` 的维度为 `R^(r x k)`
    - 其中 **秩 `r` 远小于 `d` 和 `k`**（`r ≪ min(d, k)`）。
- **前向传播**：输入数据不仅通过冻结的 `W`，也同时通过 `A` 和 `B`。路径表示为：`Output = Wx + BAx`。
- **训练与更新**：在反向传播时，**仅计算并更新小矩阵 `A` 和 `B` 的梯度**（图中绿色路径），预训练权重 `W` 始终保持不变。

#### **2. LoRA 的主要优势（图2）**
1. **训练和存储的参数大幅减少**：
    - 举例：当 `d=1000`, `k=5000` 时，原矩阵 `W` 有 5,000,000 个参数。
    - 若取 `r=5`，则 `A` 和 `B` 的总参数量仅为 `(d×r)+(r×k) = 5000 + 25000 = 30000`，**不到原参数的 1%**。

2. **存储需求显著降低**：
    - 由于只需保存微调后的小型 `A` 和 `B` 矩阵（以及原始的 `W`），而非整个更新后的大模型，极大地节省了磁盘空间。

3. **反向传播更快**：
    - 训练时只需为少量参数（`A` 和 `B`）计算梯度，无需评估绝大部分（`W`）参数的梯度，从而加速了训练过程。

4. **多任务间切换极其便捷**：
    - 针对不同任务（例如SQL生成和JavaScript代码生成）微调出的不同模型，**只需加载对应任务的 `A` 和 `B` 矩阵**，而无需反复加载或切换整个庞大的 `W` 矩阵，切换速度极快且资源消耗低。

---

### **总结**

LoRA 通过 **“冻结大权重矩阵 + 训练小型低秩适配器”** 的巧妙设计，**完美回应了传统全参数微调所面临的算力、存储和部署难题**（即您在之前问题中提到的三个问题）。它使在有限资源下高效定制和部署大型语言模型成为可能，是目前大模型高效微调领域的一项关键且流行的技术。

## 2.1 - Why does it work?

### **内容理解**

这部分内容从**理论层面**论证了LoRA的可行性。它并非直接描述LoRA的操作步骤，而是回答了 **“为什么可以对大模型的权重更新进行低秩近似？”** 这一根本问题。

核心逻辑链条是：
1.  **研究发现**：预训练大模型本身就存在“信息冗余”（低内在维度）。
2.  **理论假设**：因此，模型适应新任务时所需的“更新量”也应该具有简单结构（低内在秩）。
3.  **方法实现**：基于此假设，我们可以用两个小矩阵（低秩分解）来高效表示这个更新量。

---

### **要点总结**

#### **1. 理论源头与研究启发**
*   **关键引用**：受 **Aghajanyan等人（2020）** 研究的启发。
*   **核心发现**：预训练语言模型拥有**低的“内在维度”**。这意味着即使将模型参数**随机投影**到一个更小的子空间，它仍然能有效地学习新任务。
*   **启示**：这表明大模型中存在大量的**参数冗余**，并非所有参数都是独一无二的信息载体。

#### **2. LoRA的核心理论假设**
*   基于上述研究，作者提出了核心假说：在模型适应（微调）特定任务时，**权重的更新矩阵 `ΔW` 也具有一个低的“内在秩”**。
*   **“内在秩”远小于权重矩阵的实际维度**，即 `r << min(d, k)`。这意味着巨大的更新矩阵 `ΔW` 所包含的“有效信息”或“变化模式”其实可以用低维空间来描述。

#### **3. 方法实现：低秩分解**
*   对于一个预训练权重矩阵 `W₀ ∈ ℝ^(d×k)`，我们不直接更新它，而是将其更新约束为一个低秩分解的形式：
    `W₀ + ΔW = W₀ + BA`
    其中，
    *   `B ∈ ℝ^(d×r)`, `A ∈ ℝ^(r×k)`
    *   `r` 是远小于 `d` 和 `k` 的秩。
*   **训练时**：`W₀` 被冻结，不更新；**只训练**小矩阵 `A` 和 `B`。
*   **前向传播**：如公式 (3) 所示，输入 `x` 同时通过冻结的 `W₀` 和可训练的 `BA`，结果相加：`h = W₀x + BAx`。

#### **4. 可行性解释：秩亏缺**
*   图片底部的文字提供了一个更直观的解释：预训练模型的权重矩阵 `W` 中包含许多**传递重复信息**的参数。
*   这些参数可以被其他参数的线性组合所表示，因此**移除它们不会降低模型性能**。这类矩阵被称为**秩亏缺**矩阵。
*   正因为原始权重矩阵本身就不是“满秩”的（即信息高度密集，无冗余），所以对其进行的、旨在适应特定任务的更新，自然也可以用一种低秩（高效、无冗余）的形式来表达。

---

### **总结**

这张图片揭示了LoRA技术的**理论基石**：
它并非一个凭空想象的技巧，而是基于对预训练模型**本质属性**（低内在维度、参数冗余）的深刻洞察。通过假设“**任务适应所需的更新也是低秩的**”，从而科学地论证了使用**极小的可训练参数（A和B）** 来近似完整权重更新 `ΔW` 的合理性。这完美地解释了为什么LoRA在极大减少计算和存储开销的同时，依然能保持出色的微调效果。

## 3 - 奇异值分解（SVD）

这段代码演示了如何使用奇异值分解（SVD）对一个低秩矩阵进行因式分解，并展示了低秩近似如何减少参数数量，同时保持计算结果的近似性。这实质上是LoRA（低秩自适应）等大模型微调技术的核心数学原理。

### 1. 生成一个低秩矩阵
```python
d, k = 10, 10
W_rank = 2
W = torch.randn(d, W_rank) @ torch.randn(W_rank, k)
```
- **目的**：创建一个已知秩（rank）为2的矩阵 `W`（维度10x10）。
- **方法**：通过两个小矩阵（`d x W_rank` 和 `W_rank x k`）相乘来生成 `W`。由于 `W` 由两个秩为2的矩阵相乘得来，其最大秩就是2。
- **验证**：`print(f'Rank of W: {W_rank}')` 确认了矩阵 `W` 的秩为2。

### 2. 奇异值分解与低秩近似
```python
U, S, V = torch.svd(W)
U_r = U[:, :W_rank]
S_r = torch.diag(S[:W_rank])
V_r = V[:, :W_rank].t()
```
- **SVD分解**：`torch.svd(W)` 将矩阵 `W` 分解为三个部分：`U`、`S`（奇异值向量）和 `V`。分解满足 `W = U @ diag(S) @ V^T`。
- **低秩截断**：只保留前 `W_rank`（即2）个最大的奇异值及其对应的奇异向量。这相当于用最优的低秩矩阵来近似原始矩阵 `W`。
- **构建因子矩阵**：
  - `B = U_r @ S_r`：维度为 (10, 2)
  - `A = V_r`：维度为 (2, 10)
- **关键点**：分解后的乘积 `B @ A` 是一个秩为2的矩阵，它是对原始矩阵 `W` 的最佳低秩近似。

### 3. 计算结果对比与参数效率
```python
y = W @ x + bias
y_prime = (B @ A) @ x + bias
```
- **计算等效性**：由于 `W ≈ B @ A`，因此 `y` 和 `y_prime` 的输出结果非常接近。代码运行后会验证这一点。
- **参数效率对比**：
  - **原始矩阵 `W` 的参数数量**：`10 * 10 = 100`
  - **分解后矩阵 `B` 和 `A` 的总参数数量**：`(10 * 2) + (2 * 10) = 40`
- **核心优势**：通过低秩分解，参数数量减少了60%。在更大规模的模型（如大语言的权重矩阵可达4096x4096）中，这种压缩效应会极其显著，这正是LoRA等技术能极大减少可训练参数量的根本原因。秩 `r` 越小，参数压缩率越高。

### ▎总结与核心概念

| 代码中的概念 | 在LoRA技术中的对应与意义 |
| :--- | :--- |
| **低秩矩阵 W** | 对应大模型中一个庞大的**权重矩阵**（如Attention中的Q、K、V投影层）。 |
| **SVD分解 (B @ A)** | 体现了LoRA的**核心思想**：用一个低秩的"补丁"（`B @ A`）来模拟完整权重矩阵的更新量（`ΔW`）。 |
| **参数大量减少** | LoRA通过只训练微小的 `A` 和 `B` 矩阵（秩r通常为8或16），而**冻结原始权重W**，使得可训练参数量降至全参数微调的**1%以下**。 |
| **计算结果y ≈ y'** | 证明了低秩近似的**有效性**。LoRA的公式 `h = W₀x + (B A)x` 同样能在极小扰动下有效适应下游任务。 |

### 💡 与LoRA的细微差别

需要注意的是，这段代码是**对静态权重的分解**，而标准的LoRA用于**微调过程**，两者目标略有不同：
- **本代码**：演示了如何将一个**已有的、固定的**低秩矩阵 `W` 分解为 `B` 和 `A`，以压缩存储。
- **标准LoRA**：在微调开始前，原始权重 `W₀` 是固定的。LoRA**随机初始化**两个小矩阵 `A` 和 `B`（通常 `A` 用随机高斯分布，`B` 初始化为零），然后**通过训练来学习** `A` 和 `B`，使得合起来的低秩更新 `ΔW = B A` 能让模型适应新任务。训练初期 `ΔW=0`，确保模型从稳定的预训练状态开始学习。

## 4.0 - PyTorch LoRA（低秩自适应）微调实现

这段代码是一个精简但完整的 **PyTorch LoRA（低秩自适应）微调实现**。它通过 PyTorch 的参数化（parametrization）机制，将 LoRA 结构注入到线性层中。

### 1. **核心类：`LoRAParametrization`**

```python
class LoRAParametrization(nn.Module):
    def __init__(self, features_in, features_out, rank=10, alpha=1, device='cpu'):
        super().__init__()
        # LoRA矩阵初始化
        self.lora_A = nn.Parameter(torch.zeros((rank, features_out)).to(device))
        self.lora_B = nn.Parameter(torch.zeros((features_in, rank)).to(device))
        nn.init.normal_(self.lora_A, mean=0, std=1)
        self.scale = alpha / rank
        self.enabled = True
```

**实现细节：**
- **维度关系**：对于原始权重矩阵 `W₀ ∈ ℝ^(features_in × features_out)`：
  - `lora_A ∈ ℝ^(rank × features_out)` 对应论文中的 A
  - `lora_B ∈ ℝ^(features_in × rank)` 对应论文中的 B
- **初始化策略**（严格遵循论文4.1节）：
  - **A 用随机高斯初始化**：`nn.init.normal_(self.lora_A, mean=0, std=1)`
  - **B 用零初始化**：`torch.zeros(...)`
  - 这样 **ΔW = BA = 0** 在训练开始时，确保初始输出与原始模型一致
- **缩放因子** `scale = alpha / rank`：
  - 这是论文中的一个重要技巧，将 ΔWx 按 α/r 缩放
  - α 是超参数，当使用 Adam 优化器时，调整 α 大致相当于调整学习率
  - 这样做可以减少调整 r 时需要重新调整其他超参数的需求

```python
    def forward(self, original_weights):
        if self.enabled:
            # 计算 W + (B*A)*scale
            return original_weights + torch.matmul(self.lora_B, self.lora_A).view(original_weights.shape) * self.scale
        else:
            return original_weights
```

**前向传播逻辑：**
- 当启用 LoRA 时：`W_final = W₀ + BA × scale`
- 当禁用 LoRA 时：`W_final = W₀`（恢复原始模型）
- 这实现了论文公式(3)：`h = W₀x + ΔWx = W₀x + BAx`

### 2. **参数化注册函数**

```python
def linear_layer_parameterization(layer, device, rank=10, lora_alpha=1):
    features_in, features_out = layer.weight.shape
    return LoRAParametrization(features_in, features_out, rank=rank, alpha=lora_alpha, device=device)
```

**关键点：**
- 获取线性层的输入输出维度，创建对应的 LoRA 参数化
- **只对权重矩阵进行参数化，忽略偏置**（遵循论文4.2节）

### 3. **参数化注册与冻结机制**

```python
# 注册参数化到网络层
parametrize.register_parametrization(
    net.linear1, "weight", linear_layer_parameterization(net.linear1, device)
)

# 启用/禁用LoRA的开关
def enable_disable_lora(enabled=True):
    for layer in [net.linear1, net.linear2, net.linear3]:
        layer.parametrizations["weight"][0].enabled = enabled

# 冻结非LoRA参数
for name, param in net.named_parameters():
    if 'lora' not in name:
        param.requires_grad = False
```

**核心机制：**
1. **参数化注册**：通过 PyTorch 的 `register_parametrization` 将 LoRA 注入到权重参数
   - 这不会改变层的原始接口，但在前向传播时会应用 LoRA 变换
2. **动态开关**：可以通过 `enable_disable_lora()` 随时启用/禁用 LoRA 效果
   - 这在推理时特别有用：可以快速切换回原始模型
3. **参数冻结**：只训练 LoRA 参数（A 和 B），冻结原始模型的所有其他参数

### 4. **与论文理论的对应关系**

1. **低秩假设的实现**：代码中的 `rank` 参数控制低秩矩阵的秩 r
   - `r ≪ min(d, k)` 在代码中体现为 `rank=10`（通常远小于特征维度）

2. **训练流程**：
   - **冻结 W₀**：通过 `param.requires_grad = False` 实现
   - **只训练 A 和 B**：它们是唯一 `requires_grad=True` 的参数

3. **存储效率**：
   - 只需保存原始的 W₀ 和小的 A、B 矩阵
   - 切换任务时：加载不同的 A、B 矩阵，共享同一个 W₀

## 4.1 - parametrize.register_parametrization接口

`parametrize.register_parametrization` 是 PyTorch 中一个非常强大的工具，它允许你对神经网络的参数或缓冲区施加自定义的变换或约束。下面我将用一个清晰的表格总结其核心用法，然后通过具体示例带你深入了解。

### 🔧 接口核心用法一览

| 特性 | 说明 |
| :--- | :--- |
| **核心功能** | 为模块(`nn.Module`)的指定参数（如`weight`）或缓冲区注册一个自定义的参数化变换 |
| **基本语法** | `register_parametrization(module, tensor_name, parametrization)` |
| **参数说明** | - `module`: 目标模块（如一个`nn.Linear`层）<br>- `tensor_name`: 参数名的字符串（如`"weight"`）<br>- `parametrization`: 一个自定义的`nn.Module`对象，定义变换规则 |
| **工作方式** | 注册后，通过`module.weight`访问的不再是原始参数，而是`parametrization(原始参数)`的结果 |
| **访问原始参数** | 注册后可通过`module.parametrizations.weight.original`访问原始参数 |

### 💡 详解工作流程与示例

`register_parametrization`的核心思想是“劫持”对模型参数的访问。当你为一个层的参数（如`weight`）注册参数化后，每次访问这个参数时，PyTorch都会自动调用你定义的`parametrization`模块的`forward`方法，返回变换后的结果，而原始参数会被保留在`parametrizations`属性下。

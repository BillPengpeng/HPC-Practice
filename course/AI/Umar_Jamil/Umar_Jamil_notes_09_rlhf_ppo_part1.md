本文主要整理RLHF and PPO的主要内容。

## 1 - Topics & Prerequisites

**1. 核心学习主题 (Topics)：**
- **入门与背景**：课程始于对语言模型和AI对齐问题的简要介绍。
- **核心理论 - 强化学习**：这是课程的重点，详细展开为：
    - **基础框架**：回顾RL的基本设定（环境、智能体、奖励等）。
    - **与LM的结合**：探讨如何将RL框架应用于语言模型训练。
    - **关键组件**：重点学习奖励模型、轨迹数据、策略梯度优化。
    - **高级技巧**：涉及降低方差、优势估计、重要性采样、离策略学习等核心优化技术。
- **重点算法 - PPO**：作为RL在LM对齐中的主流算法，专门讲解其损失函数设计以及需要防范的“奖励黑客”问题。
- **实战演练**：通过代码走读将前述理论付诸实践，巩固学习效果。

**2. 必要先决条件 (Prerequisites)：**
- **数学与统计基础**：概率统计知识是理解模型不确定性和RL理论的基础。
- **深度学习基础**：需掌握梯度下降、损失函数等核心概念。
- **强化学习入门**：了解智能体、状态、环境、奖励等基本概念。
- **模型架构理解**：必须具备Transformer架构和语言模型（如GPT系列）的工作原理知识。

**总结而言**，该学习模块要求学习者已具备扎实的机器学习与深度学习基础，并引导其深入RL与语言模型交叉的前沿领域，最终目标是掌握使用PPO算法实现语言模型对齐的完整能力。

## 2 - AI alignment

### 内容概况
这张图片简洁清晰地阐述了 **AI对齐的核心概念及其在大型语言模型中的应用背景**。它首先说明了大型语言模型通过大规模数据预训练获得了广泛的“知识”，随后指出了将这样的通用模型转化为特定风格的助手（如ChatGPT）时面临的需求矛盾，并引出了AI对齐的目标。

### 要点总结

**1. LLM的通用能力来源：**
- 大型语言模型通过在海量数据上进行预训练获得能力。
- 训练数据范围极广，例如包括整个维基百科和数十亿网页。
- 这种训练赋予了模型丰富的知识，使其能够以合理的方式补全各种提示。

**2. 实际应用时的特定需求：**
- 当把LLM用作聊天助手时，需要其行为符合特定风格和约束。
- 图片中列举了三个具体的要求示例：
  - **不**使用攻击性语言。
  - **不**使用种族主义表达。
  - 以特定的风格回答问题。

**3. AI对齐的核心目标：**
- **定义**：AI对齐是一个使模型行为与人类期望行为保持一致的过程。
- **目的**：解决模型在通用能力（无所不知）与特定应用要求（安全、无害、风格化）之间的差距。即**引导或调整**一个已经拥有广泛知识的模型，使其输出和行为符合特定的、安全的、有益的准则。

## 3.0 - The RL setup

**1. 环境与设定：**
*   学习环境被简化为一个**网格世界**，猫在其中移动。
*   存在特殊单元格：**肉（目标，+100奖励）**、**浴缸（危险，-10奖励并结束回合）**、**扫帚（小惩罚，-1奖励）** 和**空单元格（中性，0奖励）**。
*   墙壁构成移动约束。

**2. 强化学习五大核心组件：**
*   **智能体（Agent）**：学习并做出决策的主体，即**猫**。
*   **状态（State）**：智能体在环境中的具体**位置（x, y坐标）**。
*   **行动（Action）**：每个时间步，智能体可采取的举动。此处是向**上、下、左、右**四个方向之一移动。若移动方向是墙壁，则停留在原地。
*   **奖励模型（Reward Model）**：环境对智能体行动的反馈信号，是学习的关键驱动力。具体规定如下：
    *   移动到空单元格：奖励 **0**。
    *   移动到扫帚：奖励 **-1**。
    *   移动到浴缸：奖励 **-10**，并且猫会“晕倒”，导致**本回合结束**，随后在初始位置重新开始。
    *   移动到肉：奖励 **+100**。
*   **策略（Policy）**：智能体在给定状态下选择行动的规则或概率分布，记为 `a_t ∼ π(·|s_t)`。策略是RL最终要学习的目标。

**3. 强化学习的终极目标：**
*   目标是找到一个**最优策略**，使得智能体（猫）在长期行动中获得的**预期累积回报（Expected Return）最大化**。在本例中，即让猫学会高效地找到肉，同时避免浴缸和扫帚。

## 3.1 - The RL setup: connection to language models

**1. 核心概念映射（RL → LM）：**
*   **智能体**：即**语言模型本身**。它是做出决策、生成文本的主体。
*   **状态**：当前的**提示词（输入标记序列）**。它代表了智能体所处的“环境”或上下文。
*   **动作**：在词汇表中选择**下一个输出标记（token）**。这是模型每一步要做的具体决策。
*   **策略**：同样指**语言模型本身**，更准确地说，是模型参数所定义的概率分布 `π(·|s_t)`。它决定了在给定当前提示词（状态）下，选择每一个可能的下一个词（动作）的概率。
*   **奖励模型**：这是一个**需要额外定义的函数或模型**，用于评估生成内容的质量。模型因生成“好回答”而获得正奖励，因生成“坏回答”而获得低奖励或无奖励。这是引导模型优化的“指挥棒”。

**2. 关键解读与意义：**
*   **序列决策过程**：将语言生成过程重新定义为RL中的**序贯决策问题**。模型每生成一个词，就是采取了一个动作，并进入一个新的状态（提示词+已生成的词），如此循环。
*   **策略与模型的同一性**：明确指出语言模型本身就是策略 `π`，这强调了**优化语言模型本质上就是优化策略**。RL的目标就是寻找能获得最高累积奖励的策略参数。
*   **奖励的核心地位**：幻灯片最后一句引导思考“如何定义奖励模型”，这指向了RL对齐技术（如RLHF）的**关键挑战与核心**——如何设计或训练出一个能准确反映人类偏好的奖励模型。

**3. 实例演示（右下角图例）：**
以提示词 **“Shanghai is a city in”**（状态）为例：
*   语言模型（策略）给出的动作概率分布可能是：生成 **“China”** 的概率为85%，**“Beijing”** 为10%，**“Cat”** 为2.5%等。
*   在RL框架下，模型选择一个词（如“China”）后，会从奖励模型获得一个**奖励信号**，这个信号将用于更新模型（策略），使其未来更倾向于生成高奖励的回应。

**总结来说**，这张图完成了**从通用RL框架到LLM具体应用的概念平移**，为理解如何使用强化学习技术（如PPO）来微调和对齐语言模型奠定了至关重要的理论基础。它清晰地表明：**用RL训练LM，就是让LM（智能体/策略）学会在对话的每一步（状态），选择下一个词（动作），以最大化从奖励模型中获得的累积评价（奖励）。**

## 4.0 - Reward model for language models

### 内容概况
1.  **第一张图（问题篇）**：标题为 **“Reward model for language models”**，重点指出**直接为回答赋予一个绝对、普适的奖励分数是极其困难的**，并通过表格示例说明了这种困境。
2.  **第二张图（方案篇）**：标题为 **“Reward model by comparison”**，承接上文，提出了一个**更实用且可行的替代方案**：通过收集人类对答案的**偏好比较**数据来间接训练奖励模型，并用表格展示了具体的数据形式。

### 要点总结

**第一张图核心要点：直接标注奖励的困境**
*   **核心挑战**：为语言模型的回答直接设计一个“普遍接受”的奖励值非常困难。因为对回答质量的评判往往是主观的、多维度的，很难用一个单一的、绝对的分数（如0.0-1.0）来精确且无争议地衡量。
*   **示例说明**：图中表格列出了三个问答对（如“上海在哪？”），其“奖励”栏均为“???”，直观地表明了为这些看似简单正确的回答分配一个具体分数本身就是一个难题。
*   **人类能力的启示**：图中文字指出，人类虽然很难对事物做出绝对评分，但**非常擅长进行比较和选择**（“good at comparing”）。这为后续的解决方案埋下了伏笔。

**第二张图核心要点：基于比较的解决方案**
*   **范式转换**：从 **“绝对评分”** 转向 **“相对比较”**。不再要求标注者给出具体分数，而是只需在**两个答案中选出更好的一个**。这大大降低了标注难度和主观不确定性。
*   **数据形式**：展示了新的数据集结构，包含`问题（Prompt）`、`答案1（Answer 1）`、`答案2（Answer 2）`和`被选中的答案（Chosen）`。示例中清晰地展示了高质量答案与低质量（或错误）答案的对比。
*   **技术路径**：使用这种**人类偏好比较数据集**，可以训练一个奖励模型。该模型的学习目标是：对于同一个问题，为其输出的**奖励分数能够反映人类的相对偏好**（即被选中的答案应获得比另一个答案更高的预测分数）。
*   **承上启下**：最后一句“Let‘s see how it‘s done”表明，这是后续具体技术（如**配对排名损失、Bradley-Terry模型**等）实现的起点。

**整体总结**
这两张图揭示了一个关键洞见：在复杂、主观的任务上，**获取可靠的人类反馈的最佳方式，往往不是直接评分，而是进行简单的偏好比较**。这种“**基于比较的奖励模型**”正是驱动**人类反馈强化学习（RLHF）** 成功运行的核心组件之一，它提供了一种将人类模糊的喜好转化为模型可优化信号的实用方法。

## 4.1 - Reward model architecture

**1. 架构总览：输入、处理与输出**
*   **输入**：将**问题（Prompt）和模型的回答（Answer）** 拼接成一个完整的令牌序列（TOK 01 - TOK 08）。
*   **核心处理**：该序列输入一个**Transformer层**。Transformer通过其注意力机制处理整个序列，为**每一个输入令牌生成一个对应的隐藏状态**（HS 01 - HS 08），该状态编码了该令牌及其之前所有上下文的信息。
*   **输出**：最终，**仅提取答案部分最后一个令牌的隐藏状态**（图中 HS 08），将其通过一个**仅有一个输出特征的线性层**，该层的输出即为预测的**奖励值**。

**2. 关键设计选择与技术要点**
*   **复用预训练模型能力**：奖励模型并非从头训练，而是以一个**预训练好的Transformer语言模型**（如GPT）作为基座。这利用了模型从海量数据中学到的强大语义理解能力。
*   **特征提取器 vs. 预测头**：
    *   Transformer层在这里充当**特征提取器**，将文本转换为富含语义的向量表示（隐藏状态）。
    *   最后的单输出线性层是一个全新的、轻量的**回归预测头**，其唯一任务是将语义向量映射到一个代表好坏的标量分数上。
*   **为何使用“最后一个令牌”的隐藏状态？**
    *   在Transformer架构中，一个序列中**最后一个令牌的隐藏状态**被认为聚合了整个输入序列的上下文信息。对于“问题-答案”对，它尤其能捕获完整答案的语义，因此是生成整体奖励分数的合适特征。

**3. 与整体目标的关系**
*   此架构是实现**基于人类偏好比较的奖励模型**的具体方式。在训练时，模型会接收成对的（问题，答案A，答案B）数据，并学习调整参数，使得被人类选中的答案（Chosen Answer）通过此流程计算出的奖励分数，**高于**未被选中的答案（Rejected Answer）的分数。
*   训练完成后，这个固化下来的模型就可以为任何新的“问题-答案”对生成一个可靠的奖励分数，从而为后续的强化学习优化提供稳定的反馈信号。

**总结**：这张图揭示了奖励模型的技术本质——它是一个**基于强大文本理解模型（Transformer）的、用于做回归预测的“打分器”**。它巧妙地将复杂的语义理解问题，转化为了一个相对简单的标量预测任务，是连接人类主观偏好与算法可优化目标的关键桥梁。

## 4.2 - Reward model loss

**1. 训练目标与数据基础：**
*   **目标**：训练一个神经网络（即奖励模型），使其能为语言模型生成的每个回应（Response）输出一个**数值奖励（Reward）**。
*   **数据**：使用之前介绍的**人类偏好比较数据集**。对于每个提示（`x`），数据集中包含一个“优胜”回答（`y_w`，即被选中的回答）和一个“落败”回答（`y_l`，即未被选中的回答）。

**2. 核心损失函数：**
*   损失函数定义为：**`Loss = -log σ(r(x, y_w) - r(x, y_l))`**
    *   `r(x, y)`：奖励模型对给定提示 `x` 和回答 `y` 预测的奖励分数。
    *   `σ`：Sigmoid函数，它将差值映射到(0,1)区间，代表“`y_w` 优于 `y_l`”的概率。
*   **函数的设计意图**：该损失函数直接优化**奖励分数的差值**，其目标是使优胜回答的奖励分数尽可能高于落败回答的奖励分数。

**3. 损失函数的工作原理（关键洞察）：**
*   **情况一（顺序正确）**：若模型预测 `r(x, y_w) > r(x, y_l)`，则 `σ(正数) > 0.5`，`-log(一个大于0.5的数)` 会得到一个**较小的负数（即较小的损失）**。这是模型期望的状态。
*   **情况二（顺序错误）**：若模型预测 `r(x, y_w) < r(x, y_l)`，则 `σ(负数) < 0.5`，`-log(一个小于0.5的正数)` 会得到一个**非常大的负数（即巨大的损失）**。模型将通过梯度下降极力避免这种情况。
*   **效果**：这种不对称的惩罚机制**强制**奖励模型学习为“优胜”回答赋予高奖励，为“落败”回答赋予低奖励，因为这是最小化整体损失的唯一途径。

**总结**：这张幻灯片揭示了奖励模型训练的**核心优化机制**。它通过一个精巧的损失函数，将**人类对答案的相对偏好**这一主观判断，转化为了一个**可微分、可优化**的机器学习目标。训练好的奖励模型，便能为后续的强化学习阶段提供稳定、可靠的评分信号，指导语言模型生成更符合人类偏好的内容。

## 4.3 - Reward model: practical implementation

**1. 获取两个回答的奖励分数**
```python
rewards_chosen = model(input_ids=inputs["input_ids_chosen"], ...)["logits"]
rewards_rejected = model(input_ids=inputs["input_ids_rejected"], ...)["logits"]
```
*   **`model`**：这就是图中描述的奖励模型（一个带线性头的语言模型）。
*   **`inputs["input_ids_chosen"]`** 和 **`inputs["input_ids_rejected"]`**：分别代表**优胜回答**和**落败回答**经过分词后的序列。
*   **`[“logits”]`**：这里`logits`指的就是模型**最后的线性层输出的单个标量值**，即**预测的奖励分数** `r(x, y)`。
*   **关键对应**：
    *   `rewards_chosen` 对应理论中的 **`r(x, y_w)`**
    *   `rewards_rejected` 对应理论中的 **`r(x, y_l)`**

**2. 核心损失计算**
```python
loss = -nn.functional.logsigmoid(rewards_chosen - rewards_rejected).mean()
```
这是整段代码的灵魂，它完美对应了理论公式：
*   **`rewards_chosen - rewards_rejected`**：计算两个奖励分数的差值。
*   **`nn.functional.logsigmoid(...)`**：这是一个高效的PyTorch函数，它**等价于先计算Sigmoid再计算Log**，即 `log(σ(...))`。这样做在数值上更稳定。
*   **`- ... .mean()`**：
    *   `-`：因为我们要最小化损失，而公式本身是 `-log σ(...)`。
    *   `.mean()`：因为模型通常以批次（batch）进行训练，这里计算的是**一个批次内所有样本损失的平均值**，作为最终的反传梯度依据。

**3. 带边际（Margin）的扩展（可选）**
```python
loss = -nn.functional.logsigmoid(rewards_chosen - rewards_rejected - inputs["margin"]).mean()
```
*   这是一个更高级的变体。`margin` 是一个正数，相当于要求`rewards_chosen`不仅要大于`rewards_rejected`，还要**至少超过一个安全边际**。这能使模型的偏好判断更加“坚定”，学习到的奖励分数差距更明显。

## 5.0 - Let’s talk about trajectories

### 内容概况
这张题为 **“Let‘s talk about trajectories...”** 的幻灯片，旨在为强化学习的数学框架奠定基础。它从最大化**期望回报**的总体目标出发，逐步引出**轨迹**的定义，并给出了计算轨迹概率和累积回报的完整公式。右侧的卡通网格图（小猫、扫把、浴缸、骨头）为抽象的数学概念提供了直观的实例背景。

### 要点总结

1.  **核心目标**：强化学习的终极目标是找到一个**最优策略 π***，使得智能体遵循该策略时获得的**期望总回报 J(π)** 最大化。

2.  **轨迹的定义**：轨迹 `τ` 是智能体与环境交互产生的完整序列，由**初始状态**和后续一系列的**（状态，动作）对**构成：`τ = (s₀, a₀, s₁, a₁, ...)`。它记录了一次完整的“经历”。

3.  **环境的随机性**：幻灯片引入了一个关键假设——**下一状态是随机的**（图中幽默地比喻为“小猫喝醉了”）。这意味着即使在同一状态采取相同动作，结果也可能不同，由状态转移概率 `P(sₜ₊₁ | sₜ, aₜ)` 决定。

4.  **轨迹的概率**：一条特定轨迹发生的概率，取决于**初始状态分布 ρ₀**、每一步的**状态转移概率 P** 和**策略 π** 选择动作的概率。其乘积公式 `P(τ|π)` 量化了该轨迹出现的可能性。

5.  **折扣奖励**：对于长期任务，通常使用**折扣因子 γ (0 ≤ γ ≤ 1)** 来计算累积回报 `R(τ)`。这体现了“即时奖励比远期奖励更有价值”的原则，并保证了无限序列的回报总和是有限的。

### 公式解释

幻灯片中出现的公式构成了RL理论的基础：

1.  **最优策略公式**
    $$ \pi^{*} = \arg\max_{\pi} J(\pi) $$
    *   **含义**：最优策略 `π*` 是那个能使**期望回报函数 J(π)** 取得最大值的策略。`arg max` 表示我们寻找的是策略本身，而非最大值。

2.  **期望回报公式**
    $$ J(\pi) = \int_{\tau} P(\tau|\pi)R(\tau) = \underset{\tau \sim \pi}{\mathrm{E}}[R(\tau)] $$
    *   **含义**：一个策略 `π` 的期望回报 `J(π)`，是**所有可能轨迹的回报按其发生概率的加权平均**。积分符号表示对全部轨迹空间求和，等号右边是更简洁的期望符号表示。

3.  **轨迹概率公式**
    $$ P(\tau|\pi) = \rho_{0}(s_{0}) \prod_{t=0}^{T-1} P(s_{t+1} | s_{t}, a_{t}) \pi(a_{t} | s_{t}) $$
    *   **含义**：该公式拆解了一条轨迹 `τ` 是如何一步步产生的：
        *   `ρ₀(s₀)`：起始于状态 `s₀` 的概率。
        *   `π(aₜ | sₜ)`：在状态 `sₜ` 下，策略 `π` 选择动作 `aₜ` 的概率。
        *   `P(sₜ₊₁ | sₜ, aₜ)`：在状态 `sₜ` 采取动作 `aₜ` 后，环境转移到状态 `sₜ₊₁` 的概率。
        *   `∏`（连乘符号）：表示从第一步到第T-1步，所有这些概率的乘积。

4.  **折扣回报公式**
    $$ R(\tau) = \sum_{t=0}^{\infty} \gamma^{t} r_{t} $$
    *   **含义**：一条轨迹的总回报是每一步即时奖励 `rₜ` 的折扣之和。`γᵗ` 是折扣因子，随着步数 `t` 增加，未来奖励的权重会指数级衰减（`γ=0` 只关注即时奖励，`γ≈1` 则近乎平等看待远期奖励）。

## 5.1 Trajectories in language models

**1. 核心目标与问题形式化：**
*   微调语言模型的核心目标被形式化为一个强化学习问题：**找到最优策略 `π*`，以最大化模型获得的期望奖励 `J(π)`**。
    *   公式为：`π* = arg max_π J(π)`

**2. 语言模型“轨迹”的明确定义：**
*   在语言模型语境下，**轨迹 `τ`** 被定义为由一系列**状态**和**动作**交替构成的序列：`τ = (s₀, a₀, s₁, a₁, …)`。
*   **关键映射**：
    *   **状态（`s_t`）**：当前的**提示词（Prompt）加上已生成的所有令牌**。它代表了生成到第 `t` 步时的完整上下文。
    *   **动作（`a_t`）**：模型从词汇表中选择的**下一个令牌（Token）**。

**3. 可视化示例（图片右侧流程图）：**
*   示例以问题 **“上海在哪里？”（Where is Shanghai?）** 开始。
*   生成回答 **“上海是一座位于中国东部的城市。”（Shanghai is a city located in Eastern China.）** 的整个过程，被分解为一个轨迹：
    1.  **初始状态（s₀）**：问题本身。
    2.  **动作（a₀）**：模型选择第一个词 **“Shanghai”**。
    3.  **新状态（s₁）**：问题 + “Shanghai”。
    4.  **动作（a₁）**：模型选择下一个词 **“is”**。
    5.  … 以此类推，直到生成完整句子。
*   这个流程图直观地展示了**一次完整的问答生成就是一个轨迹**，模型每步的选择（动作）都会改变其上下文（状态）。

## 6 - Policy Gradient Optimization

### 内容概况

这两张幻灯片构成了一个完整的逻辑单元，**从目标定义到难题揭示，最终通过数学推导给出解决方案**，系统性地介绍了强化学习中的核心算法思想——策略梯度。
1.  **第一张幻灯片（问题引入）**：明确了使用**随机梯度上升**优化策略参数以最大化期望回报的目标，但同时尖锐地指出了直接计算该梯度的**计算可行性难题**。
2.  **第二张幻灯片（推导与解决方案）**：核心部分。通过一系列精妙的数学变换，将理论上无法直接计算的梯度，推导成一个可以通过**采样有限轨迹来近似估计**的实用表达式，从而打通了理论到实践的路径。

### 要点总结

**1. 核心优化目标：**
*   将语言模型等智能体的决策规则视为一个由参数 `θ` 参数化的**策略 `π_θ`**。
*   优化的目标是调整参数 `θ`，以最大化智能体在执行该策略时获得的**期望总回报 `J(π_θ)`**。
*   由于目标是最大化，因此采用**随机梯度上升**而非梯度下降来更新参数：`θ_{k+1} = θ_k + α ∇_θ J(π_θ)`。

**2. 核心计算难题：**
*   策略梯度的定义 `∇_θ J(π_θ)` 涉及对**所有可能轨迹**的期望。在状态或动作空间巨大的实际问题（如语言生成）中，枚举所有轨迹在计算上是**不可行的（intractable）**。

**3. 关键理论突破（对数导数技巧）：**
*   通过**对数导数技巧（Log-derivative trick）** `∇_θ P(τ|θ) = P(τ|θ) ∇_θ log P(τ|θ)`，将梯度计算转换为一个与概率 `P(τ|θ)` 本身成正比的期望形式。
*   这一转换的妙处在于，它使得我们可以通过**按当前策略 `π_θ` 采样**得到的轨迹来估计梯度，而无需知道轨迹本身的绝对概率。

**4. 最终可计算的梯度公式：**
*   推导的最终成果是一个简化的梯度表达式。由于初始状态分布 `ρ_0` 和环境动态 `P(s_{t+1}|s_t, a_t)` 通常与策略参数 `θ` 无关，它们的梯度为零。
*   因此，梯度最终只与**策略本身的对数概率**有关：`∇_θ J(π_θ) = E [ Σ_t ∇_θ log π_θ(a_t|s_t) * R(τ) ]`。
*   这意味着，一条轨迹获得的**总回报 `R(τ)` 决定了该轨迹中每一步动作的“更新权重”**。高回报轨迹中的所有动作会被增强，低回报轨迹中的动作则会被抑制。

**5. 从理论到实践的桥梁（采样估计）：**
*   最终的期望形式可以直接通过**蒙特卡洛采样**进行近似。收集一个由 `N` 条轨迹组成的样本集 `D`，即可计算梯度估计值：
    `ĝ = (1/N) Σ_{τ∈D} Σ_t ∇_θ log π_θ(a_t|s_t) * R(τ)`
*   这个估计量 `ĝ` 是**无偏的**，且随着样本量增加，其估计会越来越准确。这解决了最初的计算可行性问题。

### 关键公式解释

1.  **优化更新规则**：
    $$θ_{k+1} = θ_k + α ∇_θ J(π_θ)|_{θ_k}$$
    *   **解释**：这是策略梯度算法的核心迭代步骤。`α` 是学习率，`∇_θ J(π_θ)` 是目标函数关于策略参数的梯度。通过沿梯度方向更新参数，期望回报会逐步增加。

2.  **策略梯度定理（推导结果）**：
    )$$∇_θ J(π_θ) = E_{τ∼π_θ} [ Σ_{t=0}^{T} ∇_θ log π_θ(a_t|s_t) * R(τ) ])$$
    *   **解释**：这是策略梯度最核心的公式。
    *   `∇_θ log π_θ(a_t|s_t)` 是**得分函数（Score Function）**，它指示了在当前策略下，为增加动作 `a_t` 在状态 `s_t` 下被选中的概率，参数 `θ` 应调整的方向。
    *   `R(τ)` 是**轨迹的总回报**，充当**标量权重**。它决定了上述调整方向的幅度：如果整个轨迹回报高，则大幅增强该轨迹中的每一步决策；如果回报低，则减弱。

3.  **梯度估计量（Practical Estimator）**：
    $$ĝ = (1/|D|) Σ_{τ∈D} Σ_{t=0}^{T} ∇_θ log π_θ(a_t|s_t) * R(τ)$$
    *   **解释**：这是算法实际使用的公式。通过运行当前策略 `π_θ` 收集一批轨迹数据 `D`，然后计算这个样本均值，即可得到真实梯度的近似值 `ĝ`。该估计量是后续所有策略梯度算法（包括PPO）的基础。

**总结而言**，这两张幻灯片揭示了策略梯度方法的核心智慧：通过巧妙的数学变换，将**无法计算的全局优化问题**，转化为一个**可以通过智能体自身经验（采样轨迹）来局部引导并解决的问题**，为优化复杂的策略（如大型语言模型）提供了可行的数学工具。

## 7 - Recap of what we’ve done so far

**1. 核心脉络：从理论到实践**
幻灯片清晰地勾勒出从理论推导到算法实现的全过程：
*   **理论成果**：首先回顾了已获得的策略梯度表达式 `∇_θ J(π_θ)`。
*   **核心近似**：指出在实践中，我们通过**采样求均值**来近似这个梯度。
*   **算法化**：将这一思想具体化为一个五步迭代的 **REINFORCE算法**。

**2. REINFORCE算法五步实践流程**
这是一个完整的**策略优化循环**：
1.  **初始化策略**：创建一个**神经网络**，其输入是当前状态（如网格坐标或文本序列），输出是动作空间（如移动方向或下一个词）的概率分布。这个网络就是可参数化的策略 `π_θ`。
2.  **采样交互数据**：使用当前策略网络在环境中（如图中的网格世界）运行，**收集多条轨迹 `τ`** 及其对应的累积奖励 `R(τ)`。轨迹可以运行固定步数或在特定条件下终止（如“猫晕倒”）。
3.  **计算梯度估计**：利用收集到的轨迹样本集 `D`，计算梯度估计值 `ĝ`。其公式正是图片右侧第一个公式，它是对理论梯度的蒙特卡洛估计。
4.  **更新策略参数**：使用**随机梯度上升**方法，按 `θ_{k+1} = θ_k + α * ĝ` 的规则（图片右侧第二个公式）更新网络参数 `θ`。`α` 是学习率。
5.  **循环迭代**：返回第2步，用更新后的策略继续采样、计算、更新，直至策略性能收敛。

**3. 关键数学公式**
*   **梯度估计量**：`ĝ = (1/|D|) Σ_{τ∈D} Σ_t ∇_θ log π_θ(a_t|s_t) * R(τ)`
    *   这是算法的核心计算。它用有限样本的平均值，近似了策略梯度期望值。
*   **参数更新规则**：`θ_{k+1} = θ_k + α ĝ`
    *   这是标准的梯度上升更新步骤，目的是沿着提升期望回报的方向调整策略。

**总结而言**，这张幻灯片标志着学习从**理论分析阶段**进入了**算法实现阶段**。它将先前的策略梯度定理封装为一个清晰的、可编程的迭代流程——即经典的 **REINFORCE算法**（亦称蒙特卡洛策略梯度算法）。该算法通过“**运行策略 → 收集奖励 → 根据奖励加权更新动作概率**”的循环，直观地实现了“用成功经验强化成功动作”的学习理念。

## 8.0 - Generating trajectories for LMs

**1. 核心方法：复用数据，启动训练**
- 方法起点是**先前为训练奖励模型而构建的人类偏好数据集**。该数据集中包含了大量的问题（Prompts）。
- 我们**不再需要人类标注新的比较数据**，而是直接使用这些问题作为“种子”，提示（prompt）我们当前要优化的语言模型（即“策略”）来生成回答。

**2. 生成“轨迹”并计算奖励**
- **生成轨迹**：当模型基于一个问题生成一个完整回答时，这个**逐步生成词语（Token）的过程**，就形成了一条完整的**轨迹（Trajectory）**，其中包含了一系列的状态（不断增长的文本）和动作（选择的下一个词）。
- **计算奖励**：生成答案后，将其输入到**已经训练好的奖励模型**中，从而获得一个代表该答案质量的**标量奖励值**。这个奖励值将作为评估该条轨迹好坏的依据。

**3. 连接策略梯度算法**
- 幻灯片明确指出，获得轨迹及其奖励后，便可以**按照REINFORCE算法中所描述的近似策略梯度方法来训练模型**。
- 具体而言，高奖励的答案轨迹会使得模型在后续生成中更倾向于做出导致该答案的决策（词序列），反之则会抑制低奖励的决策路径。

**4. 引导性问题：指向实践关键**
- 最后的提问 **“OK, but how do we do this in practice?”** 至关重要。它点明了从概念到代码实现之间仍需解决的具体工程问题，例如：
    - 如何高效地批量生成和存储轨迹？
    - 如何组织计算图以进行有效的梯度估计和反向传播？
    - 如何与REINFORCE或其他更高级的算法（如PPO）的具体步骤对接？

**总结来说**，这张图阐明了一个**数据驱动的训练循环的起点**：利用现有问题提示模型生成回答 → 将这些生成过程视为轨迹 → 用奖励模型为轨迹评分 → 最终利用这些（轨迹，奖励）数据对来更新模型策略。它标志着从“如何定义奖励”和“理论梯度是什么”的阶段，进入了“如何实际操作并开始训练”的阶段。

## 8.1 - Calculating log probabilities of our policy (LM)

**1. 核心计算流程（自下而上）：**
*   **输入层**：将问题分词为 tokens (`Where`, `is`, `Shanghai`, `?`, `<EOS>`) 并输入模型。这些 tokens 构成了轨迹的初始状态 `s_t`。
*   **Transformer处理层**：模型（策略 `π_θ`）的 Transformer 层处理输入序列。通过**因果掩码（Causal Mask）** 确保每个位置的隐藏状态只依赖于当前位置及之前的 tokens，从而正确表示生成过程中的“当前状态”。
*   **Logits 生成**：每个位置的隐藏状态经过线性投影，生成对应位置的 **logits**（未归一化的分数），代表了词汇表中所有候选词作为“下一个词”（即动作 `a_t`）的原始得分。
*   **对数概率计算**：对每个位置的 logits 应用 **`log_softmax`** 函数，得到该位置上所有可能动作（下一个词）的**对数概率分布**。这是由策略 `π_θ` 定义的概率分布 `π_θ(·|s_t)` 的数学实现。
*   **动作概率提取**：根据**实际生成的响应 tokens**（`Shanghai`, `is`, `in`, `China`），从对应的对数概率分布中，**选出这些特定动作（tokens）所对应的具体对数概率值**（如图中绿色的 `LOGPROB n` 所示）。这些值就是策略梯度公式中需要的 `log π_θ(a_t|s_t)`。

**2. 与策略梯度算法的连接：**
*   图片右上角再次给出了策略梯度估计公式：`ĝ = (1/|D|) Σ_{τ∈D} Σ_t ∇_θ log π_θ(a_t|s_t) * R(τ)`。
*   本图的核心内容——**计算 `log π_θ(a_t|s_t)`**——正是该公式中**唯一依赖于策略模型内部机制的部分**。得到这些值后，才能与轨迹的总奖励 `R(τ)` 结合，并通过反向传播计算梯度 `∇_θ`，从而更新模型参数 `θ`。

**3. 技术关键点：**
*   **因果建模**：使用因果掩码确保了模型在生成时只能“看到”已经生成的内容，这与强化学习中“根据当前状态选择动作”的设定完全一致。
*   **概率到梯度的桥梁**：`log_softmax` 的输出不仅提供了概率，其对数形式更是**可直接用于梯度计算**，因为 `∇_θ log π_θ(a_t|s_t)` 通常具有更简单的数学形式（如减去基线后的得分函数），这为高效的反向传播提供了便利。

## 8.2 - Calculating rewards for each trajectory

**1. 核心功能：为完整回答打分**
*   此图演示的，是在获得一条完整轨迹（例如一个完整的问答对）后，**奖励模型为其赋予一个总体质量分数**的过程。
*   图中的“Reward (t=0)”, “Reward (t=1)” 等标识容易引起误解。在此上下文中，它们**并非**指代生成过程中每一个时间步的即时奖励，而是**奖励模型内部Transformer层在处理输入序列时，每个位置（Token）对应的隐藏状态**。最终，**仅由最后一个位置的隐藏状态**通过顶部的线性层输出**一个标量奖励值**，即对整个回答的总评价 `R(τ)`。

**2. 与奖励模型架构的对应**
*   该过程完全对应之前介绍的奖励模型架构：**拼接提示与回答**作为输入 → 通过**Transformer编码器**获得上下文表示 → 取**序列末尾的隐藏状态** → 通过**单输出线性层**得到奖励分数。
*   图中特别注明线性层“只有一个输出特征”，强调了其作为回归模型输出单个标量的特性。

**3. 在策略梯度训练中的关键作用**
*   图顶部的策略梯度估计公式 `ĝ = ... Σ_t ∇_θ log π_θ(a_t|s_t) * R(τ)` 指明了此处计算的 `R(τ)` 的核心用途。
*   **`R(τ)` 是权重**：在REINFORCE等算法中，轨迹中**每一个动作的对数概率梯度** `∇_θ log π_θ(a_t|s_t)` 都要乘以**该条轨迹的总奖励** `R(τ)`。这意味着：
    *   如果整个回答获得高奖励 (`R(τ)` 为大正数)，则生成这个回答所涉及的所有**词元选择（动作）的概率都会按比例增强**。
    *   如果回答获得低奖励 (`R(τ)` 为负数或小正数)，则这些动作的概率会被抑制。
*   因此，**奖励模型扮演了“质量评判官”的角色**，其输出的 `R(τ)` 直接决定了策略模型（语言模型）参数更新的方向和幅度。

**总结来说**，这张图清晰地揭示了**奖励模型在RLHF训练循环中的实际工作方式**：它接收策略模型生成的完整回答，并为其输出一个决定训练方向的标量奖励。这个奖励与之前计算的`动作对数概率`相结合，共同驱动策略梯度更新，使语言模型逐步学会生成更高奖励、更符合人类偏好的内容。

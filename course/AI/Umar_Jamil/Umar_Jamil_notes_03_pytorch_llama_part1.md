本文主要整理LLaMA from scratch的主要内容。

## 1 - Topics and Prerequisites

### 内容概括
该幻灯片明确了学习**现代Transformer变体（以LLaMA为例）** 所需的两大板块：
1.  **左侧（先修知识）**：列出了理解后续内容所必需的数学与机器学习基础。
2.  **右侧（核心主题）**：系统性地介绍了从原始Transformer到LLaMA模型的关键架构改进点。
3.  **底部说明**：提示讲解中会对已熟悉的概念进行回顾，提供了灵活的学习路径。

整体目的在于，引导学习者从经典的Transformer模型过渡到当前高效、主流的LLaMA类模型架构。

### 要点总结

#### **（一）先修知识要点**
此为深入学习的基石，包含四个层次：
1.  **模型基础**：透彻理解Transformer的整体结构及核心的注意力机制工作原理。
2.  **流程认知**：了解Transformer模型的训练（学习参数）和推理（生成预测）过程。
3.  **数学工具**：掌握线性代数中的矩阵乘法和点积，这是实现模型计算的基础。
4.  **扩展知识（可选）**：理解复数及欧拉公式，这对理解某些高级位置编码（如RoPE）有帮助。

> 注：图片中两个重复的微分公式 `d/dx(e^x)=d/dx(cos x+sin x)` 似有笔误，正确应为欧拉公式 `e^(ix)=cos x+i sin x` 相关的导数形式，此部分不影响主体内容理解。

#### **（二）核心主题要点**
这是课程的重点，详解了七大架构改进技术，旨在提升模型的**效率、稳定性和性能**：
1.  **架构差异总览**：对比原始Transformer与LLaMA的整体设计差异，奠定比较分析的框架。
2.  **RMSNorm**：用**均方根归一化**替代LayerNorm，计算更简单，已成为LLaMA、Gemma等模型的标准配置。
3.  **旋转位置编码（Rotary Positional Embeddings）**：引入**RoPE**，将位置信息以旋转矩阵的形式融入注意力计算，能更好地建模相对位置关系，并支持可变长度。
4.  **KV缓存**：在推理时缓存注意力机制中的Key和Value向量，避免重复计算，**显著提升自回归生成的推理速度**。
5.  **多查询注意力（Multi-Query Attention）**：让多个注意力头**共享同一组Key和Value向量**，极大减少内存占用和计算量，是推理优化的关键技术。
6.  **分组多查询注意力（Grouped Multi-Query Attention）**：MQA的改进版，对Key和Value向量进行分组共享，在效率和模型表达能力之间取得更好平衡。
7.  **SwiGLU激活函数**：使用Swish门控线性单元替代传统的ReLU或GeLU，能增强模型的非线性表达能力，提升效果。

## 2 - Transformer vs LLaMA


### 核心差异总览
| 特性 | Transformer （论文原版） | LLaMA （现代高效Decoder） | 目的与优势 |
| :--- | :--- | :--- | :--- |
| **整体架构** | 完整的**编码器-解码器**结构，适用于机器翻译等序列到序列任务。 | **仅保留解码器**的架构，是当前自回归语言模型（如GPT系列）的标准。 | 专注于**生成式任务**，结构更简洁，训练目标统一。 |
| **归一化层** | 使用 **Layer Normalization**，对每个样本的特征进行归一化。 | 使用 **RMSNorm**，计算量更小，已成为LLaMA、Gemma等模型标配。 | **提升训练稳定性，加速收敛，降低计算开销。** |
| **注意力机制** | 标准的**多头注意力**，每个头有独立的Q、K、V投影。 | 采用 **分组查询注意力**，多个头**共享**一组Key和Value向量。 | **大幅减少推理时的内存占用和计算量，显著提升推理速度。** |
| **位置编码** | 使用**正弦/余弦**固定位置编码，直接加在输入词向量上。 | 使用**旋转位置编码**，将位置信息以旋转矩阵形式融入注意力计算。 | 能更好地建模**相对位置**关系，并支持**可变长度**外推。 |
| **前馈网络** | 使用带ReLU激活函数的两层线性层。 | 使用 **SwiGLU** 激活函数，替代ReLU。 | 增强模型的**非线性表达能力**，通常能带来效果提升。 |
| **推理优化** | 未在基础结构中特别优化。 | 架构设计天然支持 **KV缓存** 技术。 | 在生成文本时，可避免重复计算历史K、V，**极大加速自回归生成过程**。 |

---

### 详细结构流程对比

**🔹 Transformer （编码器-解码器）**
```
输入 → 词嵌入 → 位置编码（加性）→ 
[编码器块 × N] → 
[解码器块 × N] → 
线性层 → Softmax → 输出概率
```
*   **编码器块**：多头注意力 + Add & Norm (LayerNorm) + 前馈网络 (FFN) + Add & Norm。
*   **解码器块**：带掩码的多头注意力 + Add & Norm + 编码器-解码器注意力 + Add & Norm + FFN + Add & Norm。

**🔹 LLaMA （纯解码器）**
```
输入 → 词嵌入 → 
[解码器块 × N] → 
RMSNorm（最终层） → 线性层 → Softmax → 输出概率
```
*   **解码器块**：**RMSNorm** + **分组多头注意力**（使用RoPE）+ 残差连接 + **RMSNorm** + 前馈网络（使用SwiGLU）+ 残差连接。

---

### 总结
这张图生动地说明了LLaMA并非简单的“魔改”，而是一系列经过深思熟虑和实验验证的**系统性工程优化**。其架构演进的核心思想是：

1.  **精简与专注**：摒弃编码器-解码器结构，专注于自回归生成任务。
2.  **组件现代化**：用**RMSNorm、RoPE、分组查询注意力、SwiGLU**等更优组件，全面替换原始模块。
3.  **效率优先**：所有改进都服务于**训练更稳定、推理更快、在同等算力下扩展性更好**的目标。

因此，LLaMA的结构代表了当前大语言模型**Decoder架构的最佳实践**，是理解ChatGPT、Gemma、Mistral等众多流行模型的基础。

## 3 - RMSNorm

### 内容概况
这两张图片从**理论和实践**两个角度，共同介绍了 **Root Mean Square Normalization** 技术。
1.  **第一张图（技术论文）**：提供了RMSNorm的**学术定义、核心思想和数学公式**，解释了它是LayerNorm的一种简化变体。
2.  **第二张图（总结幻灯片）**：直接明了地列出了采用RMSNorm的**两个主要优势**：计算效率高和实际效果好。

两图结合，完整地回答了 **“RMSNorm是什么？”** 以及 **“为什么使用它？”** 这两个关键问题。

### 要点总结

#### **（一）RMSNorm的核心思想与定义**
1.  **目标**：旨在简化**LayerNorm**。论文假设LayerNorm的成功主要归功于其**重缩放不变性**，而非重新中心化不变性。
2.  **方法**：完全**移除了对输入求均值**的操作，仅根据**均方根统计量**来对输入求和进行规范化。
3.  **公式**：对于一个输入向量 $\mathbf{a}$，其RMSNorm计算为：
    $$
    \bar{a}_i = \frac{a_i}{\text{RMS}(\mathbf{a})} g_i, \quad \text{其中} \quad \text{RMS}(\mathbf{a}) = \sqrt{\frac{1}{n} \sum_{i=1}^{n} a_i^2}
    $$
    其中 $g_i$ 是一个可学习的缩放参数（类似于LayerNorm中的gamma）。
4.  **与LayerNorm的关系**：当输入总和的均值为零时，RMSNorm等同于LayerNorm。它通过**牺牲均值不变性**来换取更简单的计算。

#### **（二）使用RMSNorm的主要原因（优势）**
1.  **计算效率更高**：由于省去了计算均值和减去均值的步骤，RMSNorm比标准的LayerNorm**需要更少的计算量**，这在大规模模型训练中能带来显著的效率提升。
2.  **实践经验验证**：尽管公式更简单，但RMSNorm**在实践中被证明效果良好**。它已成为LLaMA、Gemma等许多现代大语言模型中的标准配置，稳定了训练并加速了收敛。

**总结来说**，RMSNorm是对经典LayerNorm的一种成功简化，它抓住了规范化中“重缩放”这一核心功能，以**更低的计算成本达到了同等的实践效果**，因此被广泛应用于追求效率的现代Transformer架构中。

## 4 - the difference between the absolute positional encodings and the relative ones

### 内容概况
这两张图片从**文字定义**和**图示对比**两个角度，清晰地阐述了Transformer模型中两种主要位置编码方式的区别。
*   **第一张图（文字说明）**：以问答形式，直接定义了**绝对位置编码**和**相对位置编码**的核心概念、处理方式和目的，并指明了相对位置编码的出处。
*   **第二张图（示意图）**：通过一个生动形象的卡通场景，直观展示了两种编码方式在**注意力机制**中的不同整合方式，并给出了关键的计算公式。

两张图结合起来，完美地解答了标题提出的问题：“绝对位置编码和相对位置编码有什么区别？”

### 要点总结

| 特性 | 绝对位置编码 | 相对位置编码 |
| :--- | :--- | :--- |
| **核心思想** | 为序列中**每个独立的词元**赋予一个表示其**绝对位置（如第1个、第2个）** 的向量。 | 为注意力机制中**每一对词元**创建一个表示它们**相对距离或方向**的向量。 |
| **处理单位** | **一次处理一个词元**。关注词元“自己在哪里”。 | **一次处理两个词元**。关注词元对“彼此相距多远”。 |
| **添加方式** | 直接**加到词元自身的嵌入向量**上，作为输入的一部分。 | 在计算**注意力分数时**，与Key（或Query）向量结合，直接影响两个词元的相关性计算。 |
| **类比** | 地图上的 **(经度, 纬度)**，每个地点有独一无二的坐标。 | 两个地点之间的**方向和距离**（如“A在B东边5公里”）。 |
| **数学形式 (来自图2)** | $e_{ij} = \frac{(x_iW^Q)(x_jW^K)^T}{\sqrt{d_z}}$ (标准注意力公式，位置信息隐含在$x_i, x_j$中) | $e_{ij} = \frac{x_iW^Q (x_jW^K + a_{ij}^K)^T}{\sqrt{d_z}}$ (其中 $a_{ij}^K$ 是表示相对位置的可学向量) |
| **图示解读 (图2)** | 注意力机制同时牵着两个**已经自带绝对位置**的词元。 | 注意力机制直接关注两个词元之间的**“距离”关系**。 |

#### **核心结论：**
1.  **信息类型不同**：绝对编码提供**静态的、全局的**位置信号；相对编码提供**动态的、上下文相关的**关系信号。
2.  **与注意力的整合深度不同**：绝对编码是**预处理**，相对编码是**过程参与**。相对编码更自然地与注意力机制“关注词元间关系”的本质相契合。
3.  **论文出处**：相对位置编码由Shaw等人在论文 **《Self-Attention with Relative Position Representations》** 中提出，这篇论文是Transformer架构的重要补充。

**总结来说**，相对位置编码通过让模型直接感知词元间的相对距离，提供了比绝对位置编码更灵活、更符合语言逻辑的位置信息，因此成为后续许多高效位置编码方法（如**RoPE旋转位置编码**）的思想基础。

## 5 - Rotary Position Embeddings

### 内容概况
1.  **提出与定义**：指明了RoPE的来源论文（《RoFormer》）及其作者。
2.  **核心动机与思想**：从注意力机制的内积计算出发，提出设计目标，并引入“旋转”这一核心思想。
3.  **数学推导与形式**：从二维复数形式推导到高维旋转矩阵的一般形式。
4.  **几何解释**：展示了旋转矩阵的几何意义，解释了“旋转”命名的由来。
5.  **工程实现**：给出了兼顾数学优雅与计算效率的实际实现方式。

### 要点总结

#### **1. 目标与动机**
*   **核心问题**：能否找到一种方法，使注意力机制中查询（q）和键（k）向量的**内积结果仅依赖于它们本身的语义和它们的相对位置（m-n）**，而不依赖于各自的绝对位置（m和n）？
*   **数学表述**：目标是设计函数 $f$，使得 $<f_q(x_m, m), f_k(x_n, n)> = g(x_m, x_n, m-n)$。

#### **2. 核心思想：用“旋转”编码位置**
*   **二维情形（直观理解）**：将词嵌入向量视为二维平面上的一个点。**绝对位置m和n的作用，就是将该点分别逆时针旋转 mθ 和 nθ 角度**。
*   **旋转的妙用**：两个向量在旋转后的内积，**只与它们初始的夹角和旋转的角度差有关**，恰好满足“内积只依赖于相对位置（m-n）”的设计目标。

#### **3. 数学形式**
*   **复数形式（优雅）**：
    *   位置编码函数定义为：$f_{\{q,k\}}(x, m) = (W_{\{q,k\}}x) e^{im\theta}$
    *   其内积为：$q_m^\top k_n = \text{Re}[(W_q x_m)(W_k x_n)^* e^{i(m-n)\theta}]$
*   **矩阵形式（几何实现）**：
    *   复数乘法 $e^{i m \theta}$ 在二维实数空间中等效于乘以一个**旋转矩阵**：
        $$
        R_{\theta, m} = \begin{pmatrix}
        \cos m\theta & -\sin m\theta \\
        \sin m\theta & \cos m\theta
        \end{pmatrix}
        $$
    *   因此，$f_{\{q,k\}}(x_m, m) = R_{\theta, m} W_{\{q,k\}} x_m$。

#### **4. 高维推广**
*   在真实的高维模型中，将向量维度两两分组，每一组应用一个不同的旋转矩阵。
*   **通用旋转矩阵** $R^d_{\Theta, m}$ 是一个**分块对角矩阵**，每个2x2块对应一个旋转角度 $\theta_i$，其中 $\Theta = \{\theta_i = 10000^{-2(i-1)/d}\}$ 是预先设定的参数。
*   应用到自注意力公式中，可推导出关键性质：$\boldsymbol{q}_{m}^{\top}\boldsymbol{k}_{n}=\boldsymbol{x}^{\top}\boldsymbol{W}_{q}R_{\Theta,n-m}^{d}\boldsymbol{W}_{k}\boldsymbol{x}_{n}$，完美实现了**相对位置编码**。

#### **5. 关键优势与高效计算**
*   **关键优势**：
    1.  **相对性**：内积天然依赖于相对位置差（m-n）。
    2.  **远程衰减性**：随着相对距离增大，旋转角度差增大，内积期望值会衰减，这符合自然语言中距离越远关联性越弱的先验。
    3.  **稳定性**：旋转矩阵是**正交矩阵**，模长不变，保证了数值稳定性。
*   **高效计算形式**：
    *   直接使用稀疏的旋转矩阵做乘法效率低。
    *   利用其结构，可通过**逐元素运算**高效实现：
        $$
        R_{\Theta,m}^{d}\boldsymbol{x} = 
        \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_d \end{pmatrix} \otimes 
        \begin{pmatrix} \cos m\theta_1 \\ \cos m\theta_1 \\ \vdots \\ \cos m\theta_{d/2} \\ \cos m\theta_{d/2} \end{pmatrix} + 
        \begin{pmatrix} -x_2 \\ x_1 \\ \vdots \\ -x_d \\ x_{d-1} \end{pmatrix} \otimes 
        \begin{pmatrix} \sin m\theta_1 \\ \sin m\theta_1 \\ \vdots \\ \sin m\theta_{d/2} \\ \sin m\theta_{d/2} \end{pmatrix}
        $$
    *   这就是在代码中实际使用的形式，计算量小且易于并行。

**总结来说，RoPE是一种将绝对位置信息以“旋转”方式融入注意力计算，从而在数学上等价地实现相对位置编码的优雅方法。它兼具理论上的优美和工程上的高效，已成为LLaMA、GPT-NeoX等众多主流大语言模型的标准配置。**

## 6 - Rotary Position Embeddings: practical considerations

### 内容概况
这张图片是技术讲解中的一张幻灯片，标题为 **“旋转位置嵌入：实际考量”** 。它聚焦于RoPE在工程实现中的两个关键设计细节，明确指出其在**应用对象**和**应用时机**上，与经典Transformer的位置编码方式有所不同。

### 要点总结
图片清晰地指出了两个核心的实践要点：

1.  **应用的对象：仅作用于查询和键**
    *   **具体做法**：旋转位置嵌入**只应用于**注意力机制中的**查询向量**和**键向量**。
    *   **不应用的对象**：**值向量**则不应用此嵌入。
    *   **潜在原因**：这是因为注意力权重（决定关注哪里）是通过查询（`q`）和键（`k`）的内积计算的。将位置信息融入`q`和`k`，就能让模型在计算相关性时感知词元的相对位置。而值向量（`v`）承载的是待聚合的**内容信息**，其表示通常被认为可以相对独立于精确的位置编码。

2.  **应用的时机：在线性变换之后**
    *   **RoPE的做法**：旋转位置嵌入是在输入向量 **`x` 已经与权重矩阵 `W` 相乘**（即经过线性投影得到 `q` 和 `k`）**之后**才应用的。
        *   公式表示为：`q_rotated = R(q), k_rotated = R(k)`，其中 `q = W_q * x`, `k = W_k * x`。
    *   **原始Transformer的做法**：与之对比，原始Transformer的（正弦）位置编码是**直接加到词嵌入向量上**，**早于**与 `W_q`/`W_k` 的乘法运算。
        *   公式表示为：`input_with_pe = Embedding(x) + PE(x)`，然后才计算 `q = W_q * input_with_pe`。

**总结来说**，这张图强调了RoPE的两个重要实现特征：**1）选择性作用于`q`和`k`；2）后置于线性投影层**。这些设计选择使其能够更直接、更有效地将相对位置信息注入到注意力得分的计算核心中，这是其高效性和有效性的关键所在。

## 7 - Multi-Query Attention

### 内容概况

1.  **标准批量多头注意力**：作为性能分析的基线，展示了其高计算量与内存占用。
2.  **带KV缓存的批量多头注意力**：引入了**KV缓存**这一关键推理优化，避免了重复计算，但揭示了在生成长序列时**内存访问可能成为新瓶颈**。
3.  **带KV缓存的多查询注意力**：在KV缓存的基础上，应用**MQA**技术，通过让所有注意力头共享键和值向量，**大幅减少了内存占用和访问量**，从而突破了前一种方法的性能瓶颈。

整个演进清晰地回答了“我们为何需要MQA”这一问题。

---

### 要点总结与对比

| 特性 | 1. 标准批量多头注意力 | 2. 带KV缓存的批量多头注意力 | **3. 带KV缓存的多查询注意力** |
| :--- | :--- | :--- | :--- |
| **核心特征** | 论文原始形式，每个头有独立的Q、K、V。 | 缓存历史K、V，每个解码步仅计算新token的Q、K、V。 | **在(2)的基础上，所有头共享同一份K、V。** |
| **算术操作数** | `O(b*n*d²)` | `O(b*n*d²)` | `O(b*n*d²)` |
| **总内存占用** | `O(b*n*d + b*h*n² + d²)` | `O(b*n²*d + n*d²)` | **`O(b*n*d + b*n²*k + n*d²)`** |
| **内存/操作数比** | `O(1/d + 1/(h*n))` | **`O(n/d + 1/b)`** | **`O(1/d + n/(d*h) + 1/b)`** |
| **主要瓶颈** | 计算开销巨大，不适合自回归生成。 | **当序列长度 `n` 接近模型维度 `d` 或批次 `b=1` 时，访存成为瓶颈。** | **极大缓解了访存瓶颈，特别是将昂贵的 `n/d` 项降低了 `h` 倍。** |
| **适用场景** | 模型训练，或同时处理全序列的任务。 | 自回归推理的**标准优化**。 | **追求极致推理速度**的场景（如在线聊天、长文本生成）。 |
| **质量影响** | 无（原始设计）。 | 无（逻辑等价）。 | **轻微下降**，但换取巨大速度提升。 |

**核心结论**：优化的核心脉络是**从优化计算转向优化内存访问**。MQA通过共享K、V，精准地减少了KV缓存的大小，从而在长序列生成（`n` 很大）时，将内存访问量降低约 `h` 倍，带来了显著的推理加速。

---

### 代码解释与演进

#### **1. 标准批量多头注意力 (`MultiheadAttentionBatched`)**
*   **场景**：处理**完整的**查询序列和键值序列。
*   **输入**：`X` (查询, `[b, n, d]`), `M` (键值, `[b, m, d]`)。这里 `n = m`，即处理等长序列。
*   **核心投影**：`P_q`, `P_k`, `P_v` 的形状都是 `[h, d, k]`，为每个头生成独立的Q、K、V。
*   **输出**：`Y` 形状为 `[b, n, d]`。

#### **2. 带KV缓存的批量多头注意力 (`MultiheadSelfAttentionIncremental`)**
*   **场景**：**自回归生成**的单个解码步。
*   **输入**：`X` (当前新token的查询, `[b, d]`), `M` (当前新token的键值, `[b, d]`)。
*   **核心优化**：
    *   `prev_K`, `prev_V`：缓存的**历史**K和V，形状为 `[b, h, m, k]`。
    *   `new_K`, `new_V`：通过 `concat` 将当前token的K、V拼接到缓存末尾。
*   **计算**：当前查询 `q` (`[b, h, k]`) 与整个缓存序列 `new_K` (`[b, h, m+1, k]`) 计算注意力。
*   **输出**：返回当前步的输出 `y` (`[b, d]`) 以及**更新后的KV缓存**供下一步使用。

#### **3. 带KV缓存的多查询注意力 (`MultiquerySelfAttentionIncremental`)**
*   **场景**：在(2)的基础上，应用MQA优化。
*   **核心改动**（代码中高亮部分）：
    *   **KV缓存维度**：`prev_K`, `prev_V` 形状变为 `[b, m, k]`，**移除了头维度 `h`**。
    *   **投影权重**：`P_k` 形状变为 `[d, k]`，`P_v` 形状变为 `[d, v]`（不再是 `[h, d, k]`）。这意味着所有头共用一套K和V的投影矩阵。
    *   **当前token投影**：
        *   K: `torch.einsum(“bd, dk -> bk”, M, P_k)` 得到 `[b, k]`，然后 `unsqueeze(1)` 为 `[b, 1, k]` 以拼接。
        *   V: 类似，得到 `[b, v]`。
*   **计算**：当前多头查询 `q` (`[b, h, k]`) 与**单头**的键序列 `K` (`[b, m+1, k]`) 计算注意力（`bhmk` 变为 `bhm`）。

**代码演进的核心**：从维护 `[b, h, n, k]` 的KV缓存，变为只需维护 `[b, n, k]` 的KV缓存。这使得在长序列生成时，需要从内存中加载的数据量减少了约 `h` 倍，从而大幅提升了计算核心的效率。

## 8 - Grouped Multi-Query Attention

### 内容概况
这张图片来自研究论文 **《GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints》**（作者来自Google Research），其核心目的是介绍一种名为**分组查询注意力**的注意力机制变体。该机制旨在解决Transformer模型在推理效率与模型质量之间的权衡问题，是多头注意力和多查询注意力之间的一个**高效折衷方案**。

### 要点总结

#### **1. 三种注意力机制的核心对比**

| 特性 | **多头注意力** | **分组查询注意力** | **多查询注意力** |
| :--- | :--- | :--- | :--- |
| **核心设计** | 每个注意力头都有自己独立的**查询、键、值**投影。 | 将**查询头分成若干组**，每组内的所有查询头**共享同一组键和值**。 | 所有注意力头**共享唯一的一组键和值**。 |
| **质量表现** | **高质量**：建模能力最强，能捕获最丰富的上下文关系。 | **良好的折衷**：质量接近多头注意力，损失很小。 | **存在质量损失**：表达能力受限，质量有所下降。 |
| **计算/推理速度** | **计算速度慢**：内存占用大，访存开销高，是推理瓶颈。 | **良好的折衷**：速度显著快于多头注意力。 | **计算速度快**：内存占用最小，访存效率最高，推理极快。 |
| **KV缓存大小** | `[b, h, n, k]` | `[b, g, n, k]` (其中 `g` 为分组数，`g < h`) | `[b, n, k]` |

#### **2. 分组查询注意力的核心价值**
*   **定位**：它并非一个全新的发明，而是多头注意力和多查询注意力两个极端之间的**工程化折衷**。
*   **优势**：在**几乎不牺牲模型质量**的前提下，**大幅减少了推理时的内存占用和访存量**，从而获得接近多查询注意力的推理速度。
*   **实现方式**：通过将查询头分组（例如，将32个头分为8组，每组4个查询头共享1组键值），在KV缓存大小上实现了`h/g`倍的减少（此例中为4倍）。

#### **3. 示意图解读（下图）**
示意图直观地展示了三种机制如何处理“值”、“键”、“查询”：
*   **多头（Multi-head）**：三组彩色柱子（Values, Keys, Queries）都是**多个独立的柱条**，代表三者都有独立的多份。
*   **分组查询（Grouped-query）**：**Values**和**Keys**的柱子是**按组分块的粗柱条**（数量少），而**Queries**的柱子仍是多个独立细柱条（数量多）。这形象地表示了“**键值按组共享，查询保持独立**”。
*   **多查询（Multi-query）**：**Values**和**Keys**都只有**单一柱条**，而**Queries**有多个独立柱条。这表示“**键值全局唯一，查询独立**”。

**总结来说**，分组查询注意力通过“**键值共享，查询分组**”的巧妙设计，在模型质量与推理效率之间找到了一个最佳平衡点。它既继承了多头注意力强大的表达能力，又获得了多查询注意力高效推理的大部分优势，因此被**LLaMA 2、Command R**等许多追求实用性的现代大语言模型所采用。

## 9 - SwiGLU Activation Function

### 内容概况

1.  **第一张图（理论论文）**：来自学术论文，定义了GLU（门控线性单元）系列函数，并重点对比了**FFN层从原始的两权重矩阵结构升级为三权重矩阵GLU结构**的过程，最终指明SwiGLU是其中的优选方案，并解释了保持参数恒定的关键设计。
2.  **第二张图（教学讲义）**：对比了原始Transformer与LLaMA的FFN公式，并提供了**完整的PyTorch代码实现**，清晰地展示了从公式到代码的映射。

两张图结合，完整回答了“**什么是SwiGLU？为什么用它？以及如何实现它？**”这三个核心问题。

### 要点总结

#### **1. 核心改进：从“线性+激活”到“门控机制”**
*   **原始Transformer FFN**：
    *   **结构**：`FFN(x) = max(0, xW₁ + b₁)W₂ + b₂`
    *   **流程**：线性变换 → ReLU激活 → 线性变换。**只有两个权重矩阵**（`W₁`, `W₂`）。
*   **SwiGLU FFN**：
    *   **结构**：`FFN_SwiGLU(x) = (Swish₁(xW) ⊗ xV) W₂`
    *   **流程**：引入**门控结构**，其中`Swish₁(xW)`作为“门”来控制`xV`的信息流，然后进行投影。**包含三个权重矩阵**（`W`, `V`, `W₂`）。

#### **2. SwiGLU的优势与选择**
*   **何为Swish/SiLU**：`Swish`函数定义为 `x * sigmoid(βx)`。当`β=1`时，它被称为**SiLU**函数，这是LLaMA等模型实际使用的形式。
*   **优势**：相较于ReLU，Swish（SiLU）函数是**光滑、非单调**的，提供了更丰富的非线性表达能力，在实践中（尤其是在大规模语言模型上）通常能带来更好的性能。

#### **3. 关键设计：保持参数量与计算量恒定**
*   **问题**：三矩阵结构的参数量（`dim * hidden_dim * 3`）比两矩阵结构（`dim * hidden_dim * 2`）多出50%。
*   **解决方案**：为了进行公平比较并控制计算成本，**将中间隐藏层的维度`hidden_dim`缩小为原来的`2/3`**。
    *   **公式依据**：`(2/3) * hidden_dim * 3 = 2 * hidden_dim`，因此总参数量与原始两矩阵结构相当。

#### **4. 代码实现（对应第二张图代码）**
*   **初始化**：`hidden_dim`会根据`ffn_dim_multiplier`（通常为`2/3`）和`multiple_of`（为了硬件对齐）进行调整。
*   **三个权重矩阵**：
    *   `self.w1`：对应公式中的 `W`，将输入`x`投影到`hidden_dim`。
    *   `self.w3`：对应公式中的 `V`，同样将输入`x`投影到`hidden_dim`。
    *   `self.w2`：对应公式中的 `W₂`，将门控结果投影回原始维度`dim`。
*   **前向传播**：
    *   `return self.w2(F.silu(self.w1(x)) * self.w3(x))`
    *   这正是 `FFN_SwiGLU(x) = (SiLU(xW) ⊗ xV) W₂` 的精确代码实现。

**总结来说**，SwiGLU 是通过引入**门控机制**和**更优的Swish激活函数**来增强Transformer FFN层表达能力的一项有效改进。其**三权重矩阵结构**虽然增加了模型宽度方向的灵活性，但通过**按比例缩减隐藏层维度**巧妙地维持了总参数量和计算量的恒定，使其成为LLaMA、Gemma等众多先进大语言模型中的标准配置。
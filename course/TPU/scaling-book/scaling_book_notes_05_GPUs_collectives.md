本文主要整理Part 12 GPUs (How to Think About GPUs) 的主要内容。

## 9. How Do Collectives Work on GPUs?

1.  **通用性与灵活性**：GPU支持所有主流的集合通信原语（ReduceScatters, AllGathers, AllReduces, AllToAlls），这使其在分布式计算中具备与TPU同等的能力。
2.  **分层实现机制**：GPU集合通信的一个关键特性在于其**实现方式分为两个层级**：
    *   **节点内**：通过**NVLink**高速互联实现，延迟极低，带宽极高。
    *   **跨节点**：通过**InfiniBand**等网络实现，性能取决于网络拓扑和带宽。
3.  **软件库支持**：集合通信功能由NVIDIA提供的**NVSHMEM**和**NCCL**两大软件库封装实现。其中，**NCCL** 是一个重要的开源项目，允许社区贡献和不同实现，其具体算法会根据延迟需求和网络拓扑进行优化。

## 10.0 Intra-node collectives

## 10.1 AllGather or ReduceScatter

1.  **核心算法：带宽最优的环形算法**
    *   在GPU节点内执行AllGather（全收集）或ReduceScatter（规约散播）操作时，最优策略是使用**环形算法**。
    *   此算法将节点内的N个GPU逻辑上连接成一个环，并将待通信的数据平均分成N个块。每个GPU依次向邻居发送自己拥有的数据块，并接收来自另一个邻居的数据块，经过N-1次跳转后，所有GPU都拥有全部数据（AllGather）或完成局部规约并散播结果（ReduceScatter）。

2.  **性能与延迟的权衡**
    *   **环形算法**：优点是能**持续占满所有GPU的出口带宽**，实现带宽最优。缺点是延迟与GPU数量N成正比，为N-1跳。
    *   **树形算法**：针对**小数据量**的场景，可采用树形归约（如两两配对），将跳数减少到log₂(N)，从而**优化延迟**。但总通信量（字节数）不变，因此总成本与环形算法相同。

3.  **关键性能指标（Takeaway）**
    *   **单操作成本**：在一个节点内，对大小为B字节的数据进行AllGather或ReduceScatter的**通信时间成本约为 B / W_egress**。其中W_egress是单个GPU的出口带宽。
    *   **硬件实例**：在NVIDIA H100上（450 GB/s），理论时间约为 B / 450e9 秒；在B200上（900 GB/s），时间约为 B / 900e9 秒。
    *   **AllReduce成本**：AllReduce操作可视为一个ReduceScatter followed by an AllGather，因此其通信成本通常是单个操作的**两倍**，除非网络支持硬件加速的网内归约。


3.  **单跳时间成本：**
    $$T_{hop} = bytes / (N * GPU_{bandwidth})$$
    *   $bytes$： 需要通信的总数据大小（字节）。
    *   $N$： 节点内的GPU总数。
    *   $GPU_{bandwidth}$： 单个GPU的出口带宽（如H100为450 GB/s）。
    *   在环形算法中，总数据被分成N块，每个GPU在每个跳步中只发送一个数据块。因此，每个跳步中每个GPU的实际数据发送量是 $bytes/N$。而发送这个数据块的时间，就是数据量除以链路带宽，即 $(bytes/N) / GPU_{bandwidth}$。

4.  **总通信时间成本：**
    $$T_{comms} = (bytes * (N-1)) / (N * GPU_{bandwidth})$$
    * 整个操作需要经历N-1个跳步才能完成。因此，总时间就是单跳时间乘以跳步数：$T_{hop} * (N-1)$。代入$T_{hop}$的公式即得到上述结果。

5.  **渐进成本（当N较大时）：**
    $$T_{comms} → bytes / GPU_{bandwidth}$$
    * 当GPU数量N较大时，公式中的 $(N-1)/N$ 项趋近于1。因此，总通信时间近似等于总数据量 $bytes$ 除以单个GPU的出口带宽 $GPU_{bandwidth}$。**这意味着，对于大规模节点，通信效率极高，时间成本几乎与GPU数量无关，仅取决于总数据量和单GPU的带宽。** 这也是环形算法被称为“带宽最优”的原因。


## 10.2 Pop Quiz 1 [AllGather time]

Using an 8xH100 node with 450 GB/s full-duplex bandwidth, how long does AllGather(bf16[BX, F]) take? Let B=1024, F=16,384

- We have a total of 2⋅B⋅F2⋅B⋅F bytes, with 450e9 unidirectional bandwidth. This would take roughly Tcomms=(2⋅B⋅F)/450e9, or more precisely **(2⋅B⋅F⋅(8−1))/(8⋅450e9)**. Using the provided values, this gives us roughly (2⋅1024⋅16384)/450e9=75us, or more precisely, 65us.

## 10.3 AllToAlls

1. 基础AllToAll通信时间

$$ T_{\text{AllToAll comms}} = \frac{B \cdot (N-1)}{W \cdot N^{2}} \approx \frac{B}{W \cdot N} $$ 

*   **B**： 所有GPU要交换的**数据总量**。
*   **N**： GPU的数量。
*   **W**： 单个GPU的**出口带宽**。
*   **推导过程**：
    1.  **每个GPU负责的数据量**： 总数据B被平均分配给N个GPU，因此每个GPU最初持有 $B/N$ 字节。
    2.  **每次发送的数据量**： 在AllToAll中，每个GPU需要将自己持有的 $B/N$ 字节数据，再平均发送给其他N-1个GPU。因此，每次发送给一个特定GPU的数据量是 $(B/N) / N = B/N²$ 字节。AllToAll（全交换）是一种集合通信操作，其中每个参与者都同时向所有其他参与者发送数据，也从所有其他参与者那里接收数据。
    3.  **总通信量**： 每个GPU需要执行N-1次发送，总发送量为 $(B/N²) * (N-1) = B*(N-1)/N²$ 字节。
    4.  **总时间**： 总通信时间 = 总数据量 / 带宽，即得到公式 $T = [B*(N-1)/N²] / W$。
*   **近似**： 当N较大时，$(N-1)/N ≈ 1$，因此公式可简化为 $T ≈ B/(W*N)$。这意味着**通信时间随GPU数量N的增加而线性减少**，体现了并行效率。

2. 与TPU的对比
**表述：** GPU成本为 $B/(W*N)$，TPU成本为 $B/(4W)$。
*   在8-GPU节点上，GPU的通信时间约为 $B/(W*8)$。
*   图片指出TPU的典型成本为 $B/(4W)$。
*   因此，**GPU的理论速度是TPU的** $(B/(4W)) / (B/(8W)) = 2$ **倍**。

3. MoE模型中的稀疏AllToAll
$$ T \approx \min(k/N, 1) \cdot \frac{B}{W \cdot N} $$

$$ T \approx \frac{N-1}{N} \cdot \min(k/N, 1) \cdot \frac{B}{W \cdot N} $$

*   **k**： 在输出维度上，**非零分片的数量**（远小于总分片数N）。
*   **核心思想**： 由于输出是稀疏的（只有k个分片非零），每个GPU无需向所有N-1个目标发送完整数据，只需向k个目标发送。这使**有效通信量降低了约 $k/N$ 倍**。
*   **(N-1)/N的物理意义**：它代表了潜在的发送方数量 (N-1)​ 与 总的可能路径占比 (N)​ 的比值

## 10.4 Pop Quiz 2 [AllToAll time]

Using an 8xH100 node with 450 GB/s unidirectional bandwidth, how long does AllToAllX->N(bf16[BX, N]) take? What if we know only 4 of 8 entries will be non-zero?

- From the above, we know that in the dense case, the cost is B⋅(N−1)/(W⋅N⋅N), or B/(WN). If we know only 1/2​ the entries will be non-padding, we can send **B⋅k/N/(W⋅N)=B/(2⋅W⋅N)**, roughly half the overall cost.

## 10.5 Empirical measurements

![Empirical measurements](https://jax-ml.github.io/scaling-book/assets/gpu/gpu-all-reduce-bw.png)

1.  **核心结论：理论与现实的差距**
    *   NVIDIA官方宣称的H100 NVLink带宽约为**450 GB/s**，但在实际的集合通信操作中，**极难超过370 GB/s**。
    *   这意味着所有基于理论峰值带宽的性能模型都需要进行**下调修正**（例如，将估算值调整为370 GB/s而非450 GB/s）。
    *   The blue curve is the empirical link bandwidth, calculated as **2∗bytes∗(N−1)/(N∗runtime)** from the empirical measurements.

2.  **性能依赖消息大小**
    *   要达到较高的带宽，需要非常大的消息（如图中所示的10GB）。对于分布式训练中常见的中等规模张量（如LLaMA-3 70B模型中的58MB张量），实际带宽会低得多（仅约150GB/s），因为通信启动的固定开销占比更高。

3.  **与TPU的对比**
    *   图片指出，TPU能够在**更小的消息大小**上达到其峰值带宽。这表明TPU的通信栈可能效率更高，或者其硬件设计对集合通信的优化更好，使得其在处理现实工作负载时更具优势。

4.  **对AI模型训练的意义**
    *   这个差距不是一个单纯的理论问题，它直接影响了分布式训练的效率。通信带宽是训练速度的关键瓶颈之一，实际带宽低于预期会导致训练时间延长。

## 10.6 In network reductions

![In network reductions](https://jax-ml.github.io/scaling-book/assets/gpu/sharp-algorithm.png)

SHARP是一种“网内计算”技术。传统上，数据需要在GPU之间来回传输以完成计算（如求和）。而SHARP允许网络交换机在转发数据的过程中直接完成这种计算，然后将最终结果一次性返回给所有GPU，从而大幅减少数据传输量。

### 左侧公式：无SHARP的AllReduce（传统环形算法）

$$ T_{\text{comms}} = 2 \times \frac{B \times (N-1)}{N \times W_{\text{link}}} $$

1.  **$ B / N $**: 在环形算法中，总数据量 $B$ 被平均分成 $N$ 个数据块（$N$ 为GPU数量）。每个GPU初始只负责一个数据块。
2.  **$ (N-1) $**: 为了完成AllReduce，每个数据块需要在环上传输 $N-1$ 次，才能到达所有GPU并完成规约（如求和）。
3.  **所以，$ B \times (N-1) / N $**: 这是**单个GPU在整个过程中需要发送的数据总量**。它先发送自己的数据块，然后帮忙转发其他GPU的数据块。
4.  **$ \times 2 $**: 这是关键！环形AllReduce分为两个阶段：
    *   **Reduce-Scatter阶段**：数据在环中流动并完成局部规约。
    *   **All-Gather阶段**：规约后的最终结果在环中广播，使每个GPU都获得完整结果。
    *   **每个阶段都需要 $N-1$ 步**，因此总通信量是单个阶段的两倍。
5.  **$ / W_{\text{link}} $**: 最后，总通信量除以单条链路的带宽 $W_link$，得到总通信时间。

**结论：** 无SHARP时，通信成本与数据量 $B$ 和GPU数量 $N$ 都成正比。图中橙色圆圈标出的 **“x2”** 突显了传统算法因需要两个阶段而导致的固有冗余。

### 右侧公式：使用SHARP的AllReduce (**有问题**)

$$ T_{\text{comms}} = \frac{B}{W_{\text{link}}} $$

1.  **Ingress（数据上传与聚合）**:
    *   每个GPU将自己的**全部数据（大小为 $B$）** 一次性发送到交换机。
    *   交换机在内部完成所有数据的归约计算（如求和），生成一个最终的聚合结果（大小仍为 $B$）。
    *   **此阶段耗时 ≈ $ B / W_{\text{link}} $**。

2.  **Broadcast（结果广播）**:
    *   交换机将最终聚合结果 $B$ 广播回所有 $N$ 个GPU。
    *   由于交换机的高效多播能力，这个阶段的时间可以忽略不计，或者与上传阶段并行重叠。

| 特性 | 无 SHARP | 有 SHARP |
| :--- | :--- | :--- |
| **通信模式** | GPU到GPU的多次接力（环形） | GPU到交换机再到GPU |
| **核心操作** | 数据在GPU间传输和计算 | 计算卸载到网络交换机 |
| **通信量** | $ 2 \times B \times (N-1)/N $ | $ B $（实际上传）+ $ B $（结果广播，但可优化） |
| **通信时间公式** | $ 2 \times \frac{B \times (N-1)}{N \times W_{\text{link}}} $ | $ \frac{B}{W_{\text{link}}} $ |
| **效率** | 较低，存在固有冗余 | 理论上最高可提升约一倍 |

**结论：** 使用SHARP后，通信时间**理论上**简化为仅与总数据量 $B$ 成正比，而与GPU数量 $N$ 无关。它消除了传统算法中的“x2”冗余，将通信成本**减半**。

![empirical measurements](https://jax-ml.github.io/scaling-book/assets/gpu/sharp-all-reduce-cost.png)

### **理论预期与实际表现的巨大差距**

**理论预期**：SHARP有望将AllReduce的通信时间成本从 $2 * B / W$ 降低到 $B / W$，即**性能提升接近100%（成本减半）**。

**实际表现**：实证测量显示，带宽仅提升约**30%**。有效集合带宽从约370GB/s提升至约480GB/s，远未达到理论上的翻倍效果。

###  **性能差距的核心原因**
在像LLaMA这样的大语言模型训练中，**纯粹的AllReduce操作相当罕见**。更常见的操作是AllGather和ReduceScatter的组合。SHARP对此组合的优化效果不如对纯AllReduce显著。

## 11.0 Cross-node collectives

1.  **核心问题：跨节点通信的复杂性**
    *   当集合通信操作（如规约）的范围超越单个节点时，其通信成本的计算变得更为复杂，不再像节点内通信那样直接。

2.  **核心解决方案：分层树形归约算法**
    *   图片提出了一种优化的通信策略，即采用**分层或树形结构**来组织计算。
    *   **工作流程**：归约操作从最底层开始，逐级向上进行：
        *   **第一步（节点内）**：首先在每个计算节点**内部**完成所有GPU之间的AllReduce操作。这一步充分利用了节点内的高速互联（如NVLink）。
        *   **第二步（叶层级）**：然后，每个节点将内部归约后的**最终结果**（而不是所有中间数据）发送到与之相连的**叶交换机**，在叶交换机层级进行节点间的归约。
        *   **第三步（脊层级）**：最后，在更顶层的**脊交换机**完成更大范围的归约。

3.  **关键优势：大幅减少通信数据量**
    *   这是最核心的洞见。通过先在节点内完成AllReduce，每个节点只需要向外部网络**发送一份最终的聚合数据（大小为 B 字节）**。
    *   相比之下，如果采用简单粗暴的方法（如每个GPU都直接参与跨节点通信），那么一个拥有N个GPU的节点就需要向外发送 **N * B 字节**的数据。
    *   因此，这种分层方法将节点级别的出口通信量**降低了N倍**，极大地缓解了网络带宽压力，这对于大规模分布式训练至关重要。

## 11.1 How costly is this? 

### 跨节点通信时间

$$T_{AG\ or\ RS\ comms} = bytes / W_{node\ egress} = bytes / 400e9$$

*   **$T_{AG\ or\ RS\ comms}$**： 执行AllGather或ReduceScatter操作所需的通信时间。
*   **$bytes$**： 需要通信的总数据量（字节）。例如，对所有GPU上一个特定张量进行AllGather，$bytes$即该张量的大小。
*   **$W_{node\ egress}$**： **节点的总出口带宽**。这是指整个节点（如一个包含8个GPU的服务器）通过其所有网络接口卡（NIC）所能达到的对外通信总带宽。图中以H100为例，其值为400GB/s（即 $400e9$ 字节/秒）。
*   **公式含义**： 该公式基于全二分带宽网络的理想特性，将复杂的分布式通信时间简化为一个简单的“数据量除以带宽”的计算。它表明，在无阻塞网络中，通信时间主要由数据量和节点出口带宽能力决定。

### 总操作时间

$$T_{total} = max(T_{comms\ at\ node}, T_{comms\ in\ scale-out network}) = max[ bytes / W_{GPU\ egress}, bytes / W_{node\ egress} ]$$

*   **$T_total$**： 整个集合通信操作的总时间。
*   **$T_{comms\ at\ node}$**： **节点内部的通信时间**。即节点内多个GPU之间完成数据交换所需的时间。$W_{GPU\ egress}$指单个GPU的出口带宽（如NVLink带宽）。
*   **$T_{comms\ in\ scale-out network}$**： **扩展到集群网络的通信时间**，即公式1计算的结果。
*   **公式含义**： 该公式提供了更全面的性能模型。尽管节点内通信通常极快（$W_{GPU\ egress}$ 可能远高于 $W_{node\ egress}$），但此模型强调了**跨节点网络带宽是分布式计算的关键瓶颈**。

## 11.2 You can see a more precise derivation here.

1.  **核心模型：分层并行环状归约**
    *   模型将复杂的跨节点网络通信，分解为在**节点（Node）、叶子（Leaf）、脊柱（Spine）** 等多个层级上独立且可重叠进行的环状归约操作。
    *   总通信时间由执行最慢的层级决定，即“木桶效应”。

2.  **精确的数学公式**
    *   图片给出了计算通信时间 $T$ 和可用带宽的精确公式。关键在于找出网络中各层的 $(D_i - 1) / (D_i * W_{link\_i})$ 最大值，其中 $D_i$ 是层级度数（子节点数），$W_{link\_i}$ 是链路带宽。

3.  **瓶颈分析实践**
    *   **节点层**: $D=8$, $W=450GB/s$ -> 带宽 = $(8 * 450) / (8-1) ≈ 514 GB/s$
    *   **叶子层**: $D=32$, $W=400GB/s$ -> 带宽 = $(32 * 400) / (32-1) ≈ 413 GB/s$
    *   **脊柱层**: $D=4$, $W=12.8TB/s$ -> 带宽 = $(4 * 12.8TB/s) / (4-1) ≈ 17.1 TB/s$
    *   **系统整体带宽** = $min(514GB/s, 413GB/s, 17.1TB/s) = 413 GB/s$

4.  **理论值与实际参考**
    *   图片指出，使用SHARP等技术时，实际带宽会略低于此理论值（约400GB/s）。但对于近似估算和分析，**413GB/s** 和节点内带宽 **450GB/s** 足够接近，可以作为有效的性能分析基准。

## 11.3 Other collectives

1.  **$AllToAll$ 操作的特殊性**：
    *   **非层级通信**：$AllToAll$要求每个GPU与所有其他GPU通信，这种“全互联”模式无法像$AllReduce$那样先在节点内聚合，因此**无法利用节点内的高带宽**。
    *   **性能瓶颈**：其跨节点通信成本公式为 $T ≈ B / (M * W_node egress)$，其中 $M$ 是节点数量。这意味着有效带宽远低于节点级带宽。例如，在2个节点间，有效带宽约为50GB/s，相比单节点内性能下降超过4倍。

2.  **$AllGather$/$ReduceScatter$ 的操作成本**：
    *   在具备**全二分带宽**的胖树网络中，这两种操作的成本可以简化为 $T ≈ B / W_node egress$。
    *   对于H100 SuperPod，$W_node egress$ 为 **400 GB/s**。因此，处理 $B$ 字节数据的通信时间约为 $B / 400e9$ 秒。

3.  **$AllReduce$ 的成本与SHARP技术**：
    *   $AllReduce$ 可视为一个$ReduceScatter$后跟一个$AllGather$，因此其通信成本通常是单一操作的**两倍**，即 $T ≈ 2B / W_node egress$。
    *   **SHARP** 技术通过在网络交换机内执行归约计算，可以**将$AllReduce$的通信成本减半**，使其接近 $T ≈ B / W_node egress$。

4.  **网络架构保障**：
    *   SuperPod采用**胖树拓扑**，旨在为任意两个节点对提供恒定带宽，这是上述性能模型成立的基础。

| 层级 | GPU 数量 | 分支度（子节点数） | 交换机带宽（全双工, TB/s） | 线缆带宽（全双工, TB/s） | **集合带宽（GB/s）** |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **节点** | 8 | 8 | 6.4 | 3.6 | **450** |
| **叶子** | 256 | 32 | 25.6 | 12.8 | **400** |
| **脊柱** | 1024 | 4 | 51.2 | 51.2 | **400** |

**表格说明：**
*   **集合带宽**：这是估算通信成本的关键指标，代表每个GPU或节点能够用于集合通信的有效出口带宽。
*   **瓶颈分析**：表格显示，超出节点级别后，集合带宽稳定在**400 GB/s**。这印证了要点总结中的结论：跨节点通信的瓶颈是节点的出口带宽（$W_node egress$）。


## 11.3 Cable Bandwidth & Collective Bandwidth

简单来说，**Cable Bandwidth（线缆带宽）是物理链路的理论总吞吐能力，而Collective Bandwidth（集合带宽）是运行特定算法（如环状AllReduce）时，单个GPU或节点能实际使用的有效带宽。** 后者是由前者通过一个效率公式计算得出的。

### 核心关系：总能力 vs. 单点有效能力

我们可以用一个公路系统的类比来理解：
*   **Cable Bandwidth（线缆带宽）**：就像是**一个城市所有道路的总车道数**。它衡量的是整个交通网络的最大车辆吞吐能力。
*   **Collective Bandwidth（集合带宽）**：就像是**在特定的交通规则（如环状算法）下，保证每条入口匝道都能保持通畅时，每条匝道的平均最大车流量**。它衡量的是单个入口的通行效率。

### 计算公式对应关系

第二张图片中的计算使用了环状算法的标准公式：
$集合带宽 = (Degree * W_{link\_i}) / (Degree - 1)$

这个公式正是连接 **Cable Bandwidth** 和 **Collective Bandwidth** 的桥梁。

让我们用表格中的数据来验证这个对应关系：

**1. Node层级（节点内）**

*   **表格中的Cable Bandwidth**：$3.6 TB/s$。这是节点内所有NVLink链路的总带宽。
*   **计算验证**：
    *   $Degree$ = 8（节点内有8个GPU）
    *   $W_{link\_i}$ = 450 GB/s（每个GPU的出口带宽）
    *   **Cable Bandwidth 应为** $W_{link\_i} * Degree$ = 450 GB/s * 8 = **3.6 TB/s**。这与表格第一项完全对应。
*   **推导出Collective Bandwidth**：
    *   根据公式：$集合带宽$ = (8 * 450 GB/s) / (8 - 1) ≈ 3600 / 7 ≈ **514 GB/s**
    *   **为什么表格里是450 GB/s？** 这里有一个细微, 这里的 **450 GB/s** 可以理解为是每个GPU的**点对点带宽**。在理想的全互联网络下，进行AllReduce时，有效带宽会接近这个点对点带宽。514 GB/s是环状算法下的理论计算值，而450 GB/s可能是一个更保守或基于不同模型（如全互联）的有效带宽估算值。两者是同一量级的概念，都源于3.6 TB/s的总线缆带宽。

**2. Leaf层级（叶子交换机）**

*   **表格中的Cable Bandwidth**：$12.8 TB/s$。这是一个SU内所有连接到Leaf交换机的线缆总带宽。
*   **计算验证**：
    *   $Degree$ = 32（一个SU有32个节点）
    *   $W_{link\_i}$ = 400 GB/s（每个节点到Leaf的带宽，即8条400Gbps InfiniBand线缆的聚合带宽）
    *   **Cable Bandwidth 应为** $W_{link\_i} * Degree$ = 400 GB/s * 32 = **12.8 TB/s**。完美对应。
*   **推导出Collective Bandwidth**：
    *   根据公式：$集合带宽$ = (32 * 400 GB/s) / (32 - 1) ≈ 12800 / 31 ≈ **413 GB/s**
    *   **为什么表格里是400 GB/s？** 同样，表格中的 **400 GB/s** 是一个近似和简化的值（413 GB/s的四舍五入），代表了每个节点在跨SU通信时能获得的实际有效带宽。它明确指出了性能瓶颈所在。

**3. Spine层级（脊柱交换机）**

*   **表格中的Cable Bandwidth**：$51.2 TB/s$。这是整个Spine层的总交换能力。
*   **计算验证**：
    *   $Degree$ = 4（连接了4个SU）
    *   $W_{link\_i}$ = 12.8 TB/s（每个SU到Spine的总带宽，即第二张图片中的计算结果）
    *   **Cable Bandwidth 应为** $W_{link\_i} * Degree$ = 12.8 TB/s * 4 = **51.2 TB/s**。完全对应。
*   **推导出Collective Bandwidth**：
    *   根据公式：$集合带宽$ = (4 * 12.8 TB/s) / (4 - 1) ≈ 51.2 / 3 ≈ **17.1 TB/s**
    *   **为什么表格里是400 GB/s？** 注意，Spine层的**集合带宽 400 GB/s** 并不是从17.1 TB/s计算来的。它是由**整个系统的瓶颈——Leaf层**决定的。因为数据必须先从Node到Leaf，而Leaf层只能提供约400 GB/s的带宽给每个节点，所以即使Spine层能力再强，整体的有效带宽也被限制在了 **400 GB/s**。这体现了系统的“木桶效应”。

### 总结对应关系

| 概念 | 含义 | 计算关系 | 举例（Leaf层） |
| :--- | :--- | :--- | :--- |
| **单链路带宽 ($W_{link\_i}$)** | 单个设备（GPU/节点）到交换机的带宽 | - | 400 GB/s（节点） |
| **度 ($Degree$)** | 一个父节点（交换机）连接的子节点数量 | - | 32 |
| **Cable Bandwidth** | **物理链路总能力** | $W_{link\_i} * Degree$ | 400 GB/s * 32 = **12.8 TB/s** |
| **Collective Bandwidth** | **运行算法后的单点有效带宽** | $(W_{link\_i} * Degree) / (Degree - 1)$ | 12.8 TB/s / 31 ≈ **413 GB/s** （表格简化为400 GB/s） |

**结论：** $Cable Bandwidth$ 是 $Collective Bandwidth$ 的**计算基础**。通过环状算法的效率公式，可以从物理总带宽推导出算法有效带宽。而整个系统的最终性能，由所有层级中$Collective Bandwidth$最小的那个瓶颈层级决定。

## 11.4 Reductions when array is sharded over a separate axis

### 要点总结

1.  **核心问题**：两张图片都致力于解决**多维分片数据集的集合通信成本建模**问题。当数据分布更复杂时，简单的“数据总量/带宽”模型不再适用，需要更精细的模型。

2.  **性能优化关键**：通过沿多个维度分片数据，可以**显著降低集合通信的绝对通信量**，从而提升性能。其效果类似于将一个大任务分解为许多可以并行处理的小任务。

3.  **TPU与GPU的差异**：
    *   **TPU**：处理此类操作相对简单，成本大约降为未分片版本的 $1/Y$。
    *   **GPU**：性能提升取决于**分片策略与硬件拓扑的匹配程度**。关键在于“内部”轴（Y轴）的分片是落在节点内部还是需要跨越节点网络。

4.  **分片策略与性能关系**：
    *   当Y轴分片规模（Y）**小于或等于节点内的GPU数（D_{node}）** 时，通信优化主要发生在节点内部，跨节点通信可能仍是瓶颈。
    *   当Y轴分片规模（Y）**大于节点内的GPU数（D_{node}）** 时，通信优化能惠及跨节点网络，从而带来与节点数量成正比的性能提升。

5.  **精确模型**：第二张图片的公式强调了性能受限于整个通信路径中**最慢的那个层级（由max_depth i 操作子体现）**，这是分布式系统中“木桶效应”的典型体现。

---

### 公式解释

#### **第一张图片的公式**

这些公式基于“Y轴是内部轴”的假设。

1.  **节点内通信时间**：
    $T_{comms\ at\ node} = (bytes / W_{GPU\ egress}) * (1 / min(Y, D_{node}))$
    *   **目的**：计算在单个节点内部，GPU之间完成数据归约的时间。
    *   **解释**：总数据量$bytes$被Y轴分片后，每个GPU需要处理的数据量减少为原来的$1/Y$。但通信收益最多只能达到节点内GPU数量$D_{node}$所能提供的并行度上限。因此，有效加速因子为 $1 / min(Y, D_{node})$。

2.  **跨节点网络通信时间**：
    $T_{comms\ in\ scale-out\ network} = (bytes / W_{node\ egress}) * (D_{node} / max(D_{node}, Y))$
    *   **目的**：计算数据在不同节点间传输的时间。
    *   **解释**：如果Y轴分片规模很大，超过了节点规模（Y > D_{node}），意味着归约任务被分散到多个节点上并行处理，那么每个节点需要发送到网络的数据量就会减少，因子为 $D_{node} / Y$。如果Y <= D_{node}，则优化效果仅限于节点内，跨节点数据量不变，因子为1。

3.  **总时间**：
    $T_{total} = max(T_{comms\ at\ node}, T_{comms\ in\ scale-out\ network})$
    *   **目的**：计算整个AllReduce操作的耗时。
    *   **解释**：集合通信需要等待所有阶段完成，因此总时间由最慢的阶段决定。这体现了系统瓶颈分析的重要性。

#### **第二张图片的公式**

这是一个更通用的精确模型。

$T_{AR\ or\ RS\ comms} = bytes * max_depth i [ (D_i - 1) / (D_i * max(Y, S_{i-1}) * W_{link\ i}) ]$

*   **目的**：计算在树状网络拓扑中，执行AllGather或ReduceScatter的通信时间。
*   **公式分解**：
    *   $max_depth i$： 对网络的所有层级（i）取最大值。**通信时间由最慢的层级决定**。
    *   $D_i$： 第i层的“度”，即一个父节点有多少个子节点。
    *   $(D_i - 1) / D_i$： 环状算法在该层级的效率因子。
    *   $max(Y, S_{i-1})$： **这是公式的关键**。$S_{i-1}$代表第i层以下所有子节点的总规模。它表示通信操作所能利用的并行度，受到**数据分片规模（Y）** 和**硬件子拓扑规模（S_{i-1}）** 中较大者的限制。例如，即使数据在Y轴上分了100片，但如果当前网络层级之下只连接了4个节点，那么在该层级最多只能利用4倍的并行带宽。
    *   $W_{link\ i}$： 该层级的单条链路带宽。
*   **核心思想**：该公式量化了一个重要原则：**通过分片获得的通信加速，不能超过执行该通信的硬件路径所能提供的最大并行能力。**


## 11.5 Pop Quiz 3 [Sharding along 2 axes]

Say we want to perform AllGatherX(bf16[DX,FY]) where Y is the inner axis over a single SU (256 chips). How long will this take as a function of D, F, and Y ?

### 核心概念理解

在深入计算之前，我们先明确几个关键点：

1.  **操作**：$AllGather_x$。这是在 X 维度上进行“全收集”操作。假设数据在 X 和 Y 两个维度上都被分片，执行 $AllGather_x$ 意味着：**每个设备（GPU）需要收集在 X 维度上所有其他分片的数据，但只保留自己所属的 Y 维度分片的结果**。
2.  **硬件背景**：一个 SU（可扩展单元）包含 256 个 GPU（即 32 个节点 × 8 GPU/节点）。网络是层级化的：GPU -> 节点内 NVSwitch -> 叶子交换机 -> 脊柱交换机。
3.  **关键瓶颈**：通信性能取决于数据路径中最慢的环节。如我们之前讨论，$Y$（Y轴分片数）与硬件规模（如节点内GPU数8，或整个SU的GPU数256）的相对大小，决定了瓶颈所在。
---

### 计算过程分步解析

#### **情况一：当 Y <= 8**

**公式：**
$$ T_{\text{comms}} = 2 \times D \times F \times \frac{(32 - 1)}{(32 \times 400 \times 10^9)} $$

**理解过程：**

1.  **为什么边界是 8？** 因为一个节点内有 8 个 GPU。当 $Y <= 8$ 时，意味着 Y 维度的分片数量小于或等于一个节点内的 GPU 数量。因此，**整个 $AllGather_x$ 操作所需的通信，其瓶颈发生在“节点间”**，具体来说，是在**叶子交换机（Leaf Switch）** 层级。数据需要跨越节点进行交换。

2.  **公式分解：**
    *   $2 × D × F$：这是**总通信数据量（字节）**。张量维度是 $bf16[D, F]$，bf16 格式每个元素占 2 字节。所以一个完整张量的大小是 $2 × D × F$ 字节。在 AllGather 操作中，每个设备最终需要获取所有其他设备在 X 维度上的数据，因此通信量与此成正比。
    *   $(32 - 1) / 32$：这是**叶子交换机层的环状算法效率因子**。一个 SU 有 32 个节点连接到叶子交换机层。在环状算法中，需要 $(节点数 - 1)$ 步来完成操作。这个分数 $31/32$ 代表了算法的固有开销。
    *   $400 × 10^9$：这是**每个节点的总出口带宽**（400 GB/s）。这是叶子交换机层级的性能瓶颈点。

3.  **核心结论**：在这种情况下，**通信时间是一个常数，与 Y 的大小无关！** 因为瓶颈在于节点间的固定带宽（400 GB/s），而 Y 维度的小规模分片优化效果被限制在节点内部，无法缓解跨节点的通信压力。可以把这理解为，无论节点内部的 8 个 GPU 如何高效地分配工作（因为 Y<=8），它们作为一个整体，向外发送/接收数据的速度上限就是 400 GB/s。

#### **情况二：当 Y > 8**

**公式：**
$$ T_{\text{comms}} \approx \frac{2 \times D \times F \times 256}{Y \times 12.8 \times 10^{12}} = \frac{2DF}{Y \times 50 \text{ GB/s}} $$

**理解过程：**

1.  **为什么情况变了？** 当 $Y > 8$ 时，Y 维度的分片数超过了单个节点内的 GPU 数。这意味着，**Y 维度的分片已经扩散到了多个节点**。现在，$AllGather_x$ 操作可以在**整个 SU（256个GPU）** 的更大范围内进行并行优化。

2.  **公式分解：**
    *   $2 × D × F$：同样是总数据量。
    *   $/ Y$：**这是最关键的变化！** 因为 Y 维度的分片现在遍布整个 SU，每个 GPU 只需要处理总数据量的 $1/Y$。通信量被有效地分摊了。
    *   $× 256$ 和 $/ 12.8 × 10^12$：这两项需要结合起来看。
        *   $12.8 × 10^12$ 是 **整个 SU 对外的总带宽**（12.8 TB/s）。这是一个 SU 通过所有叶子交换机连接到脊柱交换机的总容量。
        *   $256$ 是 SU 内的 GPU 总数。
        *   $(2 × D × F × 256) / (Y × 12.8 × 10^12)$ 这个整体计算的是：将总数据量分摊到 Y 个分片，并利用整个 SU 的总带宽进行计算。
    *   最终简化形式 $2DF / (Y × 50 GB/s)$ 更具洞察力。$50 GB/s$ 是**将 SU 总带宽平均分配到每个 GPU 上的值**（12.8 TB/s / 256 ≈ 50 GB/s）。这意味着，当 Y 很大时，每个 GPU 看起来像是拥有 50 GB/s 的带宽来执行这次通信。

3.  **核心结论**：在这种情况下，**通信时间与 Y 成反比**。增加 Y（在 SU 能力范围内）可以线性地减少通信时间，因为通信的并行度得到了提升，瓶颈转移到了更高、更宽裕的 SU 总带宽上。

### 总结

这个计算过程完美地演示了分布式系统中性能建模的精髓：

*   **案例一 (Y<=8)**：**通信受限于节点出口带宽**。拓扑瓶颈在叶子层，性能与 Y 无关。公式反映了跨节点通信的固定成本。
*   **案例二 (Y>8)**：**通信受益于大规模并行**。拓扑瓶颈在 SU 总出口，性能随 Y 增加而提升。公式反映了数据分片在更大规模硬件上带来的收益。

因此，理解这个计算过程的关键在于识别出：**数据分片策略（Y的值）如何与硬件拓扑结构相互作用，从而决定了整个通信操作的性能瓶颈位置。**

## 11.6 Quiz 4: Collectives

## Question 1 [SU AllGather]

Consider only a single SU with M nodes and N GPUs per node. Precisely how many bytes are ingressed and egressed by the node level switch during an AllGather? What about the top-level switch?

### 计算过程

**首先，明确设定：**
*   总数据量是 $B$ 字节。
*   SU 内有 $M$ 个节点，每个节点有 $N$ 个 GPU。
*   数据在 AllGather 前已经在所有 GPU 上分片。每个 GPU 最初持有 $B / (M * N)$ 字节的数据。

**然后，我们追踪数据的脚步：**

1.  **GPU 到节点交换机（数据上行）**
    *   **动作**：每个 GPU 将自己的数据发送给节点内部的交换机。
    *   **流量计算**：一个节点有 N 个 GPU，所以一个节点交换机的总**输入流量** = $N个GPU * (B / (M * N))字节/GPU = B / M$ 字节。

2.  **节点交换机到脊柱交换机（数据汇聚）**
    *   **动作**：节点交换机将汇总后的数据发送给脊柱交换机。
    *   **流量计算**：上一步一个节点交换机收到了 $B/M$ 字节，它需要将这些数据发送出去。所以它的**输出流量**也是 $B / M$ 字节。

3.  **脊柱交换机到节点交换机（数据下行）**
    *   **动作**：脊柱交换机从所有节点收集数据，执行 AllGather 的核心操作（聚合），然后将每个节点所需的结果发回。一个节点需要收集其他所有 M-1 个节点的数据。
    *   **流量计算**：发回给一个节点的数据量是 $(B / M) * (M - 1) = B * (M - 1) / M$ 字节。这是节点交换机从脊柱交换机接收的**输入流量**。

4.  **节点交换机到 GPU（结果分发）**
    *   **动作**：节点交换机将完整的聚合结果分发给它管辖的 N 个 GPU。每个 GPU 最终需要得到完整数据 B，但它自己已经有一份 $B/(MN)$，所以需要接收 $B - B/(MN)$。
    *   **流量计算**：一个节点交换机需要向它的 N 个 GPU 发送数据，总**输出流量** = $N个GPU * (B - B/(M*N))字节/GPU = N*B - B/M$ 字节。
基于上述流量分析，我们可以确定性能瓶颈：

| 组件 | 总输入流量 | 总输出流量 | **瓶颈与时间公式** |
| :--- | :--- | :--- | :--- |
| **节点级交换机** | $B$ 字节 | $N*B - B/M$ 字节 | **瓶颈在输出**。因为 $N*B$ 远大于 $B$，所以时间由输出流量决定：$T = (N*B) / W_node$。其中 $W_node$ 是节点出口带宽。 |
| **脊柱交换机** | $B$ 字节 | $B*(M-1)$ 字节 | **瓶颈在输出**。输出流量远大于输入，所以时间公式为：$T = (B*(M-1)) / (M * W_node)$。|


## Question 2 [Single-node SHARP AR]  (**有问题**)

Consider a single node with N GPUs per node. Precisely how many bytes are ingressed and egressed by the switch during an AllReduce using SHARP (in-network reductions)?

### 计算过程

**第一步：GPU发送数据到交换机（进行初步规约）**

*   **动作**：每个GPU将自己需要参与规约的数据发送给交换机。在环状算法中，每个GPU只发送部分数据，量是 $B*(N-1)/N$ 字节。
*   **交换机流量**：N个GPU都发送，所以交换机**接收（Ingress）** 的总数据量为：
    $N个GPU * [B*(N-1)/N]字节/GPU = B*(N-1)$ 字节。
*   **理解**：交换机收到了所有GPU的“原始数据片段”。

**第二步：交换机将部分结果发回GPU（进行本地规约）**

*   **动作**：交换机在内部对收到的数据做了一部分规约（如求和），然后将一个中间结果（大小为 $B/N$）发回给每个GPU，让GPU在本地完成剩余规约。
*   **交换机流量**：交换机向N个GPU各发送 $B/N$ 字节，所以这步交换机又**接收（Ingress）** 了 $N * (B/N) = B$ 字节。
    *   *注：这里“Ingress”可能容易混淆。从交换机角度看，数据从交换芯片发往GPU，是“Egress”。但原图可能从“数据进入交换系统”的全局视角，将GPU与交换机的所有交互都视为对交换系统的“输入”。我们暂且按原图数字理解其核心思想。关键是要理解总流量。*

**第三步：GPU将本地规约结果再次发给交换机**

*   **动作**：每个GPU在本地完成规约后，将最终属于自己的那一份完整结果（大小为 $B/N$）发回给交换机。
*   **交换机流量**：N个GPU各发送 $B/N$ 字节，所以交换机**接收（Ingress）** 了 $N * (B/N) = B$ 字节。
*   **理解**：此时，交换机汇集了所有GPU的最终规约结果。

**第四步：交换机将最终结果多播给所有GPU**

*   **动作**：交换机现在拥有完整的全局规约结果（大小为 $B$）。它需要将这个结果发送给每一个GPU。高效的做法是**多播**。
*   **交换机流量**：交换机需要将 $B$ 字节的数据流发送给N个目的地。所以交换机的**发送（Egress）** 总量为 $B$ 字节 * N个目的地 = $B*N$ 字节。
    *   *原图计算为 $B*(N-1)$，这可能是因为减去了GPU自己已有的那一份。但核心思想是Egress流量巨大。*

---

### 核心结论与理解

图片最后算出总流量为 **$B*N$ 字节**（Ingress + Egress）。**这个数字的意义需要与“无SHARP”的情况对比才能凸显。**

#### 对比无SHARP的Ring-AllReduce

在传统的Ring-AllReduce中：
*   数据需要在GPU组成的环中传递 **2*(N-1) 步**。
*   总通信量约为 **$2 * B * (N-1)/N$ 字节/GPU**。对于整个系统，总流量正比于 $2 * B * (N-1)$。

#### SHARP的巨大优势

1.  **通信量减半**：SHARP的总流量约为 $B*N$，而传统算法约是 $2*B*(N-1)$。当N较大时，SHARP的通信量几乎是传统方法的一半。这是因为SHARP避免了数据在环中流动两遍的冗余。

2.  **支持“吞吐量 = B / W_egress”**：这是最重要的结论。它意味着：
    *   **使用SHARP时，AllReduce的通信时间只取决于总数据量 $B$ 和单个GPU的出口带宽 $W_egress$。**
    *   **公式**： $T = B / W_egress$。
    *   这相当于每个GPU只需要把自己的数据完整地发送出去一次，然后就能直接拿到最终结果。交换机承担了所有的中间计算和转发工作，极大地提升了效率。

## Question 3 [Cross-node SHARP AR]  (**有问题**)

Consider an array bf16[DX, FY] sharded over a single node of N GPUs. How long does AllReduce(bf16[D, FY] { UX }) take? You can assume we do in-network reductions. Explain how this differs if we have more than a single node?


